{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Документация к реализации алгоритма PPO\n",
        "\n",
        "## Содержание\n",
        "1. [Введение](#введение)\n",
        "2. [Архитектура](#архитектура)\n",
        "3. [Основные компоненты](#основные-компоненты)\n",
        "4. [Рабочий процесс (Workflow)](#рабочий-процесс-workflow)\n",
        "5. [Детали реализации](#детали-реализации)\n",
        "6. [Гиперпараметры](#гиперпараметры)\n",
        "7. [Использование](#использование)\n",
        "\n",
        "## Введение\n",
        "\n",
        "Данный код представляет собой реализацию алгоритма Proximal Policy Optimization (PPO) для задач обучения с подкреплением. PPO - это метод оптимизации стратегии, предложенный OpenAI, который обеспечивает стабильное обучение, предотвращая слишком большие обновления политики.\n",
        "\n",
        "Реализация включает в себя:\n",
        "- Класс нейронной сети прямого распространения (`FeedForwardNN`)\n",
        "- Класс PPO для реализации алгоритма\n",
        "- Функционал для обучения на средах OpenAI Gym\n",
        "\n",
        "## Архитектура\n",
        "\n",
        "Система имеет следующую общую архитектуру:\n",
        "\n",
        "```\n",
        "                       +-------------------+\n",
        "                       |     Среда Gym     |\n",
        "                       | (напр. Pendulum)  |\n",
        "                       +--------+----------+\n",
        "                                |\n",
        "                                | Наблюдения, Награды\n",
        "                                v\n",
        "+---------------+    +-------------------------+    +-----------------+\n",
        "| Актор-нейросеть|<-->|      Алгоритм PPO      |<-->| Критик-нейросеть|\n",
        "+---------------+    +-------------------------+    +-----------------+\n",
        "                                |\n",
        "                                | Действия\n",
        "                                v\n",
        "                       +-------------------+\n",
        "                       |     Среда Gym     |\n",
        "                       +-------------------+\n",
        "```\n",
        "\n",
        "## Основные компоненты\n",
        "\n",
        "### 1. Класс FeedForwardNN\n",
        "\n",
        "Стандартная нейронная сеть прямого распространения с архитектурой:\n",
        "- Входной слой (размерность зависит от пространства наблюдений среды)\n",
        "- Два скрытых слоя по 64 нейрона каждый с функцией активации ReLU\n",
        "- Выходной слой (размерность зависит от пространства действий среды для актора или 1 для критика)\n",
        "\n",
        "Сеть используется для реализации как актора (выбор действий), так и критика (оценка ценности состояний).\n",
        "\n",
        "### 2. Класс PPO\n",
        "\n",
        "Основной класс, реализующий алгоритм PPO. Включает в себя:\n",
        "- Инициализацию актора и критика\n",
        "- Функции для сбора данных в среде (rollout)\n",
        "- Функции для вычисления Rewards-To-Go и преимуществ\n",
        "- Функции для обновления сетей\n",
        "- Логгирование процесса обучения\n",
        "\n",
        "## Рабочий процесс (Workflow)\n",
        "\n",
        "### Этап 1: Инициализация\n",
        "1. Создание среды обучения с использованием OpenAI Gym\n",
        "2. Инициализация актор-сети и критик-сети с заданной архитектурой\n",
        "3. Настройка гиперпараметров алгоритма\n",
        "4. Инициализация оптимизаторов и логгера\n",
        "\n",
        "### Этап 2: Сбор данных (Rollout)\n",
        "1. Взаимодействие с средой до тех пор, пока не будет собрано указанное количество шагов (`timesteps_per_batch`)\n",
        "2. Для каждого шага:\n",
        "   - Получение текущего наблюдения\n",
        "   - Выбор действия на основе текущей политики (актор-сеть)\n",
        "   - Выполнение действия в среде\n",
        "   - Сохранение наблюдения, действия, вероятности действия и награды\n",
        "3. Вычисление Rewards-To-Go для каждого шага (дисконтированная сумма будущих наград)\n",
        "\n",
        "### Этап 3: Обновление моделей\n",
        "1. Вычисление текущих оценок значений (Value) для всех наблюдений\n",
        "2. Вычисление преимуществ (Advantage) для каждого шага\n",
        "3. Нормализация преимуществ для стабильности обучения\n",
        "4. Выполнение нескольких эпох обучения (`n_updates_per_iteration`):\n",
        "   - Вычисление новых логарифмов вероятностей действий и оценок значений\n",
        "   - Вычисление отношения новых и старых вероятностей (ratios)\n",
        "   - Вычисление суррогатных потерь с клиппингом для предотвращения слишком больших обновлений\n",
        "   - Обновление весов актор-сети и критик-сети с использованием обратного распространения ошибки\n",
        "\n",
        "### Этап 4: Логгирование и сохранение\n",
        "1. Вывод информации о процессе обучения (средняя длина эпизода, средний возврат, потери)\n",
        "2. Периодическое сохранение моделей\n",
        "\n",
        "### Этап 5: Повторение\n",
        "Повторение этапов 2-4 до достижения указанного количества шагов (`total_timesteps`)\n",
        "\n",
        "## Детали реализации\n",
        "\n",
        "### Выбор действий\n",
        "Актор-сеть выдает среднее значение нормального распределения для каждого измерения пространства действий. Действия выбираются путем сэмплирования из многомерного нормального распределения с диагональной ковариационной матрицей.\n",
        "\n",
        "### Вычисление Rewards-To-Go\n",
        "Для каждого временного шага вычисляется сумма текущей и будущих дисконтированных наград:\n",
        "\n",
        "$$\n",
        "RTG_t = r_t + gamma * r_{t+1} + gamma^2 * r_{t+2} + ...\n",
        "$$\n",
        "\n",
        "### Вычисление преимуществ\n",
        "Преимущества вычисляются как разница между реальным Reward-To-Go и предсказанным значением Value:\n",
        "\n",
        "$$\n",
        "A_t = RTG_t - V(s_t)\n",
        "$$\n",
        "\n",
        "### Обновление политики\n",
        "Используется клиппированная функция потерь для обеспечения консервативных обновлений:\n",
        "\n",
        "$$\n",
        "L = min(r_t * A_t, clip(r_t, 1-epsilon, 1+epsilon) * A_t)\n",
        "$$\n",
        "\n",
        "где $r_t$ - отношение новой и старой вероятностей действия, а $epsilon$ - параметр клиппинга.\n",
        "\n",
        "## Гиперпараметры\n",
        "\n",
        "| Параметр | Значение по умолчанию | Описание |\n",
        "|----------|----------------------|-----------|\n",
        "| `timesteps_per_batch` | 4800 | Количество шагов в одном батче |\n",
        "| `max_timesteps_per_episode` | 1600 | Максимальное количество шагов в одном эпизоде |\n",
        "| `n_updates_per_iteration` | 5 | Количество эпох обновления на каждой итерации |\n",
        "| `lr` | 0.005 | Скорость обучения |\n",
        "| `gamma` | 0.95 | Коэффициент дисконтирования |\n",
        "| `clip` | 0.2 | Параметр клиппинга для PPO |\n",
        "| `render` | True | Включение визуализации |\n",
        "| `render_every_i` | 10 | Частота визуализации (каждые N итераций) |\n",
        "| `save_freq` | 10 | Частота сохранения модели (каждые N итераций) |\n",
        "\n",
        "## Использование\n",
        "\n",
        "```python\n",
        "# Создаем среду\n",
        "env = gym.make('Pendulum-v1')\n",
        "\n",
        "# Инициализируем PPO с пользовательскими гиперпараметрами\n",
        "hyperparameters = {\n",
        "    'timesteps_per_batch': 2048,\n",
        "    'max_timesteps_per_episode': 200,\n",
        "    'gamma': 0.99,\n",
        "    'n_updates_per_iteration': 10,\n",
        "    'lr': 3e-4,\n",
        "    'clip': 0.2,\n",
        "    'render': True,\n",
        "    'render_every_i': 10\n",
        "}\n",
        "\n",
        "# Создаем экземпляр PPO\n",
        "ppo = PPO(FeedForwardNN, env, **hyperparameters)\n",
        "\n",
        "# Запускаем обучение\n",
        "ppo.learn(total_timesteps=10000)\n",
        "```"
      ],
      "metadata": {
        "id": "JxNyLSr4NC44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импорт стандартных библиотек\n",
        "import os\n",
        "import time\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "# Импорт сторонних библиотек\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Импорт библиотек для визуализации\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import seaborn as sns\n",
        "from IPython.display import clear_output, display\n",
        "from matplotlib.figure import Figure\n",
        "from plotly.subplots import make_subplots\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Настройка стиля визуализации\n",
        "plt.style.use('ggplot')\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "\n",
        "# Создание директории для сохранения графиков\n",
        "os.makedirs('visualizations', exist_ok=True)"
      ],
      "metadata": {
        "id": "L8s1Olz_9688"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    ---------------\n",
        "        Стандартная нейронная сеть с архитектурой in_dim-64-64-out_dim.\n",
        "\n",
        "    Args:\n",
        "    ---------------\n",
        "        in_dim (int): Размерность входных данных.\n",
        "        out_dim (int): Размерность выходных данных.\n",
        "\n",
        "    Returns:\n",
        "    ---------------\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim: int, out_dim: int) -> None:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "        ---------------\n",
        "\n",
        "            Инициализация сети и настройка слоев.\n",
        "\n",
        "        Args:\n",
        "        ---------------\n",
        "            in_dim (int): Размерность входных данных.\n",
        "            out_dim (int): Размерность выходных данных.\n",
        "\n",
        "        Returns:\n",
        "        ---------------\n",
        "            None\n",
        "        \"\"\"\n",
        "        super(FeedForwardNN, self).__init__()\n",
        "\n",
        "        # Определение слоев нейронной сети\n",
        "        self.layer1 = nn.Linear(in_dim, 64)\n",
        "        self.layer2 = nn.Linear(64, 64)\n",
        "        self.layer3 = nn.Linear(64, out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, obs: np.ndarray) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "        ---------------\n",
        "            Выполняет прямой проход по нейронной сети.\n",
        "\n",
        "        Args:\n",
        "        ---------------\n",
        "            obs (np.ndarray): Входные данные для обработки.\n",
        "\n",
        "        Returns:\n",
        "        ---------------\n",
        "            torch.Tensor: Выходные данные после обработки.\n",
        "\n",
        "        Raises:\n",
        "        ---------------\n",
        "            TypeError: Если входные данные не являются экземпляром np.ndarray или torch.Tensor.\n",
        "\n",
        "        Examples:\n",
        "        ---------------\n",
        "            >>> model = FeedForwardNN(10, 2)\n",
        "            >>> output = model.forward(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))\n",
        "        \"\"\"\n",
        "        # Преобразование входных данных в тензор, если это необходимо\n",
        "        if isinstance(obs, np.ndarray):\n",
        "            obs = torch.tensor(obs, dtype=torch.float)\n",
        "\n",
        "        # Прямой проход по слоям сети с сохранением активаций для визуализации\n",
        "        activation1 = F.relu(self.layer1(obs))\n",
        "        self.activations['layer1'] = activation1.detach().numpy() if isinstance(activation1, torch.Tensor) else activation1\n",
        "\n",
        "        activation2 = F.relu(self.layer2(activation1))\n",
        "        self.activations['layer2'] = activation2.detach().numpy() if isinstance(activation2, torch.Tensor) else activation2\n",
        "\n",
        "        output = self.layer3(activation2)\n",
        "\n",
        "        # Сохраняем веса для визуализации\n",
        "        self.weights['layer1'] = self.layer1.weight.detach().numpy()\n",
        "        self.weights['layer2'] = self.layer2.weight.detach().numpy()\n",
        "        self.weights['layer3'] = self.layer3.weight.detach().numpy()\n",
        "\n",
        "        return output\n",
        "\n",
        "class PPO:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    ---------------\n",
        "        Класс PPO, используемый в качестве модели в main.py.\n",
        "\n",
        "    Args:\n",
        "    ---------------\n",
        "        policy_class: Класс политики для использования в актор/критик сетях.\n",
        "        env: Среда для обучения.\n",
        "        hyperparameters: Дополнительные параметры гиперпараметров.\n",
        "\n",
        "    Returns:\n",
        "    ---------------\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, policy_class: Any, env: gym.Env, **hyperparameters: Any) -> None:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "        ---------------\n",
        "            Инициализация модели PPO, включая гиперпараметры.\n",
        "\n",
        "        Args:\n",
        "        ---------------\n",
        "            policy_class: Класс политики для использования в актор/критик сетях.\n",
        "            env: Среда для обучения.\n",
        "            hyperparameters: Дополнительные параметры гиперпараметров.\n",
        "\n",
        "        Returns:\n",
        "        ---------------\n",
        "            None\n",
        "        \"\"\"\n",
        "        # Проверка совместимости среды\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
        "        assert isinstance(env.action_space, gym.spaces.Box)\n",
        "\n",
        "        # Инициализация гиперпараметров для обучения с PPO\n",
        "        self._init_hyperparameters(hyperparameters)\n",
        "\n",
        "        # Извлечение информации о среде\n",
        "        self.env = env\n",
        "        self.obs_dim = env.observation_space.shape[0]\n",
        "        self.act_dim = env.action_space.shape[0]\n",
        "\n",
        "        # Инициализация актор и критик сетей\n",
        "        self.actor = policy_class(self.obs_dim, self.act_dim)\n",
        "        self.critic = policy_class(self.obs_dim, 1)\n",
        "\n",
        "        # Инициализация оптимизаторов для актор и критик сетей\n",
        "        self.actor_optim = Adam(self.actor.parameters(), lr=self.lr)\n",
        "        self.critic_optim = Adam(self.critic.parameters(), lr=self.lr)\n",
        "\n",
        "        # Инициализация ковариационной матрицы для запроса действий у актора\n",
        "        self.cov_var = torch.full(size=(self.act_dim,), fill_value=0.5)\n",
        "        self.cov_mat = torch.diag(self.cov_var)\n",
        "\n",
        "        # Логгер для вывода сводок каждой итерации\n",
        "        self.logger = {\n",
        "            'delta_t': time.time_ns(),\n",
        "            't_so_far': 0,          # количество временных шагов на данный момент\n",
        "            'i_so_far': 0,          # количество итераций на данный момент\n",
        "            'batch_lens': [],       # длины эпизодов в батче\n",
        "            'batch_rews': [],       # возвраты эпизодов в батче\n",
        "            'actor_losses': [],     # потери актор сети в текущей итерации\n",
        "        }\n",
        "\n",
        "    def learn(self, total_timesteps: int) -> None:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "        ---------------\n",
        "            Обучение актор и критик сетей. Основной алгоритм PPO.\n",
        "\n",
        "        Args:\n",
        "        ---------------\n",
        "            total_timesteps: Общее количество временных шагов для обучения.\n",
        "\n",
        "        Returns:\n",
        "        ---------------\n",
        "            None\n",
        "        \"\"\"\n",
        "        print(f\"Обучение... Выполняется {self.max_timesteps_per_episode} временных шагов за эпизод, \", end='')\n",
        "        print(f\"{self.timesteps_per_batch} временных шагов за батч, всего {total_timesteps} временных шагов\")\n",
        "\n",
        "        t_so_far = 0  # Количество временных шагов, симулированных на данный момент\n",
        "        i_so_far = 0  # Количество выполненных итераций\n",
        "\n",
        "        while t_so_far < total_timesteps:\n",
        "            # Сбор батча симуляций\n",
        "            batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = self.rollout()\n",
        "\n",
        "            # Подсчет количества временных шагов, собранных в этом батче\n",
        "            t_so_far += np.sum(batch_lens)\n",
        "\n",
        "            # Увеличение количества итераций\n",
        "            i_so_far += 1\n",
        "\n",
        "            # Логирование временных шагов и итераций\n",
        "            self.logger['t_so_far'] = t_so_far\n",
        "            self.logger['i_so_far'] = i_so_far\n",
        "\n",
        "            # Вычисление преимущества на k-й итерации\n",
        "            V, _ = self.evaluate(batch_obs, batch_acts)\n",
        "            A_k = batch_rtgs - V.detach()\n",
        "\n",
        "            # Нормализация преимуществ для уменьшения дисперсии\n",
        "            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
        "\n",
        "            # Обновление сети в течение нескольких эпох\n",
        "            for _ in range(self.n_updates_per_iteration):\n",
        "                # Вычисление V_phi и pi_theta(a_t | s_t)\n",
        "                V, curr_log_probs = self.evaluate(batch_obs, batch_acts)\n",
        "\n",
        "                # Вычисление отношения pi_theta(a_t | s_t) / pi_theta_k(a_t | s_t)\n",
        "                ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
        "\n",
        "                # Вычисление суррогатных потерь\n",
        "                surr1 = ratios * A_k\n",
        "                surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
        "\n",
        "                # Вычисление потерь актор и критик сетей\n",
        "                actor_loss = (-torch.min(surr1, surr2)).mean()\n",
        "                critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
        "\n",
        "                # Обратное распространение для актор сети\n",
        "                self.actor_optim.zero_grad()\n",
        "                actor_loss.backward(retain_graph=True)\n",
        "                self.actor_optim.step()\n",
        "\n",
        "                # Обратное распространение для критик сети\n",
        "                self.critic_optim.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                self.critic_optim.step()\n",
        "\n",
        "                # Логирование потерь актора\n",
        "                self.logger['actor_losses'].append(actor_loss.detach())\n",
        "\n",
        "            # Вывод сводки обучения\n",
        "            self._log_summary()\n",
        "\n",
        "            # Сохранение модели\n",
        "            if i_so_far % self.save_freq == 0:\n",
        "                torch.save(self.actor.state_dict(), './ppo_actor.pth')\n",
        "                torch.save(self.critic.state_dict(), './ppo_critic.pth')\n",
        "\n",
        "    def rollout(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, List[int]]:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "        ---------------\n",
        "            Сбор батча данных из симуляции.\n",
        "\n",
        "        Args:\n",
        "        ---------------\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "        ---------------\n",
        "            batch_obs: Наблюдения, собранные в этом батче.\n",
        "            batch_acts: Действия, собранные в этом батче.\n",
        "            batch_log_probs: Логарифмы вероятностей каждого действия в этом батче.\n",
        "            batch_rtgs: Rewards-To-Go каждого временного шага в этом батче.\n",
        "            batch_lens: Длины каждого эпизода в этом батче.\n",
        "        \"\"\"\n",
        "        # Данные батча\n",
        "        batch_obs = []\n",
        "        batch_acts = []\n",
        "        batch_log_probs = []\n",
        "        batch_rews = []\n",
        "        batch_rtgs = []\n",
        "        batch_lens = []\n",
        "\n",
        "        # Данные эпизода\n",
        "        ep_rews = []\n",
        "\n",
        "        t = 0  # Количество временных шагов, выполненных в этом батче\n",
        "\n",
        "        # Симуляция до достижения указанного количества временных шагов\n",
        "        while t < self.timesteps_per_batch:\n",
        "            ep_rews = []  # награды, собранные за эпизод\n",
        "\n",
        "            # Сброс среды\n",
        "            obs, _ = self.env.reset()\n",
        "            done = False\n",
        "\n",
        "            # Выполнение эпизода\n",
        "            for ep_t in range(self.max_timesteps_per_episode):\n",
        "                # Рендеринг среды, если указано\n",
        "                if self.render and (self.logger['i_so_far'] % self.render_every_i == 0) and len(batch_lens) == 0:\n",
        "                    self.env.render()\n",
        "\n",
        "                t += 1  # Увеличение количества временных шагов в этом батче\n",
        "\n",
        "                # Отслеживание наблюдений в этом батче\n",
        "                batch_obs.append(obs)\n",
        "\n",
        "                # Вычисление действия и выполнение шага в среде\n",
        "                action, log_prob = self.get_action(obs)\n",
        "                obs, rew, terminated, truncated, _ = self.env.step(action)\n",
        "\n",
        "                # Объединение terminated и truncated\n",
        "                done = terminated or truncated\n",
        "\n",
        "                # Отслеживание наград, действий и логарифмов вероятностей\n",
        "                ep_rews.append(rew)\n",
        "                batch_acts.append(action)\n",
        "                batch_log_probs.append(log_prob)\n",
        "\n",
        "                # Завершение эпизода, если среда указала на его окончание\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            # Отслеживание длины и наград эпизода\n",
        "            batch_lens.append(ep_t + 1)\n",
        "            batch_rews.append(ep_rews)\n",
        "\n",
        "        # Преобразование данных в тензоры\n",
        "        batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
        "        batch_acts = torch.tensor(batch_acts, dtype=torch.float)\n",
        "        batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float)\n",
        "        batch_rtgs = self.compute_rtgs(batch_rews)\n",
        "\n",
        "        # Логирование возвратов и длины эпизодов в этом батче\n",
        "        self.logger['batch_rews'] = batch_rews\n",
        "        self.logger['batch_lens'] = batch_lens\n",
        "\n",
        "        return batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens\n",
        "\n",
        "    def compute_rtgs(self, batch_rews: List[List[float]]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "        ---------------\n",
        "            Вычисление Reward-To-Go каждого временного шага в батче.\n",
        "\n",
        "        Args:\n",
        "        ---------------\n",
        "            batch_rews: Награды в батче.\n",
        "\n",
        "        Returns:\n",
        "        ---------------\n",
        "            batch_rtgs: Rewards-To-Go.\n",
        "        \"\"\"\n",
        "        # Rewards-To-Go для каждого эпизода в батче\n",
        "        batch_rtgs = []\n",
        "\n",
        "        # Итерация по каждому эпизоду\n",
        "        for ep_rews in reversed(batch_rews):\n",
        "            discounted_reward = 0  # Накопленная дисконтированная награда\n",
        "\n",
        "            # Итерация по всем наградам в эпизоде\n",
        "            for rew in reversed(ep_rews):\n",
        "                discounted_reward = rew + discounted_reward * self.gamma\n",
        "                batch_rtgs.insert(0, discounted_reward)\n",
        "\n",
        "        # Преобразование Rewards-To-Go в тензор\n",
        "        batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
        "\n",
        "        return batch_rtgs\n",
        "\n",
        "    def get_action(self, obs: np.ndarray) -> Tuple[np.ndarray, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "        ---------------\n",
        "            Запрос действия у актор сети.\n",
        "\n",
        "        Args:\n",
        "        ---------------\n",
        "            obs: Наблюдение на текущем временном шаге.\n",
        "\n",
        "        Returns:\n",
        "        ---------------\n",
        "            action: Действие для выполнения.\n",
        "            log_prob: Логарифм вероятности выбранного действия.\n",
        "        \"\"\"\n",
        "        # Запрос среднего действия у актор сети\n",
        "        mean = self.actor(obs)\n",
        "\n",
        "        # Создание распределения с средним действием\n",
        "        dist = MultivariateNormal(mean, self.cov_mat)\n",
        "\n",
        "        # Выбор действия из распределения\n",
        "        action = dist.sample()\n",
        "\n",
        "        # Вычисление логарифма вероятности действия\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        # Возврат выбранного действия и логарифма вероятности\n",
        "        return action.detach().numpy(), log_prob.detach()\n",
        "\n",
        "    def evaluate(self, batch_obs: torch.Tensor, batch_acts: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "        ---------------\n",
        "            Оценка значений каждого наблюдения и логарифмов вероятностей действий.\n",
        "\n",
        "        Args:\n",
        "        ---------------\n",
        "            batch_obs: Наблюдения из последнего собранного батча.\n",
        "            batch_acts: Действия из последнего собранного батча.\n",
        "\n",
        "        Returns:\n",
        "        ---------------\n",
        "            V: Предсказанные значения batch_obs.\n",
        "            log_probs: Логарифмы вероятностей действий batch_acts при batch_obs.\n",
        "        \"\"\"\n",
        "        # Запрос значений V у критик сети\n",
        "        V = self.critic(batch_obs).squeeze()\n",
        "\n",
        "        # Вычисление логарифмов вероятностей действий\n",
        "        mean = self.actor(batch_obs)\n",
        "        dist = MultivariateNormal(mean, self.cov_mat)\n",
        "        log_probs = dist.log_prob(batch_acts)\n",
        "\n",
        "        # Возврат значений и логарифмов вероятностей\n",
        "        return V, log_probs\n",
        "\n",
        "    def _init_hyperparameters(self, hyperparameters: dict) -> None:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "        ---------------\n",
        "            Инициализация значений гиперпараметров.\n",
        "\n",
        "        Args:\n",
        "        ---------------\n",
        "            hyperparameters: Дополнительные параметры гиперпараметров.\n",
        "\n",
        "        Returns:\n",
        "        ---------------\n",
        "            None\n",
        "        \"\"\"\n",
        "        # Инициализация значений гиперпараметров по умолчанию\n",
        "        self.timesteps_per_batch = 4800\n",
        "        self.max_timesteps_per_episode = 1600\n",
        "        self.n_updates_per_iteration = 5\n",
        "        self.lr = 0.005\n",
        "        self.gamma = 0.95\n",
        "        self.clip = 0.2\n",
        "        self.render = True\n",
        "        self.render_every_i = 10\n",
        "        self.save_freq = 10\n",
        "        self.seed = None\n",
        "\n",
        "        # Изменение значений по умолчанию на пользовательские\n",
        "        for param, val in hyperparameters.items():\n",
        "            setattr(self, param, val)\n",
        "\n",
        "        # Установка seed, если указано\n",
        "        if self.seed is not None:\n",
        "            assert isinstance(self.seed, int)\n",
        "            torch.manual_seed(self.seed)\n",
        "            print(f\"Seed успешно установлен на {self.seed}\")\n",
        "\n",
        "    def _log_summary(self) -> None:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "        ---------------\n",
        "            Вывод сводки обучения.\n",
        "\n",
        "        Args:\n",
        "        ---------------\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "        ---------------\n",
        "            None\n",
        "        \"\"\"\n",
        "        # Вычисление значений для логирования\n",
        "        delta_t = self.logger['delta_t']\n",
        "        self.logger['delta_t'] = time.time_ns()\n",
        "        delta_t = (self.logger['delta_t'] - delta_t) / 1e9\n",
        "        delta_t = str(round(delta_t, 2))\n",
        "\n",
        "        t_so_far = self.logger['t_so_far']\n",
        "        i_so_far = self.logger['i_so_far']\n",
        "        avg_ep_lens = np.mean(self.logger['batch_lens'])\n",
        "        avg_ep_rews = np.mean([np.sum(ep_rews) for ep_rews in self.logger['batch_rews']])\n",
        "        avg_actor_loss = np.mean([losses.float().mean() for losses in self.logger['actor_losses']])\n",
        "\n",
        "        # Округление значений для более эстетичного вывода\n",
        "        avg_ep_lens = str(round(avg_ep_lens, 2))\n",
        "        avg_ep_rews = str(round(avg_ep_rews, 2))\n",
        "        avg_actor_loss = str(round(avg_actor_loss, 5))\n",
        "\n",
        "        # Вывод сводки\n",
        "        print(flush=True)\n",
        "        print(f\"-------------------- Итерация #{i_so_far} --------------------\", flush=True)\n",
        "        print(f\"Средняя длина эпизода: {avg_ep_lens}\", flush=True)\n",
        "        print(f\"Средний возврат эпизода: {avg_ep_rews}\", flush=True)\n",
        "        print(f\"Средняя потеря: {avg_actor_loss}\", flush=True)\n",
        "        print(f\"Временных шагов на данный момент: {t_so_far}\", flush=True)\n",
        "        print(f\"Итерация заняла: {delta_t} сек\", flush=True)\n",
        "        print(f\"------------------------------------------------------\", flush=True)\n",
        "        print(flush=True)\n",
        "\n",
        "        # Сброс данных логирования для батча\n",
        "        self.logger['batch_lens']   = []\n",
        "        self.logger['batch_rews']   = []\n",
        "        self.logger['actor_losses'] = []"
      ],
      "metadata": {
        "id": "g8F11Ns1ZUDy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем среду\n",
        "env = gym.make('Pendulum-v1')  # Можно попробовать другие среды Box2D или MuJoCo\n",
        "\n",
        "# Инициализируем PPO\n",
        "hyperparameters = {\n",
        "    'timesteps_per_batch': 2048,\n",
        "    'max_timesteps_per_episode': 200,\n",
        "    'gamma': 0.99,\n",
        "    'n_updates_per_iteration': 10,\n",
        "    'lr': 3e-4,\n",
        "    'clip': 0.2,\n",
        "    'render': True,\n",
        "    'render_every_i': 10\n",
        "}\n",
        "\n",
        "ppo = PPO(FeedForwardNN, env, **hyperparameters)\n",
        "\n",
        "# Запускаем обучение\n",
        "ppo.learn(total_timesteps=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAmwAaAVE0be",
        "outputId": "72a68b76-fc6b-47f0-d108-070ea379beab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обучение... Выполняется 200 временных шагов за эпизод, 2048 временных шагов за батч, всего 10000 временных шагов\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/classic_control/pendulum.py:178: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------------- Итерация #1 --------------------\n",
            "Средняя длина эпизода: 200.0\n",
            "Средний возврат эпизода: -1223.48\n",
            "Средняя потеря: -0.00306\n",
            "Временных шагов на данный момент: 2200\n",
            "Итерация заняла: 3.33 сек\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "-------------------- Итерация #2 --------------------\n",
            "Средняя длина эпизода: 200.0\n",
            "Средний возврат эпизода: -1165.36\n",
            "Средняя потеря: -0.00077\n",
            "Временных шагов на данный момент: 4400\n",
            "Итерация заняла: 3.42 сек\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "-------------------- Итерация #3 --------------------\n",
            "Средняя длина эпизода: 200.0\n",
            "Средний возврат эпизода: -1221.3\n",
            "Средняя потеря: -0.00152\n",
            "Временных шагов на данный момент: 6600\n",
            "Итерация заняла: 2.96 сек\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "-------------------- Итерация #4 --------------------\n",
            "Средняя длина эпизода: 200.0\n",
            "Средний возврат эпизода: -1235.85\n",
            "Средняя потеря: -0.00168\n",
            "Временных шагов на данный момент: 8800\n",
            "Итерация заняла: 2.93 сек\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "-------------------- Итерация #5 --------------------\n",
            "Средняя длина эпизода: 200.0\n",
            "Средний возврат эпизода: -1210.64\n",
            "Средняя потеря: -0.00058\n",
            "Временных шагов на данный момент: 11000\n",
            "Итерация заняла: 3.04 сек\n",
            "------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Тестирование обученного агента\n",
        "def test_agent(ppo: Any, env: gym.Env, n_episodes: int = 10) -> None:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    ---------------\n",
        "        Тестирование обученного агента в заданной среде.\n",
        "\n",
        "    Args:\n",
        "    ---------------\n",
        "        ppo: Обученный агент PPO.\n",
        "        env: Среда для тестирования.\n",
        "        n_episodes: Количество эпизодов для тестирования.\n",
        "\n",
        "    Returns:\n",
        "    ---------------\n",
        "        None\n",
        "    \"\"\"\n",
        "    for episode in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        # Выполнение эпизода до завершения\n",
        "        while not done:\n",
        "            # Получение действия от агента\n",
        "            action, _ = ppo.get_action(obs)\n",
        "\n",
        "            # Выполнение шага в среде\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Накопление награды\n",
        "            total_reward += reward\n",
        "\n",
        "            # Рендеринг среды\n",
        "            env.render()\n",
        "\n",
        "        # Вывод общей награды за эпизод\n",
        "        print(f\"Эпизод {episode + 1}, Общая награда: {total_reward}\")\n",
        "\n",
        "    # Закрытие среды после тестирования\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "fLMPyINXGx37"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_agent(ppo, env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5FGk9X5KU7d",
        "outputId": "9f0f5e52-f30d-4e5b-e4ed-aa2cfecd6dfe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпизод 1, Общая награда: -1515.1427226525689\n",
            "Эпизод 2, Общая награда: -1309.9893423038995\n",
            "Эпизод 3, Общая награда: -1082.6582799700243\n",
            "Эпизод 4, Общая награда: -1239.5453566172735\n",
            "Эпизод 5, Общая награда: -1110.7083583066783\n",
            "Эпизод 6, Общая награда: -916.0654989379681\n",
            "Эпизод 7, Общая награда: -1047.2785855014663\n",
            "Эпизод 8, Общая награда: -1306.5437231937635\n",
            "Эпизод 9, Общая награда: -1161.8973842454343\n",
            "Эпизод 10, Общая награда: -859.6355777379283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ztsn1iQEKVSJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}