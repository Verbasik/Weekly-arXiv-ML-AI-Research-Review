# **DAPO: RL-алгоритм от ByteDance**

## **Аннотация**

В данной работе представлена система **DAPO (Decoupled Clip and Dynamic sAmpling Policy Optimization)**, представляющая собой открытую платформу для обучения больших языковых моделей (LLM) с использованием методов обучения с подкреплением (Reinforcement Learning, RL). Несмотря на значительные успехи современных LLM, таких как OpenAI o1 и DeepSeek R1, ключевые технические детали их RL-обучения остаются недоступными для научного сообщества, что существенно затрудняет воспроизводимость результатов и дальнейшие исследования. В ответ на эту проблему авторы предлагают инновационный алгоритм DAPO, который не только демонстрирует высокую эффективность, но и предоставляет полную открытость кода, данных и методологии.

Система DAPO достигла рекордного результата в 50 баллов на математическом конкурсе **AIME 2024**, превзойдя предыдущий рекорд модели DeepSeek-R1 (47 баллов). При этом DAPO добилась такого результата, сократив количество шагов обучения вдвое. В основе алгоритма лежат четыре ключевые технологии: **стратегия Clip-Higher**, **динамическая выборка**, **оптимизация градиента на уровне токенов** и **интеллектуальный штраф длины**. Эти методы направлены на решение основных проблем RL-обучения, таких как коллапс энтропии, шум вознаграждения и неэффективность обучения на длинных текстах.

Авторы подчеркивают, что масштабное обучение с подкреплением является критически важным для развития способности LLM к сложным рассуждениям. Однако, в отличие от предыдущих работ, где детали RL-обучения оставались скрытыми (например, в блогах OpenAI и технических отчетах DeepSeek R1), DAPO предоставляет полную прозрачность. В открытый доступ выложены не только исходные коды обучения, разработанные на базе фреймворка **verl**, но и тщательно подготовленные датасеты. Это способствует повышению воспроизводимости результатов и открывает новые возможности для исследований в области крупномасштабного RL-обучения LLM.

Таким образом, работа представляет собой значительный вклад в развитие открытых и воспроизводимых методов обучения больших языковых моделей, предлагая как теоретические инновации, так и практические инструменты для научного сообщества.

## **1. Введение**

Появление моделей с расширенным временем тестирования и рассуждения (Test-time Compute), таких как O1 от OpenAI и R1 от DeepSeek, а так же совсем недавно Claude от Antropic, ознаменовало фундаментальный сдвиг парадигмы в области больших языковых моделей (LLM) на основе обучения с подкреплением (Reinforcement Learning, RL). Эти модели продемонстрировали беспрецедентные способности к комплексным рассуждениям, позволяющие им успешно решать сложные математические и программистские задачи уровня соревнований AIME и Codeforces.

Центральной технологией, обеспечивающей этот прорыв, выступает масштабное обучение с подкреплением (Reinforcement Learning, RL), которое стимулирует развитие сложных форм рассуждения, включая самопроверку, итеративное уточнение и рефлексию. Несмотря на впечатляющие результаты, конкретные алгоритмы и методологические подходы к масштабируемому RL-обучению остаются в значительной степени скрытыми в технических отчетах существующих моделей. Как отмечают авторы, "ключевые технические детали современных рассуждающих LLM скрыты (например, в блоге OpenAI о модели o1 и техническом отчете DeepSeek R1)", из-за чего исследовательское сообщество испытывает трудности с воспроизведением их результатов.

В ходе экспериментов авторы использовали Qwen2.5-32B в качестве предварительно обученной модели для применения обучения с подкреплением на основе обратной связи. При первоначальных запусках с использованием базового алгоритма GRPO (Generalized Reward-weighted Policy Optimization) модель достигла лишь 30 баллов на тесте AIME, что значительно уступает 47 баллам модели DeepSeek-RL. Углубленный анализ выявил, что наивная имплементация GRPO сталкивается с рядом критических проблем, среди которых:

1. **Коллапс энтропии** — тенденция модели к сужению разнообразия генерируемых ответов;
2. **Шум вознаграждения** — некорректное присвоение наград за частично правильные или слишком длинные ответы;
3. **Нестабильность обучения** — трудности при масштабировании процесса обучения на длинных цепочках рассуждений.

Аналогичные проблемы отмечались в более широком сообществе при попытках воспроизвести результаты DeepSeek, что указывает на возможное отсутствие ключевых деталей обучения в опубликованной статье о R1 — деталей, критически важных для разработки масштабируемых и воспроизводимых систем RL-обучения промышленного уровня.

Для преодоления этого разрыва авторы предоставляют современную систему масштабного RL-обучения LLM с открытым исходным кодом, которая достигает 50 баллов на AIME 2024 на базе модели Qwen2.5-32B. Данный результат превосходит предыдущий рекорд DeepSeek-RL-Zero-Qwen-32B (47 баллов), при этом требуя лишь 50% объема тренировочных шагов (см. рисунок 1). В основе системы лежит предложенный авторами алгоритм Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO), включающий четыре ключевые инновации, которые принципиально улучшают эффективность RL-обучения в сценариях длинных цепочек рассуждений:

![Figure_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_1.png)

- **Clip-Higher** — стратегия, способствующая разнообразию генерации и позволяющая проводить адаптивную выборку, разделяя нижние и верхние границы клиппирования (ε-low и ε-high);
- **Dynamic Sampling** — метод динамической выборки, повышающий эффективность и стабильность обучения путем исключения примеров с нулевым градиентом;
- **Token-Level Policy Gradient Loss** — расчет градиента политики на уровне отдельных токенов, что критически важно для эффективного обучения на длинных последовательностях;
- **Overlong Reward Shaping** — интеллектуальная система штрафов за превышение длины, снижающая шум вознаграждения и стабилизирующая процесс обучения.

Особую ценность работы составляет полная открытость всех аспектов системы: весь код реализации, построенный на фреймворке verl, вместе с тщательно подготовленным датасетом DAPO-Math-17K, доступны в открытом репозитории. Эта открытость контрастирует с предыдущими работами, которые, несмотря на впечатляющие результаты, не раскрывали критически важных деталей обучения.

В ходе экспериментов авторы также наблюдали интересное явление: модель не только усиливает существующие шаблоны рассуждения, но и постепенно развивает принципиально новые способности, в частности, поведение, связанное с самопроверкой и переосмыслением предыдущих шагов. Это открывает новые перспективы для понимания фундаментальных механизмов обучения LLM сложным формам рассуждения.

Далее мы шаг за шагом рассмотрим переход от PPO к GRPO, а затем к DAPO, чтобы увидеть, как был разработан этот новый алгоритм обучения с подкреплением.

## **2. Препдпосылки**

### **2.1 Проксимальная оптимизация политики (PPO)**

#### Основная концепция

Проксимальная оптимизация политики (PPO) — это один из наиболее популярных и эффективных алгоритмов обучения с подкреплением, разработанный исследователями из OpenAI в 2017 году. PPO представляет собой усовершенствование предыдущих методов оптимизации политики, таких как TRPO (Trust Region Policy Optimization), но с более простой реализацией и сравнимой или лучшей производительностью.

#### Ключевые особенности PPO

1. **Стабильность обучения**: основная идея PPO заключается в ограничении изменений политики между итерациями, что предотвращает слишком резкие обновления, которые могут дестабилизировать процесс обучения;

2. **Механизм отсечения**: PPO использует функцию отсечения для ограничения коэффициента важности выборки, что гарантирует, что новая политика не будет сильно отклоняться от старой;

3. **Эффективность выборки**: по сравнению с другими алгоритмами, PPO более эффективно использует собранные данные, что позволяет достигать лучших результатов при меньшем количестве взаимодействий со средой.

#### Математическая формализация

Целевая функция PPO определяется следующим образом:

![Image_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Image_1.png)

![Image_2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Image_2.png)

**Обобoбщим: что "под капотом" без математических деталей:**

* **Политика (Policy):**  это как "инструкция" для программы, говорящая, какое действие нужно выбрать в каждой ситуации. В PPO политика представлена нейронной сетью.

* **Функция ценности (Value Function):** это оценка того, насколько "хорошо" находиться в определенной ситуации.  Она помогает программе понимать, к каким ситуациям стоит стремиться.  Тоже обычно нейронная сеть.

* **Преимущество (Advantage):**  это разница между тем, насколько хорошо действие, которое программа выбрала, и насколько "в среднем" хорошо действовать в этой ситуации.  Помогает понять, какие действия были действительно удачными.

* **Целевая функция PPO (формула (1)):**  это главная "цель" обучения.  Программа пытается ее максимизировать, чтобы стать лучше.  Самое важное в ней - это механизм "отсечения" (clip), который ограничивает изменения политики.

* **Оценка преимущества GAE (формула (2)):**  способ посчитать, насколько хороши были действия программы, учитывая не только немедленный результат, но и будущие последствия.

* **Временная разница (формула (3)):**  помогает функции ценности учиться, сравнивая предсказанную ценность ситуации с тем, что реально произошло.

#### Процесс работы PPO

1. **Сбор опыта**: агенты взаимодействуют со средой, следуя текущей политике $\pi_{\theta_{\text{old}}}$, и собирают траектории (последовательности состояний, действий, вознаграждений).

2. **Оценка преимущества**: для каждого временного шага в собранных траекториях рассчитывается оценка преимущества $\hat{A}_t$ с использованием формул (2) и (3).

3. **Оптимизация политики**: параметры политики $\theta$ обновляются путем оптимизации целевой функции (1) с использованием стохастического градиентного восхождения в течение нескольких эпох.

4. **Обновление функции ценности**: параметры функции ценности обновляются путем минимизации среднеквадратичной ошибки между предсказанными и фактическими ценностями состояний.

5. **Итерация**: шаги 1-4 повторяются до достижения желаемой производительности или заданного числа итераций.

#### Преимущества PPO

1. **Простота реализации**: PPO проще в реализации, чем TRPO и другие сложные алгоритмы обучения с подкреплением;

2. **Высокая производительность**: PPO показывает отличные результаты во многих задачах, от игр Atari до сложных задач робототехники;

3. **Совместимость с нейронными сетями**: PPO прекрасно работает с глубокими нейронными сетями и может использоваться для обучения сложных политик;

4. **Хорошая масштабируемость**: PPO можно эффективно распараллеливать для ускорения обучения.

#### Применение PPO

PPO широко используется в различных областях:

1. **Игры**: от простых игр до сложных стратегий и симуляторов;

2. **Робототехника**: обучение управлению роботами для выполнения сложных физических задач;

3. **Системы рекомендаций**: оптимизация стратегий рекомендаций в интерактивных системах;

4. **Большие языковые модели**: в последние годы PPO активно применяется для настройки языковых моделей через обучение с подкреплением на основе обратной связи от человека (RLHF).

#### Заключение

Проксимальная оптимизация политики (PPO) представляет собой мощный и гибкий алгоритм обучения с подкреплением, который благодаря своей простоте и эффективности стал стандартом де-факто в области глубокого обучения с подкреплением. Его применение простирается от игр и робототехники до обучения современных языковых моделей, демонстрируя универсальность и мощь данного подхода.

### **2.2 Оптимизация групповой относительной политики (GRPO)**

Group Relative Policy Optimization (GRPO) — это алгоритм обучения с подкреплением, предназначенный для оптимизации LLM в задачах, требующих структурированного рассуждения, таких как математика и логика. Он был представлен в работах DeepSeekMath и DeepSeek-R1 **как ответ на вызовы обучения моделей с миллиардами параметров**. GRPO предлагает более эффективный подход по сравнению с традиционными методами, такими как Proximal Policy Optimization (PPO), **за счет устранения ключевых узких мест, связанных с вычислением advantage-функций**.

#### **Новаторский подход GRPO к Advantage-функциям**

![Figure_19](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-07_%26_08/assets/Figure_19.jpg)

GRPO полностью устраняет необходимость в value-сети, используя **групповую относительную нормализацию**:
для каждого промпта $P$ генерируется группа из $N$ ответов $G = \{O_1, O_2, ..., O_N\}$ с использованием политики $\pi$.  Каждому ответу $O_i$ присваивается награда $R_i = R(O_i)$, отражающая его качество.  Advantage-функция для $i$-го ответа $O_i$ относительно группы $G$ вычисляется по формуле:

![Image_3](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Image_3.png)

> По сути, Advantage-функция в GRPO для каждого конкретного ответа рассчитывается как награда конкретного ответа  минус  среднее арифметическое наград всех ответов в группе.

#### **Математическая формализация**

**Функция целевой оптимизации в GRPO**:

![Image_4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Image_4.png)

**Обобщение: что "под капотом" GRPO без математических деталей:**

* **Политика (Policy)**: Как и в PPO, это нейронная сеть, которая определяет, какие действия (или какие токены для LLM) следует выбирать в каждом состоянии.

* **Групповая генерация ответов**: В отличие от PPO, GRPO генерирует не один ответ на запрос, а сразу группу различных ответов (обычно 4-8) на один и тот же промпт.

* **Относительная нормализация вместо функции ценности**: GRPO не использует отдельную функцию ценности (Value Function). Вместо этого качество каждого ответа оценивается относительно других ответов в группе – это ключевое отличие.

* **Преимущество (Advantage)**: Рассчитывается как разница между наградой конкретного ответа и средней наградой всех ответов в группе. Это позволяет понять, какие ответы лучше среднего.

* **KL-дивергенция**: Жёстко интегрирована в функцию потерь, чтобы ограничить отклонение обновленной политики от исходной, обеспечивая стабильность обучения.

* **Экономия ресурсов**: Устранение value-сети снижает использование памяти на 40-60% и ускоряет обучение на ~35%.

#### Процесс работы GRPO

1. **Генерация группы ответов**: Для каждого запроса модель генерирует группу из N различных ответов, используя текущую политику.

2. **Оценка наград**: Каждый ответ получает награду от внешней функции награды (например, от модели-критика или по заданным правилам).

3. **Расчет относительного преимущества**: Для каждого ответа вычисляется преимущество как разница между его наградой и средней наградой по группе:
   
$$
A_i = R_i - (R_1 + R_2 + ... + R_N)/N
$$

4. **Оптимизация политики**: Параметры модели обновляются так, чтобы увеличить вероятность генерации ответов с положительным преимуществом (лучше среднего) и уменьшить вероятность ответов с отрицательным преимуществом (хуже среднего).

5. **Регуляризация с KL-дивергенцией**: Обновление ограничивается, чтобы новая политика не слишком отличалась от предыдущей версии или от референсной модели.

6. **Итерация**: Шаги 1-5 повторяются до достижения желаемой производительности.

**Ключевые особенности GRPO подхода:**

*   **Групповая относительная нормализация:** advantage-функция вычисляется относительно группы ответов, сгенерированных для одного и того же промпта, что обеспечивает относительную оценку качества;
*   **Устранение value-сети:**  средняя награда по группе $\bar{R}_G$ служит в качестве baseline, заменяя необходимость в отдельной value-сети для оценки ценности состояний или действий;
*   **Обучение на основе сравнения:**  GRPO фокусируется на обучении политики, которая генерирует ответы, превосходящие в среднем другие ответы в группе, что делает его эффективным в задачах, где важна относительная оценка качества;
* **KL-дивергенция: Жесткая интеграция в loss-функцию через относительные веса**: KL-дивергенция вводится в функцию потерь для регуляризации, ограничивая величину изменения политики на каждом шаге обучения и предотвращая её резкие колебания, что способствует стабильности обучения.

**Ограничения и замечания:**

*   Эффективность GRPO подхода зависит от качества функции награды $R(O)$.  Необходимо корректно определить функцию награды, чтобы она адекватно отражала желаемые свойства ответов.
*   Размер группы $N$ является гиперпараметром, который может влиять на стабильность и эффективность обучения.  Выбор оптимального значения $N$ может потребовать экспериментальной настройки.
*   GRPO, как и другие методы обучения с подкреплением, может быть чувствителен к выбору гиперпараметров оптимизации и архитектуры модели.

---

#### **Практическая интерпретация для LLM**

В GRPO advantage-функция становится **инструментом ранжирования вариантов ответа**:
- Модель учится генерировать ответы, которые не просто "хороши", но **значительно лучше среднего в своей группе**.
- Это стимулирует:
  - Поиск неочевидных, но эффективных цепочек рассуждений;
  - Избегание шаблонных ошибок, типичных для группы.

**Эффект**: модель фокусируется на **качественных различиях между ответами**, а не на абсолютных значениях наград, что критично для сложных задач с неоднозначными критериями успеха.

**Контекст проблемы**:
- В задачах рассуждения LLM часто генерируют множественные "рассуждения-цепочки" (chain-of-thought), но стандартные алгоритмы RL слабо адаптированы для их оценки.
- **Value-сети в PPO требуют значительных ресурсов для обучения и склонны к ошибкам в многомодальных распределениях наград**.

---

#### **Основные отличия GRPO от PPO**

| **Характеристика**                   | **PPO**                               | **GRPO**                                                                 |
|-------------------------------------|---------------------------------------|---------------------------------------------------------------------------|
| Наличие value-сети                   | Требуется                             | Исключена                                                                |
| Оценка преимущества                  | На основе value-сети                  | **Групповая относительная нормализация внутри траекторий**               |
| KL-дивергенция                       | Опциональная регуляризация            | **Жесткая интеграция в loss-функцию через относительные веса**           |
| Использование памяти                 | Высокое (2 модели)                    | **Снижено на 40-60% за счет удаления value-сети**                         |
| Сходимость                           | Зависит от точности value-сети        | **Стабильнее благодаря групповой стабилизации градиентов**               |

---

### **2.3 Устранение расхождения KL**

**Устранение расхождения KL в алгоритме DAPO для обучения моделей с длинными цепочками рассуждений**  

В сценарии RLHF (обучение с подкреплением и обратной связью с человеком) штрафной член, основанный на расхождении Кульбака-Лейблера (KL), традиционно используется для регулирования отклонений между обновляемой онлайн-политикой и замороженной эталонной политикой. Его основная цель — обеспечить, чтобы в процессе обучения модель корректировала свое поведение, не отдаляясь слишком сильно от исходного распределения данных, что особенно важно для сохранения предсказуемости и стабильности.  

Однако при обучении моделей, генерирующих длинные цепочки рассуждений (Chain-of-Thought, CoT), это ограничение теряет свою актуальность. В таких задачах распределение модели в процессе обучения может закономерно и значительно отклоняться от исходного из-за сложности и многошаговости выводов. Жесткое регулирование через KL-дивергенцию в данном случае становится избыточным, так как искусственно ограничивает способность модели к исследованию альтернативных стратегий генерации, необходимых для эффективного решения многоэтапных задач.  

Алгоритм DAPO (Decoupled Adaptive Policy Optimization) предлагает устранить KL-штраф, чтобы смягчить это ограничение. Отказ от термина расхождения KL позволяет модели свободно адаптироваться в процессе обучения, не будучи привязанной к изначальному распределению эталонной политики. Это особенно важно для сценариев, где успешное выполнение задачи требует выхода за рамки шаблонных решений, например, при генерации сложных логических заключений или творческих текстов. Таким образом, DAPO фокусируется на балансе между исследованием новых стратегий и эффективной оптимизацией политики, что повышает гибкость модели в контексте длинных выводов без ущерба для качества генерации.  

Данный подход подчеркивает, что в определенных сценариях RLHF строго контроля за отклонением от исходной политики можно избежать, чтобы раскрыть полный потенциал адаптивности модели в условиях сложных и неоднозначных задач.

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
  <p style="margin: 0; font-weight: bold; color: #2c3e50;">First Checkpoint:</p>
  <p style="margin: 8px 0 0 0; color: #2c3e50;">DAPO устраняет штрафы за расхождение KL в RLHF для задач в стиле long-CoT, что позволяет повысить гибкость политики и улучшить возможности рассуждений.</p>
</div>

### **2.4 Моделирование вознаграждения на основе правил**

Традиционные модели вознаграждения часто сталкиваются с проблемой взлома вознаграждения (reward hacking), когда модель манипулирует сигналом вознаграждения для получения высоких оценок вместо того, чтобы действительно улучшить способности к рассуждению. DAPO напрямую использует в качестве вознаграждения окончательную точность проверяемой задачи, избегая при этом сложности модели вознаграждения. В частности, функция вознаграждения выглядит следующим образом:

![Image_5](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Image_5.png)

Этот подход доказал свою эффективность в различных областях, включая автоматическое доказательство теорем, компьютерное программирование и математические соревнования.

> ИМХО: в данном случае моделирование вознаграждения работает только для детерменированных задач, где ответ однозначен. Для задач с неопределенными ответами (например, ответ LLM на вопрос, где в качестве ответа используется эвристика, а не строгое доказательство) это не сработает.

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
  <p style="margin: 0; font-weight: bold; color: #2c3e50;">Second Checkpoint:</p>
  <p style="margin: 8px 0 0 0; color: #2c3e50;">DAPO заменяет сложные модели вознаграждения на прямое использование итоговой точности задачи, что устраняет проблему взлома вознаграждения (reward hacking) и упрощает обучение.</p>
</div>

## **3. Алгоритм DAPO**

Исследователи предложили алгоритмы Decouple Clip и Dynamic Sampling Strategy Optimization (DAPO). DAPO делает выборку группы выходных данных ${o_i}_{i=1}^G$ для каждого вопроса $q$, связанного с ответом $a$, и оптимизирует политику через следующую целевую функцию:

![Image_6](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Image_6.png)

Ниже попробуем разобрать ключевые технологии, связанные с DAPO.

### **📌 3.1 Clip-Higher: повышение лимита**

В алгоритмах обучения с подкреплением (RL), таких как Proximal Policy Optimization (PPO) и Generalized Proximal Policy Optimization (GRPO), часто наблюдается явление коллапса энтропии, когда энтропия политики быстро уменьшается по мере обучения. Это приводит к тому, что генерируемые ответы становятся практически идентичными, что свидетельствует об ограниченном исследовании пространства возможных действий и преждевременной детерминированности стратегии. В данной работе предлагается стратегия Clip-Higher — модификация стандартного механизма отсечения в PPO, направленная на решение этой проблемы путем улучшения возможностей исследования для токенов с низкой вероятностью.

#### Проблема коллапса энтропии

В ходе первоначальных экспериментов с использованием стандартных реализаций PPO и GRPO было обнаружено, что энтропия политики быстро снижается по мере обучения, что можно наблюдать на рисунке 2b. Выборочные ответы для некоторых групп часто оказываются практически идентичными, что указывает на ограниченное исследование пространства возможных действий и раннюю детерминированность стратегии, потенциально препятствующую процессу расширения.

В основе этой проблемы лежит механизм отсечения коэффициента важности выборки, введенный в алгоритме PPO-Clip для ограничения области доверия и повышения стабильности обучения с подкреплением. Хотя этот механизм обеспечивает стабильность обучения, он также может ограничивать исследование пространства политики, особенно для токенов с низкой вероятностью.

![Figure_3](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_3.png)

#### Асимметрия ограничения коэффициента важности

Стандартный механизм отсечения в PPO использует единый параметр ε (обычно установленный на 0.2) для ограничения изменения вероятностей как в большую, так и в меньшую сторону. Однако это создает асимметрию в возможностях изменения вероятностей для различных токенов.

Рассмотрим пример с двумя действиями, имеющими вероятности $\pi_{\text{data}}(o_i | q) = 0,01$ и $0,9$ соответственно в исходном распределении. При стандартном ограничении коэффициента важности с ε = 0.2, максимально возможные обновленные вероятности составят $\pi(o_i | q) = 0,012$ и $1,08$ соответственно. Это означает, что для токенов с высокой исходной вероятностью (например, 0.9) существует меньше ограничений на рост их вероятности, тогда как для токенов с низкой исходной вероятностью (например, 0.01) возможности значительного увеличения вероятности сильно ограничены.

Эмпирические наблюдения также подтверждают, что максимальная вероятность обрезки токена обычно составляет $\pi(o_i | q) < 0,2$, как показано на рисунке 3а. Это подтверждает теоретический анализ, согласно которому верхний порог отсечения ограничивает рост вероятности токенов с низкой исходной вероятностью, тем самым потенциально ограничивая разнообразие системы.

![Figure_4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_4.png)

#### Стратегия Clip-Higher

Для решения вышеописанной проблемы предлагается стратегия Clip-Higher, основанная на разделении нижнего и верхнего диапазонов отсечения на ε_low и ε_high соответственно. Математически это выражается следующей формулой:

![Image_7](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Image_7.png)

где $r_{i,t}(θ)$ представляет отношение вероятностей новой политики к базовой политике, а $Â_{i,t}$ — оценка преимущества.

В отличие от стандартного подхода PPO, где ε_low = ε_high = 0.2, стратегия Clip-Higher использует различные значения для этих параметров: ε_low остается равным 0.2, а ε_high увеличивается до 0.28. Это увеличение верхнего порога отсечения оставляет больше пространства для роста вероятности токенов с низкой исходной вероятностью, тем самым стимулируя исследование "токенов с длинным хвостом".

#### Экспериментальные результаты

Как показано на графиках выше, предложенная корректировка механизма отсечения эффективно улучшает энтропию стратегии и способствует созданию более разнообразных выборок. Исследователи сознательно решили сохранить ε_low относительно небольшим (0.2), поскольку увеличение этого параметра может привести к чрезмерному снижению вероятности некоторых токенов, что в конечном итоге может вызвать коллапс пространства выборки.

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
  <p style="margin: 0; font-weight: bold; color: #2c3e50;">Third Checkpoint: Clip-Higher</p>
  <p style="margin: 8px 0 0 0; color: #2c3e50;">Стратегия Clip-Higher борется с коллапсом энтропии в PPO/GRPO, вводя асимметричные пороги отсечения (ε_low < ε_high). Увеличение верхнего порога (ε_high) стимулирует исследование токенов с низкой вероятностью, повышая разнообразие генерируемых ответов.</p>
</div>

### **📌 3.2 Dynamic Sampling: повышение эффективности градиентного обучения**

#### Проблема снижения градиента

Существующие алгоритмы обучения с подкреплением (RL) часто страдают от проблемы снижения градиента, которая возникает, когда точность некоторых подсказок достигает значения 1. Например, в алгоритме GRPO, если все выходные данные для конкретной подсказки верны и получают одинаковое вознаграждение 1, результирующее преимущество группы становится равным нулю. Это ведет к обновлению политики без градиентов, что существенно снижает эффективность выборки.

Эмпирические наблюдения показывают (Figure3.b - график чуть выще был), что количество образцов с точностью, равной 1, продолжает увеличиваться в процессе обучения. В результате количество действительных сигналов в каждой партии уменьшается, что приводит к:

- Увеличению дисперсии градиента;
- Ослаблению градиентного сигнала для обучения модели.

#### Решение: метод динамической выборки

Для решения этой проблемы предлагается метод динамической выборки. Основная идея заключается в следующем:

1. Выполнение избыточной выборки подсказок;
2. Фильтрация подсказок с точностью, равной 1 или 0;
3. Сохранение только подсказок с допустимыми градиентами в пакете;
4. Поддержание постоянного количества подсказок в обучающем пакете.

Процесс выборки продолжается до тех пор, пока пакет не будет полностью заполнен примерами, точность которых строго находится в диапазоне (0,1).

#### Математическая формализация

Опять же, наша целевая функция DAPO, тут нас интересует ограничение, выделенное красным:

![Image_8](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Image_8.png)

Это ограничение гарантирует, что в пакете содержатся только подсказки с точностью в диапазоне между 0 и 1. В случае динамической выборки эксперимент может достичь той же производительности быстрее. Наблюдение показано на рисунке 6.

![Figure_5](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_5.png)

Метод динамической выборки представляет собой эффективное решение проблемы снижения градиента в алгоритмах обучения с подкреплением. Путем целенаправленной фильтрации подсказок с крайними значениями точности (0 или 1) и сосредоточения вычислительных ресурсов на подсказках с промежуточной точностью, данный метод позволяет существенно повысить эффективность обучения и ускорить достижение конвергенции модели.

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
  <p style="margin: 0; font-weight: bold; color: #2c3e50;">Fourth Checkpoint: Dynamic Sampling</p>
  <p style="margin: 8px 0 0 0; color: #2c3e50;">Метод динамической выборки решает проблему снижения градиента в RL-алгоритмах, исключая подсказки с точностью 0 или 1. Фильтрация примеров с промежуточной точностью (0 < acc < 1) позволяет сохранять значимые градиенты в пакете, уменьшает их дисперсию и усиливает сигнал для обучения. Поддержание постоянного размера пакета с «полезными» примерами ускоряет конвергенцию модели.</p>
</div>

### **📌 3.3 Token-Level Policy Gradient Loss: действие по перебалансировке**

Исходный алгоритм GRPO использует расчет потерь на уровне выборки, который включает в себя сначала усреднение потерь по токенам в каждой выборке, а затем агрегирование потерь по всем выборкам. При таком подходе каждому образцу присваивается одинаковый вес при окончательном расчете потерь. Однако авторы обнаружили, что этот подход к сокращению потерь создает ряд проблем в сценариях RL с длинной цепочкой мысли.

Поскольку при расчете потерь всем образцам присваивается одинаковый вес, токены в более длинных ответах (содержащих больше токенов) могут вносить непропорционально меньший вклад в общие потери, что может привести к двум нежелательным эффектам. Во-первых, для высококачественных длинных выборок этот эффект может помешать модели изучить закономерности, связанные с рассуждениями. Во-вторых, слишком длинные образцы часто демонстрируют некачественные паттерны, такие как бессмысленная тарабарщина и повторяющиеся слова. Таким образом, расчет потерь на уровне выборки не может эффективно устранить эти плохие закономерности в длинных выборках, что приводит к нездоровому увеличению энтропии и длины отклика, как показано на рисунках 4а и 4б.

![Figure_6](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_6.png)

Авторы вводят градиентную потерю политики на уровне токенов в сценарий долгосрочной цепочки мышления (RL) для решения вышеуказанных ограничений:

![Image_9](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Image_9.png)

Ключевые отличия:
1. Нормализация производится по общему количеству токенов во всех ответах: $\sum_{i=1}^G |o_i|$;
2. Токены агрегируются напрямую, без предварительного усреднения по выборкам.

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
  <p style="margin: 0; font-weight: bold; color: #2c3e50;">Fifth Checkpoint: Token-level Policy Gradient Loss</p>
  <p style="margin: 8px 0 0 0; color: #2c3e50;">Стандартный расчет потерь GRPO на уровне сэмпла ослабляет градиенты от токенов в длинных ответах long-CoT, ухудшая обучение и подавление ошибок. Предложенный расчет потерь на уровне токенов решает эту проблему путем прямой агрегации и нормализации по всем токенам батча, обеспечивая каждому токену пропорциональный вклад в градиент.</p>
</div>

### **📌 3.4 Overlong Reward Shaping: супердлинное формирование вознаграждения**
 
При обучении с подкреплением обычно устанавливается фиксированная максимальная длина генерации и соответствующим образом обрезаются очень длинные образцы. Авторы обнаружили, что неправильное формирование вознаграждения для усеченных выборок может привести к появлению шума вознаграждения и существенно нарушить процесс обучения.

По умолчанию назначается штрафное вознаграждение за усеченные образцы. Такой подход может внести шум в процесс обучения, поскольку разумный процесс вывода может быть наказан просто за то, что он слишком длинный. Этот штраф может запутать модель относительно эффективности ее процесса рассуждения.

Чтобы изучить влияние этого шума вознаграждения, исследователи сначала применили очень длинную стратегию фильтрации, чтобы замаскировать потерю усеченных выборок. Было обнаружено, что такой подход значительно стабилизировал обучение и улучшил результаты, как показано на рисунке 5.

![Figure_7](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_7.png)

Кроме того, исследователи предложили мягкий штраф за превышение длины (Формула 13) — механизм штрафа, учитывающий длину, для усеченных выборок. В частности, определяется штрафной интервал, когда длина ответа превышает предопределенное максимальное значение. В пределах этого интервала, чем длиннее ответ, тем больше штраф. Этот штраф добавляется к изначальной награде за правильность, основанной на правилах, сигнализируя модели о необходимости избегать слишком длинных ответов.

![Figure_8](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_8.png)

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
<p style="margin: 0; font-weight: bold; color: #2c3e50;">Sixth Checkpoint: Overlong Reward Shaping</p>
<p style="margin: 8px 0 0 0; color: #2c3e50;">Традиционное бинарное пенальти за превышение длины ответа вносит шум в обучение, наказывая даже частично корректные длинные решения. Предложенный подход Overlong Reward Shaping заменяет жесткий штраф на постепенную линейную функцию в интервале 16-20К токенов, снижая уровень шума и позволяя модели эффективно учиться на длинных последовательностях без резкого отбрасывания данных.</p>
</div>

## **4 Эксперименты**

### **4.1 Подробности обучения**

Исследователи сосредоточили свое внимание на математических задачах для оценки разработанного алгоритма, который может быть легко адаптирован для других задач с четкими и точными сигналами вознаграждения. Для обучения был использован фреймворк verl с GRPO в качестве базового алгоритма. Преимущество оценивалось с помощью нормализации группового вознаграждения.

В работе были применены следующие гиперпараметры: оптимизатор AdamW с постоянной скоростью обучения $1 \times 10^{-6}$ и линейным разогревом в 20 шагов. Размер пакета подсказок составил 512, с выбором 16 ответов на каждую подсказку. Для сверхдлинного формирования вознаграждения была установлена ожидаемая максимальная длина в 16 384 токенов с дополнительным мягким штрафным буфером в 4906 токенов. Параметры обрезки $c_{\text{low}}$ и $c_{\text{high}}$ были установлены на уровне 0,2 и 0,28 соответственно.

![Figure_9](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_9.png)

### **4.2 Основные результаты**

В экспериментах на AIME 2024 метод DAPO успешно обучил базовую модель Qwen-32B, превратив её в мощную модель вывода, превзойдя результаты DeepSeek с использованием метода R1 на Qwen2.5-32B. Было показано значительное улучшение производительности AIME 2024: точность увеличилась с почти 0% до 50% при использовании только 50% шагов обучения, необходимых для DeepSeek-R1-Zero-Qwen-32B.

Исследователи проанализировали вклад каждой методики обучения в их подход. Улучшения демонстрируют эффективность этих методик в обучении с подкреплением. В условиях наивной настройки GRPO обучение на основе базовой модели Qwen.2-5-32B достигло только 30% точности.

![Figure_10](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_10.png)

### **4.3 Динамика тренировки**

Процесс обучения DAPO продемонстрировал сложность RL в больших языковых моделях. Исследователи обеспечили стабильность обучения, отслеживая ключевые показатели. Эксперименты показали, что DAPO не только улучшает способность модели к рассуждению, но и усиливает её исследовательские способности.

![Figure_11](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_11.png)

### **4.4 Анализ случая**

В ходе обучения с подкреплением модель DAPO продемонстрировала динамически развивающуюся модель рассуждений. По мере обучения модель не только укрепляла существующие шаблоны рассуждений, но и постепенно формировала новые модели поведения.

![Figure_12](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_12.png)

## **5. Заключение**

Запуск системы DAPO стал крупным прорывом в области крупномасштабного обучения языковым моделям с подкреплением. Благодаря открытому исходному коду алгоритмов, кодов и наборов данных, система получила отличную оценку 50 на AIME и предоставила ценные ресурсы для будущих исследований.

Четыре основные технологии DAPO — Clip-Higher, Dynamic Sampling, Token-Level Policy Gradient Loss и Overlong Reward Shaping — предлагают новые решения для обучения с подкреплением сложным задачам рассуждения. Выпуск DAPO с открытым исходным кодом позволил мировому исследовательскому сообществу лучше понимать и применять методы обучения с подкреплением для крупномасштабных языковых моделей.

Наконец, позвольте мне добавить некоторые ограничения, которые пришли мне в голову:

- С точки зрения итоговой производительности 50%-ная точность AIME все еще отстает от 72,6% DeepSeek-R1-Distill-Qwen-32B;
- Эффективность этого метода была проверена только на одном обучающем наборе, одном тестовом наборе и одной модели, и его обобщение сомнительно;
- С другой стороны, даже если DAPO обладает средней степенью обобщения, мы можем представить четыре приема, описанные в этой статье, как набор инструментов, из которого мы можем брать различные инструменты для конкретных сценариев, а не использовать весь DAPO как черный ящик. Фактически, из четырех приемов три предназначены для формирования вознаграждения, которое используется для поощрения исследовательской деятельности, лучшей обработки длинных ответов и лучшей обработки штрафа за длину, а оставшийся — для повышения эффективности выборки. Видно, что между ними нет зависимости, и любое подмножество рационально