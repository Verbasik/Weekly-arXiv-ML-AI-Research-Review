# **DAPO: революционный RL-алгоритм от ByteDance**

## **Аннотация**

В данной работе представлена система **DAPO (Decoupled Clip and Dynamic sAmpling Policy Optimization)**, представляющая собой открытую платформу для обучения больших языковых моделей (LLM) с использованием методов обучения с подкреплением (Reinforcement Learning, RL). Несмотря на значительные успехи современных LLM, таких как OpenAI o1 и DeepSeek R1, ключевые технические детали их RL-обучения остаются недоступными для научного сообщества, что существенно затрудняет воспроизводимость результатов и дальнейшие исследования. В ответ на эту проблему авторы предлагают инновационный алгоритм DAPO, который не только демонстрирует высокую эффективность, но и предоставляет полную открытость кода, данных и методологии.

Система DAPO достигла рекордного результата в 50 баллов на математическом конкурсе **AIME 2024**, превзойдя предыдущий рекорд модели DeepSeek-R1 (47 баллов). При этом DAPO добилась такого результата, сократив количество шагов обучения вдвое. В основе алгоритма лежат четыре ключевые технологии: **стратегия Clip-Higher**, **динамическая выборка**, **оптимизация градиента на уровне токенов** и **интеллектуальный штраф длины**. Эти методы направлены на решение основных проблем RL-обучения, таких как коллапс энтропии, шум вознаграждения и неэффективность обучения на длинных текстах.

Авторы подчеркивают, что масштабное обучение с подкреплением является критически важным для развития способности LLM к сложным рассуждениям. Однако, в отличие от предыдущих работ, где детали RL-обучения оставались скрытыми (например, в блогах OpenAI и технических отчетах DeepSeek R1), DAPO предоставляет полную прозрачность. В открытый доступ выложены не только исходные коды обучения, разработанные на базе фреймворка **verl**, но и тщательно подготовленные датасеты. Это способствует повышению воспроизводимости результатов и открывает новые возможности для исследований в области крупномасштабного RL-обучения LLM.

Таким образом, работа представляет собой значительный вклад в развитие открытых и воспроизводимых методов обучения больших языковых моделей, предлагая как теоретические инновации, так и практические инструменты для научного сообщества.

## **1. Введение**

Появление моделей с расширенным временем тестирования и рассуждения (Test-time Compute), таких как O1 от OpenAI и R1 от DeepSeek, а так же совсем недавно Claude от Antropic, ознаменовало фундаментальный сдвиг парадигмы в области больших языковых моделей (LLM) на основе обучения с подкреплением (Reinforcement Learning, RL). Эти модели продемонстрировали беспрецедентные способности к комплексным рассуждениям, позволяющие им успешно решать сложные математические и программистские задачи уровня соревнований AIME и Codeforces.

Центральной технологией, обеспечивающей этот прорыв, выступает масштабное обучение с подкреплением (Reinforcement Learning, RL), которое стимулирует развитие сложных форм рассуждения, включая самопроверку, итеративное уточнение и рефлексию. Несмотря на впечатляющие результаты, конкретные алгоритмы и методологические подходы к масштабируемому RL-обучению остаются в значительной степени скрытыми в технических отчетах существующих моделей. Как отмечают авторы, "ключевые технические детали современных рассуждающих LLM скрыты (например, в блоге OpenAI о модели o1 и техническом отчете DeepSeek R1)", из-за чего исследовательское сообщество испытывает трудности с воспроизведением их результатов.

В ходе экспериментов авторы использовали Qwen2.5-32B в качестве предварительно обученной модели для применения обучения с подкреплением на основе обратной связи. При первоначальных запусках с использованием базового алгоритма GRPO (Generalized Reward-weighted Policy Optimization) модель достигла лишь 30 баллов на тесте AIME, что значительно уступает 47 баллам модели DeepSeek-RL. Углубленный анализ выявил, что наивная имплементация GRPO сталкивается с рядом критических проблем, среди которых:

1. **Коллапс энтропии** — тенденция модели к сужению разнообразия генерируемых ответов;
2. **Шум вознаграждения** — некорректное присвоение наград за частично правильные или слишком длинные ответы;
3. **Нестабильность обучения** — трудности при масштабировании процесса обучения на длинных цепочках рассуждений.

Аналогичные проблемы отмечались в более широком сообществе при попытках воспроизвести результаты DeepSeek, что указывает на возможное отсутствие ключевых деталей обучения в опубликованной статье о R1 — деталей, критически важных для разработки масштабируемых и воспроизводимых систем RL-обучения промышленного уровня.

Для преодоления этого разрыва авторы предоставляют современную систему масштабного RL-обучения LLM с открытым исходным кодом, которая достигает 50 баллов на AIME 2024 на базе модели Qwen2.5-32B. Данный результат превосходит предыдущий рекорд DeepSeek-RL-Zero-Qwen-32B (47 баллов), при этом требуя лишь 50% объема тренировочных шагов (см. рисунок 1). В основе системы лежит предложенный авторами алгоритм Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO), включающий четыре ключевые инновации, которые принципиально улучшают эффективность RL-обучения в сценариях длинных цепочек рассуждений:

![Figure_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_1.png)

- **Clip-Higher** — стратегия, способствующая разнообразию генерации и позволяющая проводить адаптивную выборку, разделяя нижние и верхние границы клиппирования (ε-low и ε-high);
- **Dynamic Sampling** — метод динамической выборки, повышающий эффективность и стабильность обучения путем исключения примеров с нулевым градиентом;
- **Token-Level Policy Gradient Loss** — расчет градиента политики на уровне отдельных токенов, что критически важно для эффективного обучения на длинных последовательностях;
- **Overlong Reward Shaping** — интеллектуальная система штрафов за превышение длины, снижающая шум вознаграждения и стабилизирующая процесс обучения.

Особую ценность работы составляет полная открытость всех аспектов системы: весь код реализации, построенный на фреймворке verl, вместе с тщательно подготовленным датасетом DAPO-Math-17K, доступны в открытом репозитории. Эта открытость контрастирует с предыдущими работами, которые, несмотря на впечатляющие результаты, не раскрывали критически важных деталей обучения.

В ходе экспериментов авторы также наблюдали интересное явление: модель не только усиливает существующие шаблоны рассуждения, но и постепенно развивает принципиально новые способности, в частности, поведение, связанное с самопроверкой и переосмыслением предыдущих шагов. Это открывает новые перспективы для понимания фундаментальных механизмов обучения LLM сложным формам рассуждения.

---

> Раскрой, если хочешь очень глубоко закопаться 🤓 Будет душно, но интересно 📚

<details> 
    <summary><em><strong>Сущность Test-time Compute</strong></em></summary>

### Сущность Test-time Compute

"**Test-time compute**" (вычисления во время тестирования/инференса) представляет собой парадигму масштабирования RL LLM, которая акцентирует внимание на увеличении вычислительных ресурсов, доступных модели непосредственно в момент обработки пользовательского запроса (inference time).  В отличие от традиционного подхода, "Test-time compute" позволяет улучшить производительность уже обученной модели, предоставляя ей больше **ВРЕМЕНИ** и **ВЫЧИСЛИТЕЛЬНОЙ МОЩНОСТИ** для "размышления" над каждым конкретным запросом.

### Отличие от традиционного масштабирования

Традиционное масштабирование LLM фокусировалось на следующих аспектах **во время обучения**:

* **Размер модели:** увеличение количества параметров и сложности архитектуры;
* **Объем данных:** расширение и разнообразие обучающих данных;
* **Вычислительные ресурсы для обучения:** использование более мощных GPU и увеличение времени обучения.

"Test-time compute" вводит **дополнительное измерение масштабирования**, применяемое **после обучения модели**.  Это позволяет повысить эффективность модели, не изменяя ее архитектуру или параметры, а оптимизируя вычислительные ресурсы в момент инференса.

### Механизм и преимущества Test-time Compute

Предоставление модели больше вычислительных ресурсов во время инференса позволяет:

* **Углубленная обработка запросов:**  модель может проводить более детальный анализ входного текста и контекста, на основе более глубоких цепочек рассуждения;
* **Улучшение рассуждений:**  дополнительные вычисления способствуют более эффективному планированию, поиску оптимальных решений и генерации логически обоснованных ответов;
* **Использование сложных алгоритмов инференса:**  возможность применения ресурсоемких, но более качественных методов декодирования и генерации.

### Как итог

"Test-time compute" знаменует собой важный сдвиг в подходах к масштабированию LLM.  Он дополняет традиционные методы, сосредотачиваясь на оптимизации вычислительных ресурсов в момент использования модели.  Это открывает перспективы для создания более интеллектуальных и reasoning-ориентированных языковых моделей, особенно в задачах, требующих глубокого анализа и логического вывода.

</details>

<details> 
    <summary><em><strong>Краткий обзор алгоритма GRPO</strong></em></summary>

### **Введение в GRPO**
Group Relative Policy Optimization (GRPO) — это алгоритм обучения с подкреплением, предназначенный для оптимизации LLM в задачах, требующих структурированного рассуждения, таких как математика и логика. Он был представлен в работах DeepSeekMath и DeepSeek-R1 **как ответ на вызовы обучения моделей с миллиардами параметров**. GRPO предлагает более эффективный подход по сравнению с традиционными методами, такими как Proximal Policy Optimization (PPO), **за счет устранения ключевых узких мест, связанных с вычислением advantage-функций**.


<details> 
    <summary><em><strong>Объяснение Advantage-функций</strong></em></summary>

**Advantage-функция** — это ключевое понятие в обучении с подкреплением (Reinforcement Learning, RL), которое **количественно оценивает преимущество выбора конкретного действия `a` в состоянии `s` по сравнению со средним действием, предписанным текущей политикой модели**. Формально она выражается как разница между **Q-функцией** (ожидаемая суммарная награда за действие `a` в состоянии `s`) и **V-функцией** (средняя ожидаемая награда в состоянии `s` при текущей политике):

$$
A(s, a) = Q(s, a) - V(s)
$$

---

### **Зачем нужна Advantage-функция?**
1. **Оценка относительной ценности действий**:
   - Помогает модели понять, насколько конкретное действие лучше или хуже "стандартного" поведения в данном контексте при текущей политике.
   - Пример: В математической задаче действие "выбрать метод интегрирования по частям" может иметь высокий advantage, если приводит к правильному ответу, и низкий — если усложняет решение.

2. **Снижение дисперсии градиентов**:
   - Использование относительных advantage-значений вместо абсолютных наград делает обновления политики более стабильными.

---

### **Как вычисляются Advantage-функции в классическом RL (например, PPO)?**
В Proximal Policy Optimization (PPO):
1. **Value-сеть** (отдельная нейросеть) обучается предсказывать `V(s)` — ожидаемую награду для состояния `s`.
2. **Q(s, a)** оценивается через фактическую полученную награду + дисконтированные будущие награды.
3. **Advantage** вычисляется как:
   $$
   A(s, a) = R_{\text{total}} - V(s)
   $$
   где $( R_{\text{total}} )$ — дисконтированная сумма наград за траекторию.

**Проблемы PPO**:
- Value-сеть требует дополнительных вычислительных ресурсов и памяти.
- Ошибки в предсказаниях `V(s)` (особенно в задачах с **многомодальным распределением наград**, как в LLM) искажают advantage-значения.

> Тут будет ссылка на тетрадку
</details> 

---

### **Новаторский подход GRPO к Advantage-функциям**

GRPO полностью устраняет необходимость в value-сети, используя **групповую относительную нормализацию**:
для каждого промпта $P$ генерируется группа из $N$ ответов $G = \{O_1, O_2, ..., O_N\}$ с использованием политики $\pi$.  Каждому ответу $O_i$ присваивается награда $R_i = R(O_i)$, отражающая его качество.  Advantage-функция для $i$-го ответа $O_i$ относительно группы $G$ вычисляется по формуле:

$$
A_i(O_i, G) = R_i - \bar{R}_G = R_i - \frac{1}{N} \sum_{j=1}^N R_j
$$

где:

- В GRPO оценка $R_i$ (награда для ответа $O_i$) в **первой итерации** берётся из **внешней функции награды (reward function)** $R(O_i)$, которая:

    1. **Не зависит от текущей политики $\pi$.**  
    - Это может быть:  
        - Ручная аннотация (например, экспертные оценки качества ответов).  
        - Автоматизированный алгоритм (например, модель-критик, отдельная ML-модель, оценивающая тексты).  
        - Правило на основе heuristics (например, соответствие формату, наличие ключевых слов).  

    2. **Требует предварительной настройки.**  
    - Если используется модель-критик, её нужно предварительно обучить на размеченных данных.  
    - Если аннотации ручные, требуется подготовка датасета с оценками.

- $\bar{R}_G = \frac{1}{N} \sum_{j=1}^N R_j$ — средняя награда по группе $G$.

> По сути, Advantage-функция в GRPO для каждого конкретного ответа рассчитывается как награда конкретного ответа  минус  среднее арифметическое наград всех ответов в группе.

Группа $G$ в контексте GRPO представляет собой **горизонтально различные вариативные ответы на один и тот же промпт $P$**, а не последовательные шаги в траектории.  

**Пояснение:**  
1. **Горизонтальная вариативность:** для каждого промпта $P$ политика $\pi$ генерирует $N$ **альтернативных ответов** ($O_1, O_2, \dots, O_N$), которые являются независимыми вариантами, а не частями одной цепочки (траектории). Это похоже на сэмплирование нескольких возможных ответов на один вопрос.  

2. **Сравнение внутри группы:** advantage-функция $A_i$ вычисляет, насколько конкретный ответ $O_i$ лучше или хуже **среднего по группе** ($\bar{R}_G$). Это требует, чтобы все ответы в $G$ были параллельными вариантами, иначе среднее значение теряет смысл как относительный базис.  

3. **Устранение value-сети:** GRPO заменяет оценку "абсолютной" полезности (через value-сеть) на **относительное сравнение внутри группы**. Для этого группа должна содержать разнообразные ответы на один промпт, чтобы средняя награда $\bar{R}_G$ отражала общее качество группы.  

**Ключевые особенности GRPO подхода:**

*   **Групповая относительная нормализация:** advantage-функция вычисляется относительно группы ответов, сгенерированных для одного и того же промпта, что обеспечивает относительную оценку качества;
*   **Устранение value-сети:**  средняя награда по группе $\bar{R}_G$ служит в качестве baseline, заменяя необходимость в отдельной value-сети для оценки ценности состояний или действий;
*   **Обучение на основе сравнения:**  GRPO фокусируется на обучении политики, которая генерирует ответы, превосходящие в среднем другие ответы в группе, что делает его эффективным в задачах, где важна относительная оценка качества;
* **KL-дивергенция: Жесткая интеграция в loss-функцию через относительные веса**: KL-дивергенция вводится в функцию потерь для регуляризации, ограничивая величину изменения политики на каждом шаге обучения и предотвращая её резкие колебания, что способствует стабильности обучения.

**Ограничения и замечания:**

*   Эффективность GRPO подхода зависит от качества функции награды $R(O)$.  Необходимо корректно определить функцию награды, чтобы она адекватно отражала желаемые свойства ответов.
*   Размер группы $N$ является гиперпараметром, который может влиять на стабильность и эффективность обучения.  Выбор оптимального значения $N$ может потребовать экспериментальной настройки.
*   GRPO, как и другие методы обучения с подкреплением, может быть чувствителен к выбору гиперпараметров оптимизации и архитектуры модели.

---

### **Практическая интерпретация для LLM**
В GRPO advantage-функция становится **инструментом ранжирования вариантов ответа**:
- Модель учится генерировать ответы, которые не просто "хороши", но **значительно лучше среднего в своей группе**.
- Это стимулирует:
  - Поиск неочевидных, но эффективных цепочек рассуждений.
  - Избегание шаблонных ошибок, типичных для группы.

**Эффект**: Модель фокусируется на **качественных различиях между ответами**, а не на абсолютных значениях наград, что критично для сложных задач с неоднозначными критериями успеха.

**Контекст проблемы**:
- В задачах рассуждения LLM часто генерируют множественные "рассуждения-цепочки" (chain-of-thought), но стандартные алгоритмы RL слабо адаптированы для их оценки.
- **Value-сети в PPO требуют значительных ресурсов для обучения и склонны к ошибкам в многомодальных распределениях наград**.

---

### **Основные отличия GRPO от PPO**

| **Характеристика**                   | **PPO**                               | **GRPO**                                                                 |
|-------------------------------------|---------------------------------------|---------------------------------------------------------------------------|
| Наличие value-сети                   | Требуется                             | Исключена                                                                |
| Оценка преимущества                  | На основе value-сети                  | **Групповая относительная нормализация внутри траекторий**               |
| KL-дивергенция                       | Опциональная регуляризация            | **Жесткая интеграция в loss-функцию через относительные веса**           |
| Использование памяти                 | Высокое (2 модели)                    | **Снижено на 40-60% за счет удаления value-сети**                         |
| Сходимость                           | Зависит от точности value-сети        | **Стабильнее благодаря групповой стабилизации градиентов**               |

---

### **Математические основы GRPO**
**Функция потерь в GRPO**:

$$
L(\theta) = \mathbb{E}_{(s,a) \sim \pi_{\text{old}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\text{old}}(a|s)} \, A(s,a) \;-\; \beta \cdot D_{KL}(\pi_\theta \,\|\, \pi_{\text{old}}) \right],
$$

где:
- **$\theta$** — параметры **текущей политики** (нейронной сети), которые оптимизируются в процессе обучения.
- **$s$** — текущее **состояние** (state) среды, в котором находится агент.
- **$a$** — **действие** (action), выбранное агентом в состоянии $s$.
- **$\pi_\theta(a|s)$** — вероятность выбора действия $a$ в состоянии $s$ согласно **текущей политике**.
- **$\pi_{\text{old}}(a|s)$** — вероятность выбора действия $a$ в состоянии $s$ согласно **старой политике**, зафиксированной на момент сбора данных.
- **$A(s,a)$** — **преимущество** (advantage) действия $a$ в состоянии $s$, вычисляемое как разница между ожидаемой наградой при выборе $a$ и средней наградой в состоянии $s$. Формально:  
  $$A(s,a) = Q(s,a) - V(s),$$  
  где $Q(s,a)$ — оценка общей награды за выбор $a$ в $s$, а $V(s)$ — средняя ценность состояния $s$.
- **$\mathbb{E}_{(s,a) \sim \pi_{\text{old}}}$** — математическое ожидание, взятое по состояниям и действиям из **опыта**, собранного старой политикой $\pi_{\text{old}}$ (off-policy данные).
- **$D_{KL}(\pi_\theta \,\|\, \pi_{\text{old}})$** — KL-дивергенция между распределениями действий текущей и старой политик в состоянии $s$:  
  $$D_{KL}(\pi_\theta \,\|\, \pi_{\text{old}}) = \mathbb{E}_{a \sim \pi_\theta} \left[ \log \frac{\pi_\theta(a|s)}{\pi_{\text{old}}(a|s)} \right].$$
- **$\beta$** — гиперпараметр, регулирующий силу KL-регуляризации (**типичные значения: 0.05–0.2**).

---

### **Пояснения**
1. **Off-policy обучение**: Градиенты вычисляются на данных, собранных старой политикой ($\pi_{\text{old}}$), но оптимизируется новая политика ($\pi_\theta$).  
2. **Importance weighting** $\frac{\pi_\theta}{\pi_{\text{old}}}$ корректирует градиенты с учетом различий между политиками, предотвращая смещение оценок.  
3. **KL-дивергенция** ограничивает скорость изменения политики, обеспечивая устойчивость обучения.  
4. **Преимущество $A(s,a)$** направляет обновление в сторону действий с большей ожидаемой наградой. Если $A(s,a) > 0$, действие $a$ в состоянии $s$ считается лучше среднего.

**Оптимизация**:
- Градиенты обновляются только для токенов, критически влияющих на награду (**например, ключевых шагов в математическом выводе**).  
  - *Формально*, это можно представить как применение маски $( M )$ к градиентам, где $( M_i = 1 )$ для «критических» токенов и $( M_i = 0 )$ для остальных. Таким образом, обновляются только параметры, связанные с «критическими» токенами, что повышает эффективность обучения, фокусируясь на наиболее значимых частях рассуждения.
- **Сэмплирование ответов**: Для каждого промпта параллельно генерируются 4–8 вариантов, что улучшает покрытие пространства решений.

---

### **Немного цифр**
1. **Эффективность**:
   - Удаление value-сети сокращает объем памяти на **18.2 GB для модели с 33B параметров** (эксперименты DeepSeek-R1).
   - Время обучения сокращается на **35%** при решении задач уровня MATH dataset.

2. **Стабильность**:
   - Групповая нормализация уменьшает дисперсию градиентов (**на 60% по сравнению с PPO**).
   - KL-регуляризация предотвращает "распад политики" — типичную проблему PPO.

3. **Результативность**:
   - На бенчмарке MATH GRPO повысил точность модели DeepSeek-Math-7B с **51.2% до 58.7%**.
   - В логических задачах (например, FOLIO) улучшение составило **12.3%**.

---

### **Практическая реализация GRPO**

**Шаги внедрения**:
1. **Супервизионное дообучение (SFT)**:
   - Используются данные формата:  
     ```json
     {"prompt": "Решите уравнение ∫₀¹ x² dx", "response": "∫₀¹ x² dx = [x³/3]₀¹ = 1/3"}
     ```
   - **Ключевой аспект**: очистка данных от ошибок через self-consistency проверку.

2. **Моделирование награды**:
   - Для математических задач (пример):  
     
    $$
     [
       R = \text{Correctness} + 0.5 \cdot \text{StepQuality} \;-\; 0.3 \cdot \text{LengthPenalty}.
     ]
    $$

   - Разработка эффективной функции награды является ключевым аспектом GRPO. В общем случае, она должна быть спроектирована так, чтобы поощрять желаемые свойства рассуждений — корректность, логическую последовательность, краткость и эффективность решения. Веса коэффициентов (например, 1, 0.5, -0.3 в примере) могут быть настроены эмпирически для достижения оптимального баланса между этими свойствами.

3. **Обучение с GRPO**:
   - **Гиперпараметры**:
     - Batch size: 512 промптов (по 4 ответа на промпт → 2048 примеров/шаг).
     - Learning rate: 1e-6 с линейным затуханием.
   - **Трюк**: Заморозка первых 10% слоев модели для сохранения общих знаний.

---

### **Кейсы применения**
1. **DeepSeek-Math-33B**:
   - Решение задач Международной математической олимпиады (IMO) с точностью **44.5%**.
   - **Особенность**: Использование GRPO + деревоискока (MCTS) для генерации шагов.

2. **Логический планировщик AlphaLogic**:
   - Автоматическое доказательство теорем в Coq с успешностью **68%** (против 52% у PPO).

---

### **Заключение**
GRPO представляет собой значительный шаг вперёд в области обучения с подкреплением для LLM, особенно в задачах, требующих сложного рассуждения. **Его применение уже выходит за рамки математики — текущие исследования тестируют GRPO в юридическом анализе и генерации научных гипотез.** Несмотря на ограничения, алгоритм демонстрирует потенциал для создания "мыслящих" ИИ-систем, способных к глубокому абстрактному мышлению.

</details> 

---

Далее мы шаг за шагом рассмотрим переход от PPO к GRPO, а затем к DAPO, чтобы увидеть, как был разработан этот новый алгоритм обучения с подкреплением.

## **2. Препдпосылки**

