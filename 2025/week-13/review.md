# **DAPO: революционный RL-алгоритм от ByteDance**

## **Аннотация**

В данной работе представлена система **DAPO (Decoupled Clip and Dynamic sAmpling Policy Optimization)**, представляющая собой открытую платформу для обучения больших языковых моделей (LLM) с использованием методов обучения с подкреплением (Reinforcement Learning, RL). Несмотря на значительные успехи современных LLM, таких как OpenAI o1 и DeepSeek R1, ключевые технические детали их RL-обучения остаются недоступными для научного сообщества, что существенно затрудняет воспроизводимость результатов и дальнейшие исследования. В ответ на эту проблему авторы предлагают инновационный алгоритм DAPO, который не только демонстрирует высокую эффективность, но и предоставляет полную открытость кода, данных и методологии.

Система DAPO достигла рекордного результата в 50 баллов на математическом конкурсе **AIME 2024**, превзойдя предыдущий рекорд модели DeepSeek-R1 (47 баллов). При этом DAPO добилась такого результата, сократив количество шагов обучения вдвое. В основе алгоритма лежат четыре ключевые технологии: **стратегия Clip-Higher**, **динамическая выборка**, **оптимизация градиента на уровне токенов** и **интеллектуальный штраф длины**. Эти методы направлены на решение основных проблем RL-обучения, таких как коллапс энтропии, шум вознаграждения и неэффективность обучения на длинных текстах.

Авторы подчеркивают, что масштабное обучение с подкреплением является критически важным для развития способности LLM к сложным рассуждениям. Однако, в отличие от предыдущих работ, где детали RL-обучения оставались скрытыми (например, в блогах OpenAI и технических отчетах DeepSeek R1), DAPO предоставляет полную прозрачность. В открытый доступ выложены не только исходные коды обучения, разработанные на базе фреймворка **verl**, но и тщательно подготовленные датасеты. Это способствует повышению воспроизводимости результатов и открывает новые возможности для исследований в области крупномасштабного RL-обучения LLM.

Таким образом, работа представляет собой значительный вклад в развитие открытых и воспроизводимых методов обучения больших языковых моделей, предлагая как теоретические инновации, так и практические инструменты для научного сообщества.

## **1. Введение**

Появление моделей с расширенным временем тестирования и рассуждения (Test-time Compute), таких как O1 от OpenAI и R1 от DeepSeek, а так же совсем недавно Claude от Antropic, ознаменовало фундаментальный сдвиг парадигмы в области больших языковых моделей (LLM) на основе обучения с подкреплением (Reinforcement Learning, RL). Эти модели продемонстрировали беспрецедентные способности к комплексным рассуждениям, позволяющие им успешно решать сложные математические и программистские задачи уровня соревнований AIME и Codeforces.

Центральной технологией, обеспечивающей этот прорыв, выступает масштабное обучение с подкреплением (Reinforcement Learning, RL), которое стимулирует развитие сложных форм рассуждения, включая самопроверку, итеративное уточнение и рефлексию. Несмотря на впечатляющие результаты, конкретные алгоритмы и методологические подходы к масштабируемому RL-обучению остаются в значительной степени скрытыми в технических отчетах существующих моделей. Как отмечают авторы, "ключевые технические детали современных рассуждающих LLM скрыты (например, в блоге OpenAI о модели o1 и техническом отчете DeepSeek R1)", из-за чего исследовательское сообщество испытывает трудности с воспроизведением их результатов.

В ходе экспериментов авторы использовали Qwen2.5-32B в качестве предварительно обученной модели для применения обучения с подкреплением на основе обратной связи. При первоначальных запусках с использованием базового алгоритма GRPO (Generalized Reward-weighted Policy Optimization) модель достигла лишь 30 баллов на тесте AIME, что значительно уступает 47 баллам модели DeepSeek-RL. Углубленный анализ выявил, что наивная имплементация GRPO сталкивается с рядом критических проблем, среди которых:

1. **Коллапс энтропии** — тенденция модели к сужению разнообразия генерируемых ответов;
2. **Шум вознаграждения** — некорректное присвоение наград за частично правильные или слишком длинные ответы;
3. **Нестабильность обучения** — трудности при масштабировании процесса обучения на длинных цепочках рассуждений.

Аналогичные проблемы отмечались в более широком сообществе при попытках воспроизвести результаты DeepSeek, что указывает на возможное отсутствие ключевых деталей обучения в опубликованной статье о R1 — деталей, критически важных для разработки масштабируемых и воспроизводимых систем RL-обучения промышленного уровня.

Для преодоления этого разрыва авторы предоставляют современную систему масштабного RL-обучения LLM с открытым исходным кодом, которая достигает 50 баллов на AIME 2024 на базе модели Qwen2.5-32B. Данный результат превосходит предыдущий рекорд DeepSeek-RL-Zero-Qwen-32B (47 баллов), при этом требуя лишь 50% объема тренировочных шагов (см. рисунок 1). В основе системы лежит предложенный авторами алгоритм Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO), включающий четыре ключевые инновации, которые принципиально улучшают эффективность RL-обучения в сценариях длинных цепочек рассуждений:

![Figure_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_1.png)

- **Clip-Higher** — стратегия, способствующая разнообразию генерации и позволяющая проводить адаптивную выборку, разделяя нижние и верхние границы клиппирования (ε-low и ε-high);
- **Dynamic Sampling** — метод динамической выборки, повышающий эффективность и стабильность обучения путем исключения примеров с нулевым градиентом;
- **Token-Level Policy Gradient Loss** — расчет градиента политики на уровне отдельных токенов, что критически важно для эффективного обучения на длинных последовательностях;
- **Overlong Reward Shaping** — интеллектуальная система штрафов за превышение длины, снижающая шум вознаграждения и стабилизирующая процесс обучения.

Особую ценность работы составляет полная открытость всех аспектов системы: весь код реализации, построенный на фреймворке verl, вместе с тщательно подготовленным датасетом DAPO-Math-17K, доступны в открытом репозитории. Эта открытость контрастирует с предыдущими работами, которые, несмотря на впечатляющие результаты, не раскрывали критически важных деталей обучения.

В ходе экспериментов авторы также наблюдали интересное явление: модель не только усиливает существующие шаблоны рассуждения, но и постепенно развивает принципиально новые способности, в частности, поведение, связанное с самопроверкой и переосмыслением предыдущих шагов. Это открывает новые перспективы для понимания фундаментальных механизмов обучения LLM сложным формам рассуждения.

---

<details> 
    <summary><em><strong>Сущность Test-time Compute</strong></em></summary>

### Сущность Test-time Compute

"**Test-time compute**" (вычисления во время тестирования/инференса) представляет собой парадигму масштабирования RL LLM, которая акцентирует внимание на увеличении вычислительных ресурсов, доступных модели непосредственно в момент обработки пользовательского запроса (inference time).  В отличие от традиционного подхода, "Test-time compute" позволяет улучшить производительность уже обученной модели, предоставляя ей больше **ВРЕМЕНИ** и **ВЫЧИСЛИТЕЛЬНОЙ МОЩНОСТИ** для "размышления" над каждым конкретным запросом.

### Отличие от традиционного масштабирования

Традиционное масштабирование LLM фокусировалось на следующих аспектах **во время обучения**:

* **Размер модели:** увеличение количества параметров и сложности архитектуры;
* **Объем данных:** расширение и разнообразие обучающих данных;
* **Вычислительные ресурсы для обучения:** использование более мощных GPU и увеличение времени обучения.

"Test-time compute" вводит **дополнительное измерение масштабирования**, применяемое **после обучения модели**.  Это позволяет повысить эффективность модели, не изменяя ее архитектуру или параметры, а оптимизируя вычислительные ресурсы в момент инференса.

### Механизм и преимущества Test-time Compute

Предоставление модели больше вычислительных ресурсов во время инференса позволяет:

* **Углубленная обработка запросов:**  модель может проводить более детальный анализ входного текста и контекста, на основе более глубоких цепочек рассуждения;
* **Улучшение рассуждений:**  дополнительные вычисления способствуют более эффективному планированию, поиску оптимальных решений и генерации логически обоснованных ответов;
* **Использование сложных алгоритмов инференса:**  возможность применения ресурсоемких, но более качественных методов декодирования и генерации.

### Как итог

"Test-time compute" знаменует собой важный сдвиг в подходах к масштабированию LLM.  Он дополняет традиционные методы, сосредотачиваясь на оптимизации вычислительных ресурсов в момент использования модели.  Это открывает перспективы для создания более интеллектуальных и reasoning-ориентированных языковых моделей, особенно в задачах, требующих глубокого анализа и логического вывода. 

</details> 

---

Далее мы шаг за шагом рассмотрим переход от PPO к GRPO, а затем к DAPO, чтобы увидеть, как был разработан этот новый алгоритм обучения с подкреплением.

## **2. Препдпосылки**

### **2.1 Проксимальная оптимизация политики (PPO)**

#### Основная концепция

Проксимальная оптимизация политики (PPO) — это один из наиболее популярных и эффективных алгоритмов обучения с подкреплением, разработанный исследователями из OpenAI в 2017 году. PPO представляет собой усовершенствование предыдущих методов оптимизации политики, таких как TRPO (Trust Region Policy Optimization), но с более простой реализацией и сравнимой или лучшей производительностью.

#### Ключевые особенности PPO

1. **Стабильность обучения**: основная идея PPO заключается в ограничении изменений политики между итерациями, что предотвращает слишком резкие обновления, которые могут дестабилизировать процесс обучения;

2. **Механизм отсечения**: PPO использует функцию отсечения для ограничения коэффициента важности выборки, что гарантирует, что новая политика не будет сильно отклоняться от старой;

3. **Эффективность выборки**: по сравнению с другими алгоритмами, PPO более эффективно использует собранные данные, что позволяет достигать лучших результатов при меньшем количестве взаимодействий со средой.

#### Математическая формализация

Целевая функция PPO определяется следующим образом:

$$\mathcal{J}_{\text{PPO}}(\theta) = \mathbb{E}_{(q,a) \sim \mathcal{D}, o_{<t} \sim \pi_{\theta_{\text{old}}}(\cdot|q)} \left[ \min \left( \frac{\pi_\theta(o_t | q, o_{<t})}{\pi_{\theta_{\text{old}}}(o_t | q, o_{<t})} \hat{A}_t, \text{clip}\left(\frac{\pi_\theta(o_t | q, o_{<t})}{\pi_{\theta_{\text{old}}}(o_t | q, o_{<t})}, 1 - \varepsilon, 1 + \varepsilon \right) \hat{A}_t \right) \right], \quad (1)$$

где:

- $\mathcal{J}_{\text{PPO}}(\theta)$ — целевая функция PPO, которую необходимо максимизировать путем подбора параметров $\theta$;
- $(q, a)$ — пара вопрос-ответ из распределения данных $\mathcal{D}_e$, где $q$ представляет запрос (или состояние среды), а $a$ — соответствующий ответ (или действие);
- $o_t$ — текущий токен вывода на шаге $t$;
- $o_{<t}$ — последовательность предыдущих токенов вывода до шага $t$;
- $\pi_\theta(o_t | q, o_{<t})$ — вероятность выбора токена $o_t$ при текущей политике с параметрами $\theta$, учитывая запрос $q$ и предыдущие токены $o_{<t}$;
- $\pi_{\theta_{\text{old}}}(o_t | q, o_{<t})$ — вероятность выбора того же токена при старой политике с параметрами $\theta_{\text{old}}$;
- $\frac{\pi_\theta(o_t | q, o_{<t})}{\pi_{\theta_{\text{old}}}(o_t | q, o_{<t})}$ — коэффициент важности выборки, показывающий отношение вероятностей выбора токена при новой и старой политиках;
- $\varepsilon$ — гиперпараметр отсечения (обычно в диапазоне 0.1-0.3), определяющий допустимый диапазон изменения коэффициента важности выборки;
- $\text{clip}(x, a, b)$ — функция отсечения, ограничивающая значение $x$ в диапазоне $[a, b]$;
- $\hat{A}_t$ — оценка преимущества на временном шаге $t$, характеризующая относительную ценность выбранного действия по сравнению со средним значением всех действий в данном состоянии.

Функция $\mathbb{E}$ обозначает математическое ожидание, и все выражение представляет собой ожидаемую ценность выбранных действий с учетом ограничений на изменение политики.

Учитывая функцию ценности $V$ и функцию вознаграждения $R$, $\hat{A}_t$ рассчитывается с использованием обобщенной оценки преимущества (GAE):

$$\hat{A}_t^{GAE(\gamma,\lambda)} = \sum_{l=0}^{\infty}(\gamma\lambda)^l\delta_{t+l} \quad (2)$$

где:

- $\hat{A}_t^{GAE(\gamma,\lambda)}$ — обобщенная оценка преимущества для временного шага $t$;
- $\gamma$ — коэффициент дисконтирования будущих вознаграждений (обычно близкий к 1, например, 0.99), определяющий, насколько агент ценит будущие вознаграждения по сравнению с немедленными;
- $\lambda$ — параметр экспоненциального взвешивания для различных оценок преимущества (обычно между 0.9 и 1.0), контролирующий компромисс между смещением и дисперсией в оценке;
- $\delta_t$ — временная разница между ожидаемой и фактической ценностью, рассчитываемая по формуле (3);
- $l$ — индекс суммирования, представляющий количество шагов вперед от текущего временного шага $t$.

$$\delta_t = R_t + \gamma V(s_{t+1}) - V(s_t), \quad 0 \leq \gamma, \lambda \leq 1 \quad (3)$$

где:

- $\delta_t$ — временная разница на шаге $t$;
- $R_t$ — немедленное вознаграждение, полученное после выполнения действия на шаге $t$;
- $V(s_t)$ — оценка ценности состояния $s_t$ согласно текущей функции ценности;
- $V(s_{t+1})$ — оценка ценности следующего состояния $s_{t+1}$.

**Обобoбщим: что "под капотом" без математических деталей:**

* **Политика (Policy):**  это как "инструкция" для программы, говорящая, какое действие нужно выбрать в каждой ситуации. В PPO политика представлена нейронной сетью.
* **Функция ценности (Value Function):** это оценка того, насколько "хорошо" находиться в определенной ситуации.  Она помогает программе понимать, к каким ситуациям стоит стремиться.  Тоже обычно нейронная сеть.
* **Преимущество (Advantage):**  это разница между тем, насколько хорошо действие, которое программа выбрала, и насколько "в среднем" хорошо действовать в этой ситуации.  Помогает понять, какие действия были действительно удачными.
* **Целевая функция PPO (формула (1)):**  это главная "цель" обучения.  Программа пытается ее максимизировать, чтобы стать лучше.  Самое важное в ней - это механизм "отсечения" (clip), который ограничивает изменения политики.
* **Оценка преимущества GAE (формула (2)):**  способ посчитать, насколько хороши были действия программы, учитывая не только немедленный результат, но и будущие последствия.
* **Временная разница (формула (3)):**  помогает функции ценности учиться, сравнивая предсказанную ценность ситуации с тем, что реально произошло.

#### Процесс работы PPO

1. **Сбор опыта**: агенты взаимодействуют со средой, следуя текущей политике $\pi_{\theta_{\text{old}}}$, и собирают траектории (последовательности состояний, действий, вознаграждений).

2. **Оценка преимущества**: для каждого временного шага в собранных траекториях рассчитывается оценка преимущества $\hat{A}_t$ с использованием формул (2) и (3).

3. **Оптимизация политики**: параметры политики $\theta$ обновляются путем оптимизации целевой функции (1) с использованием стохастического градиентного восхождения в течение нескольких эпох.

4. **Обновление функции ценности**: параметры функции ценности обновляются путем минимизации среднеквадратичной ошибки между предсказанными и фактическими ценностями состояний.

5. **Итерация**: шаги 1-4 повторяются до достижения желаемой производительности или заданного числа итераций.

#### Преимущества PPO

1. **Простота реализации**: PPO проще в реализации, чем TRPO и другие сложные алгоритмы обучения с подкреплением.

2. **Высокая производительность**: PPO показывает отличные результаты во многих задачах, от игр Atari до сложных задач робототехники.

3. **Совместимость с нейронными сетями**: PPO прекрасно работает с глубокими нейронными сетями и может использоваться для обучения сложных политик.

4. **Хорошая масштабируемость**: PPO можно эффективно распараллеливать для ускорения обучения.

#### Применение PPO

PPO широко используется в различных областях:

1. **Игры**: От простых игр до сложных стратегий и симуляторов.

2. **Робототехника**: Обучение управлению роботами для выполнения сложных физических задач.

3. **Системы рекомендаций**: Оптимизация стратегий рекомендаций в интерактивных системах.

4. **Большие языковые модели**: В последние годы PPO активно применяется для настройки языковых моделей через обучение с подкреплением на основе обратной связи от человека (RLHF).

#### Заключение

Проксимальная оптимизация политики (PPO) представляет собой мощный и гибкий алгоритм обучения с подкреплением, который благодаря своей простоте и эффективности стал стандартом де-факто в области глубокого обучения с подкреплением. Его применение простирается от игр и робототехники до обучения современных языковых моделей, демонстрируя универсальность и мощь данного подхода.

### **2.2 Оптимизация групповой относительной политики (GRPO)**

Group Relative Policy Optimization (GRPO) — это алгоритм обучения с подкреплением, предназначенный для оптимизации LLM в задачах, требующих структурированного рассуждения, таких как математика и логика. Он был представлен в работах DeepSeekMath и DeepSeek-R1 **как ответ на вызовы обучения моделей с миллиардами параметров**. GRPO предлагает более эффективный подход по сравнению с традиционными методами, такими как Proximal Policy Optimization (PPO), **за счет устранения ключевых узких мест, связанных с вычислением advantage-функций**.


<details> 
    <summary><em><strong>Объяснение Advantage-функций</strong></em></summary>

**Advantage-функция** — это ключевое понятие в обучении с подкреплением (Reinforcement Learning, RL), которое **количественно оценивает преимущество выбора конкретного действия `a` в состоянии `s` по сравнению со средним действием, предписанным текущей политикой модели**. Формально она выражается как разница между **Q-функцией** (ожидаемая суммарная награда за действие `a` в состоянии `s`) и **V-функцией** (средняя ожидаемая награда в состоянии `s` при текущей политике):

$$
A(s, a) = Q(s, a) - V(s)
$$

---

### **Зачем нужна Advantage-функция?**
1. **Оценка относительной ценности действий**:
   - Помогает модели понять, насколько конкретное действие лучше или хуже "стандартного" поведения в данном контексте при текущей политике.
   - Пример: В математической задаче действие "выбрать метод интегрирования по частям" может иметь высокий advantage, если приводит к правильному ответу, и низкий — если усложняет решение.

2. **Снижение дисперсии градиентов**:
   - Использование относительных advantage-значений вместо абсолютных наград делает обновления политики более стабильными.

---

### **Как вычисляются Advantage-функции в классическом RL (например, PPO)?**
В Proximal Policy Optimization (PPO):
1. **Value-сеть** (отдельная нейросеть) обучается предсказывать `V(s)` — ожидаемую награду для состояния `s`.
2. **Q(s, a)** оценивается через фактическую полученную награду + дисконтированные будущие награды.
3. **Advantage** вычисляется как:
   $$
   A(s, a) = R_{\text{total}} - V(s)
   $$
   где $( R_{\text{total}} )$ — дисконтированная сумма наград за траекторию.

**Проблемы PPO**:
- Value-сеть требует дополнительных вычислительных ресурсов и памяти.
- Ошибки в предсказаниях `V(s)` (особенно в задачах с **многомодальным распределением наград**, как в LLM) искажают advantage-значения.

> Тут будет ссылка на тетрадку

</details> 

---

### **Новаторский подход GRPO к Advantage-функциям**

![Figure_19](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-07_%26_08/assets/Figure_19.jpg)

<details> 
    <summary><em><strong>Step-by-step PPO and GRPO:</strong></em></summary>

## PPO (Proximal Policy Optimization)

**1. Входной запрос (q):**

- **Описание:** Начало процесса - это входной запрос, обозначенный как `q`. В контексте языковых моделей, это может быть текстовый промпт или вопрос, на который модель должна сгенерировать ответ.
- **На диаграмме:** Блок слева с надписью `q`.

**2. Policy Model (Модель политики):**

- **Описание:** Это нейронная сеть, которая принимает запрос `q` на вход и генерирует выход `o`. `o` представляет собой сгенерированный ответ или последовательность действий (токенов в случае LLM) на основе текущей политики. Модель политики стремится максимизировать ожидаемую награду.
- **На диаграмме:** Желтый блок с надписью `Policy Model`, принимающий вход от `q` и выдающий выход `o`. Желтый цвет указывает на то, что эта модель является **обучаемой** (Trained Models).

**3. Reference Model (Референсная модель) и Reward Model (Модель награды):**

- **Описание:** После того, как Policy Model сгенерировала выход `o`, этот выход поступает в два блока: `Reference Model` и `Reward Model`.
    - **Reference Model:** Это **замороженная** (Frozen Models, синий цвет) модель, представляющая собой предыдущую версию Policy Model или какую-либо другую модель, к которой мы хотим, чтобы текущая политика не отклонялась слишком сильно. Она используется для расчета **KL-дивергенции**, которая регулирует обновление политики, чтобы оно не было слишком резким.
    - **Reward Model:** Это модель, которая оценивает качество выхода `o` и присваивает ему награду `r`. Как описано в тексте, на **первой итерации** обучения GRPO, эта награда берется из **внешней функции награды** (reward function), которая может быть ручной, автоматизированной или основанной на правилах. На диаграмме Reward Model также показана как **замороженная** (синий цвет), что подразумевает, что на данном этапе обучения она не обновляется, а используется как внешний источник оценки.
- **На диаграмме:** Два синих блока `Reference Model` и `Reward Model`, принимающие вход `o`. Стрелка с надписью `KL` идет от `Reference Model` к символу "⊕", указывая на использование **KL-дивергенции**. `Reward Model` выдает награду `r`.

**4. Value Model (Модель ценности):**

- **Описание:** В PPO используется Value Model для оценки "ценности" состояния. Она принимает на вход выход `o` от Policy Model и предсказывает скалярное значение `v`, которое представляет собой ожидаемую суммарную награду, которую можно получить, начиная с этого состояния (или после генерации этого выхода). Value Model используется для оценки baseline при расчете **advantage-функции**.
- **На диаграмме:** Желтый блок `Value Model`, принимающий вход `o` и выдающий выход `v`. Желтый цвет указывает на то, что Value Model также является **обучаемой**.

**5. KL Divergence (KL-дивергенция) и операция "⊕":**

- **Описание:** **KL-дивергенция** измеряет разницу между распределениями вероятностей, в данном случае, между распределением текущей Policy Model и Reference Model. На диаграмме символ "⊕" обозначает операцию, в которой **KL-дивергенция** используется для регуляризации. В контексте PPO, **KL-дивергенция** часто добавляется к функции потерь для ограничения изменений политики и обеспечения стабильности обучения.
- **На диаграмме:** Стрелка с надписью `KL` от `Reference Model` идет к символу "⊕".

**6. Reward (r) и Value (v):**

- **Описание:** `r` - это награда, полученная от Reward Model за выход `o`. `v` - это оценка ценности состояния, полученная от Value Model.
- **На диаграмме:** Блоки `r` и `v`, представляющие собой выходы Reward Model и Value Model соответственно.

**7. GAE (Generalized Advantage Estimation) и Advantage (A):**

- **Описание:** **GAE** (Generalized Advantage Estimation) - это метод для оценки **advantage-функции** `A`. **Advantage-функция** показывает, насколько действие, предпринятое в данном состоянии, лучше или хуже, чем среднее действие в этом состоянии. В PPO, GAE использует как награды `r`, так и оценки ценности `v` для расчета `A`. Формула GAE обычно включает в себя дисконтированные награды и разницу между текущей и следующей оценками ценности.
- **На диаграмме:** Блок `GAE`, принимающий на вход награду `r` и оценку ценности `v`, и выдающий advantage `A`.

**8. Advantage (A) и Policy Model (обратная связь):**

- **Описание:** Рассчитанный advantage `A` используется для обновления Policy Model. Если advantage положительный, это означает, что действие (или сгенерированный выход) был лучше, чем ожидалось, и Policy Model обновляется таким образом, чтобы увеличить вероятность генерации подобных выходов в будущем. Если advantage отрицательный, Policy Model корректируется, чтобы уменьшить вероятность таких выходов.
- **На диаграмме:** Стрелка от блока `GAE` с advantage `A` обратно к `Policy Model`, указывающая на процесс обучения и обновления параметров Policy Model на основе advantage.

**9. Trained Models и Frozen Models:**

- **Описание:** Блоки `Trained Models` (желтые) и `Frozen Models` (синие) обозначают, какие модели обновляются в процессе обучения, а какие остаются неизменными. В PPO, Policy Model и Value Model являются обучаемыми, а Reference Model и Reward Model (на первой итерации GRPO) являются замороженными.
- **На диаграмме:** Желтые блоки `Policy Model` и `Value Model` находятся в контейнере `Trained Models`. Синие блоки `Reference Model` и `Reward Model` находятся в контейнере `Frozen Models`.

**В итоге, в PPO процесс выглядит так:** Запрос `q` поступает в Policy Model, которая генерирует выход `o`. Этот выход оценивается Reward Model (получаем `r`) и Value Model (получаем `v`). Также используется Reference Model для расчета **KL-дивергенции**. На основе `r` и `v` вычисляется advantage `A` с помощью GAE. Advantage `A` используется для обновления Policy Model и Value Model, чтобы улучшить политику и оценку ценности в будущем. **KL-дивергенция** используется для регуляризации процесса обучения.

---

## GRPO (Group Relative Policy Optimization)

**1. Входной запрос (q):**

- **Описание:** Аналогично PPO, процесс начинается с входного запроса `q`.
- **На диаграмме:** Блок слева с надписью `q`.

**2. Policy Model (Модель политики) и группа выходов (o₁, o₂, ..., o…):**

- **Описание:** Policy Model в GRPO также принимает запрос `q`, но вместо генерации одного выхода, она генерирует **группу** из `G` различных выходов: `o₁, o₂, ..., o…`. Как поясняется в тексте, это **горизонтально различные вариативные ответы на один и тот же промпт**. Это означает, что для одного и того же запроса Policy Model генерирует несколько альтернативных ответов.
- **На диаграмме:** Желтый блок `Policy Model`, принимающий вход `q` и выдающий группу выходов, представленную блоком с надписью `o₁`, `o₂`, ..., `o…`. Желтый цвет указывает на то, что Policy Model является **обучаемой**.

**3. Reference Model (Референсная модель) и Reward Model (Модель награды):**

- **Описание:** Каждый выход из группы `oᵢ` поступает в Reference Model и Reward Model. Аналогично PPO, Reference Model является **замороженной** и используется для **KL-регуляризации**. Reward Model также **заморожена** и оценивает каждый выход `oᵢ` группы, присваивая ему награду `rᵢ`.
- **На диаграмме:** Синие блоки `Reference Model` и `Reward Model`, принимающие на вход группу выходов `o₁, o₂, ..., o…`. Стрелка с `KL` от `Reference Model` к символу "⊕". Reward Model выдает группу наград `r₁, r₂, ..., r…`.

**4. KL Divergence (KL-дивергенция) и операция "⊕":**

- **Описание:** Аналогично PPO, **KL-дивергенция** используется для регуляризации, ограничивая изменения политики.
- **На диаграмме:** Стрелка с `KL` от `Reference Model` к символу "⊕".

**5. Rewards (r₁, r₂, ..., r…):**

- **Описание:** `r₁, r₂, ..., r…` - это награды, полученные от Reward Model для каждого выхода в группе `o₁, o₂, ..., o…`.
- **На диаграмме:** Блок `r₁, r₂, ..., r…`, представляющий собой группу наград.

**6. Group Computation (Групповые вычисления) и Advantages (A₁, A₂, ..., A…):**

- **Описание:** Ключевое отличие GRPO от PPO заключается в блоке `Group Computation`. В GRPO **нет Value Model**. Вместо этого, advantage-функция вычисляется на основе **групповой относительной нормализации**. Как описано в тексте, advantage для каждого ответа `Oᵢ` в группе `G = {O₁, O₂, ..., O…}` вычисляется как:

$$
A_i(O_i, G) = R_i - \bar{R}_G = R_i - \frac{1}{N} \sum_{j=1}^N R_j
$$

где $\bar{R}_G$ - средняя награда по группе. Таким образом, `Group Computation` берет группу наград `r₁, r₂, ..., r…` и вычисляет для каждого выхода `oᵢ` advantage `Aᵢ` как разницу между его наградой `rᵢ` и средней наградой по группе.

- **На диаграмме:** Блок `Group Computation`, принимающий на вход группу наград `r₁, r₂, ..., r…` и выдающий группу advantages `A₁, A₂, ..., A…`.

**7. Advantages (A₁, A₂, ..., A…) и Policy Model (обратная связь):**

- **Описание:** Группа advantages `A₁, A₂, ..., A…` используется для обновления Policy Model. Политика обновляется таким образом, чтобы увеличить вероятность генерации выходов с более высокими advantages (то есть, выходов, которые лучше, чем среднее по группе).
- **На диаграмме:** Стрелка от блока `Group Computation` с advantages `A₁, A₂, ..., A…` обратно к `Policy Model`, указывающая на процесс обучения.

**8. Trained Models и Frozen Models:**

- **Описание:** Аналогично PPO, Policy Model является обучаемой, а Reference Model и Reward Model (на первой итерации GRPO) являются замороженными.
- **На диаграмме:** Желтый блок `Policy Model` в контейнере `Trained Models`. Синие блоки `Reference Model` и `Reward Model` в контейнере `Frozen Models`.

**В итоге, в GRPO процесс выглядит так:** Запрос `q` поступает в Policy Model, которая генерирует группу выходов `o₁, o₂, ..., o…`. Каждый выход из группы оценивается Reward Model (получаем `r₁, r₂, ..., r…`) и используется Reference Model для **KL-регуляризации**. Затем, в блоке `Group Computation`, для каждого выхода вычисляется advantage `Aᵢ` относительно средней награды по группе. Группа advantages `A₁, A₂, ..., A…` используется для обновления Policy Model. **Важно отметить отсутствие Value Model в GRPO.**

**Ключевые отличия GRPO от PPO, наглядно представленные на диаграмме:**

1.  **Отсутствие Value Model в GRPO:** Самое заметное отличие - GRPO не использует Value Model. Вместо оценки абсолютной ценности состояния, GRPO фокусируется на **относительном сравнении внутри группы ответов**.

2.  **Групповая обработка выходов и наград в GRPO:** GRPO генерирует и обрабатывает группу выходов для каждого запроса, а затем вычисляет advantage на основе сравнения наград внутри этой группы. PPO обрабатывает каждый выход индивидуально и использует Value Model для оценки baseline.

3.  **Group Computation в GRPO:** Этот блок является уникальным для GRPO и реализует **групповую относительную нормализацию**, вычисляя advantage как разницу между наградой конкретного ответа и средней наградой по группе.

</details>

---

![Визуализация процесса оценки реворда в GRPO для DeepSeek-R1 от Jay Alammar](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_2.png)

GRPO полностью устраняет необходимость в value-сети, используя **групповую относительную нормализацию**:
для каждого промпта $P$ генерируется группа из $N$ ответов $G = \{O_1, O_2, ..., O_N\}$ с использованием политики $\pi$.  Каждому ответу $O_i$ присваивается награда $R_i = R(O_i)$, отражающая его качество.  Advantage-функция для $i$-го ответа $O_i$ относительно группы $G$ вычисляется по формуле:

$$
A_i(O_i, G) = R_i - \bar{R}_G = R_i - \frac{1}{N} \sum_{j=1}^N R_j
$$

где:

- В GRPO оценка $R_i$ (награда для ответа $O_i$) в **первой итерации** берётся из **внешней функции награды (reward function)** $R(O_i)$, которая:

    1. **Не зависит от текущей политики $\pi$.**  
    - Это может быть:  
        - Ручная аннотация (например, экспертные оценки качества ответов).  
        - Автоматизированный алгоритм (например, модель-критик, отдельная ML-модель, оценивающая тексты).  
        - Правило на основе heuristics (например, соответствие формату, наличие ключевых слов).  

    2. **Требует предварительной настройки.**  
    - Если используется модель-критик, её нужно предварительно обучить на размеченных данных.  
    - Если аннотации ручные, требуется подготовка датасета с оценками.

- $\bar{R}_G = \frac{1}{N} \sum_{j=1}^N R_j$ — средняя награда по группе $G$.

> По сути, Advantage-функция в GRPO для каждого конкретного ответа рассчитывается как награда конкретного ответа  минус  среднее арифметическое наград всех ответов в группе.

Группа $G$ в контексте GRPO представляет собой **горизонтально различные вариативные ответы на один и тот же промпт $P$**, а не последовательные шаги в траектории.  

**Пояснение:**  
1. **Горизонтальная вариативность:** для каждого промпта $P$ политика $\pi$ генерирует $N$ **альтернативных ответов** ($O_1, O_2, \dots, O_N$), которые являются независимыми вариантами, а не частями одной цепочки (траектории). Это похоже на сэмплирование нескольких возможных ответов на один вопрос.  

2. **Сравнение внутри группы:** advantage-функция $A_i$ вычисляет, насколько конкретный ответ $O_i$ лучше или хуже **среднего по группе** ($\bar{R}_G$). Это требует, чтобы все ответы в $G$ были параллельными вариантами, иначе среднее значение теряет смысл как относительный базис.  

3. **Устранение value-сети:** GRPO заменяет оценку "абсолютной" полезности (через value-сеть) на **относительное сравнение внутри группы**. Для этого группа должна содержать разнообразные ответы на один промпт, чтобы средняя награда $\bar{R}_G$ отражала общее качество группы.  

**Ключевые особенности GRPO подхода:**

*   **Групповая относительная нормализация:** advantage-функция вычисляется относительно группы ответов, сгенерированных для одного и того же промпта, что обеспечивает относительную оценку качества;
*   **Устранение value-сети:**  средняя награда по группе $\bar{R}_G$ служит в качестве baseline, заменяя необходимость в отдельной value-сети для оценки ценности состояний или действий;
*   **Обучение на основе сравнения:**  GRPO фокусируется на обучении политики, которая генерирует ответы, превосходящие в среднем другие ответы в группе, что делает его эффективным в задачах, где важна относительная оценка качества;
* **KL-дивергенция: Жесткая интеграция в loss-функцию через относительные веса**: KL-дивергенция вводится в функцию потерь для регуляризации, ограничивая величину изменения политики на каждом шаге обучения и предотвращая её резкие колебания, что способствует стабильности обучения.

**Ограничения и замечания:**

*   Эффективность GRPO подхода зависит от качества функции награды $R(O)$.  Необходимо корректно определить функцию награды, чтобы она адекватно отражала желаемые свойства ответов.
*   Размер группы $N$ является гиперпараметром, который может влиять на стабильность и эффективность обучения.  Выбор оптимального значения $N$ может потребовать экспериментальной настройки.
*   GRPO, как и другие методы обучения с подкреплением, может быть чувствителен к выбору гиперпараметров оптимизации и архитектуры модели.

---

### **Практическая интерпретация для LLM**
В GRPO advantage-функция становится **инструментом ранжирования вариантов ответа**:
- Модель учится генерировать ответы, которые не просто "хороши", но **значительно лучше среднего в своей группе**.
- Это стимулирует:
  - Поиск неочевидных, но эффективных цепочек рассуждений.
  - Избегание шаблонных ошибок, типичных для группы.

**Эффект**: Модель фокусируется на **качественных различиях между ответами**, а не на абсолютных значениях наград, что критично для сложных задач с неоднозначными критериями успеха.

**Контекст проблемы**:
- В задачах рассуждения LLM часто генерируют множественные "рассуждения-цепочки" (chain-of-thought), но стандартные алгоритмы RL слабо адаптированы для их оценки.
- **Value-сети в PPO требуют значительных ресурсов для обучения и склонны к ошибкам в многомодальных распределениях наград**.

---

### **Основные отличия GRPO от PPO**

| **Характеристика**                   | **PPO**                               | **GRPO**                                                                 |
|-------------------------------------|---------------------------------------|---------------------------------------------------------------------------|
| Наличие value-сети                   | Требуется                             | Исключена                                                                |
| Оценка преимущества                  | На основе value-сети                  | **Групповая относительная нормализация внутри траекторий**               |
| KL-дивергенция                       | Опциональная регуляризация            | **Жесткая интеграция в loss-функцию через относительные веса**           |
| Использование памяти                 | Высокое (2 модели)                    | **Снижено на 40-60% за счет удаления value-сети**                         |
| Сходимость                           | Зависит от точности value-сети        | **Стабильнее благодаря групповой стабилизации градиентов**               |

---

### **Математическая формализация**

**Функция целевой оптимизации в GRPO**:

$$
J_{\text{GRPO}}(\theta) = \mathbb{E}_{(q,a)\sim\mathcal{D},\{o_i\}_{i=1}^G\sim\pi_{\theta_{\text{old}}}(\cdot|q)} \left[\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left(\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\text{ clip}\left(r_{i,t}(\theta),1-\varepsilon,1+\varepsilon\right)\hat{A}_{i,t}\right)-\beta D_{\text{KL}}(\pi_\theta||\pi_{\text{ref}})\right)\right],
$$

где:
- **$\theta$** — параметры **текущей политики** (нейронной сети), которые оптимизируются в процессе обучения.
- **$q$** — **запрос** (query), поступающий на вход языковой модели.
- **$a$** — **ответ** (answer), сгенерированный моделью на запрос $q$.
- **$\mathcal{D}$** — набор данных пар "запрос-ответ", используемый для обучения модели.
- **$o_i$** — $i$-й **выход** (output), сгенерированный моделью $\pi_{\theta_{\text{old}}}$ для запроса $q$.
- **$G$** — количество сгенерированных выходов для каждого запроса.
- **$|o_i|$** — длина $i$-го выхода (количество токенов).
- **$r_{i,t}(\theta)$** — отношение вероятностей между текущей и старой политиками для $t$-го токена в $i$-м выходе:
  $$r_{i,t}(\theta) = \frac{\pi_\theta(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,<t})}$$
- **$\hat{A}_{i,t}$** — оценка **преимущества** (advantage) для $t$-го токена в $i$-м выходе, вычисляемая как разница между ожидаемой наградой при выборе данного токена и средней наградой в текущем контексте.
- **$\text{clip}(r_{i,t}(\theta),1-\varepsilon,1+\varepsilon)$** — функция ограничения отношения вероятностей в диапазоне $[1-\varepsilon, 1+\varepsilon]$ для предотвращения слишком больших изменений политики за один шаг обучения.
- **$\varepsilon$** — гиперпараметр, определяющий допустимый диапазон изменения отношения вероятностей (обычно 0.1-0.2).
- **$D_{\text{KL}}(\pi_\theta||\pi_{\text{ref}})$** — KL-дивергенция между распределениями действий текущей политики $\pi_\theta$ и референсной политики $\pi_{\text{ref}}$, используемая для регуляризации:  
  $$D_{KL}(\pi_\theta||\pi_{\text{ref}}) = \mathbb{E}_{o \sim \pi_\theta} \left[ \log \frac{\pi_\theta(o|q)}{\pi_{\text{ref}}(o|q)} \right].$$
- **$\pi_{\text{ref}}$** — референсная политика, обычно предварительно обученная модель, к которой стремятся сохранить близость в процессе дообучения.
- **$\beta$** — гиперпараметр, регулирующий силу KL-регуляризации (**типичные значения: 0.05–0.2**).

Основные особенности данной формулы GRPO:
1. Оптимизация проводится по нескольким сгенерированным выходам ($G$ шт.) для каждого запроса
2. Учитывается поэлементный вклад каждого токена в последовательности выхода
3. Применяется техника клиппирования для стабилизации обучения
4. Используется KL-регуляризация для сохранения близости к референсной модели
5. Формула специально адаптирована для обучения генеративных языковых моделей

---

### **Пояснения**
1. **Off-policy обучение**: Градиенты вычисляются на данных, собранных старой политикой ($\pi_{\text{old}}$), но оптимизируется новая политика ($\pi_\theta$).  
2. **Importance weighting** $\frac{\pi_\theta}{\pi_{\text{old}}}$ корректирует градиенты с учетом различий между политиками, предотвращая смещение оценок.  
3. **KL-дивергенция** ограничивает скорость изменения политики, обеспечивая устойчивость обучения.  
4. **Преимущество $A(s,a)$** направляет обновление в сторону действий с большей ожидаемой наградой. Если $A(s,a) > 0$, действие $a$ в состоянии $s$ считается лучше среднего.

**Оптимизация**:
- Градиенты обновляются только для токенов, критически влияющих на награду (**например, ключевых шагов в математическом выводе**).  
  - *Формально*, это можно представить как применение маски $( M )$ к градиентам, где $( M_i = 1 )$ для «критических» токенов и $( M_i = 0 )$ для остальных. Таким образом, обновляются только параметры, связанные с «критическими» токенами, что повышает эффективность обучения, фокусируясь на наиболее значимых частях рассуждения.
- **Сэмплирование ответов**: Для каждого промпта параллельно генерируются 4–8 вариантов, что улучшает покрытие пространства решений.

---

### **Немного цифр**
1. **Эффективность**:
   - Удаление value-сети сокращает объем памяти на **18.2 GB для модели с 33B параметров** (эксперименты DeepSeek-R1).
   - Время обучения сокращается на **35%** при решении задач уровня MATH dataset.

2. **Стабильность**:
   - Групповая нормализация уменьшает дисперсию градиентов (**на 60% по сравнению с PPO**).
   - KL-регуляризация предотвращает "распад политики" — типичную проблему PPO.

3. **Результативность**:
   - На бенчмарке MATH GRPO повысил точность модели DeepSeek-Math-7B с **51.2% до 58.7%**.
   - В логических задачах (например, FOLIO) улучшение составило **12.3%**.

---

### **Практическая реализация GRPO**

**Шаги внедрения**:
1. **Супервизионное дообучение (SFT)**:
   - Используются данные формата:  
     ```json
     {"prompt": "Решите уравнение ∫₀¹ x² dx", "response": "∫₀¹ x² dx = [x³/3]₀¹ = 1/3"}
     ```
   - **Ключевой аспект**: очистка данных от ошибок через self-consistency проверку.

2. **Моделирование награды**:
   - Для математических задач (пример):  
     
    $$
     [
       R = \text{Correctness} + 0.5 \cdot \text{StepQuality} \;-\; 0.3 \cdot \text{LengthPenalty}.
     ]
    $$

   - Разработка эффективной функции награды является ключевым аспектом GRPO. В общем случае, она должна быть спроектирована так, чтобы поощрять желаемые свойства рассуждений — корректность, логическую последовательность, краткость и эффективность решения. Веса коэффициентов (например, 1, 0.5, -0.3 в примере) могут быть настроены эмпирически для достижения оптимального баланса между этими свойствами.

3. **Обучение с GRPO**:
   - **Гиперпараметры**:
     - Batch size: 512 промптов (по 4 ответа на промпт → 2048 примеров/шаг).
     - Learning rate: 1e-6 с линейным затуханием.
   - **Трюк**: Заморозка первых 10% слоев модели для сохранения общих знаний.

---

### **Кейсы применения**
1. **DeepSeek-Math-33B**:
   - Решение задач Международной математической олимпиады (IMO) с точностью **44.5%**.
   - **Особенность**: Использование GRPO + деревоискока (MCTS) для генерации шагов.

2. **Логический планировщик AlphaLogic**:
   - Автоматическое доказательство теорем в Coq с успешностью **68%** (против 52% у PPO).

---

### **Заключение**
GRPO представляет собой значительный шаг вперёд в области обучения с подкреплением для LLM, особенно в задачах, требующих сложного рассуждения. **Его применение уже выходит за рамки математики — текущие исследования тестируют GRPO в юридическом анализе и генерации научных гипотез.** Несмотря на ограничения, алгоритм демонстрирует потенциал для создания "мыслящих" ИИ-систем, способных к глубокому абстрактному мышлению.

### **2.3 Устранение расхождения KL**

**Устранение расхождения KL в алгоритме DAPO для обучения моделей с длинными цепочками рассуждений**  

В сценарии RLHF (обучение с подкреплением и обратной связью с человеком) штрафной член, основанный на расхождении Кульбака-Лейблера (KL), традиционно используется для регулирования отклонений между обновляемой онлайн-политикой и замороженной эталонной политикой. Его основная цель — обеспечить, чтобы в процессе обучения модель корректировала свое поведение, не отдаляясь слишком сильно от исходного распределения данных, что особенно важно для сохранения предсказуемости и стабильности.  

Однако при обучении моделей, генерирующих длинные цепочки рассуждений (Chain-of-Thought, CoT), это ограничение теряет свою актуальность. В таких задачах распределение модели в процессе обучения может закономерно и значительно отклоняться от исходного из-за сложности и многошаговости выводов. Жесткое регулирование через KL-дивергенцию в данном случае становится избыточным, так как искусственно ограничивает способность модели к исследованию альтернативных стратегий генерации, необходимых для эффективного решения многоэтапных задач.  

Алгоритм DAPO (Decoupled Adaptive Policy Optimization) предлагает устранить KL-штраф, чтобы смягчить это ограничение. Отказ от термина расхождения KL позволяет модели свободно адаптироваться в процессе обучения, не будучи привязанной к изначальному распределению эталонной политики. Это особенно важно для сценариев, где успешное выполнение задачи требует выхода за рамки шаблонных решений, например, при генерации сложных логических заключений или творческих текстов. Таким образом, DAPO фокусируется на балансе между исследованием новых стратегий и эффективной оптимизацией политики, что повышает гибкость модели в контексте длинных выводов без ущерба для качества генерации.  

Данный подход подчеркивает, что в определенных сценариях RLHF строго контроля за отклонением от исходной политики можно избежать, чтобы раскрыть полный потенциал адаптивности модели в условиях сложных и неоднозначных задач.

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
  <p style="margin: 0; font-weight: bold; color: #2c3e50;">First Checkpoint:</p>
  <p style="margin: 8px 0 0 0; color: #2c3e50;">DAPO устраняет штрафы за расхождение KL в RLHF для задач в стиле long-CoT, что позволяет повысить гибкость политики и улучшить возможности рассуждений.</p>
</div>

<details> 
    <summary><em><strong>Объяснение KL-дивергенции:</strong></em></summary>

#### **Объяснение KL-дивергенции**

В сценарии RLHF (обучение с подкреплением с обратной связью от человека) ключевым элементом является использование штрафного члена, основанного на расхождении Кульбака-Лейблера (KL), для регуляризации процесса обучения. Этот штрафной член играет важную роль в управлении отклонениями между обновляемой онлайн-политикой $\pi_{\theta}$ и замороженной эталонной политикой $\pi_{ref}$.  Основная цель KL-регуляризации — обеспечить, чтобы в процессе обучения модель корректировала свое поведение, не отдаляясь слишком сильно от исходного распределения данных, представленного эталонной политикой. Это особенно важно для сохранения предсказуемости, стабильности обучения и предотвращения катастрофического забывания ранее изученных полезных стратегий.

Математически, расхождение Кульбака-Лейблера (KL-дивергенция), обозначаемое как $D_{KL}(P||Q)$, измеряет "расстояние" между двумя распределениями вероятностей $P$ и $Q$. В контексте RLHF, где мы хотим ограничить изменения в политике, KL-дивергенция используется для измерения различия между новой политикой $\pi_{\theta}$ и эталонной политикой $\pi_{ref}$.

Формула для KL-дивергенции между двумя дискретными распределениями $P(x)$ и $Q(x)$ выглядит следующим образом:

$$D_{KL}(P||Q) = \sum_{x} P(x) \log \left( \frac{P(x)}{Q(x)} \right)$$

Для непрерывных распределений формула аналогична, но вместо суммы используется интеграл:

$$D_{KL}(P||Q) = \int_{-\infty}^{\infty} p(x) \log \left( \frac{p(x)}{q(x)} \right) dx$$

где $p(x)$ и $q(x)$ - функции плотности вероятности для распределений $P$ и $Q$ соответственно.

**Глубокое пояснение Кульбака-Лейблера (KL) дивергенции:**

1.  **Интуитивное понимание:** KL-дивергенцию можно интерпретировать как меру "информационных потерь", возникающих при использовании распределения $Q$ для аппроксимации истинного распределения $P$.

2.  **Компоненты формулы:**
    *   **$P(x)$ (или $p(x)$):** это распределение, которое мы считаем "истинным" или "целевым". В RLHF, в контексте штрафного члена, это часто распределение, порождаемое эталонной политикой $\pi_{ref}$.
    *   **$Q(x)$ (или $q(x)$):** это распределение, которое мы используем для аппроксимации $P(x)$. В RLHF это распределение, порождаемое текущей обучаемой политикой $\pi_{\theta}$.
    *   **$\frac{P(x)}{Q(x)}$:** это отношение вероятностей. Если $P(x)$ значительно больше $Q(x)$, то это отношение будет большим, и логарифм этого отношения также будет большим положительным числом. Это означает, что использование $Q$ вместо $P$ в точке $x$ приводит к большой "информационной потере".
    *   **$\log \left( \frac{P(x)}{Q(x)} \right)$:** логарифм делает меру аддитивной и преобразует отношение вероятностей в более удобную шкалу. Обычно используется натуральный логарифм (основание $e$), но также может использоваться логарифм по основанию 2 (в контексте теории информации, где единицей измерения является бит).
    *   **$P(x) \log \left( \frac{P(x)}{Q(x)} \right)$:** каждое значение логарифмического отношения взвешивается вероятностью $P(x)$. Это означает, что точки $x$, которые имеют высокую вероятность в распределении $P$, вносят больший вклад в общую KL-дивергенцию.
    *   **$\sum_{x}$ (или $\int_{-\infty}^{\infty}$):** суммирование (или интегрирование) по всем возможным значениям $x$ дает общую KL-дивергенцию между распределениями $P$ и $Q$.

3.  **Свойства KL-дивергенции:**
    *   **Неотрицательность:** $D_{KL}(P||Q) \ge 0$. KL-дивергенция всегда неотрицательна. Она равна нулю тогда и только тогда, когда $P = Q$ почти всюду;
    *   **Несимметричность:**  $D_{KL}(P||Q) \neq D_{KL}(Q||P)$ в общем случае. Это означает, что "расстояние" от $P$ до $Q$ не равно "расстоянию" от $Q$ до $P$. Важно понимать, какое распределение является "целевым" ($P$) и какое является "аппроксимирующим" ($Q$). В формуле $D_{KL}(P||Q)$, мы измеряем, насколько $Q$ отличается от $P$.

4.  **KL-дивергенция в RLHF:**
    *   **Регуляризация политики:** в RLHF, KL-дивергенция используется как штрафной член в целевой функции обучения с подкреплением. Цель состоит в том, чтобы обучить политику $\pi_{\theta}$, которая максимизирует награду, но при этом не слишком сильно отклоняется от эталонной политики $\pi_{ref}$.
    *   **Стабилизация обучения:** ограничение изменений политики с помощью KL-дивергенции помогает стабилизировать процесс обучения. Без этого ограничения, политика может резко меняться от итерации к итерации, что может привести к нестабильности и ухудшению производительности.
    *   **Предотвращение "policy drift":**  эталонная политика $\pi_{ref}$ часто представляет собой политику, обученную на начальном этапе или политику, которая демонстрирует желаемое поведение.  KL-регуляризация помогает предотвратить "дрейф" политики $\pi_{\theta}$ слишком далеко от $\pi_{ref}$, сохраняя таким образом важные характеристики исходной политики.
    *   **Баланс между исследованием и использованием:**  KL-штраф позволяет модели исследовать новые стратегии, но при этом удерживает ее в разумных пределах, не позволяя ей полностью забыть или игнорировать ранее изученное поведение, представленное эталонной политикой.

5.  **Применение в целевой функции RLHF:**  в типичной целевой функции RLHF, оптимизируемой с помощью алгоритмов, таких как PPO (Proximal Policy Optimization), KL-дивергенция добавляется как штрафной член:

    $J(\theta) = \mathbb{E}_{s \sim d_{\pi_{\theta}}, a \sim \pi_{\theta}} \left[ r(s, a) - \beta D_{KL}(\pi_{\theta}(. | s) || \pi_{ref}(. | s)) \right]$

    где:
    *   $J(\theta)$ - целевая функция, которую мы максимизируем;
    *   $r(s, a)$ - функция награды;
    *   $\beta$ - коэффициент регуляризации, определяющий силу KL-штрафа. Чем больше $\beta$, тем сильнее штраф за отклонение от эталонной политики;
    *   $D_{KL}(\pi_{\theta}(. | s) || \pi_{ref}(. | s))$ - KL-дивергенция между распределением действий текущей политики $\pi_{\theta}$ и эталонной политики $\pi_{ref}$ для состояния $s$.

В заключение, KL-дивергенция является мощным инструментом для регуляризации обучения в RLHF, обеспечивая баланс между оптимизацией награды и сохранением стабильности и предсказуемости поведения модели, путем контроля за отклонением новой политики от заданной эталонной политики.

</details>

### **2.4 Моделирование вознаграждения на основе правил**

Традиционные модели вознаграждения часто сталкиваются с проблемой взлома вознаграждения (reward hacking), когда модель манипулирует сигналом вознаграждения для получения высоких оценок вместо того, чтобы действительно улучшить способности к рассуждению. DAPO напрямую использует в качестве вознаграждения окончательную точность проверяемой задачи, избегая при этом сложности модели вознаграждения. В частности, функция вознаграждения выглядит следующим образом:

$$ R(\hat{y}, y) = \begin{cases} 1, & \text{если } \hat{y} \text{ эквивалентен } y \\ -1, & \text{иначе} \end{cases}, \quad (7) $$

Этот подход доказал свою эффективность в различных областях, включая автоматическое доказательство теорем, компьютерное программирование и математические соревнования.

> ИМХО: в данном случае моделирование вознаграждения работает только для детерменированных задач, где ответ однозначен. Для задач с неопределенными ответами (например, ответ LLM на вопрос, где в качестве ответа используется эвристика, а не строгое доказательство) это не сработает.

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
  <p style="margin: 0; font-weight: bold; color: #2c3e50;">Second Checkpoint:</p>
  <p style="margin: 8px 0 0 0; color: #2c3e50;">DAPO заменяет сложные модели вознаграждения на прямое использование итоговой точности задачи, что устраняет проблему взлома вознаграждения (reward hacking) и упрощает обучение.</p>
</div>

## **3. Алгоритм DAPO**

Исследователи предложили алгоритмы Decouple Clip и Dynamic Sampling Strategy Optimization (DAPO). DAPO делает выборку группы выходных данных ${o_i}_{i=1}^G$ для каждого вопроса $q$, связанного с ответом $a$, и оптимизирует политику через следующую целевую функцию:

$$J_{DAPO}(\theta) = \mathbb{E}_{(q,a) \sim \mathcal{D}, \{o_i\}_{i=1}^G \sim \pi_{\theta,\text{old}}(·|q)}$$

$$\left[ \frac{1}{\sum_{i=1}^G |o_i|} \sum_{i=1}^G \sum_{t=1}^{|o_i|} \min\left(r_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}\left(r_{i,t}(\theta), 1 - \epsilon_{\text{low}}, 1 + \epsilon_{\text{high}}\right)\hat{A}_{i,t}\right) \right], \quad (8)$$

$$\text{s.t.}\ 0 < |\{o_i | \text{is\_equivalent}(a, o_i)\}| < G,$$

где

$$r_{i,t}(\theta) = \frac{\pi_\theta(o_{i,t} | q, o_{i,<t})}{\pi_{\theta,\text{old}}(o_{i,t} | q, o_{i,<t})}, \quad \hat{A}_{i,t} = \frac{R_i - \text{mean}(\{R_i\}_{i=1}^G)}{\text{std}(\{R_i\}_{i=1}^G)}, \quad (9)$$

Ниже попробуем разобрать ключевые технологии, связанные с DAPO.

## **📌 3.1 Clip-Higher: повышение лимита**

В алгоритмах обучения с подкреплением (RL), таких как Proximal Policy Optimization (PPO) и Generalized Proximal Policy Optimization (GRPO), часто наблюдается явление коллапса энтропии, когда энтропия политики быстро уменьшается по мере обучения. Это приводит к тому, что генерируемые ответы становятся практически идентичными, что свидетельствует об ограниченном исследовании пространства возможных действий и преждевременной детерминированности стратегии. В данной работе предлагается стратегия Clip-Higher — модификация стандартного механизма отсечения в PPO, направленная на решение этой проблемы путем улучшения возможностей исследования для токенов с низкой вероятностью.

<details> 
    <summary><em><strong>Энтропия Шеннона:</strong></em></summary>

### Дополнение о концепции энтропии в алгоритмах обучения с подкреплением

В контексте представленного текста энтропия является фундаментальной метрикой, которая измеряет уровень неопределенности или случайности в политике агента. Математически энтропия политики π определяется как:

$$H(\pi(\cdot|s)) = -\sum_{a} \pi(a|s) \log \pi(a|s)$$

где:
- π(a|s) — вероятность выбора действия a в состоянии s согласно текущей политике;
- Суммирование идет по всем возможным следующим токенам a из словаря, а π(a|s) для каждого из них вычисляется описанным выше способом с помощью языковой модели;
- Таким образом, энтропия политики — это скалярное значение, которое суммирует характеристики вероятностного распределения π(a|s), но само по себе не является распределением.

Представленная формула описывает энтропию Шеннона для распределения вероятностей действий, заданного политикой π в конкретном состоянии s. Она измеряет степень неопределенности или "случайности" в выборе действия агентом в этом состоянии. Чем выше энтропия, тем более непредсказуем выбор агента, тем больше он "исследует". Чем ниже энтропия, тем более предсказуем (детерминирован) выбор, тем больше агент "использует" известные хорошие действия.

Формула вычисляет среднее количество информации, которое мы ожидаем получить, наблюдая за выбором действия агентом в состоянии s.

### Роль энтропии в обучении с подкреплением

Энтропия выполняет несколько критических функций в алгоритмах RL:

1. **Баланс между исследованием и использованием**: высокая энтропия означает более равномерное распределение вероятностей между различными действиями, что способствует исследованию (exploration) пространства действий. Низкая энтропия указывает на концентрацию вероятности на небольшом подмножестве действий, что соответствует использованию (exploitation) уже известных стратегий.

2. **Предотвращение преждевременной сходимости**: сохранение достаточного уровня энтропии помогает избежать застревания в локальных оптимумах, позволяя агенту продолжать исследовать потенциально лучшие стратегии.

3. **Разнообразие генерируемых ответов**: в контексте генеративных моделей, таких как языковые модели, обучаемые с помощью RL, высокая энтропия обеспечивает разнообразие в генерируемых ответах.

### Проблема коллапса энтропии

Когда энтропия политики быстро снижается во время обучения (коллапс энтропии), это приводит к следующим негативным последствиям:

1. **Детерминированность политики**: агент начинает выбирать одни и те же действия с очень высокой вероятностью, фактически превращая стохастическую политику в детерминированную.

2. **Сокращение пространства исследования**: токены с изначально низкой вероятностью практически исключаются из рассмотрения, что ограничивает разнообразие генерируемых последовательностей.

3. **Потеря способности к адаптации**: агент теряет способность адаптироваться к изменяющимся условиям, поскольку новые стратегии не получают достаточного представления в обучении.

</details>

### Проблема коллапса энтропии

В ходе первоначальных экспериментов с использованием стандартных реализаций PPO и GRPO было обнаружено, что энтропия политики быстро снижается по мере обучения, что можно наблюдать на рисунке 2b. Выборочные ответы для некоторых групп часто оказываются практически идентичными, что указывает на ограниченное исследование пространства возможных действий и раннюю детерминированность стратегии, потенциально препятствующую процессу расширения.

В основе этой проблемы лежит механизм отсечения коэффициента важности выборки, введенный в алгоритме PPO-Clip для ограничения области доверия и повышения стабильности обучения с подкреплением. Хотя этот механизм обеспечивает стабильность обучения, он также может ограничивать исследование пространства политики, особенно для токенов с низкой вероятностью.

![Figure_3](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_3.png)

### Асимметрия ограничения коэффициента важности

Стандартный механизм отсечения в PPO использует единый параметр ε (обычно установленный на 0.2) для ограничения изменения вероятностей как в большую, так и в меньшую сторону. Однако это создает асимметрию в возможностях изменения вероятностей для различных токенов.

Рассмотрим пример с двумя действиями, имеющими вероятности $\pi_{\text{data}}(o_i | q) = 0,01$ и $0,9$ соответственно в исходном распределении. При стандартном ограничении коэффициента важности с ε = 0.2, максимально возможные обновленные вероятности составят $\pi(o_i | q) = 0,012$ и $1,08$ соответственно. Это означает, что для токенов с высокой исходной вероятностью (например, 0.9) существует меньше ограничений на рост их вероятности, тогда как для токенов с низкой исходной вероятностью (например, 0.01) возможности значительного увеличения вероятности сильно ограничены.

Эмпирические наблюдения также подтверждают, что максимальная вероятность обрезки токена обычно составляет $\pi(o_i | q) < 0,2$, как показано на рисунке 3а. Это подтверждает теоретический анализ, согласно которому верхний порог отсечения ограничивает рост вероятности токенов с низкой исходной вероятностью, тем самым потенциально ограничивая разнообразие системы.

![Figure_4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_4.png)

### Стратегия Clip-Higher

Для решения вышеописанной проблемы предлагается стратегия Clip-Higher, основанная на разделении нижнего и верхнего диапазонов отсечения на ε_low и ε_high соответственно. Математически это выражается следующей формулой:

$$\left[ \frac{1}{\sum_{i=1}^G |o_i|} \sum_{i=1}^G \sum_{t=1}^{|o_i|} \min\left(r_{i,t}(\theta)\hat{A}{i,t}, \text{clip}\left(r{i,t}(\theta), 1 - \textcolor{red}{\epsilon_{\text{low}}}, 1 + \textcolor{red}{\epsilon_{\text{high}}}\right)\hat{A}_{i,t}\right) \right], \quad (10)$$

где $r_{i,t}(θ)$ представляет отношение вероятностей новой политики к базовой политике, а $Â_{i,t}$ — оценка преимущества.

В отличие от стандартного подхода PPO, где ε_low = ε_high = 0.2, стратегия Clip-Higher использует различные значения для этих параметров: ε_low остается равным 0.2, а ε_high увеличивается до 0.28. Это увеличение верхнего порога отсечения оставляет больше пространства для роста вероятности токенов с низкой исходной вероятностью, тем самым стимулируя исследование "токенов с длинным хвостом".

### Экспериментальные результаты

Как показано на графиках выше, предложенная корректировка механизма отсечения эффективно улучшает энтропию стратегии и способствует созданию более разнообразных выборок. Исследователи сознательно решили сохранить ε_low относительно небольшим (0.2), поскольку увеличение этого параметра может привести к чрезмерному снижению вероятности некоторых токенов, что в конечном итоге может вызвать коллапс пространства выборки.

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
  <p style="margin: 0; font-weight: bold; color: #2c3e50;">Third Checkpoint: Clip-Higher</p>
  <p style="margin: 8px 0 0 0; color: #2c3e50;">Стратегия Clip-Higher борется с коллапсом энтропии в PPO/GRPO, вводя асимметричные пороги отсечения (ε_low < ε_high). Увеличение верхнего порога (ε_high) стимулирует исследование токенов с низкой вероятностью, повышая разнообразие генерируемых ответов.</p>
</div>

## **📌 3.2 Dynamic Sampling: повышение эффективности градиентного обучения**

### Проблема снижения градиента

Существующие алгоритмы обучения с подкреплением (RL) часто страдают от проблемы снижения градиента, которая возникает, когда точность некоторых подсказок достигает значения 1. Например, в алгоритме GRPO, если все выходные данные для конкретной подсказки верны и получают одинаковое вознаграждение 1, результирующее преимущество группы становится равным нулю. Это ведет к обновлению политики без градиентов, что существенно снижает эффективность выборки.

Эмпирические наблюдения показывают (Figure3.b - график чуть выще был), что количество образцов с точностью, равной 1, продолжает увеличиваться в процессе обучения. В результате количество действительных сигналов в каждой партии уменьшается, что приводит к:

- Увеличению дисперсии градиента;
- Ослаблению градиентного сигнала для обучения модели.

### Решение: метод динамической выборки

Для решения этой проблемы предлагается метод динамической выборки. Основная идея заключается в следующем:

1. Выполнение избыточной выборки подсказок;
2. Фильтрация подсказок с точностью, равной 1 или 0;
3. Сохранение только подсказок с допустимыми градиентами в пакете;
4. Поддержание постоянного количества подсказок в обучающем пакете.

Процесс выборки продолжается до тех пор, пока пакет не будет полностью заполнен примерами, точность которых строго находится в диапазоне (0,1).

### Математическая формализация

Опять же, наша целевая функция DAPO, тут нас интересует ограничение, выделенное красным:

$$J_{DAPO}(\theta) = \mathbb{E}_{(q,a) \sim \mathcal{D}, \{o_i\}_{i=1}^G \sim \pi_{\theta,\text{old}}(·|q)}$$

$$\left[ \frac{1}{\sum_{i=1}^G |o_i|} \sum_{i=1}^G \sum_{t=1}^{|o_i|} \min\left(r_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}\left(r_{i,t}(\theta), 1 - \epsilon_{\text{low}}, 1 + \epsilon_{\text{high}}\right)\hat{A}_{i,t}\right) \right], \quad (11)$$

с ограничением: $$\text{s.t.}\textcolor{red}{\ 0 < |\{o_i | \text{is\_equivalent}(a, o_i)\}| < G},$$

Это ограничение гарантирует, что в пакете содержатся только подсказки с точностью в диапазоне между 0 и 1. В случае динамической выборки эксперимент может достичь той же производительности быстрее. Наблюдение показано на рисунке 6.

![Figure_5](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_5.png)

Метод динамической выборки представляет собой эффективное решение проблемы снижения градиента в алгоритмах обучения с подкреплением. Путем целенаправленной фильтрации подсказок с крайними значениями точности (0 или 1) и сосредоточения вычислительных ресурсов на подсказках с промежуточной точностью, данный метод позволяет существенно повысить эффективность обучения и ускорить достижение конвергенции модели.

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
  <p style="margin: 0; font-weight: bold; color: #2c3e50;">Fourth Checkpoint: Dynamic Sampling</p>
  <p style="margin: 8px 0 0 0; color: #2c3e50;">Метод динамической выборки решает проблему снижения градиента в RL-алгоритмах, исключая подсказки с точностью 0 или 1. Фильтрация примеров с промежуточной точностью (0 < acc < 1) позволяет сохранять значимые градиенты в пакете, уменьшает их дисперсию и усиливает сигнал для обучения. Поддержание постоянного размера пакета с «полезными» примерами ускоряет конвергенцию модели.</p>
</div>

## **📌 3.3 Token-Level Policy Gradient Loss: действие по перебалансировке**

Исходный алгоритм GRPO использует расчет потерь на уровне выборки, который включает в себя сначала усреднение потерь по токенам в каждой выборке, а затем агрегирование потерь по всем выборкам. При таком подходе каждому образцу присваивается одинаковый вес при окончательном расчете потерь. Однако авторы обнаружили, что этот подход к сокращению потерь создает ряд проблем в сценариях RL с длинной цепочкой мысли.

<details> 
    <summary><em><strong>Как исходный алгоритм GRPO использует расчет потерь на уровне выборки:</strong></em></summary>

### **Как исходный алгоритм GRPO использует расчет потерь на уровне выборки**

#### 1. **Основной принцип расчета потерь**

В GRPO потери вычисляются на уровне **выборок**, то есть для каждой генерации (или ответа) $ o_i $ в группе $ G $, а затем усредняются по всем выборкам. Этот подход можно разбить на несколько шагов:

#### Шаг 1: Расчет потерь для отдельных токенов

Для каждого ответа $o_i$ в группе $G$ и каждого токена $t$ в ответе вычисляется:

$$L_{i,t} = \min\left(r_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}\left(r_{i,t}(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_{i,t}\right)$$

где:
- $r_{i,t}(\theta) = \frac{\pi_\theta(t|q, o_{i,<t})}{\pi_{\text{old}}(t|q, o_{i,<t})}$ - отношение вероятностей между текущей и старой политиками
- $\hat{A}_{i,t}$ - оценка преимущества для токена $t$ в ответе $o_i$
- $\epsilon$ - параметр клиппирования для ограничения обновления политики

#### Шаг 2: Усреднение потерь по токенам для каждого ответа

Потери для каждого ответа $o_i$ усредняются по всем токенам:

$$L_i = \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} L_{i,t}$$

где $|o_i|$ - длина ответа $o_i$ в токенах.

#### Шаг 3: Агрегирование потерь по всем выборкам

Окончательная функция потерь:

$$L_{\text{GRPO}}(\theta) = \frac{1}{G} \sum_{i=1}^G L_i = \frac{1}{G} \sum_{i=1}^G \left[ \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} L_{i,t} \right]$$

#### 2. **Почему используется такой подход?**
- **Упрощение расчетов**: усреднение потерь по токенам внутри одного ответа позволяет игнорировать различия в длинах ответов на этапе агрегации. Это делает алгоритм более простым в реализации.
- **Равномерное влияние выборок**: каждый ответ $ o_i $ вносит одинаковый вклад в общие потери, независимо от его длины. Это обеспечивает равномерное распределение весов между выборками.

#### 3. **Проблемы такого подхода**
Однако этот метод имеет ряд ограничений, особенно в задачах с длинными цепочками рассуждений:

- **Непропорциональный вклад длинных ответов**:  
  В длинных ответах каждый токен оказывает меньшее влияние на общие потери, так как потери усредняются по большому количеству токенов. Это может привести к тому, что модель недоучивает важные паттерны в длинных последовательностях.

- **Некачественные длинные ответы**:  
  Длинные ответы часто содержат "шум" (например, повторяющиеся или бессмысленные фрагменты). Поскольку такие ответы усредняются на уровне выборки, модель может неэффективно подавлять эти нежелательные паттерны.

- **Искажение обучения**:  
  Равномерное взвешивание выборок может привести к тому, что модель будет предпочитать короткие ответы, так как они легче оптимизируются.

---

### **Как это работает на практике**
Рассмотрим пример:

1. **Генерация группы ответов**:  
   Для промпта $ P $ генерируется группа из $ G = 4 $ ответов:
   - $ o_1 $: "Краткий ответ" (3 токена).
   - $ o_2 $: "Более длинный ответ с деталями" (10 токенов).
   - $ o_3 $: "Еще более длинный ответ с повторами" (20 токенов).
   - $ o_4 $: "Самый длинный ответ с шумом" (30 токенов).

2. **Вычисление потерь для каждого ответа**:  
   - Для $ o_1 $: Потери усредняются по 3 токенам.
   - Для $ o_2 $: Потери усредняются по 10 токенам.
   - Для $ o_3 $: Потери усредняются по 20 токенам.
   - Для $ o_4 $: Потери усредняются по 30 токенам.

3. **Агрегирование потерь**:  
   Все ответы вносят одинаковый вклад в общие потери, даже если их длины сильно различаются.

---

### **Заключение**
Расчет потерь на уровне выборки в GRPO обеспечивает простоту и равномерность в обучении, но имеет ограничения, связанные с обработкой длинных последовательностей. Эти ограничения побудили авторов ввести альтернативный подход — **расчет потерь на уровне токенов**, который устраняет указанные проблемы, позволяя каждому токену оказывать пропорциональное влияние на обновление градиента.

</details>

---

Поскольку при расчете потерь всем образцам присваивается одинаковый вес, токены в более длинных ответах (содержащих больше токенов) могут вносить непропорционально меньший вклад в общие потери, что может привести к двум нежелательным эффектам. Во-первых, для высококачественных длинных выборок этот эффект может помешать модели изучить закономерности, связанные с рассуждениями. Во-вторых, слишком длинные образцы часто демонстрируют некачественные паттерны, такие как бессмысленная тарабарщина и повторяющиеся слова. Таким образом, расчет потерь на уровне выборки не может эффективно устранить эти плохие закономерности в длинных выборках, что приводит к нездоровому увеличению энтропии и длины отклика, как показано на рисунках 4а и 4б.

![Figure_6](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_6.png)

Авторы вводят градиентную потерю политики на уровне токенов в сценарий долгосрочной цепочки мышления (RL) для решения вышеуказанных ограничений:

$$J_{DAPO}(\theta) = \mathbb{E}_{(q,a) \sim \mathcal{D}, \{o_i\}_{i=1}^G \sim \pi_{\theta,\text{old}}(·|q)}$$

$$\left[ \frac{1}{\textcolor{red}{\sum_{i=1}^G |o_i|}} \textcolor{red}{\sum_{i=1}^G \sum_{t=1}^{|o_i|}} \min\left(r_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}\left(r_{i,t}(\theta), 1 - \epsilon_{\text{low}}, 1 + \epsilon_{\text{high}}\right)\hat{A}_{i,t}\right) \right], \quad (12)$$

с ограничением: $$\text{s.t.}\ 0 < |\{o_i | \text{is\_equivalent}(a, o_i)\}| < G,$$

Ключевые отличия:
1. Нормализация производится по общему количеству токенов во всех ответах: $\sum_{i=1}^G |o_i|$
2. Токены агрегируются напрямую, без предварительного усреднения по выборкам

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
  <p style="margin: 0; font-weight: bold; color: #2c3e50;">Fifth Checkpoint: Token-level Policy Gradient Loss</p>
  <p style="margin: 8px 0 0 0; color: #2c3e50;">Стандартный расчет потерь GRPO на уровне сэмпла ослабляет градиенты от токенов в длинных ответах long-CoT, ухудшая обучение и подавление ошибок. Предложенный расчет потерь на уровне токенов решает эту проблему путем прямой агрегации и нормализации по всем токенам батча, обеспечивая каждому токену пропорциональный вклад в градиент.</p>
</div>

## **📌 3.4 Overlong Reward Shaping: супердлинное формирование вознаграждения**
 
При обучении с подкреплением обычно устанавливается фиксированная максимальная длина генерации и соответствующим образом обрезаются очень длинные образцы. Авторы обнаружили, что неправильное формирование вознаграждения для усеченных выборок может привести к появлению шума вознаграждения и существенно нарушить процесс обучения.

По умолчанию назначается штрафное вознаграждение за усеченные образцы. Такой подход может внести шум в процесс обучения, поскольку разумный процесс вывода может быть наказан просто за то, что он слишком длинный. Этот штраф может запутать модель относительно эффективности ее процесса рассуждения.

Чтобы изучить влияние этого шума вознаграждения, исследователи сначала применили очень длинную стратегию фильтрации, чтобы замаскировать потерю усеченных выборок. Было обнаружено, что такой подход значительно стабилизировал обучение и улучшил результаты, как показано на рисунке 5.

![Figure_7](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_7.png)

Кроме того, исследователи предложили мягкий штраф за превышение длины (Формула 13) — механизм штрафа, учитывающий длину, для усеченных выборок. В частности, определяется штрафной интервал, когда длина ответа превышает предопределенное максимальное значение. В пределах этого интервала, чем длиннее ответ, тем больше штраф. Этот штраф добавляется к изначальной награде за правильность, основанной на правилах, сигнализируя модели о необходимости избегать слишком длинных ответов.

![Figure_8](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_8.png)

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
<p style="margin: 0; font-weight: bold; color: #2c3e50;">Sixth Checkpoint: Overlong Reward Shaping</p>
<p style="margin: 8px 0 0 0; color: #2c3e50;">Традиционное бинарное пенальти за превышение длины ответа вносит шум в обучение, наказывая даже частично корректные длинные решения. Предложенный подход Overlong Reward Shaping заменяет жесткий штраф на постепенную линейную функцию в интервале 16-20К токенов, снижая уровень шума и позволяя модели эффективно учиться на длинных последовательностях без резкого отбрасывания данных.</p>
</div>

## 4 Эксперименты

### 4.1 Подробности обучения

Исследователи сосредоточили свое внимание на математических задачах для оценки разработанного алгоритма, который может быть легко адаптирован для других задач с четкими и точными сигналами вознаграждения. Для обучения был использован фреймворк verl с GRPO в качестве базового алгоритма. Преимущество оценивалось с помощью нормализации группового вознаграждения.

В работе были применены следующие гиперпараметры: оптимизатор AdamW с постоянной скоростью обучения $1 \times 10^{-6}$ и линейным разогревом в 20 шагов. Размер пакета подсказок составил 512, с выбором 16 ответов на каждую подсказку. Для сверхдлинного формирования вознаграждения была установлена ожидаемая максимальная длина в 16 384 токенов с дополнительным мягким штрафным буфером в 4906 токенов. Параметры обрезки $c_{\text{low}}$ и $c_{\text{high}}$ были установлены на уровне 0,2 и 0,28 соответственно.

![Figure_9](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_9.png)

### 4.2 Основные результаты

В экспериментах на AIME 2024 метод DAPO успешно обучил базовую модель Qwen-32B, превратив её в мощную модель вывода, превзойдя результаты DeepSeek с использованием метода R1 на Qwen2.5-32B. Было показано значительное улучшение производительности AIME 2024: точность увеличилась с почти 0% до 50% при использовании только 50% шагов обучения, необходимых для DeepSeek-R1-Zero-Qwen-32B.

Исследователи проанализировали вклад каждой методики обучения в их подход. Улучшения демонстрируют эффективность этих методик в обучении с подкреплением. В условиях наивной настройки GRPO обучение на основе базовой модели Qwen.2-5-32B достигло только 30% точности.

![Figure_10](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_10.png)

### 4.3 Динамика тренировки

Процесс обучения DAPO продемонстрировал сложность RL в больших языковых моделях. Исследователи обеспечили стабильность обучения, отслеживая ключевые показатели. Эксперименты показали, что DAPO не только улучшает способность модели к рассуждению, но и усиливает её исследовательские способности.

![Figure_11](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_11.png)

### 4.4 Анализ случая

В ходе обучения с подкреплением модель DAPO продемонстрировала динамически развивающуюся модель рассуждений. По мере обучения модель не только укрепляла существующие шаблоны рассуждений, но и постепенно формировала новые модели поведения.

![Figure_12](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_12.png)

## 5. Заключение

Запуск системы DAPO стал крупным прорывом в области крупномасштабного обучения языковым моделям с подкреплением. Благодаря открытому исходному коду алгоритмов, кодов и наборов данных, система получила отличную оценку 50 на AIME и предоставила ценные ресурсы для будущих исследований.

Четыре основные технологии DAPO — Clip-Higher, Dynamic Sampling, Token-Level Policy Gradient Loss и Overlong Reward Shaping — предлагают новые решения для обучения с подкреплением сложным задачам рассуждения. Выпуск DAPO с открытым исходным кодом позволил мировому исследовательскому сообществу лучше понимать и применять методы обучения с подкреплением для крупномасштабных языковых моделей.

Наконец, позвольте мне добавить некоторые ограничения, которые пришли мне в голову:

- С точки зрения итоговой производительности 50%-ная точность AIME все еще отстает от 72,6% DeepSeek-R1-Distill-Qwen-32B.
- Эффективность этого метода была проверена только на одном обучающем наборе, одном тестовом наборе и одной модели, и его обобщение сомнительно.
- С другой стороны, даже если DAPO обладает средней степенью обобщения, мы можем представить четыре приема, описанные в этой статье, как набор инструментов, из которого мы можем брать различные инструменты для конкретных сценариев, а не использовать весь DAPO как черный ящик. Фактически, из четырех приемов три предназначены для формирования вознаграждения, которое используется для поощрения исследовательской деятельности, лучшей обработки длинных ответов и лучшей обработки штрафа за длину, а оставшийся — для повышения эффективности выборки. Видно, что между ними нет зависимости, и любое подмножество рационально
