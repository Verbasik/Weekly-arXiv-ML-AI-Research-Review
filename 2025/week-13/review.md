# **DAPO: революционный RL-алгоритм от ByteDance**

## **Аннотация**

В данной работе представлена система **DAPO (Decoupled Clip and Dynamic sAmpling Policy Optimization)**, представляющая собой открытую платформу для обучения больших языковых моделей (LLM) с использованием методов обучения с подкреплением (Reinforcement Learning, RL). Несмотря на значительные успехи современных LLM, таких как OpenAI o1 и DeepSeek R1, ключевые технические детали их RL-обучения остаются недоступными для научного сообщества, что существенно затрудняет воспроизводимость результатов и дальнейшие исследования. В ответ на эту проблему авторы предлагают инновационный алгоритм DAPO, который не только демонстрирует высокую эффективность, но и предоставляет полную открытость кода, данных и методологии.

Система DAPO достигла рекордного результата в 50 баллов на математическом конкурсе **AIME 2024**, превзойдя предыдущий рекорд модели DeepSeek-R1 (47 баллов). При этом DAPO добилась такого результата, сократив количество шагов обучения вдвое. В основе алгоритма лежат четыре ключевые технологии: **стратегия Clip-Higher**, **динамическая выборка**, **оптимизация градиента на уровне токенов** и **интеллектуальный штраф длины**. Эти методы направлены на решение основных проблем RL-обучения, таких как коллапс энтропии, шум вознаграждения и неэффективность обучения на длинных текстах.

Авторы подчеркивают, что масштабное обучение с подкреплением является критически важным для развития способности LLM к сложным рассуждениям. Однако, в отличие от предыдущих работ, где детали RL-обучения оставались скрытыми (например, в блогах OpenAI и технических отчетах DeepSeek R1), DAPO предоставляет полную прозрачность. В открытый доступ выложены не только исходные коды обучения, разработанные на базе фреймворка **verl**, но и тщательно подготовленные датасеты. Это способствует повышению воспроизводимости результатов и открывает новые возможности для исследований в области крупномасштабного RL-обучения LLM.

Таким образом, работа представляет собой значительный вклад в развитие открытых и воспроизводимых методов обучения больших языковых моделей, предлагая как теоретические инновации, так и практические инструменты для научного сообщества.

## **1. Введение**

Появление моделей с расширенным временем тестирования и рассуждения (Test-time Compute), таких как O1 от OpenAI и R1 от DeepSeek, а так же совсем недавно Claude от Antropic, ознаменовало фундаментальный сдвиг парадигмы в области больших языковых моделей (LLM) на основе обучения с подкреплением (Reinforcement Learning, RL). Эти модели продемонстрировали беспрецедентные способности к комплексным рассуждениям, позволяющие им успешно решать сложные математические и программистские задачи уровня соревнований AIME и Codeforces.

Центральной технологией, обеспечивающей этот прорыв, выступает масштабное обучение с подкреплением (Reinforcement Learning, RL), которое стимулирует развитие сложных форм рассуждения, включая самопроверку, итеративное уточнение и рефлексию. Несмотря на впечатляющие результаты, конкретные алгоритмы и методологические подходы к масштабируемому RL-обучению остаются в значительной степени скрытыми в технических отчетах существующих моделей. Как отмечают авторы, "ключевые технические детали современных рассуждающих LLM скрыты (например, в блоге OpenAI о модели o1 и техническом отчете DeepSeek R1)", из-за чего исследовательское сообщество испытывает трудности с воспроизведением их результатов.

В ходе экспериментов авторы использовали Qwen2.5-32B в качестве предварительно обученной модели для применения обучения с подкреплением на основе обратной связи. При первоначальных запусках с использованием базового алгоритма GRPO (Generalized Reward-weighted Policy Optimization) модель достигла лишь 30 баллов на тесте AIME, что значительно уступает 47 баллам модели DeepSeek-RL. Углубленный анализ выявил, что наивная имплементация GRPO сталкивается с рядом критических проблем, среди которых:

1. **Коллапс энтропии** — тенденция модели к сужению разнообразия генерируемых ответов;
2. **Шум вознаграждения** — некорректное присвоение наград за частично правильные или слишком длинные ответы;
3. **Нестабильность обучения** — трудности при масштабировании процесса обучения на длинных цепочках рассуждений.

Аналогичные проблемы отмечались в более широком сообществе при попытках воспроизвести результаты DeepSeek, что указывает на возможное отсутствие ключевых деталей обучения в опубликованной статье о R1 — деталей, критически важных для разработки масштабируемых и воспроизводимых систем RL-обучения промышленного уровня.

Для преодоления этого разрыва авторы предоставляют современную систему масштабного RL-обучения LLM с открытым исходным кодом, которая достигает 50 баллов на AIME 2024 на базе модели Qwen2.5-32B. Данный результат превосходит предыдущий рекорд DeepSeek-RL-Zero-Qwen-32B (47 баллов), при этом требуя лишь 50% объема тренировочных шагов (см. рисунок 1). В основе системы лежит предложенный авторами алгоритм Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO), включающий четыре ключевые инновации, которые принципиально улучшают эффективность RL-обучения в сценариях длинных цепочек рассуждений:

![Figure_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_1.png)

- **Clip-Higher** — стратегия, способствующая разнообразию генерации и позволяющая проводить адаптивную выборку, разделяя нижние и верхние границы клиппирования (ε-low и ε-high);
- **Dynamic Sampling** — метод динамической выборки, повышающий эффективность и стабильность обучения путем исключения примеров с нулевым градиентом;
- **Token-Level Policy Gradient Loss** — расчет градиента политики на уровне отдельных токенов, что критически важно для эффективного обучения на длинных последовательностях;
- **Overlong Reward Shaping** — интеллектуальная система штрафов за превышение длины, снижающая шум вознаграждения и стабилизирующая процесс обучения.

Особую ценность работы составляет полная открытость всех аспектов системы: весь код реализации, построенный на фреймворке verl, вместе с тщательно подготовленным датасетом DAPO-Math-17K, доступны в открытом репозитории. Эта открытость контрастирует с предыдущими работами, которые, несмотря на впечатляющие результаты, не раскрывали критически важных деталей обучения.

В ходе экспериментов авторы также наблюдали интересное явление: модель не только усиливает существующие шаблоны рассуждения, но и постепенно развивает принципиально новые способности, в частности, поведение, связанное с самопроверкой и переосмыслением предыдущих шагов. Это открывает новые перспективы для понимания фундаментальных механизмов обучения LLM сложным формам рассуждения.

---

<details> 
    <summary><em><strong>Сущность Test-time Compute</strong></em></summary>

### Сущность Test-time Compute

"**Test-time compute**" (вычисления во время тестирования/инференса) представляет собой парадигму масштабирования RL LLM, которая акцентирует внимание на увеличении вычислительных ресурсов, доступных модели непосредственно в момент обработки пользовательского запроса (inference time).  В отличие от традиционного подхода, "Test-time compute" позволяет улучшить производительность уже обученной модели, предоставляя ей больше **ВРЕМЕНИ** и **ВЫЧИСЛИТЕЛЬНОЙ МОЩНОСТИ** для "размышления" над каждым конкретным запросом.

### Отличие от традиционного масштабирования

Традиционное масштабирование LLM фокусировалось на следующих аспектах **во время обучения**:

* **Размер модели:** увеличение количества параметров и сложности архитектуры;
* **Объем данных:** расширение и разнообразие обучающих данных;
* **Вычислительные ресурсы для обучения:** использование более мощных GPU и увеличение времени обучения.

"Test-time compute" вводит **дополнительное измерение масштабирования**, применяемое **после обучения модели**.  Это позволяет повысить эффективность модели, не изменяя ее архитектуру или параметры, а оптимизируя вычислительные ресурсы в момент инференса.

### Механизм и преимущества Test-time Compute

Предоставление модели больше вычислительных ресурсов во время инференса позволяет:

* **Углубленная обработка запросов:**  модель может проводить более детальный анализ входного текста и контекста, на основе более глубоких цепочек рассуждения;
* **Улучшение рассуждений:**  дополнительные вычисления способствуют более эффективному планированию, поиску оптимальных решений и генерации логически обоснованных ответов;
* **Использование сложных алгоритмов инференса:**  возможность применения ресурсоемких, но более качественных методов декодирования и генерации.

### Как итог

"Test-time compute" знаменует собой важный сдвиг в подходах к масштабированию LLM.  Он дополняет традиционные методы, сосредотачиваясь на оптимизации вычислительных ресурсов в момент использования модели.  Это открывает перспективы для создания более интеллектуальных и reasoning-ориентированных языковых моделей, особенно в задачах, требующих глубокого анализа и логического вывода. 

</details> 

---

Далее мы шаг за шагом рассмотрим переход от PPO к GRPO, а затем к DAPO, чтобы увидеть, как был разработан этот новый алгоритм обучения с подкреплением.

## **2. Препдпосылки**

### **2.1 Проксимальная оптимизация политики (PPO)**

#### Основная концепция

Проксимальная оптимизация политики (PPO) — это один из наиболее популярных и эффективных алгоритмов обучения с подкреплением, разработанный исследователями из OpenAI в 2017 году. PPO представляет собой усовершенствование предыдущих методов оптимизации политики, таких как TRPO (Trust Region Policy Optimization), но с более простой реализацией и сравнимой или лучшей производительностью.

#### Ключевые особенности PPO

1. **Стабильность обучения**: основная идея PPO заключается в ограничении изменений политики между итерациями, что предотвращает слишком резкие обновления, которые могут дестабилизировать процесс обучения;

2. **Механизм отсечения**: PPO использует функцию отсечения для ограничения коэффициента важности выборки, что гарантирует, что новая политика не будет сильно отклоняться от старой;

3. **Эффективность выборки**: по сравнению с другими алгоритмами, PPO более эффективно использует собранные данные, что позволяет достигать лучших результатов при меньшем количестве взаимодействий со средой.

#### Математическая формализация

Целевая функция PPO определяется следующим образом:

$$\mathcal{J}_{\text{PPO}}(\theta) = \mathbb{E}_{(q,a) \sim \mathcal{D}, o_{<t} \sim \pi_{\theta_{\text{old}}}(\cdot|q)} \left[ \min \left( \frac{\pi_\theta(o_t | q, o_{<t})}{\pi_{\theta_{\text{old}}}(o_t | q, o_{<t})} \hat{A}_t, \text{clip}\left(\frac{\pi_\theta(o_t | q, o_{<t})}{\pi_{\theta_{\text{old}}}(o_t | q, o_{<t})}, 1 - \varepsilon, 1 + \varepsilon \right) \hat{A}_t \right) \right], \quad (1)$$

где:

- $\mathcal{J}_{\text{PPO}}(\theta)$ — целевая функция PPO, которую необходимо максимизировать путем подбора параметров $\theta$;
- $(q, a)$ — пара вопрос-ответ из распределения данных $\mathcal{D}_e$, где $q$ представляет запрос (или состояние среды), а $a$ — соответствующий ответ (или действие);
- $o_t$ — текущий токен вывода на шаге $t$;
- $o_{<t}$ — последовательность предыдущих токенов вывода до шага $t$;
- $\pi_\theta(o_t | q, o_{<t})$ — вероятность выбора токена $o_t$ при текущей политике с параметрами $\theta$, учитывая запрос $q$ и предыдущие токены $o_{<t}$;
- $\pi_{\theta_{\text{old}}}(o_t | q, o_{<t})$ — вероятность выбора того же токена при старой политике с параметрами $\theta_{\text{old}}$;
- $\frac{\pi_\theta(o_t | q, o_{<t})}{\pi_{\theta_{\text{old}}}(o_t | q, o_{<t})}$ — коэффициент важности выборки, показывающий отношение вероятностей выбора токена при новой и старой политиках;
- $\varepsilon$ — гиперпараметр отсечения (обычно в диапазоне 0.1-0.3), определяющий допустимый диапазон изменения коэффициента важности выборки;
- $\text{clip}(x, a, b)$ — функция отсечения, ограничивающая значение $x$ в диапазоне $[a, b]$;
- $\hat{A}_t$ — оценка преимущества на временном шаге $t$, характеризующая относительную ценность выбранного действия по сравнению со средним значением всех действий в данном состоянии.

Функция $\mathbb{E}$ обозначает математическое ожидание, и все выражение представляет собой ожидаемую ценность выбранных действий с учетом ограничений на изменение политики.

Учитывая функцию ценности $V$ и функцию вознаграждения $R$, $\hat{A}_t$ рассчитывается с использованием обобщенной оценки преимущества (GAE):

$$\hat{A}_t^{GAE(\gamma,\lambda)} = \sum_{l=0}^{\infty}(\gamma\lambda)^l\delta_{t+l} \quad (2)$$

где:

- $\hat{A}_t^{GAE(\gamma,\lambda)}$ — обобщенная оценка преимущества для временного шага $t$;
- $\gamma$ — коэффициент дисконтирования будущих вознаграждений (обычно близкий к 1, например, 0.99), определяющий, насколько агент ценит будущие вознаграждения по сравнению с немедленными;
- $\lambda$ — параметр экспоненциального взвешивания для различных оценок преимущества (обычно между 0.9 и 1.0), контролирующий компромисс между смещением и дисперсией в оценке;
- $\delta_t$ — временная разница между ожидаемой и фактической ценностью, рассчитываемая по формуле (3);
- $l$ — индекс суммирования, представляющий количество шагов вперед от текущего временного шага $t$.

$$\delta_t = R_t + \gamma V(s_{t+1}) - V(s_t), \quad 0 \leq \gamma, \lambda \leq 1 \quad (3)$$

где:

- $\delta_t$ — временная разница на шаге $t$;
- $R_t$ — немедленное вознаграждение, полученное после выполнения действия на шаге $t$;
- $V(s_t)$ — оценка ценности состояния $s_t$ согласно текущей функции ценности;
- $V(s_{t+1})$ — оценка ценности следующего состояния $s_{t+1}$.

**Обобoбщим: что "под капотом" без математических деталей:**

* **Политика (Policy):**  это как "инструкция" для программы, говорящая, какое действие нужно выбрать в каждой ситуации. В PPO политика представлена нейронной сетью.
* **Функция ценности (Value Function):** это оценка того, насколько "хорошо" находиться в определенной ситуации.  Она помогает программе понимать, к каким ситуациям стоит стремиться.  Тоже обычно нейронная сеть.
* **Преимущество (Advantage):**  это разница между тем, насколько хорошо действие, которое программа выбрала, и насколько "в среднем" хорошо действовать в этой ситуации.  Помогает понять, какие действия были действительно удачными.
* **Целевая функция PPO (формула (1)):**  это главная "цель" обучения.  Программа пытается ее максимизировать, чтобы стать лучше.  Самое важное в ней - это механизм "отсечения" (clip), который ограничивает изменения политики.
* **Оценка преимущества GAE (формула (2)):**  способ посчитать, насколько хороши были действия программы, учитывая не только немедленный результат, но и будущие последствия.
* **Временная разница (формула (3)):**  помогает функции ценности учиться, сравнивая предсказанную ценность ситуации с тем, что реально произошло.

#### Процесс работы PPO

1. **Сбор опыта**: агенты взаимодействуют со средой, следуя текущей политике $\pi_{\theta_{\text{old}}}$, и собирают траектории (последовательности состояний, действий, вознаграждений).

2. **Оценка преимущества**: для каждого временного шага в собранных траекториях рассчитывается оценка преимущества $\hat{A}_t$ с использованием формул (2) и (3).

3. **Оптимизация политики**: параметры политики $\theta$ обновляются путем оптимизации целевой функции (1) с использованием стохастического градиентного восхождения в течение нескольких эпох.

4. **Обновление функции ценности**: параметры функции ценности обновляются путем минимизации среднеквадратичной ошибки между предсказанными и фактическими ценностями состояний.

5. **Итерация**: шаги 1-4 повторяются до достижения желаемой производительности или заданного числа итераций.

#### Преимущества PPO

1. **Простота реализации**: PPO проще в реализации, чем TRPO и другие сложные алгоритмы обучения с подкреплением.

2. **Высокая производительность**: PPO показывает отличные результаты во многих задачах, от игр Atari до сложных задач робототехники.

3. **Совместимость с нейронными сетями**: PPO прекрасно работает с глубокими нейронными сетями и может использоваться для обучения сложных политик.

4. **Хорошая масштабируемость**: PPO можно эффективно распараллеливать для ускорения обучения.

#### Применение PPO

PPO широко используется в различных областях:

1. **Игры**: От простых игр до сложных стратегий и симуляторов.

2. **Робототехника**: Обучение управлению роботами для выполнения сложных физических задач.

3. **Системы рекомендаций**: Оптимизация стратегий рекомендаций в интерактивных системах.

4. **Большие языковые модели**: В последние годы PPO активно применяется для настройки языковых моделей через обучение с подкреплением на основе обратной связи от человека (RLHF).

#### Заключение

Проксимальная оптимизация политики (PPO) представляет собой мощный и гибкий алгоритм обучения с подкреплением, который благодаря своей простоте и эффективности стал стандартом де-факто в области глубокого обучения с подкреплением. Его применение простирается от игр и робототехники до обучения современных языковых моделей, демонстрируя универсальность и мощь данного подхода.

### **2.2 Оптимизация групповой относительной политики (GRPO)**

Group Relative Policy Optimization (GRPO) — это алгоритм обучения с подкреплением, предназначенный для оптимизации LLM в задачах, требующих структурированного рассуждения, таких как математика и логика. Он был представлен в работах DeepSeekMath и DeepSeek-R1 **как ответ на вызовы обучения моделей с миллиардами параметров**. GRPO предлагает более эффективный подход по сравнению с традиционными методами, такими как Proximal Policy Optimization (PPO), **за счет устранения ключевых узких мест, связанных с вычислением advantage-функций**.


<details> 
    <summary><em><strong>Объяснение Advantage-функций</strong></em></summary>

**Advantage-функция** — это ключевое понятие в обучении с подкреплением (Reinforcement Learning, RL), которое **количественно оценивает преимущество выбора конкретного действия `a` в состоянии `s` по сравнению со средним действием, предписанным текущей политикой модели**. Формально она выражается как разница между **Q-функцией** (ожидаемая суммарная награда за действие `a` в состоянии `s`) и **V-функцией** (средняя ожидаемая награда в состоянии `s` при текущей политике):

$$
A(s, a) = Q(s, a) - V(s)
$$

---

### **Зачем нужна Advantage-функция?**
1. **Оценка относительной ценности действий**:
   - Помогает модели понять, насколько конкретное действие лучше или хуже "стандартного" поведения в данном контексте при текущей политике.
   - Пример: В математической задаче действие "выбрать метод интегрирования по частям" может иметь высокий advantage, если приводит к правильному ответу, и низкий — если усложняет решение.

2. **Снижение дисперсии градиентов**:
   - Использование относительных advantage-значений вместо абсолютных наград делает обновления политики более стабильными.

---

### **Как вычисляются Advantage-функции в классическом RL (например, PPO)?**
В Proximal Policy Optimization (PPO):
1. **Value-сеть** (отдельная нейросеть) обучается предсказывать `V(s)` — ожидаемую награду для состояния `s`.
2. **Q(s, a)** оценивается через фактическую полученную награду + дисконтированные будущие награды.
3. **Advantage** вычисляется как:
   $$
   A(s, a) = R_{\text{total}} - V(s)
   $$
   где $( R_{\text{total}} )$ — дисконтированная сумма наград за траекторию.

**Проблемы PPO**:
- Value-сеть требует дополнительных вычислительных ресурсов и памяти.
- Ошибки в предсказаниях `V(s)` (особенно в задачах с **многомодальным распределением наград**, как в LLM) искажают advantage-значения.

> Тут будет ссылка на тетрадку

</details> 

---

### **Новаторский подход GRPO к Advantage-функциям**

![Figure_19](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-07_%26_08/assets/Figure_19.jpg)

<details> 
    <summary><em><strong>Step-by-step PPO and GRPO:</strong></em></summary>

## PPO (Proximal Policy Optimization)

**1. Входной запрос (q):**

- **Описание:** Начало процесса - это входной запрос, обозначенный как `q`. В контексте языковых моделей, это может быть текстовый промпт или вопрос, на который модель должна сгенерировать ответ.
- **На диаграмме:** Блок слева с надписью `q`.

**2. Policy Model (Модель политики):**

- **Описание:** Это нейронная сеть, которая принимает запрос `q` на вход и генерирует выход `o`. `o` представляет собой сгенерированный ответ или последовательность действий (токенов в случае LLM) на основе текущей политики. Модель политики стремится максимизировать ожидаемую награду.
- **На диаграмме:** Желтый блок с надписью `Policy Model`, принимающий вход от `q` и выдающий выход `o`. Желтый цвет указывает на то, что эта модель является **обучаемой** (Trained Models).

**3. Reference Model (Референсная модель) и Reward Model (Модель награды):**

- **Описание:** После того, как Policy Model сгенерировала выход `o`, этот выход поступает в два блока: `Reference Model` и `Reward Model`.
    - **Reference Model:** Это **замороженная** (Frozen Models, синий цвет) модель, представляющая собой предыдущую версию Policy Model или какую-либо другую модель, к которой мы хотим, чтобы текущая политика не отклонялась слишком сильно. Она используется для расчета **KL-дивергенции**, которая регулирует обновление политики, чтобы оно не было слишком резким.
    - **Reward Model:** Это модель, которая оценивает качество выхода `o` и присваивает ему награду `r`. Как описано в тексте, на **первой итерации** обучения GRPO, эта награда берется из **внешней функции награды** (reward function), которая может быть ручной, автоматизированной или основанной на правилах. На диаграмме Reward Model также показана как **замороженная** (синий цвет), что подразумевает, что на данном этапе обучения она не обновляется, а используется как внешний источник оценки.
- **На диаграмме:** Два синих блока `Reference Model` и `Reward Model`, принимающие вход `o`. Стрелка с надписью `KL` идет от `Reference Model` к символу "⊕", указывая на использование **KL-дивергенции**. `Reward Model` выдает награду `r`.

**4. Value Model (Модель ценности):**

- **Описание:** В PPO используется Value Model для оценки "ценности" состояния. Она принимает на вход выход `o` от Policy Model и предсказывает скалярное значение `v`, которое представляет собой ожидаемую суммарную награду, которую можно получить, начиная с этого состояния (или после генерации этого выхода). Value Model используется для оценки baseline при расчете **advantage-функции**.
- **На диаграмме:** Желтый блок `Value Model`, принимающий вход `o` и выдающий выход `v`. Желтый цвет указывает на то, что Value Model также является **обучаемой**.

**5. KL Divergence (KL-дивергенция) и операция "⊕":**

- **Описание:** **KL-дивергенция** измеряет разницу между распределениями вероятностей, в данном случае, между распределением текущей Policy Model и Reference Model. На диаграмме символ "⊕" обозначает операцию, в которой **KL-дивергенция** используется для регуляризации. В контексте PPO, **KL-дивергенция** часто добавляется к функции потерь для ограничения изменений политики и обеспечения стабильности обучения.
- **На диаграмме:** Стрелка с надписью `KL` от `Reference Model` идет к символу "⊕".

**6. Reward (r) и Value (v):**

- **Описание:** `r` - это награда, полученная от Reward Model за выход `o`. `v` - это оценка ценности состояния, полученная от Value Model.
- **На диаграмме:** Блоки `r` и `v`, представляющие собой выходы Reward Model и Value Model соответственно.

**7. GAE (Generalized Advantage Estimation) и Advantage (A):**

- **Описание:** **GAE** (Generalized Advantage Estimation) - это метод для оценки **advantage-функции** `A`. **Advantage-функция** показывает, насколько действие, предпринятое в данном состоянии, лучше или хуже, чем среднее действие в этом состоянии. В PPO, GAE использует как награды `r`, так и оценки ценности `v` для расчета `A`. Формула GAE обычно включает в себя дисконтированные награды и разницу между текущей и следующей оценками ценности.
- **На диаграмме:** Блок `GAE`, принимающий на вход награду `r` и оценку ценности `v`, и выдающий advantage `A`.

**8. Advantage (A) и Policy Model (обратная связь):**

- **Описание:** Рассчитанный advantage `A` используется для обновления Policy Model. Если advantage положительный, это означает, что действие (или сгенерированный выход) был лучше, чем ожидалось, и Policy Model обновляется таким образом, чтобы увеличить вероятность генерации подобных выходов в будущем. Если advantage отрицательный, Policy Model корректируется, чтобы уменьшить вероятность таких выходов.
- **На диаграмме:** Стрелка от блока `GAE` с advantage `A` обратно к `Policy Model`, указывающая на процесс обучения и обновления параметров Policy Model на основе advantage.

**9. Trained Models и Frozen Models:**

- **Описание:** Блоки `Trained Models` (желтые) и `Frozen Models` (синие) обозначают, какие модели обновляются в процессе обучения, а какие остаются неизменными. В PPO, Policy Model и Value Model являются обучаемыми, а Reference Model и Reward Model (на первой итерации GRPO) являются замороженными.
- **На диаграмме:** Желтые блоки `Policy Model` и `Value Model` находятся в контейнере `Trained Models`. Синие блоки `Reference Model` и `Reward Model` находятся в контейнере `Frozen Models`.

**В итоге, в PPO процесс выглядит так:** Запрос `q` поступает в Policy Model, которая генерирует выход `o`. Этот выход оценивается Reward Model (получаем `r`) и Value Model (получаем `v`). Также используется Reference Model для расчета **KL-дивергенции**. На основе `r` и `v` вычисляется advantage `A` с помощью GAE. Advantage `A` используется для обновления Policy Model и Value Model, чтобы улучшить политику и оценку ценности в будущем. **KL-дивергенция** используется для регуляризации процесса обучения.

---

## GRPO (Group Relative Policy Optimization)

**1. Входной запрос (q):**

- **Описание:** Аналогично PPO, процесс начинается с входного запроса `q`.
- **На диаграмме:** Блок слева с надписью `q`.

**2. Policy Model (Модель политики) и группа выходов (o₁, o₂, ..., o…):**

- **Описание:** Policy Model в GRPO также принимает запрос `q`, но вместо генерации одного выхода, она генерирует **группу** из `G` различных выходов: `o₁, o₂, ..., o…`. Как поясняется в тексте, это **горизонтально различные вариативные ответы на один и тот же промпт**. Это означает, что для одного и того же запроса Policy Model генерирует несколько альтернативных ответов.
- **На диаграмме:** Желтый блок `Policy Model`, принимающий вход `q` и выдающий группу выходов, представленную блоком с надписью `o₁`, `o₂`, ..., `o…`. Желтый цвет указывает на то, что Policy Model является **обучаемой**.

**3. Reference Model (Референсная модель) и Reward Model (Модель награды):**

- **Описание:** Каждый выход из группы `oᵢ` поступает в Reference Model и Reward Model. Аналогично PPO, Reference Model является **замороженной** и используется для **KL-регуляризации**. Reward Model также **заморожена** и оценивает каждый выход `oᵢ` группы, присваивая ему награду `rᵢ`.
- **На диаграмме:** Синие блоки `Reference Model` и `Reward Model`, принимающие на вход группу выходов `o₁, o₂, ..., o…`. Стрелка с `KL` от `Reference Model` к символу "⊕". Reward Model выдает группу наград `r₁, r₂, ..., r…`.

**4. KL Divergence (KL-дивергенция) и операция "⊕":**

- **Описание:** Аналогично PPO, **KL-дивергенция** используется для регуляризации, ограничивая изменения политики.
- **На диаграмме:** Стрелка с `KL` от `Reference Model` к символу "⊕".

**5. Rewards (r₁, r₂, ..., r…):**

- **Описание:** `r₁, r₂, ..., r…` - это награды, полученные от Reward Model для каждого выхода в группе `o₁, o₂, ..., o…`.
- **На диаграмме:** Блок `r₁, r₂, ..., r…`, представляющий собой группу наград.

**6. Group Computation (Групповые вычисления) и Advantages (A₁, A₂, ..., A…):**

- **Описание:** Ключевое отличие GRPO от PPO заключается в блоке `Group Computation`. В GRPO **нет Value Model**. Вместо этого, advantage-функция вычисляется на основе **групповой относительной нормализации**. Как описано в тексте, advantage для каждого ответа `Oᵢ` в группе `G = {O₁, O₂, ..., O…}` вычисляется как:

$$
A_i(O_i, G) = R_i - \bar{R}_G = R_i - \frac{1}{N} \sum_{j=1}^N R_j
$$

где $\bar{R}_G$ - средняя награда по группе. Таким образом, `Group Computation` берет группу наград `r₁, r₂, ..., r…` и вычисляет для каждого выхода `oᵢ` advantage `Aᵢ` как разницу между его наградой `rᵢ` и средней наградой по группе.

- **На диаграмме:** Блок `Group Computation`, принимающий на вход группу наград `r₁, r₂, ..., r…` и выдающий группу advantages `A₁, A₂, ..., A…`.

**7. Advantages (A₁, A₂, ..., A…) и Policy Model (обратная связь):**

- **Описание:** Группа advantages `A₁, A₂, ..., A…` используется для обновления Policy Model. Политика обновляется таким образом, чтобы увеличить вероятность генерации выходов с более высокими advantages (то есть, выходов, которые лучше, чем среднее по группе).
- **На диаграмме:** Стрелка от блока `Group Computation` с advantages `A₁, A₂, ..., A…` обратно к `Policy Model`, указывающая на процесс обучения.

**8. Trained Models и Frozen Models:**

- **Описание:** Аналогично PPO, Policy Model является обучаемой, а Reference Model и Reward Model (на первой итерации GRPO) являются замороженными.
- **На диаграмме:** Желтый блок `Policy Model` в контейнере `Trained Models`. Синие блоки `Reference Model` и `Reward Model` в контейнере `Frozen Models`.

**В итоге, в GRPO процесс выглядит так:** Запрос `q` поступает в Policy Model, которая генерирует группу выходов `o₁, o₂, ..., o…`. Каждый выход из группы оценивается Reward Model (получаем `r₁, r₂, ..., r…`) и используется Reference Model для **KL-регуляризации**. Затем, в блоке `Group Computation`, для каждого выхода вычисляется advantage `Aᵢ` относительно средней награды по группе. Группа advantages `A₁, A₂, ..., A…` используется для обновления Policy Model. **Важно отметить отсутствие Value Model в GRPO.**

**Ключевые отличия GRPO от PPO, наглядно представленные на диаграмме:**

1.  **Отсутствие Value Model в GRPO:** Самое заметное отличие - GRPO не использует Value Model. Вместо оценки абсолютной ценности состояния, GRPO фокусируется на **относительном сравнении внутри группы ответов**.

2.  **Групповая обработка выходов и наград в GRPO:** GRPO генерирует и обрабатывает группу выходов для каждого запроса, а затем вычисляет advantage на основе сравнения наград внутри этой группы. PPO обрабатывает каждый выход индивидуально и использует Value Model для оценки baseline.

3.  **Group Computation в GRPO:** Этот блок является уникальным для GRPO и реализует **групповую относительную нормализацию**, вычисляя advantage как разницу между наградой конкретного ответа и средней наградой по группе.

</details>

---

![Визуализация процесса оценки реворда в GRPO для DeepSeek-R1 от Jay Alammar](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-13/assets/Figure_2.png)

GRPO полностью устраняет необходимость в value-сети, используя **групповую относительную нормализацию**:
для каждого промпта $P$ генерируется группа из $N$ ответов $G = \{O_1, O_2, ..., O_N\}$ с использованием политики $\pi$.  Каждому ответу $O_i$ присваивается награда $R_i = R(O_i)$, отражающая его качество.  Advantage-функция для $i$-го ответа $O_i$ относительно группы $G$ вычисляется по формуле:

$$
A_i(O_i, G) = R_i - \bar{R}_G = R_i - \frac{1}{N} \sum_{j=1}^N R_j
$$

где:

- В GRPO оценка $R_i$ (награда для ответа $O_i$) в **первой итерации** берётся из **внешней функции награды (reward function)** $R(O_i)$, которая:

    1. **Не зависит от текущей политики $\pi$.**  
    - Это может быть:  
        - Ручная аннотация (например, экспертные оценки качества ответов).  
        - Автоматизированный алгоритм (например, модель-критик, отдельная ML-модель, оценивающая тексты).  
        - Правило на основе heuristics (например, соответствие формату, наличие ключевых слов).  

    2. **Требует предварительной настройки.**  
    - Если используется модель-критик, её нужно предварительно обучить на размеченных данных.  
    - Если аннотации ручные, требуется подготовка датасета с оценками.

- $\bar{R}_G = \frac{1}{N} \sum_{j=1}^N R_j$ — средняя награда по группе $G$.

> По сути, Advantage-функция в GRPO для каждого конкретного ответа рассчитывается как награда конкретного ответа  минус  среднее арифметическое наград всех ответов в группе.

Группа $G$ в контексте GRPO представляет собой **горизонтально различные вариативные ответы на один и тот же промпт $P$**, а не последовательные шаги в траектории.  

**Пояснение:**  
1. **Горизонтальная вариативность:** для каждого промпта $P$ политика $\pi$ генерирует $N$ **альтернативных ответов** ($O_1, O_2, \dots, O_N$), которые являются независимыми вариантами, а не частями одной цепочки (траектории). Это похоже на сэмплирование нескольких возможных ответов на один вопрос.  

2. **Сравнение внутри группы:** advantage-функция $A_i$ вычисляет, насколько конкретный ответ $O_i$ лучше или хуже **среднего по группе** ($\bar{R}_G$). Это требует, чтобы все ответы в $G$ были параллельными вариантами, иначе среднее значение теряет смысл как относительный базис.  

3. **Устранение value-сети:** GRPO заменяет оценку "абсолютной" полезности (через value-сеть) на **относительное сравнение внутри группы**. Для этого группа должна содержать разнообразные ответы на один промпт, чтобы средняя награда $\bar{R}_G$ отражала общее качество группы.  

**Ключевые особенности GRPO подхода:**

*   **Групповая относительная нормализация:** advantage-функция вычисляется относительно группы ответов, сгенерированных для одного и того же промпта, что обеспечивает относительную оценку качества;
*   **Устранение value-сети:**  средняя награда по группе $\bar{R}_G$ служит в качестве baseline, заменяя необходимость в отдельной value-сети для оценки ценности состояний или действий;
*   **Обучение на основе сравнения:**  GRPO фокусируется на обучении политики, которая генерирует ответы, превосходящие в среднем другие ответы в группе, что делает его эффективным в задачах, где важна относительная оценка качества;
* **KL-дивергенция: Жесткая интеграция в loss-функцию через относительные веса**: KL-дивергенция вводится в функцию потерь для регуляризации, ограничивая величину изменения политики на каждом шаге обучения и предотвращая её резкие колебания, что способствует стабильности обучения.

**Ограничения и замечания:**

*   Эффективность GRPO подхода зависит от качества функции награды $R(O)$.  Необходимо корректно определить функцию награды, чтобы она адекватно отражала желаемые свойства ответов.
*   Размер группы $N$ является гиперпараметром, который может влиять на стабильность и эффективность обучения.  Выбор оптимального значения $N$ может потребовать экспериментальной настройки.
*   GRPO, как и другие методы обучения с подкреплением, может быть чувствителен к выбору гиперпараметров оптимизации и архитектуры модели.

---

### **Практическая интерпретация для LLM**
В GRPO advantage-функция становится **инструментом ранжирования вариантов ответа**:
- Модель учится генерировать ответы, которые не просто "хороши", но **значительно лучше среднего в своей группе**.
- Это стимулирует:
  - Поиск неочевидных, но эффективных цепочек рассуждений.
  - Избегание шаблонных ошибок, типичных для группы.

**Эффект**: Модель фокусируется на **качественных различиях между ответами**, а не на абсолютных значениях наград, что критично для сложных задач с неоднозначными критериями успеха.

**Контекст проблемы**:
- В задачах рассуждения LLM часто генерируют множественные "рассуждения-цепочки" (chain-of-thought), но стандартные алгоритмы RL слабо адаптированы для их оценки.
- **Value-сети в PPO требуют значительных ресурсов для обучения и склонны к ошибкам в многомодальных распределениях наград**.

---

### **Основные отличия GRPO от PPO**

| **Характеристика**                   | **PPO**                               | **GRPO**                                                                 |
|-------------------------------------|---------------------------------------|---------------------------------------------------------------------------|
| Наличие value-сети                   | Требуется                             | Исключена                                                                |
| Оценка преимущества                  | На основе value-сети                  | **Групповая относительная нормализация внутри траекторий**               |
| KL-дивергенция                       | Опциональная регуляризация            | **Жесткая интеграция в loss-функцию через относительные веса**           |
| Использование памяти                 | Высокое (2 модели)                    | **Снижено на 40-60% за счет удаления value-сети**                         |
| Сходимость                           | Зависит от точности value-сети        | **Стабильнее благодаря групповой стабилизации градиентов**               |

---

### **Математическая формализация**

**Функция целевой оптимизации в GRPO**:

$$
J_{\text{GRPO}}(\theta) = \mathbb{E}_{(q,a)\sim\mathcal{D},\{o_i\}_{i=1}^G\sim\pi_{\theta_{\text{old}}}(\cdot|q)} \left[\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left(\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\text{ clip}\left(r_{i,t}(\theta),1-\varepsilon,1+\varepsilon\right)\hat{A}_{i,t}\right)-\beta D_{\text{KL}}(\pi_\theta||\pi_{\text{ref}})\right)\right],
$$

где:
- **$\theta$** — параметры **текущей политики** (нейронной сети), которые оптимизируются в процессе обучения.
- **$q$** — **запрос** (query), поступающий на вход языковой модели.
- **$a$** — **ответ** (answer), сгенерированный моделью на запрос $q$.
- **$\mathcal{D}$** — набор данных пар "запрос-ответ", используемый для обучения модели.
- **$o_i$** — $i$-й **выход** (output), сгенерированный моделью $\pi_{\theta_{\text{old}}}$ для запроса $q$.
- **$G$** — количество сгенерированных выходов для каждого запроса.
- **$|o_i|$** — длина $i$-го выхода (количество токенов).
- **$r_{i,t}(\theta)$** — отношение вероятностей между текущей и старой политиками для $t$-го токена в $i$-м выходе:
  $$r_{i,t}(\theta) = \frac{\pi_\theta(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,<t})}$$
- **$\hat{A}_{i,t}$** — оценка **преимущества** (advantage) для $t$-го токена в $i$-м выходе, вычисляемая как разница между ожидаемой наградой при выборе данного токена и средней наградой в текущем контексте.
- **$\text{clip}(r_{i,t}(\theta),1-\varepsilon,1+\varepsilon)$** — функция ограничения отношения вероятностей в диапазоне $[1-\varepsilon, 1+\varepsilon]$ для предотвращения слишком больших изменений политики за один шаг обучения.
- **$\varepsilon$** — гиперпараметр, определяющий допустимый диапазон изменения отношения вероятностей (обычно 0.1-0.2).
- **$D_{\text{KL}}(\pi_\theta||\pi_{\text{ref}})$** — KL-дивергенция между распределениями действий текущей политики $\pi_\theta$ и референсной политики $\pi_{\text{ref}}$, используемая для регуляризации:  
  $$D_{KL}(\pi_\theta||\pi_{\text{ref}}) = \mathbb{E}_{o \sim \pi_\theta} \left[ \log \frac{\pi_\theta(o|q)}{\pi_{\text{ref}}(o|q)} \right].$$
- **$\pi_{\text{ref}}$** — референсная политика, обычно предварительно обученная модель, к которой стремятся сохранить близость в процессе дообучения.
- **$\beta$** — гиперпараметр, регулирующий силу KL-регуляризации (**типичные значения: 0.05–0.2**).

Основные особенности данной формулы GRPO:
1. Оптимизация проводится по нескольким сгенерированным выходам ($G$ шт.) для каждого запроса
2. Учитывается поэлементный вклад каждого токена в последовательности выхода
3. Применяется техника клиппирования для стабилизации обучения
4. Используется KL-регуляризация для сохранения близости к референсной модели
5. Формула специально адаптирована для обучения генеративных языковых моделей

---

### **Пояснения**
1. **Off-policy обучение**: Градиенты вычисляются на данных, собранных старой политикой ($\pi_{\text{old}}$), но оптимизируется новая политика ($\pi_\theta$).  
2. **Importance weighting** $\frac{\pi_\theta}{\pi_{\text{old}}}$ корректирует градиенты с учетом различий между политиками, предотвращая смещение оценок.  
3. **KL-дивергенция** ограничивает скорость изменения политики, обеспечивая устойчивость обучения.  
4. **Преимущество $A(s,a)$** направляет обновление в сторону действий с большей ожидаемой наградой. Если $A(s,a) > 0$, действие $a$ в состоянии $s$ считается лучше среднего.

**Оптимизация**:
- Градиенты обновляются только для токенов, критически влияющих на награду (**например, ключевых шагов в математическом выводе**).  
  - *Формально*, это можно представить как применение маски $( M )$ к градиентам, где $( M_i = 1 )$ для «критических» токенов и $( M_i = 0 )$ для остальных. Таким образом, обновляются только параметры, связанные с «критическими» токенами, что повышает эффективность обучения, фокусируясь на наиболее значимых частях рассуждения.
- **Сэмплирование ответов**: Для каждого промпта параллельно генерируются 4–8 вариантов, что улучшает покрытие пространства решений.

---

### **Немного цифр**
1. **Эффективность**:
   - Удаление value-сети сокращает объем памяти на **18.2 GB для модели с 33B параметров** (эксперименты DeepSeek-R1).
   - Время обучения сокращается на **35%** при решении задач уровня MATH dataset.

2. **Стабильность**:
   - Групповая нормализация уменьшает дисперсию градиентов (**на 60% по сравнению с PPO**).
   - KL-регуляризация предотвращает "распад политики" — типичную проблему PPO.

3. **Результативность**:
   - На бенчмарке MATH GRPO повысил точность модели DeepSeek-Math-7B с **51.2% до 58.7%**.
   - В логических задачах (например, FOLIO) улучшение составило **12.3%**.

---

### **Практическая реализация GRPO**

**Шаги внедрения**:
1. **Супервизионное дообучение (SFT)**:
   - Используются данные формата:  
     ```json
     {"prompt": "Решите уравнение ∫₀¹ x² dx", "response": "∫₀¹ x² dx = [x³/3]₀¹ = 1/3"}
     ```
   - **Ключевой аспект**: очистка данных от ошибок через self-consistency проверку.

2. **Моделирование награды**:
   - Для математических задач (пример):  
     
    $$
     [
       R = \text{Correctness} + 0.5 \cdot \text{StepQuality} \;-\; 0.3 \cdot \text{LengthPenalty}.
     ]
    $$

   - Разработка эффективной функции награды является ключевым аспектом GRPO. В общем случае, она должна быть спроектирована так, чтобы поощрять желаемые свойства рассуждений — корректность, логическую последовательность, краткость и эффективность решения. Веса коэффициентов (например, 1, 0.5, -0.3 в примере) могут быть настроены эмпирически для достижения оптимального баланса между этими свойствами.

3. **Обучение с GRPO**:
   - **Гиперпараметры**:
     - Batch size: 512 промптов (по 4 ответа на промпт → 2048 примеров/шаг).
     - Learning rate: 1e-6 с линейным затуханием.
   - **Трюк**: Заморозка первых 10% слоев модели для сохранения общих знаний.

---

### **Кейсы применения**
1. **DeepSeek-Math-33B**:
   - Решение задач Международной математической олимпиады (IMO) с точностью **44.5%**.
   - **Особенность**: Использование GRPO + деревоискока (MCTS) для генерации шагов.

2. **Логический планировщик AlphaLogic**:
   - Автоматическое доказательство теорем в Coq с успешностью **68%** (против 52% у PPO).

---

### **Заключение**
GRPO представляет собой значительный шаг вперёд в области обучения с подкреплением для LLM, особенно в задачах, требующих сложного рассуждения. **Его применение уже выходит за рамки математики — текущие исследования тестируют GRPO в юридическом анализе и генерации научных гипотез.** Несмотря на ограничения, алгоритм демонстрирует потенциал для создания "мыслящих" ИИ-систем, способных к глубокому абстрактному мышлению.

### **2.3 Устранение расхождения KL**

**Устранение расхождения KL в алгоритме DAPO для обучения моделей с длинными цепочками рассуждений**  

В сценарии RLHF (обучение с подкреплением и обратной связью с человеком) штрафной член, основанный на расхождении Кульбака-Лейблера (KL), традиционно используется для регулирования отклонений между обновляемой онлайн-политикой и замороженной эталонной политикой. Его основная цель — обеспечить, чтобы в процессе обучения модель корректировала свое поведение, не отдаляясь слишком сильно от исходного распределения данных, что особенно важно для сохранения предсказуемости и стабильности.  

Однако при обучении моделей, генерирующих длинные цепочки рассуждений (Chain-of-Thought, CoT), это ограничение теряет свою актуальность. В таких задачах распределение модели в процессе обучения может закономерно и значительно отклоняться от исходного из-за сложности и многошаговости выводов. Жесткое регулирование через KL-дивергенцию в данном случае становится избыточным, так как искусственно ограничивает способность модели к исследованию альтернативных стратегий генерации, необходимых для эффективного решения многоэтапных задач.  

Алгоритм DAPO (Decoupled Adaptive Policy Optimization) предлагает устранить KL-штраф, чтобы смягчить это ограничение. Отказ от термина расхождения KL позволяет модели свободно адаптироваться в процессе обучения, не будучи привязанной к изначальному распределению эталонной политики. Это особенно важно для сценариев, где успешное выполнение задачи требует выхода за рамки шаблонных решений, например, при генерации сложных логических заключений или творческих текстов. Таким образом, DAPO фокусируется на балансе между исследованием новых стратегий и эффективной оптимизацией политики, что повышает гибкость модели в контексте длинных выводов без ущерба для качества генерации.  

Данный подход подчеркивает, что в определенных сценариях RLHF строго контроля за отклонением от исходной политики можно избежать, чтобы раскрыть полный потенциал адаптивности модели в условиях сложных и неоднозначных задач.

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
  <p style="margin: 0; font-weight: bold; color: #2c3e50;">First Checkpoint:</p>
  <p style="margin: 8px 0 0 0; color: #2c3e50;">DAPO устраняет штрафы за расхождение KL в RLHF для задач в стиле long-CoT, что позволяет повысить гибкость политики и улучшить возможности рассуждений.</p>
</div>

<details> 
    <summary><em><strong>Объяснение KL-дивергенции:</strong></em></summary>

#### **Объяснение KL-дивергенции**

В сценарии RLHF (обучение с подкреплением с обратной связью от человека) ключевым элементом является использование штрафного члена, основанного на расхождении Кульбака-Лейблера (KL), для регуляризации процесса обучения. Этот штрафной член играет важную роль в управлении отклонениями между обновляемой онлайн-политикой $\pi_{\theta}$ и замороженной эталонной политикой $\pi_{ref}$.  Основная цель KL-регуляризации — обеспечить, чтобы в процессе обучения модель корректировала свое поведение, не отдаляясь слишком сильно от исходного распределения данных, представленного эталонной политикой. Это особенно важно для сохранения предсказуемости, стабильности обучения и предотвращения катастрофического забывания ранее изученных полезных стратегий.

Математически, расхождение Кульбака-Лейблера (KL-дивергенция), обозначаемое как $D_{KL}(P||Q)$, измеряет "расстояние" между двумя распределениями вероятностей $P$ и $Q$. В контексте RLHF, где мы хотим ограничить изменения в политике, KL-дивергенция используется для измерения различия между новой политикой $\pi_{\theta}$ и эталонной политикой $\pi_{ref}$.

Формула для KL-дивергенции между двумя дискретными распределениями $P(x)$ и $Q(x)$ выглядит следующим образом:

$$D_{KL}(P||Q) = \sum_{x} P(x) \log \left( \frac{P(x)}{Q(x)} \right)$$

Для непрерывных распределений формула аналогична, но вместо суммы используется интеграл:

$$D_{KL}(P||Q) = \int_{-\infty}^{\infty} p(x) \log \left( \frac{p(x)}{q(x)} \right) dx$$

где $p(x)$ и $q(x)$ - функции плотности вероятности для распределений $P$ и $Q$ соответственно.

**Глубокое пояснение Кульбака-Лейблера (KL) дивергенции:**

1.  **Интуитивное понимание:** KL-дивергенцию можно интерпретировать как меру "информационных потерь", возникающих при использовании распределения $Q$ для аппроксимации истинного распределения $P$.

2.  **Компоненты формулы:**
    *   **$P(x)$ (или $p(x)$):** это распределение, которое мы считаем "истинным" или "целевым". В RLHF, в контексте штрафного члена, это часто распределение, порождаемое эталонной политикой $\pi_{ref}$.
    *   **$Q(x)$ (или $q(x)$):** это распределение, которое мы используем для аппроксимации $P(x)$. В RLHF это распределение, порождаемое текущей обучаемой политикой $\pi_{\theta}$.
    *   **$\frac{P(x)}{Q(x)}$:** это отношение вероятностей. Если $P(x)$ значительно больше $Q(x)$, то это отношение будет большим, и логарифм этого отношения также будет большим положительным числом. Это означает, что использование $Q$ вместо $P$ в точке $x$ приводит к большой "информационной потере".
    *   **$\log \left( \frac{P(x)}{Q(x)} \right)$:** логарифм делает меру аддитивной и преобразует отношение вероятностей в более удобную шкалу. Обычно используется натуральный логарифм (основание $e$), но также может использоваться логарифм по основанию 2 (в контексте теории информации, где единицей измерения является бит).
    *   **$P(x) \log \left( \frac{P(x)}{Q(x)} \right)$:** каждое значение логарифмического отношения взвешивается вероятностью $P(x)$. Это означает, что точки $x$, которые имеют высокую вероятность в распределении $P$, вносят больший вклад в общую KL-дивергенцию.
    *   **$\sum_{x}$ (или $\int_{-\infty}^{\infty}$):** суммирование (или интегрирование) по всем возможным значениям $x$ дает общую KL-дивергенцию между распределениями $P$ и $Q$.

3.  **Свойства KL-дивергенции:**
    *   **Неотрицательность:** $D_{KL}(P||Q) \ge 0$. KL-дивергенция всегда неотрицательна. Она равна нулю тогда и только тогда, когда $P = Q$ почти всюду;
    *   **Несимметричность:**  $D_{KL}(P||Q) \neq D_{KL}(Q||P)$ в общем случае. Это означает, что "расстояние" от $P$ до $Q$ не равно "расстоянию" от $Q$ до $P$. Важно понимать, какое распределение является "целевым" ($P$) и какое является "аппроксимирующим" ($Q$). В формуле $D_{KL}(P||Q)$, мы измеряем, насколько $Q$ отличается от $P$.

4.  **KL-дивергенция в RLHF:**
    *   **Регуляризация политики:** в RLHF, KL-дивергенция используется как штрафной член в целевой функции обучения с подкреплением. Цель состоит в том, чтобы обучить политику $\pi_{\theta}$, которая максимизирует награду, но при этом не слишком сильно отклоняется от эталонной политики $\pi_{ref}$.
    *   **Стабилизация обучения:** ограничение изменений политики с помощью KL-дивергенции помогает стабилизировать процесс обучения. Без этого ограничения, политика может резко меняться от итерации к итерации, что может привести к нестабильности и ухудшению производительности.
    *   **Предотвращение "policy drift":**  эталонная политика $\pi_{ref}$ часто представляет собой политику, обученную на начальном этапе или политику, которая демонстрирует желаемое поведение.  KL-регуляризация помогает предотвратить "дрейф" политики $\pi_{\theta}$ слишком далеко от $\pi_{ref}$, сохраняя таким образом важные характеристики исходной политики.
    *   **Баланс между исследованием и использованием:**  KL-штраф позволяет модели исследовать новые стратегии, но при этом удерживает ее в разумных пределах, не позволяя ей полностью забыть или игнорировать ранее изученное поведение, представленное эталонной политикой.

5.  **Применение в целевой функции RLHF:**  в типичной целевой функции RLHF, оптимизируемой с помощью алгоритмов, таких как PPO (Proximal Policy Optimization), KL-дивергенция добавляется как штрафной член:

    $J(\theta) = \mathbb{E}_{s \sim d_{\pi_{\theta}}, a \sim \pi_{\theta}} \left[ r(s, a) - \beta D_{KL}(\pi_{\theta}(. | s) || \pi_{ref}(. | s)) \right]$

    где:
    *   $J(\theta)$ - целевая функция, которую мы максимизируем;
    *   $r(s, a)$ - функция награды;
    *   $\beta$ - коэффициент регуляризации, определяющий силу KL-штрафа. Чем больше $\beta$, тем сильнее штраф за отклонение от эталонной политики;
    *   $D_{KL}(\pi_{\theta}(. | s) || \pi_{ref}(. | s))$ - KL-дивергенция между распределением действий текущей политики $\pi_{\theta}$ и эталонной политики $\pi_{ref}$ для состояния $s$.

В заключение, KL-дивергенция является мощным инструментом для регуляризации обучения в RLHF, обеспечивая баланс между оптимизацией награды и сохранением стабильности и предсказуемости поведения модели, путем контроля за отклонением новой политики от заданной эталонной политики.

</details> 