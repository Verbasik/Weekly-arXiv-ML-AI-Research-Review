# **MoE: ðŸ¤– How Mixture of Experts is Changing the Rules of AI ðŸš€**

When reviewing recently published papers, you may notice the term "MoE" appearing frequently in titles. What exactly does "MoE" mean, and why is it being used so often today? In this visual guide, we will thoroughly examine this critical component with over 50 illustrations: **Mixture of Experts (MoE)**!

![Table_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_1.png  )

## **Introduction**

This guide discusses the application of two core MoE componentsâ€”experts and routersâ€”in a typical LLM-based architecture.

---

## **What is a Mixture of Experts Model?**

The Mixture of Experts (MoE) method improves the training quality of LLM-based models by employing multiple distinct submodels (or "experts").

### Core MoE Components:

1. **Experts**:
   - Each FFNN layer now has a set of selectable "experts".
   - These "experts" are often themselves feedforward neural networks (FFNNs).

2. **Router or Gating Network**:
   - Decides which tokens to send to which experts.

At each level of an MoE-enabled LLM, we find some (relatively specialized) experts:

![Table_2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_2.png  )

Note that these "experts" are not narrowly specialized specialists in specific domains like "psychology" or "biology" as in the humanities. Rather, they capture more syntactic information at the lexical level and excel at processing certain tokens in specific contexts:

![Table_3](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_3.png  )

The router (or gating network) is responsible for selecting the most suitable expert for each input:

![Table_4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_4.png  )

Each expert is not a full LLM but rather a subcomponent within the LLM architecture.

---

## **Role of Experts**

To understand what experts are and how they function, we must first understand what MoE replaces: dense layers.

### (1) Dense Layers

The Mixture of Experts (MoE) method originated from a relatively basic function in large language models (LLMs): the feedforward neural network (FFNN).

In a standard decoder-only Transformer architecture, FFNNs are typically applied after layer normalization:

![Table_5](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_5.png  )

The FFNN enables the model to utilize contextual information generated by the attention mechanism and further transform it to capture more complex relationships in the data.

However, the size of the FFNN grows rapidly. To learn these complex relationships, it typically needs to expand the input dimensions:

![Table_6](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_6.png  )

<details>
  <summary>Feed Forward Network (FFN):</summary>

  **Purpose of FFN:**

  FFN is a key component in each Transformer block, responsible for **nonlinear transformation of token representations at individual positions**. While Multi-Head Attention enables tokens to interact and consider context, FFN processes each token's representation **individually**, yet with context provided by the attention layer.

  - Input to the FFN sublayer: $\text{Output}_{Norm1}$.
  - FFN consists of two linear layers with an activation function (e.g., ReLU, GeLU) between them:
  $$
  \text{FFN}(\text{Output}_{Norm1}) = \text{Activation}(\text{Output}_{Norm1} W_1 + b_1) W_2 + b_2
  $$
  where:

   - $W_1 \in \mathbb{R}^{D_{model} \times D_{ff}}$
   - $W_2 \in \mathbb{R}^{D_{ff} \times D_{model}}$ â€” weight matrices
   - $b_1 \in \mathbb{R}^{D_{ff}}$, $b_2 \in \mathbb{R}^{D_{model}}$ â€” bias vectors
   - $D_{ff}$ â€” internal FFN dimension (typically $4 \times D_{model}$).

   **Output:** FFN transforms the input and outputs a tensor of the **same dimension** $(N, L, D_{model})$, where:

   - $N$ â€” batch size (number of examples in batch),
   - $L$ â€” sequence length (number of tokens),
   - $D_{model}$ â€” hidden dimension (embedding size).

   This output is then passed to the next Transformer layer or used for downstream tasks (e.g., classification or text generation).

  **FFN Structure:**

  FFN consists of **two sequential linear layers** with an **activation function** between them. This can be viewed as a two-layer fully connected neural network applied to each position in the sequence.

  **FFN Components and Formula:**

  $$
  \text{FFN}(x) = \text{Activation}(x W_1 + b_1) W_2 + b_2
  $$

  Letâ€™s break down each component of the formula:

  1.  **First Linear Layer (Expansion Layer):**  $(x W_1 + b_1)$

   *   **Input:**  $x$ â€” the FFN input, i.e., $\text{Output}_{Norm1}$ of dimension `[batch_size, sequence_length, hidden_size]` ($D_{model}$).

   *   **Weight matrix $W_1$**:  $W_1 \in \mathbb{R}^{D_{model} \times D_{ff}}$ â€” the **weight matrix of the first linear layer**. It is an **adjustable parameter**.

   *   **Bias vector $b_1$**: $b_1 \in \mathbb{R}^{D_{ff}}$ â€” the **bias vector of the first linear layer**. Also an **adjustable parameter**.
   
   *   **Internal dimension $D_{ff}$**: $D_{ff}$ â€” the **internal (intermediate) dimension** of the FFN.  It is typically **larger than $D_{model}$**, often 4 times larger ($D_{ff} = 4 \times D_{model}$). For example, if $D_{model} = 512$, then $D_{ff} = 2048$.  **Increasing the dimension** at this stage is called **"expansion"**.

   *   **Operation:**  A **linear transformation** of input $x$ via matrix multiplication with $W_1$ and addition of bias $b_1$.

   *   **Output of first linear layer:**  The result is a tensor of dimension `[batch_size, sequence_length, D_{ff}]`. The feature space dimension has **increased** from $D_{model}$ to $D_{ff}$.

  2.  **Activation Function:**  $\text{Activation(...)}$

   *   **Input:**  Output of first linear layer of dimension `[batch_size, sequence_length, D_{ff}]`.
   
   *   **Activation Function:**  $\text{Activation}$ â€” a **nonlinear activation function**. In Transformers, commonly used are:
      *   **ReLU (Rectified Linear Unit):**  $\text{ReLU}(z) = \max(0, z)$. A simple and efficient function that zeros negative values.
      *   **GeLU (Gaussian Error Linear Unit):** A smoother activation function that sometimes yields better results than ReLU. The GeLU formula is slightly more complex, but its essence is introducing nonlinearity.

   *   **Purpose of Activation Function:**  The activation function **introduces nonlinearity** into the transformation. Without nonlinearity, FFN would be just another linear layer, and the entire Transformer would be equivalent to a linear model, severely limiting its expressiveness. Nonlinearity enables the model to learn **complex, nonlinear dependencies** in the data.
   
   *   **Output of Activation Function:**  The tensor dimension **remains unchanged** after applying the activation function. The output still has dimension `[batch_size, sequence_length, D_{ff}]`.

  3.  **Second Linear Layer (Contraction Layer):**  $FFN(... ) W_2 + b_2$

   *   **Input:**  Output of activation function of dimension `[batch_size, sequence_length, D_{ff}]`.

   *   **Weight matrix $W_2$**:  $W_2 \in \mathbb{R}^{D_{ff} \times D_{model}}$ â€” the **weight matrix of the second linear layer**. Also an **adjustable parameter**.

   *   **Bias vector $b_2$**: $b_2 \in \mathbb{R}^{D_{model}}$ â€” the **bias vector of the second linear layer**. Also an **adjustable parameter**.

   *   **Operation:**  A **linear transformation** of the activation output via matrix multiplication with $W_2$ and addition of bias $b_2$.

   *   **Output of second linear layer (and FFN overall):**  The result is a tensor of dimension `[batch_size, sequence_length, D_{model}]`. The feature space dimension is **restored** to the original $D_{model}$. This is **"contraction"** of dimension.

  **FFN Dimensions in Example:**

  Assume $D_{model} = 512$ and $D_{ff} = 4 \times D_{model} = 2048$.

  1.  **Input $x$**:  `[batch_size, sequence_length, 512]`

  2.  **First linear layer $(x W_1 + b_1)$**:

   *   $W_1$ has dimension `[512, 2048]`

   *   Output: `[batch_size, sequence_length, 2048]` (dimension expanded)

  3.  **Activation Function $\text{Activation}$**:

   *   Input: `[batch_size, sequence_length, 2048]`

   *   Output: `[batch_size, sequence_length, 2048]` (dimension unchanged)

  4.  **Second linear layer $(... ) W_2 + b_2)$**:

   *   $W_2$ has dimension `[2048, 512]`

   *   Output: `[batch_size, sequence_length, 512]` (dimension contracted back to original)

  **Purpose of Matrices $W_1$ and $W_2$:**

  *   **$W_1$ (Expansion Matrix):**  Matrix $W_1$ is responsible for **projecting the input space of dimension $D_{model}$ into a wider space of dimension $D_{ff}$**. This allows FFN to **increase expressiveness** and "remember" more information at the intermediate stage.
  *   **$W_2$ (Contraction Matrix):**  Matrix $W_2$ is responsible for **projecting back from the $D_{ff}$ space into the original $D_{model}$ space**. This is necessary so that the FFN output has the same dimension as the input and can be integrated into the rest of the Transformer architecture. Additionally, matrix $W_2$ allows **mixing and aggregating** information obtained at the higher-dimensional intermediate stage.

  **Why is FFN needed in Transformer?**

  *   **Introducing Nonlinearity:**  FFN introduces **nonlinearity** into the model, which is critical for learning complex dependencies in data.
  *   **Processing Information at Position Level:**  FFN is applied **independently to each position** in the sequence. This enables the model to perform **more complex, nonlinear transformations** of each token's representation after context has been captured by the attention layer.
  *   **Increasing Model Expressiveness:**  By expanding the dimension to $D_{ff}$ and then contracting back to $D_{model}$, FFN allows the model to **increase its expressiveness** and ability to learn more complex patterns. The intermediate higher-dimensional space acts as a kind of "hidden space" where the model can more flexibly manipulate data representations.
</details>

### (2) Sparse Layers

In a traditional Transformer, the FFNN (Feedforward Neural Network) is called a dense model because all its parameters (including weights and biases) are activated. All parameters are used to compute outputs, and none are discarded.

If we examine a dense model closely, we see that input data in some way activates all parameters:

![Table_7](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_7.png  )

In contrast, sparse models activate only a subset of overall parameters, closely related to the Mixture of Experts (MoE) model.

To illustrate this, we can decompose the dense model into several parts (called experts) and retrain it.

Then, only some experts are activated simultaneously:

![Table_8](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_8.png  )

The core idea is that each expert learns different information during training. During inference, only specific experts most relevant to the task are used.

Faced with a problem, we can select the expert most suited to solve it:

![Table_9](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_9.png  )

### (3) What Have Experts Learned?

As we have seen, the information acquired by an expert is more granular than information about the entire domain. Therefore, calling them "experts" may sometimes be misleading.

![Table_10](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_10.png  )

However, experts in the decoder model do not appear to exhibit the same type of specialization. This does not mean all experts are equal.

A good example is the Mixtral 8x7B paper, where each token is colored by its first selected expert.

![Table_11](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_11.png  )

This visualization also shows that experts tend to focus more on syntax rather than domain-specific content.

Thus, although decoder experts seemingly lack specific specialization, they behave more consistently with certain token types.

### (4) Expert Architecture

Although it is useful to visualize experts as dense models with hidden layers divided into parts, in reality, they are often full FFNNs themselves.

![Table_11](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_12.png  )

Since most LLMs have multiple decoder blocks, a given text passes through several experts before being generated:

![Table_13](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_13.png  )

Experts selected for different tokens may differ, leading to different "paths":

![Table_14](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_14.png  )

If we update the decoder block visualization, it now contains multiple FFNNs (one for each "expert"):

![Table_15](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_15.png  )

The decoder block now contains multiple FFNNs (i.e., "experts") that can be used during inference.

## **Routing Mechanism**

Now that we have a set of experts, how does the model learn which experts to use?

A router (also called a gating network) is placed before the experts and is trained to select the expert that should be activated for each token.

### (1) Router

The router (or gating network) itself is also an FFNN that selects an expert based on specific inputs.

The router outputs probability values and uses these to select the most suitable expert:

![Table_16](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_16.png  )

The expert layer returns the output of the selected expert and multiplies it by the gate value (selection probability).

Routers and experts (only a few of them) together form an MoE layer:

![Table_17](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_17.png  )

There are two types of MoE layers: sparse Mixture of Experts and dense Mixture of Experts.

Both variants use routers to select experts, but sparse MoE selects only a few experts, whereas dense MoE selects all experts but may distribute them in different proportions.

![Table_18](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_18.png  )

For example, faced with a set of tokens, MoE distributes these tokens among all experts, whereas sparse MoE selects only a few experts.

In modern LLMs, when you see "MoE," it usually refers to sparse MoE models, since sparse models allow partial expert usage, thereby reducing computational costâ€”a crucial feature for LLMs.

### (2) Expert Selection

The routing network is perhaps the most critical component of MoE, as it determines not only which experts to select during inference but also during training.

In the simplest form, we multiply the input (x) by the routerâ€™s weight matrix (W):

![Table_19](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_19.png  )

Then we apply the SoftMax operation to the output to create a probability distribution G(x) for each expert:

![Table_20](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_20.png  )

The router uses this probability distribution to select the most suitable expert for the given input.

Finally, we multiply each router output by the output of the corresponding selected expert and sum the results:

![Table_21](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_21.png  )

Letâ€™s put it all together and trace how input data passes through the router and experts:

![Table_22](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_22.png  )

### (3) Routing Complexity

However, this simple function often leads to the router always selecting the same expert, since some experts may learn faster than others:

![Table_23](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_23.png  )

This would lead not only to uneven expert selection distribution but also to some experts being practically untrained. This causes problems during both training and inference.

Therefore, we want to use each expert with equal importance during training and inference, known as **load balancing**. This is partially done to prevent the model from overfitting to a single set of experts.

## **Load Balancing and Optimization**

To balance expert importance, we must focus on the router, as it is the primary component determining which experts are selected at any given time.

### (1) KeepTopK

One way to balance router load is to use a simple scaling policy called KeepTopK.

By introducing trainable (Gaussian) noise, we can prevent the selection of the same expert:

![Table_24](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_24.png  )

Then, weights of all experts except the top k experts (e.g., 2) we wish to activate are set to -âˆž:

![Table_25](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_25.png  )

If these weights are set to -âˆž, the output probability after SoftMax becomes 0:

![Table_26](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_26.png  )

Note that KeepTopK can be implemented without additional noise.

**Token Selection Strategy**

The KeepTopK strategy directs each token to several selected experts.

This approach is called **token selection** and allows directing a given token to one expert (top-1 routing):

![Table_27](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_27.png  )

Or to multiple experts (top-k routing):

![Table_28](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_28.png  )

The main advantage of this strategy is that it weighs individual expert contributions and combines them.

**Auxiliary Losses**

To achieve uniform expert distribution during training, auxiliary losses (also called load balancing losses) are added to the networkâ€™s standard losses.

The auxiliary loss adds a constraint forcing experts to have equal importance during training.

The first component of the auxiliary loss is the sum of routing values for each expert across the entire batch:

![Table_29](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_29.png  )

This gives us an importance score for each expertâ€”that is, the probability of selecting a given expert independent of input.

We can use these importance scores to compute the **Coefficient of Variation (CV)**, representing the degree of variation between importance scores of different experts.

![Table_30](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_30.png  )

For example, if differences in importance scores are large, the CV value will be high:

![Table_31](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_31.png  )

Conversely, if all experts have similar scores, the CV value will be low (which is what we expect):

![Table_32](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_32.png  )

Using this CV score, we can update the auxiliary loss during training to minimize the CV score (thereby assigning equal importance to each expert):

![Table_33](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_33.png  )

Finally, the auxiliary loss is used as an independent term in the loss function during optimization.

### (2) Expert Capacity

Expert imbalance is reflected not only in selected experts but also in the distribution of tokens assigned to these experts.

For example, if input tokens are disproportionately distributed among certain experts, some experts may become undertrained:

![Table_34](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_34.png  )

Here, we must consider not only which experts are engaged but also how frequently these experts are engaged.

The solution to this problem is to limit the number of tokens each expert can processâ€”known as **expert capacity**.

When an expert reaches its limit, excess tokens are passed to the next expert:

![Table_35](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_35.png  )

If both experts reach their capacity, the token is not processed by any expert and is passed directly to the next level. This situation is called **token overflow**.

![Table_36](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_36.png  )

### (3) Using Switch Transformer to Simplify MoE

The first MoE model based on Transformer that solves MoE training instability problems, such as load balancing, is the Switch Transformer. The switching technology improves training stability by simplifying the architecture and training process.

**Switch Layer**

Switch Transformer is a T5 model (encoder-decoder structure) that replaces the traditional FFNN layer with a switch layer.

The switch layer is a sparse MoE layer that selects one expert (top-1 routing) for each token.

![Table_37](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_37.png  )

The router does not use a special method for expert selection; it simply takes the softmax of the result of multiplying input data by expert weights (same as the previous method).

![Table_38](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_38.png  )

**Capacity Factor**

The capacity factor is a critical parameter determining the number of tokens each expert can handle. The Switch Transformer extends this concept by introducing a capacity factor that directly affects expert capacity.

![Table_39](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_39.png  )

Expert capacity components are quite simple:

![Table_40](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_40.png  )

If we increase the capacity factor, each expert can process more tokens.

![Table_41](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_41.png  )

However, if the capacity factor is too large, computational resources are wasted. Conversely, if the capacity factor is too small, model performance degrades due to token overflow.

**Auxiliary Losses**

To further prevent token dropping, Switch Transformer introduces a simplified version of auxiliary loss.

In the simplified auxiliary loss variant, the coefficient of variation is no longer calculated; instead, the number of assigned tokens is weighted against the routing probability of each expert:

![Table_42](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_42.png  )

Since the goal is to uniformly distribute tokens among N experts, we want the values in vectors P and f to equal 1/N.

Î± is a hyperparameter used for fine-tuning the importance of this loss during training. Too large a value will affect the main loss function, while too small a value will not enable effective load balancing.

## **Mixture of Experts in Vision Modeling**

MoE is not limited to language models. Vision models such as ViT use Transformer architecture and can therefore also adopt MoE.

Briefly recalling, ViT (Vision Transformer) is an architecture that splits an image into multiple patches and processes them as tokens.

![Table_43](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_43.png  )

These image patches (or tokens) are projected into representation vectors (plus an additional positional embedding vector) and then fed into a standard encoder:

![Table_44](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_44.png  )

When these image patches enter the encoder, they are processed as tokens, making this architecture well-suited for MoE.

### (1) Vision-MoE

Vision-MoE (V-MoE) is one of the first examples of implementing MoE in an image model. It replaces dense FFNN layers in ViT with sparse MoE.

![Table_45](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_45.png  )

This improvement allows ViT models (which are typically smaller than language models) to scale significantly by increasing the number of experts.

To reduce hardware constraints for each expert, a small predefined capacity is set, as images typically contain many patches.

However, low capacity often leads to discarded image patches (analogous to token overflow).

![Table_46](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_46.png  )

To maintain low capacity, the network assigns each patch an importance score and prioritizes patches with higher scores, thereby avoiding loss from overflowed patches.

This approach is called **batched priority routing**.

![Table_47](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_47.png  )

Thus, even with reduced token count, we can still see that important image patches are successfully routed.

![Table_48](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_48.png  )

Priority routing allows focusing on the most important image patches by processing fewer patches.

### (2) From Sparse MoE to Soft MoE

In V-MoE, the priority scoring mechanism allows distinguishing between important and unimportant image patches. However, once image patches are assigned to each expert, information in unprocessed patches is lost.

The goal of Soft-MoE is to transition from discrete distribution of image patches (tokens) to a soft distribution by mixing image patches.

In the first step, we multiply the input x (image patch embedding) by a trainable matrix Î¦. This generates routing information indicating how relevant a token is to a specific expert.

![Table_49](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_49.png  )

Then, the routing information matrix undergoes a softmax operation (along columns) to update each image patch embedding.

![Table_50](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_50.png  )

The updated image patch embedding essentially represents a weighted average of all image patch embeddings.

![Table_51](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_51.png  )

Visually, this looks as if all image patches are mixed. Combined image patches are sent to each expert. After generating outputs, they are multiplied again by the routing matrix.

![Table_52](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_52.png  )

The routing matrix affects input at the token level and output at the expert level.

As a result, we obtain "soft" image patches/tokens that are processed instead of discrete inputs.

## **Activation of Mixtral 8x7B and Comparison of Sparse Parameters**

An important characteristic of MoE is its computational requirements. Since only a portion of experts is used simultaneously, we can have more parameters than are actually activated.

Although this MoE has more parameters (sparse parameters), fewer parameters are activated because during inference we use only a subset of experts (active parameters).

![Table_53](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_53.png  )

In other words, we still need to load the entire model (including all experts) into the device (sparse parameters), but during actual inference, we only need to use a subset of parameters (active parameters). An MoE model requires more GPU memory to load all experts but operates faster during inference.

Letâ€™s take Mixtral 8x7B as an example to examine the difference between sparse and active parameters.

![Table_54](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_54.png  )

In this example, we see that the number of parameters per expert is 5.6B, not 7B (though there are 8 experts total).

![Table_55](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_55.png  )

We need to load 8x5.6B (46.7B) parameters (plus all shared parameters), but for inference, we need only 2x5.6B (12.8B) parameters.

## **Conclusion**

Our investigation of Mixture of Experts (MoE) models is now complete! We hope this article has helped you better understand the potential of this fascinating technology. Today, nearly every architecture has an MoE variant, indicating that it will likely persist for a long time.