# Mixture of Experts

# Руководство по Смеси Экспертов (MoE)

При просмотре последних опубликованных статей вы можете заметить в названиях слово «MoE». Что же означает это «MoE» и почему его так часто используют сейчас? В этом наглядном руководстве мы подробно рассмотрим этот важный компонент с более чем 50 иллюстрациями: **Смесь Экспертов (MoE)**!

![Table_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_1.jpg)

## Введение

В этом руководстве обсуждается применение двух основных компонентов MoE — экспертов и маршрутизаторов — в типичной архитектуре на основе LLM.

---

## 01. Что такое модель «Смешанные Эксперты»?

Метод «Смешанные Эксперты» (MoE) позволяет улучшить качество обучения моделей на основе LLM за счет использования нескольких различных подмоделей (или «экспертов»).

### Основные компоненты MoE:

1. **Эксперты**:
   - Каждый слой FFNN теперь имеет набор «экспертов», которых можно выбрать.
   - Эти «эксперты» зачастую сами являются нейронными сетями прямого распространения (FFNN).

2. **Маршрутизатор или сеть шлюзов**:
   - Решает, какие токены отправлять тем или иным экспертам.

На каждом уровне LLM с MoE мы можем найти некоторых (относительно специализированных) экспертов. 

![Table_2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_2.jpg)

Обратите внимание, что эти «эксперты» не являются узкоспециализированными специалистами в конкретной области, как эксперты по «психологии» или «биологии» в области гуманитарных наук. На самом деле они усваивают больше синтаксической информации на лексическом уровне и отлично справляются с обработкой определенных токенов в определенных контекстах.

![Table_3](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_3.jpg)

Маршрутизатор (или сеть шлюзов) отвечает за выбор наиболее подходящего эксперта для каждого входа. 

![Table_4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_4.jpg)

Каждый эксперт не является полноценным LLM, а лишь частью подмодели в архитектуре LLM.

---

## 02. Роль экспертов

Чтобы понять, что имеют в виду эксперты и как они работают, сначала нужно понять, что заменяет MoE: плотные слои.

### (1) Плотные слои

Модель «Смесь Экспертов» (MoE) изначально возникла из относительно базовой функции в больших языковых моделях (LLM), а именно нейронной сети прямого распространения (FFNN).

В стандартной архитектуре Transformer, работающей только с декодированием, FFNN обычно применяются после нормализации слоев. 

![Table_5](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_5.jpg)

FFNN позволяет модели использовать контекстную информацию, генерируемую механизмом внимания, и дополнительно преобразовывать эту информацию для фиксации более сложных взаимосвязей в данных.

Однако размер FFNN быстро растет. Чтобы изучить эти сложные взаимосвязи, ему обычно необходимо расширить получаемые входные данные.

![Table_6](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-06/assets/Table_6.jpg)

<details>
  <summary>Нейронная сеть прямого распространения (Feed Forward Network):</summary>

  **Назначение FFN:**

  FFN - это ключевой компонент в каждом блоке Transformer, который отвечает за **нелинейное преобразование представлений токенов на уровне отдельных позиций**.  В то время как Multi-Head Attention позволяет токенам взаимодействовать друг с другом и учитывать контекст, FFN обрабатывает представление каждого токена **индивидуально**, но уже с учетом контекста, полученного от слоя внимания.

  - На вход подслоя FFN поступает $\text{Output}_{Norm1}$.
  - FFN состоит из двух линейных слоев с функцией активации (например, ReLU, GeLU) между ними:
  $$
  \text{FFN}(\text{Output}_{Norm1}) = \text{Activation}(\text{Output}_{Norm1} W_1 + b_1) W_2 + b_2
  $$
  где:

     - $W_1 \in \mathbb{R}^{D_{model} \times D_{ff}}$
     - $W_2 \in \mathbb{R}^{D_{ff} \times D_{model}}$ - весовые матрицы
     - $b_1 \in \mathbb{R}^{D_{ff}}$, $b_2 \in \mathbb{R}^{D_{model}}$ - векторы смещений
     - $D_{ff}$ - внутренняя размерность FFN (обычно $4 \times D_{model}$).

     **Выход:** FFN преобразует вход и выдает тензор **той же размерности** $(N, L, D_{model})$, где:

     - $N$ — размер батча (количество примеров в батче),
     - $L$ — длина последовательности (число токенов),
     - $D_{model}$ — скрытая размерность (размер эмбеддингов).

     Этот выход затем передается в следующий слой Transformer или используется для решения задачи (например, классификации или генерации текста).

  **Структура FFN:**

  FFN состоит из **двух последовательных линейных слоев** с **функцией активации** между ними.  Это можно представить как двухслойную полносвязную нейронную сеть, применяемую к каждой позиции в последовательности.

  **Компоненты FFN и формула:**

  $$
  \text{FFN}(x) = \text{Activation}(x W_1 + b_1) W_2 + b_2
  $$

  Разберем каждый компонент формулы:

  1.  **Первый линейный слой (Expansion Layer):**  `(x W_1 + b_1)`
     *   **Вход:**  $x$ - это вход FFN, то есть $\text{Output}_{Norm1}$ размерности `[batch_size, sequence_length, hidden_size]` ($D_{model}$).
     *   **Весовая матрица $W_1$**:  $W_1 \in \mathbb{R}^{D_{model} \times D_{ff}}$ - это **матрица весов первого линейного слоя**.  Она является **обучаемым параметром**.
     *   **Вектор смещения $b_1$**: $b_1 \in \mathbb{R}^{D_{ff}}$ - это **вектор смещения первого линейного слоя**. Он также является **обучаемым параметром**.
     *   **Внутренняя размерность $D_{ff}$**: $D_{ff}$ - это **внутренняя (промежуточная) размерность FFN**.  Обычно она **больше, чем $D_{model}$**, часто в 4 раза больше ($D_{ff} = 4 \times D_{model}$).  Например, если $D_{model} = 512$, то $D_{ff} = 2048$.  **Увеличение размерности** на этом этапе называется **"расширением" (expansion)**.
     *   **Операция:**  Происходит **линейное преобразование** входа $x$ путем матричного умножения на $W_1$ и добавления смещения $b_1$.
     *   **Выход первого линейного слоя:**  Результатом является тензор размерности `[batch_size, sequence_length, D_{ff}]`.  Размерность признакового пространства **увеличилась** с $D_{model}$ до $D_{ff}$.

  2.  **Функция активации (Activation Function):**  `Activation(...)`
     *   **Вход:**  Выход первого линейного слоя размерности `[batch_size, sequence_length, D_{ff}]`.
     *   **Функция активации:**  $\text{Activation}$ - это **нелинейная функция активации**.  В Transformer обычно используются:
        *   **ReLU (Rectified Linear Unit):**  $\text{ReLU}(z) = \max(0, z)$.  Простая и эффективная функция, обнуляющая отрицательные значения.
        *   **GeLU (Gaussian Error Linear Unit):**  Более гладкая функция активации, которая в некоторых случаях показывает лучшие результаты, чем ReLU.  Формула GeLU немного сложнее, но суть в том, что она также вносит нелинейность.
     *   **Назначение функции активации:**  Функция активации **вводит нелинейность** в преобразование.  Без нелинейности, FFN был бы просто еще одним линейным слоем, и Transformer в целом был бы эквивалентен линейной модели, что сильно ограничило бы его выразительность.  Нелинейность позволяет модели учить **сложные, нелинейные зависимости** в данных.
     *   **Выход функции активации:**  Размерность тензора **не меняется** после применения функции активации.  Выход по-прежнему имеет размерность `[batch_size, sequence_length, D_{ff}]`.

  3.  **Второй линейный слой (Contraction Layer):**  `(... ) W_2 + b_2`
     *   **Вход:**  Выход функции активации размерности `[batch_size, sequence_length, D_{ff}]`.
     *   **Весовая матрица $W_2$**:  $W_2 \in \mathbb{R}^{D_{ff} \times D_{model}}$ - это **матрица весов второго линейного слоя**.  Также является **обучаемым параметром**.
     *   **Вектор смещения $b_2$**: $b_2 \in \mathbb{R}^{D_{model}}$ - это **вектор смещения второго линейного слоя**.  Также **обучаемый параметр**.
     *   **Операция:**  Происходит **линейное преобразование** выхода функции активации путем матричного умножения на $W_2$ и добавления смещения $b_2$.
     *   **Выход второго линейного слоя (и FFN в целом):**  Результатом является тензор размерности `[batch_size, sequence_length, D_{model}]`.  Размерность признакового пространства **возвращается** к исходной $D_{model}$.  Это **"сжатие" (contraction)** размерности.

  **Размерности в FFN на примере:**

  Предположим, $D_{model} = 512$ и $D_{ff} = 4 \times D_{model} = 2048$.

  1.  **Вход $x$**:  `[batch_size, sequence_length, 512]`
  2.  **Первый линейный слой $(x W_1 + b_1)$**:
     *   $W_1$ имеет размерность `[512, 2048]`
     *   Выход: `[batch_size, sequence_length, 2048]` (размерность расширилась)
  3.  **Функция активации $\text{Activation}$**:
     *   Вход: `[batch_size, sequence_length, 2048]`
     *   Выход: `[batch_size, sequence_length, 2048]` (размерность не меняется)
  4.  **Второй линейный слой $(... ) W_2 + b_2)$**:
     *   $W_2$ имеет размерность `[2048, 512]`
     *   Выход: `[batch_size, sequence_length, 512]` (размерность сжалась обратно к исходной)

  **Назначение матрицы $W_1$ и $W_2$:**

  *   **$W_1$ (матрица расширения):**  Матрица $W_1$ отвечает за **проекцию входного пространства размерности $D_{model}$ в более широкое пространство размерности $D_{ff}$**.  Это позволяет FFN **увеличить выразительность** и "запомнить" больше информации на промежуточном этапе.
  *   **$W_2$ (матрица сжатия):**  Матрица $W_2$ отвечает за **проекцию обратно из пространства размерности $D_{ff}$ в исходное пространство размерности $D_{model}$**.  Это необходимо, чтобы выход FFN имел ту же размерность, что и вход, и мог быть интегрирован в остальную часть архитектуры Transformer.  Также, матрица $W_2$ позволяет **смешать и агрегировать информацию**, полученную на промежуточном этапе в пространстве большей размерности.

  **Зачем нужен FFN в Transformer?**

  *   **Введение нелинейности:**  FFN вносит **нелинейность** в модель, что критически важно для обучения сложных зависимостей в данных.
  *   **Обработка информации на уровне позиций:**  FFN применяется **независимо к каждой позиции** в последовательности.  Это позволяет модели выполнять **более сложное, нелинейное преобразование** представления каждого токена после того, как контекст был учтен слоем внимания.
  *   **Увеличение выразительности модели:**  За счет расширения размерности до $D_{ff}$ и последующего сжатия обратно до $D_{model}$, FFN позволяет модели **увеличить свою выразительность** и способность к обучению более сложным закономерностям.  Промежуточное пространство большей размерности действует как своего рода "скрытое пространство", где модель может более гибко манипулировать представлениями данных.
</details>