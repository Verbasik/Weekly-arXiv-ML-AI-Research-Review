# Диффузионные языковые модели: новая парадигма NLP

## Введение

Диффузионные языковые модели (Diffusion Language Models, **DLM**) представляют собой новую парадигму генерации текста, альтернативную классическим авторегрессионным трансформерам. В традиционных больших языковых моделях (LLM) текст генерируется последовательно, токен за токеном, слева направо. Это приводит к высоким затратам времени и вычислений: длинные ответы требуют пропорционально больше шагов, а параллельное ускорение ограничено зависимостями последовательности. Диффузионный подход предлагает иной механизм: модель учится восстанавливать текст из зашумленной версии, постепенно **«размывая» шум и уточняя ответ**. Идея заимствована из успешных диффузионных моделей для изображений и аудио, где генерация идет через многократное денойзинг-преобразование случайного шума в осмысленный сигнал. Применительно к языку это означает, что DLM генерирует фразу целиком и **итеративно улучшает ее качество**, вместо предсказания следующего слова по одному. Такой подход открывает возможности для более **быстрой и когерентной** генерации: модель может создавать целые блоки текста за раз, параллельно обновляя несколько токенов, и имеет шанс исправлять ошибки в ходе процесса.

В этом обзоре мы рассмотрим четыре передовые архитектуры DLM, демонстрирующие новый подход: **Gemini Diffusion** от Google DeepMind, **Mercury Coder** от стартапа Inception Labs, **LLaDA** (Large Language Diffusion Models) разработанный группой китайских исследователей, и **Eso-LM** (Esoteric Language Models) – совместную работу NVIDIA и Корнеллского университета. Для каждой модели мы проанализируем архитектуру и механизм диффузии, опишем алгоритм генерации (параллельный, последовательный или гибридный) и ключевые инженерные новшества (например, расписания восстановления токенов, модификации внимания и использование кеша ключ-значение). Далее мы представим математическую формализацию диффузионных LLM: как они определяют распределение над текстом и какие функции потерь оптимизируют. Затем сравним экспериментальные результаты – как по качеству текста (перплексия, BLEU, MMLU и др.), так и по производительности (скорость в токенах/с, латентность, энергоэффективность) на современных аппаратных платформах (GPU H100/A100, TPU и пр.). Наконец, сопоставим диффузионные модели с классическими трансформерами, отметив их преимущества, текущие ограничения и потенциал сместить авторегрессионные модели, а также обсудим перспективы масштабирования и применения на практике.

> Денойзинг (denoising) — процесс постепенного удаления случайного шума из зашумленной последовательности с целью восстановления осмысленного текста.

## Архитектурный анализ: ключевые модели Diffusion LM

### Gemini Diffusion (Google DeepMind)

**Gemini Diffusion** – экспериментальная текстовая модель от Google DeepMind, демонстрирующая диффузионный подход в генерации языка. В отличие от стандартных LLM, последовательно предсказывающих токены, Gemini Diffusion генерирует сразу целый фрагмент текста и итеративно его улучшает. **Механизм диффузии:** модель обучается преобразовывать случайный шум в осмысленный текст, аналогично тому, как Stable Diffusion генерирует изображения из шума. В ходе *forward*-процесса случайный шум накладывается на данные: исходные предложения постепенно повреждаются (например, токены маскируются или заменяются шумом) до неузнаваемости. Затем модель учится выполнять *reverse*-процесс – шаг за шагом убирать шум, восстанавливая исходное содержимое текста. Каждый шаг денойзинга – это проход трансформера, который берет текущую зашумленную последовательность и выдает очищенную версию на один шаг ближе к настоящему тексту.

**Генерация и архитектура:** при синтезе текста Gemini Diffusion начинает с *полностью зашумленной последовательности* фиксированной длины (например, все токены маскированы или заполнены случайными символами) и затем за несколько итераций преобразует ее в осмысленный ответ. Все токены обновляются в параллельном режиме – по сути, модель генерирует *целый блок текста одновременно*, а не по одному слову. Благодаря этому скорость ответа резко возрастает: внутренняя оценка Google показала, что Gemini Diffusion достигает **1000–2000 токенов/с**, тогда как даже самая быстрая версия авторегрессионной Gemini (режим Flash) выдает лишь \~272 токена/с. Кроме того, диффузионная модель может использовать *бидирекциональное внимание* внутри генерируемого блока: при денойзинге токены свободно видят контекст с обеих сторон, в отличие от строго причинного (лево-направленного) внимания в обычных трансформерах. Это дает Gemini Diffusion способность к **нелокальному “взгляду” на предложение** – модель учитывает будущие слова при выборе ранних, что улучшает глобальную связность фразы.

**Инженерные новшества:** для эффективной генерации Gemini Diffusion использует специальное *расписание восстановления* токенов и модифицированный трансформер. Хотя подробности реализации не раскрыты полностью, известно, что в процессе обучения применяются сотни шагов шума и восстановления. Вероятно, как и в других MDM (masked diffusion models), используется случайная перестановка позиций: порядок, в котором токены “очищаются” на каждом шаге, может задаваться *пермутацией*, позволяющей раскрывать слова в любой последовательности. Это устраняет жёсткое левонаправленное ограничение и помогает модели исправлять ошибки: если на раннем шаге какой-то токен сгенерирован неудачно, на следующем итерация денойзинга может его заменить на более подходящий вариант. В денойзере Gemini применяется стандартный трансформер, но с **полным вниманием по всему блоку**, что повышает когерентность ответа за счёт глобальной оптимизации текста, а не локальной, как в AR-моделях. Разработчики отмечают также режим *Instant Edit*: модель способна получать на вход готовый текст и **редактировать его “на лету”**, исправляя грамматику, меняя стиль или дополняя код – всё это естественным образом поддерживается итеративным диффузионным механизмом. В сумме, Gemini Diffusion демонстрирует, что при сравнимом качестве с классической моделью, диффузионный LM может обеспечивать существенно меньшую задержку и более высокую связанность текста.

> Бидирекциональное внимание (bidirectional attention) — механизм внимания в нейронных сетях, при котором каждый токен в последовательности может учитывать информацию как из предыдущих, так и из последующих токенов. 

### Mercury Coder (Inception Labs)

**Mercury** – первая коммерческая масштабная диффузионная LLM, представленная стартапом Inception Labs под руководством профессора Стефано Эрмона. Ее версия **Mercury Coder** предназначена для генерации кода и уже доступна для тестирования, что ознаменовало практическое применение dLLM в индустрии. **Механизм диффузии:** Mercury реализует так называемый *coarse-to-fine* (“от черновика к деталям”) генеративный процесс. На первом этапе модель генерирует грубый набросок выходной последовательности, заполнив ее “белым шумом” текста. Например, для задачи кодирования Mercury может начать с шаблона, где часть символов отсутствует (замаскирована) или случайна. Далее за несколько итераций этот черновик *параллельно уточняется*: модель убирает шум и заполняет пропуски, постепенно приближаясь к финальному решению. Этот процесс аналогичен диффузионным моделям изображений, где из зернистого эскиза вырисовываются четкие детали изображения.

**Алгоритм генерации:** Mercury генерирует полный ответ за фиксированное число шагов денойзинга. **Параллельность** – ключевое отличие: модель может обновлять сразу множество токенов за один шаг (например, сразу несколько слов или символов кода). В результате скорость впечатляет – **свыше 1000 токенов/с на GPU NVIDIA H100**. Тесты показали, что Mercury Coder генерирует код в 5–10 раз быстрее, чем оптимизированные авторегрессионные аналоги (так, GPT-4о Mini \~59 токенов/с), достигая \~1100 токенов/с на H100. При этом используется относительно небольшой трансформер (версии *Small* и *Mini*) – ускорение достигается именно за счет алгоритмических улучшений.

**Инновации и особенности:** точные архитектурные детали Mercury не полностью раскрыты (параметры модели, обучающий корпус и пр. держатся в секрете). Однако из научных публикаций соавторов известно, что Mercury базируется на методе **score-based diffusion** для дискретных данных. Так, в октябре 2023 г. группа Эрмона предложила новый подход – *score entropy*, дискретный аналог выравнивания скорораспределений, обеспечивающий стабильное обучение диффузии текста и формирующий вариационную нижнюю оценку правдоподобия (ELBO). В Mercury модель учится не напрямую предсказывать токены, а **оценивать отношение правдоподобий** между правильным токеном и текущим (зашумленным) токеном. Это позволяет вводить специальную метрику неопределенности для каждого символа.

В практике генерации Mercury осуществляет *прогрессивное случайное маскирование*: на этапе *forward* все больший процент токенов входного текста маскируется случайно на каждом шаге. К концу forward-процесса значительная часть последовательности скрыта. *Reverse*-процесс затем запускается с полностью замаскированной заготовки и поэтапно **открывает маски**. На каждом шаге денойзинга Mercury вычисляет для каждого токена *относительную уверенность* (тот самый transition ratio) в том, что данный токен должен быть определенным словом `y` вместо текущего `x`. Если уверенность высока, токен “открывается” (маска заменяется прогнозом); если нет – может остаться маской до более позднего шага. Таким образом, применяется *адаптивное расписание восстановления*: более легкие для предсказания элементы раскрываются раньше, сложные – позже, позволяя модели тратить больше итераций именно на трудные участки текста. Инженеры Inception Labs описывают, что за один шаг диффузии Mercury обновляет **сразу несколько токенов параллельно**, используя предобученное распределение масок по шагам. Это делает генерацию не только быстрой, но и устойчивой – исправление ошибок возможно на последующих итерациях. В отличие от авторегрессии, где одна неверно выбранная буква задает неправильный контекст для всех следующих, здесь ошибка не фатальна: уже на следующем шаге маска может быть переопределена корректно.

Для повышения эффективности Mercury также реализует некоторые классические приемы, как и AR-модели: например, **кеширование key-value** активаций трансформера между шагами, чтобы не пересчитывать неизменную часть последовательности заново. В открытых источниках не указано явно, используется ли кеш в Mercury, но общий принцип dLLM позволяет это – особенно если фиксировать порядок раскрытия масок. Известно, что другая команда (NVIDIA/Корнелл в Eso-LM) достигла существенного ускорения, введя каузальные ограничения для кеширования при диффузии. Возможно, Mercury также применяет оптимизации внимания, раз его достижимая скорость столь высока без специального железа. В целом, Mercury Coder продемонстрировал жизнеспособность диффузионных LLM в реальных приложениях, особенно там, где критична **мгновенная генерация длинных ответов** (например, автодополнение кода).

### LLaDA (Large Language Diffusion Models)

**LLaDA** – открытая исследовательская инициатива, впервые показавшая, что диффузионные модели могут конкурировать с авторегрессией на уровне больших LLM. Авторы LLaDA (Nie et al., 2025) поставили цель: обучить диффузионную модель **с нуля** на масштабном корпусе (триллионы токенов) и проверить, сможет ли она достичь ключевых способностей LLM – обобщения знаний, понимания инструкций, обучения в контексте и т.п..

**Архитектура и механизм:** название LLaDA расшифровывается как *Large Language Diffusion with Masking* – то есть это **маскированная диффузионная модель** для текста. Forward-процесс реализован через *дискретное случайное маскирование*: исходная последовательность постепенно “разрушается” путём замены отдельных токенов на специальный символ MASK независимо друг от друга, пока на последнем шаге все токены не станут масками. Каждая маска интерпретируется как “шум” в данных. Reverse-процесс – это *итеративное восстановление масок*: обученный трансформер получает на вход частично замаскированный текст и предсказывает, какой токен должен стоять на месте каждой маски (т.е. выполняет задачу **masked language modeling** на каждом шаге диффузии). LLaDA использует **стандартный Transformer** без каких-либо специальных модулей: отличие только в том, что на этапе денойзинга разрешено **двунаправленное внимание** по всей последовательности (никакого causal mask), ведь порядок заполнения масок не фиксирован слева-направо. Таким образом модель учит распределение над текстом, оптимизируя **нижнюю вариационную оценку правдоподобия**: вместо точного максимизации $\log P(text)$ вводится последовательность вспомогательных распределений (шагов диффузии), связанных неравенством ELBO. На практике итоговая функция потерь LLaDA – это взвешенная сумма кросс-энтропии на предсказание замаскированных токенов на каждом шаге диффузии. Проще говоря, модель учится хорошо выполнять заполнение масок на всех уровнях “зашумленности” данных – от почти чистых предложений до полностью маскированных.

**Алгоритм генерации:** LLaDA генерирует последовательность длины $L$ за $T$ шагов. Изначально берется $z_T$ – полностью замаскированный вход (все $L$ позиций = \[MASK]). На каждом обратном шаге $t=T, T-1, \dots, 1$ модель параллельно заполняет некоторый *поднабор* масок своими прогнозами. Порядок, в котором маски раскрываются, задается либо случайно, либо по специальному расписанию. В оригинальной работе применяется независимое случайное маскирование: на каждом шаге *forward* с вероятностью $p_t$ каждый токен маскируется. Это соответствует тому, что и обратное раскрытие идет не строго по позициям слева направо, а в случайном порядке – что критично для снятия ограничений AR. Математически, *reverse* шага $t$ моделируется условным распределением $p_\theta(x_{t-1} | x_t)$, параметризованным трансформером, которое разлагается по всем позициям с масками. Предсказав часть масок на шаге $t$, модель переходит к шагу $t-1$, где масок станет ещё меньше, и так до $t=0$, когда ни одной маски не останется и получится окончательный текст. **Важно**: как только позиция открылaсь (маска заменена на токен), **она остается зафиксированной** на всех последующих шагах. Это предотвращает зацикливание и повышает устойчивость: каждый шаг добавляет новые “чистые” токены, и к концу все восстановится. Такой параллельный процесс существенно быстрее автодегенерации, т.к. общее число сетевых обращений (NFEs) может быть намного меньше $L$ (например, $T=50$ шагов для $L=100$ токенов против 100 шагов у AR).

**Инженерные достижения:** LLaDA стала первой диффузионной моделью языка, успешно масштабированной до **8 миллиардов параметров**, с обучением от нуля на огромном корпусе (\~2.3 трлн токенов). Для этого потребовалось \~0.13 млн GPU-часов на ускорителях NVIDIA H800. Архитектурно модель аналогична трансформерам семейства GPT/LLaMA (они же выступали в роли baseline для сравнения). Однако авторам пришлось решить проблему, известную как *“curse of diffusion in discrete space”*: ранние попытки применять диффузию к тексту давали куда худшие перплексии, чем AR-модели. LLaDA показывает, что с правильной настройкой (случайное маскирование, оптимизация ELBO, последующий supervised fine-tuning) эти ограничения снимаются. В частности, LLaDA продемонстрировала **масштабируемость** диффузионного подхода: качество модели стабильно росло с увеличением параметров и данных, аналогично авторегрессии. Было подтверждено, что ключевые способности LLM не зависят критически от AR-парадигмы: после стандартной донастройки по инструкциям (SFT) диффузионная LLaDA-8B успешно выполняет сложные интерактивные задачи (диалоги, следование инструкциям) на уровне лучших 8-миллиардных ARM. Также интересен эксперимент с т.н. *reverse reasoning* – когда вопрос задан “задом наперед” (например, стихотворение, слова в котором переставлены наоборот). AR-модели обычно терпят неудачу из-за жёсткого направления контекста, а LLaDA справилась и даже превзошла GPT-4o в задаче восстановления перевернутого поэтического текста. Это демонстрирует естественное преимущество диффузионного подхода в задачах, требующих двунаправленной обработки последовательности.

### Eso-LM (NVIDIA & Cornell)

**Eso-LM** (*Esoteric Language Models*) – экспериментальная гибридная архитектура, совмещающая лучшие черты авторегрессии и диффузии. Разработчики поставили вопрос: можно ли объединить высокое качество AR-моделей с высокой скоростью и гибкостью MDM, создав единый подход с обучением «по всем фронтам»? Результатом стали Eso-LM вариантов A и B – модели, которые генерируют текст **в два этапа** и вводят специальные изменения в механизм внимания трансформера.

**Двухфазная генерация:** процесс разделяется на *диффузионную фазу* и *последовательную фазу*. Сначала **MDM-модель параллельно генерирует черновик** фразы, заполнив часть позиций, а часть оставив масками. Проще говоря, на этом шаге получается предложение, где уже стоят некоторые слова, а остальные места – пустые «бланки». Далее наступает вторая фаза: **AR-модель завершает предложение**, последовательно заполняя оставшиеся маски слева направо обычным автодополнением. Формально, если $z_0$ – частично замаскированная последовательность после первого этапа, то на втором этапе генерируется финальный вывод $x$ как $x = \text{AR}(z_0)$, где AR-модель видит уже раскрытые «чистые» токены и дополняет недостающее. Важно, что при таком подходе *часть токенов генерируется параллельно, а часть – последовательно*. Параметром разбиения служит доля $\alpha_0$: если $\alpha_0 = 1$, то весь текст генерирует только диффузия (чисто параллельный режим), если $\alpha_0 = 0$ – весь текст порождается AR (классический режим). Обычно выбирают промежуточное значение, например, $\alpha_0 = 0.5$ – половина токенов сразу ставится диффузией, половина дозаполняется AR. Такой гибрид позволяет **интерполировать между AR и MDM** по качеству и скорости, а также вносит дополнительную гибкость: к примеру, можно первые и последние слова предложения определить параллельно (учитывая глобальный контекст), а детали середины уточнить последовательно. Eso-LM явно сочетает сильные стороны: высокое качество моделирования (AR-часть гарантирует гладкость переходов, особенно для сложных фрагментов) и высокую скорость на большую часть последовательности (параллельная MDM-часть экономит время).

**Единая архитектура и внимание:** ключевая сложность – обучить **один трансформер**, способный работать и в режиме диффузии, и в режиме авторегрессии. В обычном случае требования конфликтуют: для AR нужно causal-маскирование (токен видит только предшествующие), для MDM – полное внимание по всем токенам (маски можно раскрывать в любом порядке). Авторы Eso-LM решили эту проблему с помощью **кастомного механизма внимания** с маской $A$. В трансформер вводится матрица смещений внимания $A_{i,j}$, где $A_{i,j} = 0$ разрешает внимание от позиции $i$ к $j$, а $-\infty$ запрещает. Настраивая эту матрицу, можно эмулировать любой шаблон внимания. Например, для AR-части $A$ будет задавать треугольную каузальную маску, а для MDM-части – полное внимание между «чистыми» токенами и ограниченное для масок. Конкретно, **Eso-LM (A)** убирает двунаправленное внимание *между масками* в диффузионной фазе. Это означает, что маски при денойзинге не «видят» друг друга – тем самым исключается избыточная зависимость, и трансформер может работать быстрее. Эту экономию авторы усиливают с помощью *разреженного внимания*: на каждом шаге диффузии обрабатываются не все позиции, а только те маски, которые выбраны для раскрытия на данном шаге, плюс все уже раскрытые токены. Такой подход существенно снижает затраты при длинных последовательностях, потому что вместо полного прохода по 10k токенам, например, можно обновить только 1k масок, оставив 9k постоянных. **Eso-LM (B)** идёт ещё дальше: он вводит каузальное ограничение даже для «чистых» токенов во время диффузии, позволяя **кешировать KV**-активации для них. Проще говоря, вариант B жертвует частью двунаправленного контекста (чистые токены видят только предшествующие чистые), но взамен может сохранять их представления и не пересчитывать на каждом шаге. Это даёт дополнительное ускорение – по оценкам, поддержка KV-кеша в диффузионной фазе увеличивает скорость до **65%** по сравнению с базовым MDM без кеширования. При этом небольшое снижение качества (перплексии) приемлемо: вариант B показывает чуть хуже PPL, чем A, но всё ещё лучше, чем чисто диффузионные модели, и **единственный** среди известных, кто может кешировать в параллельной генерации.

**Итоги архитектуры:** Eso-LM продемонстрировал, что **симбиотическое объединение AR и диффузии возможно** в рамках одного трансформера. На практике оба варианта модели достигли **лучшей на сегодня перплексии среди диффузионных моделей** (на датасетах LM1B и OpenWebText) и предоставили непрерывный спектр качество/скорость между AR и MDM. Особо отмечается, что Eso-LM (A) может приблизиться по PPL к чисто AR-модели, но генерирует существенно быстрее, а Eso-LM (B) слегка уступает по PPL, зато **работает быстрее всех** (благодаря кешированию). В тестах на скорость генерации Eso-LM превосходил предыдущие диффузионные модели, попадая на новую **Pareto-границу качества и быстродействия** (т. е. ни одна прежняя модель не давала одновременно такой же перплексии при такой же скорости). При малом числе шагов генерации гибрид не страдает коллапсом (в отличие от некоторых упрощённых интерполирующих схем), а при большом числе шагов выдает образцы лучшего качества, чем все прошлые диффузионные LM. Эти результаты делают Eso-LM важным ориентиром в дизайне будущих архитектур: возможно, именно комбинация параллельной черновой генерации с последовательной доводкой позволит достичь наилучшего баланса.

## Математическая и вероятностная формализация

Все рассмотренные диффузионные языковые модели стремятся моделировать *распределение вероятностей над текстом*, аналогично традиционным LLM, но не через авторегрессионное разложение, а посредством **марковского диффузионного процесса**.

Формально цель — максимизировать вероятность $P_\theta(X)$ для последовательности $X = (x_1, \dots, x_L)$ из обучающего корпуса. В авторегрессионных моделях применяется факторизация по токенам:  
$$
P(X) = \prod_{i=1}^L P(x_i \mid x_{<i})
$$  
и обучение сводится к минимизации кросс-энтропии предсказания следующего токена.

<details> 
    <summary><em><strong>Кросс-энтропия (Cross-Entropy)</strong></em></summary>

## Кросс-энтропия (Cross-Entropy Loss)

Функция кросс-энтропии (Cross-Entropy Loss) — это центральная функция потерь в задачах **классификации**, особенно бинарной и многоклассовой. Она тесно связана с **максимизацией логарифма правдоподобия**, а также с фундаментальными концепциями информационной теории.

### 1. Постановка задачи классификации

Пусть дана обучающая выборка:

$$
D = \{(x_i, y_i)\}_{i=1}^n,\quad x_i \in \mathcal{X} \subseteq \mathbb{R}^d,\ y_i \in \{1, 2, \dots, K\}
$$

Цель: найти параметризованную функцию $f_\theta(x)$, которая аппроксимирует распределение вероятностей классов:

$$
f_\theta(x) = \hat{\mathbf{p}}(x) = (\hat{p}_1(x), \hat{p}_2(x), \dots, \hat{p}_K(x)), \quad \sum_{k=1}^K \hat{p}_k(x) = 1,\ \hat{p}_k(x) \ge 0
$$

(например, выход softmax'а).

Пусть $y_i$ — истинный класс, тогда целевая one-hot вектор-метка:

$$
\mathbf{y}_i = (0,\dots, 1, \dots, 0), \text{ где 1 стоит на } y_i\text{-й позиции}
$$

### 2. Определение кросс-энтропии

Кросс-энтропия между истинным распределением $P$ и предсказанным $Q$:

$$
\boxed{
H(P, Q) = -\sum_{k=1}^K P(k) \log Q(k)
}
$$

В контексте supervised learning:

* $P(k) = \mathbb{I}[y_i = k]$ — one-hot распределение;
* $Q(k) = \hat{p}_k(x_i)$ — предсказанная вероятность;

- Если говорить интуитивно, кросс-энтропия помогает нам понять, насколько наши предсказания отличаются от истинных значений;
- Логарифм используется для того, чтобы преобразовать произведение вероятностей в сумму, что упрощает расчёты и делает их более стабильными;
- Знак минус используется, потому что кросс-энтропия учитывает логарифмы вероятностей, которые всегда меньше или равны нуля. Когда мы суммируем эти логарифмы, результат получается отрицательным. Чтобы сделать функцию потерь положительной и минимизировать её, мы добавляем этот минус.

Тогда:

$$
\text{Loss}(x_i, y_i) = - \log \hat{p}_{y_i}(x_i)
$$

А по всей выборке:

$$
\boxed{
\mathcal{L}_{CE}(\theta) = -\frac{1}{n} \sum_{i=1}^n \log \hat{p}_{y_i}(x_i)
}
$$

### 3. Интерпретации

#### (а) Информационная теория

Кросс-энтропия измеряет **среднее количество бит**, необходимое для кодирования истинных меток $P$, если используется кодировка, основанная на распределении $Q$:

* Если $Q \approx P$, то $H(P,Q) \approx H(P)$ — энтропия.
* Если $Q$ сильно отличается от $P$, то $H(P,Q)$ увеличивается.

> ⇒ **Минимизация кросс-энтропии ⇔ максимизация точности предсказания вероятностей.**

#### (б) Вероятностная интерпретация

Предположим, что модель предсказывает вероятности $Q = f_\theta(x)$, и данные метки $y_i$ независимы, тогда:

$$
\log L(\theta) = \sum_{i=1}^n \log P(y_i | x_i, \theta) = \sum_{i=1}^n \log \hat{p}_{y_i}(x_i)
$$

Тогда:

$$
\boxed{
\mathcal{L}_{CE} = - \log L(\theta)
}
$$

То есть, **кросс-энтропия — это отрицательный логарифм правдоподобия**. Поэтому она возникает естественно при выводе из принципа максимального правдоподобия (MLE).

#### (в) Связь с KL-дивергенцией

Напомним, KL-дивергенция:

$$
D_{KL}(P \| Q) = \sum_{k=1}^K P(k) \log \frac{P(k)}{Q(k)} = H(P, Q) - H(P)
$$

При one-hot разметке $P(k) = \delta_{ky}$ ⇒ $H(P)$ = 0 ⇒

$$
\boxed{
D_{KL}(P \| Q) = H(P, Q)
}
$$

<details> 
    <summary><em><strong>Кросс-энтропия vs KL-дивергенция</strong></em></summary>

## 🔍 **Оптимизация кросс-энтропии vs KL-дивергенции: от простого к сложному**

Когда мы работаем с задачами машинного обучения, особенно в классификации, нам часто нужно измерять, насколько предсказания модели отличаются от истинных значений. Два популярных способа сделать это — **кросс-энтропия (Cross-Entropy, CE)** и **KL-дивергенция (Kullback-Leibler Divergence, KLD)**.

На первый взгляд, они кажутся очень похожими, но есть важные различия. Давайте разберёмся по шагам!

### **1️⃣ Что такое энтропия $H(P)$?**

Энтропия распределения $ P $ — это мера его **неопределённости**. Формула:

$$
H(P) = -\sum_{x \in \mathcal{X}} P(x) \log P(x)
$$

где:
- $ \mathcal{X} $ — все возможные токены (слова/символы),
- $ P(x) $ — вероятность токена $ x $ в истинном распределении.

Чем выше $ H(P) $, тем более "размазано" распределение (больше неопределённости).

### **2️⃣ Пример: Next-Token Prediction**

Допустим, у нас есть:
- **Контекст:** `"Кошка лежит на ___"`
- **Возможные следующие токены:** `"ковре"` (0.7), `"полу"` (0.2), `"диване"` (0.1)

Тогда **истинное распределение $ P $** может быть:

#### **🔹 Случай 1: One-hot (детерминированное)**
Если правильный токен только `"ковре"`, то:

$$
P = [1, 0, 0]
$$

Энтропия:

$$
H(P) = - \left( 1 \cdot \log 1 + 0 \cdot \log 0 + 0 \cdot \log 0 \right) = 0
$$

(поскольку $ \lim_{p \to 0} p \log p = 0 $)

**Вывод:**

- $ H(P) = 0 $ → нет неопределённости.
- В этом случае **KLD и CE совпадают**:

$$
D_{KL}(P \| Q) = H(P, Q) - H(P) = H(P, Q)
$$

#### **🔹 Случай 2: Вероятностное (soft)**
Пусть правильные токены имеют вероятности:

$$
P = [0.7, 0.2, 0.1]
$$

Тогда энтропия:

$$
H(P) = - (0.7 \log 0.7 + 0.2 \log 0.2 + 0.1 \log 0.1)
$$

Допустим, логарифм по основанию 2 (биты):

$$
H(P) \approx - (0.7 \cdot (-0.514) + 0.2 \cdot (-2.321) + 0.1 \cdot (-3.321)) \approx 1.157 \text{ бит}
$$

**Что это значит?**
- Энтропия **не нулевая**, значит, есть неопределённость в правильном ответе.
- Если модель предсказывает $ Q = [0.6, 0.3, 0.1] $, то:

$$
D_{KL}(P \| Q) = H(P, Q) - H(P)
$$

Здесь $ H(P, Q) $ — кросс-энтропия, а $ H(P) $ — "базовая" неопределённость данных.

### **3️⃣ Кросс-энтропия (Cross-Entropy, CE)**

#### **🔹 Что это?**

Кросс-энтропия измеряет, насколько "удивительны" предсказания модели относительно истинного распределения. Чем меньше CE, тем лучше модель предсказывает.

#### **🔹 Формула**

Для дискретного случая (например, классификация):

$$
H(P, Q) = -\sum_{i} P(i) \log Q(i)
$$

где:
- $ P $ — истинное распределение (обычно one-hot encoded, например, $[0, 1, 0]$ для класса 2).
- $ Q $ — предсказанное распределение (например, $[0.1, 0.8, 0.1]$).

#### **🔹 Особенности**
✅ **Простота**: в ML чаще используют CE, потому что если $ P $ — one-hot, то формула упрощается до $ -\log Q(\text{true class}) $.  
✅ **Эффективность**: градиенты легко вычисляются, что ускоряет обучение.

## **4️⃣ KL-дивергенция (Kullback-Leibler Divergence, KLD)**

#### **🔹 Что это?**

KLD измеряет, насколько одно распределение $ Q $ отличается от другого $ P $. Это **не метрика расстояния** (не симметрична: $ D_{KL}(P \| Q) \neq D_{KL}(Q \| P) $).

#### **🔹 Формула**

$$
D_{KL}(P \| Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)} = H(P, Q) - H(P)
$$

где:
- $ H(P, Q) $ — кросс-энтропия между $ P $ и $ Q $,
- $ H(P) $ — энтропия $ P $ (мера неопределённости).

#### **🔹 Особенности**
✅ **Информационная разница**: KLD показывает, сколько дополнительных бит информации нужно для кодирования $ P $, если использовать $ Q $.  
❌ **Зависит от $ H(P) $**: если $ P $ фиксировано (например, one-hot), то $ H(P) = 0 $, и KLD становится равным CE!

### **5️⃣ Связь между CE и KLD**

Из формулы KLD видно:
$$
D_{KL}(P \| Q) = H(P, Q) - H(P)
$$

#### **🔹 Если $ P $ — one-hot (как в классификации):**
- $ H(P) = 0 $ (энтропия детерминированного распределения нулевая),
- Тогда **KLD = CE**!

#### **🔹 Если $ P $ не one-hot (например, сглаженные метки):**
- $ H(P) > 0 $, значит, KLD и CE различаются.
- Оптимизация KLD учитывает ещё и энтропию $ P $, а CE — нет.

### **6️⃣ Когда что использовать?**

| **Критерий**       | **Кросс-энтропия (CE)** | **KL-дивергенция (KLD)** |
|--------------------|------------------------|--------------------------|
| **One-hot метки**  | ✅ Лучше (проще и быстрее) | ⚠️ То же самое (KLD = CE) |
| **Сглаженные метки** | ❌ Не учитывает $ H(P) $ | ✅ Учитывает разницу распределений |
| **Интерпретация**  | "Удивление" модели | "Информационная стоимость" ошибки |

#### **🔹 Практический вывод:**
- **В большинстве задач классификации CE и KLD эквивалентны** (так как метки one-hot).
- **Если метки вероятностные (например, soft targets в Distillation) — KLD лучше**, так как учитывает энтропию истинного распределения.

### **🎯 Итог**

- **Кросс-энтропия** — это "удивление" модели относительно истинных меток.
- **KL-дивергенция** — это "стоимость" использования $ Q $ вместо $ P $.
- **Если $ P $ детерминировано (one-hot) → CE = KLD.**
- **Если $ P $ вероятностно → KLD учитывает его энтропию.**

Теперь вы знаете разницу и можете осознанно выбирать функцию потерь! 🚀

</details>

### 4. Частные случаи

#### 🔹 Бинарная кросс-энтропия (логистическая регрессия)

Если $y_i \in {0, 1}$ и $f(x_i) = \hat{p}_i \in (0, 1)$, тогда:

$$
\mathcal{L}_{BCE} = -\frac{1}{n} \sum_{i=1}^n \left[y_i \log \hat{p}_i + (1 - y_i)\log(1 - \hat{p}_i)\right]
$$

### 5. Свойства кросс-энтропии

| Свойство              | Описание                                                                        |
| --------------------- | ------------------------------------------------------------------------------- |
| 📈 Выпуклость         | Если $\hat{p}*k$ — аффинная функция параметров, $\mathcal{L}*{CE}$ выпукла. |
| ⚙️ Гладкость          | Дифференцируема по $\hat{p}_k$, подходит для градиентного спуска.            |
| 🎯 Интерпретируемость | Потери равны $-log$ предсказанной вероятности правильного класса.            |
| ⚠️ Чувствительность   | Наказывает сильнее за большую уверенность в неправильных классах.               |

> **Пример:** если правильный класс предсказан с вероятностью 0.9: $-log(0.9) ≈ 0.105$, а если 0.01 — $-\log(0.01) ≈ 4.6$.

### 6. Градиент кросс-энтропии

Рассмотрим softmax выход модели:

$$
\hat{p}_k = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}},\quad z_k = \text{логиты}
$$

И функцию потерь:

$$
\mathcal{L}_{CE}(z, y) = -\log \hat{p}_y
$$

Градиент по $z_j$:

$$
\frac{\partial \mathcal{L}}{\partial z_j} = \hat{p}_j - \mathbb{I}[j = y]
$$

То есть:

$$
\nabla_z \mathcal{L} = \hat{\mathbf{p}} - \mathbf{y}
$$

> 💡 Это очень удобно: градиент — просто разность между предсказанным распределением и истинным one-hot вектором.

### 7. Практические аспекты

**Когда применять:**

* Классификация (бинарная / многоклассовая / multi-label).
* Когда важно корректно оценивать вероятности.
* При выводе через MLE.

**Когда избегать:**

* Задачи с шумной разметкой: кросс-энтропия чувствительна к label noise.
* Имбаланс классов без корректировки (возможна переоценка частых классов).

### 8. Заключение

Кросс-энтропия — фундаментальная функция потерь в классификации, строго выведенная из теории вероятностей и информационной теории. Её выпуклость, простота градиента и интерпретируемость делают её предпочтительным выбором в большинстве задач обучения с учителем. Однако чувствительность к переуверенным ошибкам требует внимательного контроля и настройки моделей.

> 🧪 В реальных задачах часто применяют её модификации: **focal loss** (для борьбы с дисбалансом), **label smoothing**, **soft targets**, **weighted CE** и др.

</details>

---

В диффузионных же моделях вводится *скрытая переменная* — **индекс шага диффузии $t$**, и определяются два процесса:
- **прямой (forward)** $q$ — постепенное добавление шума к данным,
- **обратный (reverse)** $p_\theta$ — обучаемый процесс удаления шума.

Forward-процесс строится так, что при $t = T$ данные полностью разрушены (например, все токены замаскированы или заменены на равномерный шум), а при $t = 0$ — данные чистые (оригинальный текст). Конкретно, на каждом шаге $t \to t-1$ мы можем определять распределение $q(x_{t-1} \mid x_t, X)$, где $X$ — исходные данные.

В дискретных диффузионных моделях обычно используется **маскирование** как вид шума: с некоторой возрастающей вероятностью $\beta_t$ каждый токен заменяется на `[MASK]` независимо. В пределе $t = T$ получаем полностью замаскированную последовательность (полный шум). Этот forward-процесс $q$ известен точно (задано распределение масок). Согласно теории диффузии, *апостериорное* распределение $q(x_{t-1} \mid x_t, X)$ тоже может быть получено в аналитическом виде (для маскирования оно пропорционально либо дельта-функции на истинном токене, либо на маске).

Обратный процесс $p_\theta$ параметризуется нейросетью. Он должен аппроксимировать распределение восстановления: $p_\theta(x_{t-1} \mid x_t)$ — вероятность получить менее зашумленное состояние на шаге $t-1$, исходя из состояния на шаге $t$. Чтобы сделать задачу tractable, обычно предполагается *полная факторизация* этого распределения по позициям, содержащим шум на шаге $t$. Например, если на шаге $t$ замаскированы индексы $\mathcal{M}_t$, то  
$$
p_\theta(x_{t-1} \mid x_t) = \prod_{i\in \mathcal{M}_t} p_\theta(x^{(i)}_{t-1} \mid x_t)
$$  
— модель предсказывает каждый замаскированный токен независимо (условно на текущее состояние всей последовательности $x_t$). Это в точности эквивалентно задаче **маскированного языкового моделирования** на текущем контексте $x_t$.

<details> 
    <summary><em><strong>Суть диффузионной модели для текста</strong></em></summary>

## **Суть диффузионной модели для текста:**

Представьте, что у вас есть осмысленный текст. Процесс **"диффузии"** постепенно **портит** этот текст, например, заменяя случайные слова (токены) на "маски" (`[MASK]`) или просто на случайный шум. Это похоже на то, как будто кто-то стирает слова на странице. Конечная цель модели — научиться выполнять **обратный процесс**: взять этот зашумленный, "испорченный" текст и **восстановить** изначальный осмысленный вариант.

**Ключевой компонент: обратный процесс ($p_\theta$)**

*   **Что это?** это "мозг" модели восстановления. Это нейронная сеть (параметризованная весами $\theta$), которая **учится предсказывать, как выглядел текст на предыдущем, *менее* зашумленном шаге ($x_{t-1}$), исходя из текущего, *более* зашумленного состояния ($x_t$)**.
*   **Формально:** $p_\theta(x_{t-1} \mid x_t)$ — это *условное распределение вероятностей*. Оно говорит: "Если текущий текст выглядит как $x_t$ (содержит какие-то шумы/маски), то какова вероятность того, что на предыдущем шаге он выглядел как $x_{t-1}$?".
*   **Цель обучения:** настроить параметры $\theta$ нейронной сети так, чтобы это распределение $p_\theta$ было как можно ближе к *истинному* (но неизвестному нам) распределению восстановления $q(x_{t-1} \mid x_t)$.

**Проблема: слишком сложно!**

*   Предсказывать *весь* вектор $x_{t-1}$ (который представляет всю последовательность слов/токенов) сразу, на основе $x_t$ — это *чрезвычайно* сложная задача. Это потребовало бы от модели учитывать все возможные комбинации слов во всей последовательности одновременно. Вычислительно это неподъемно.

**Решение: полная факторизация (условная независимость)**

*   Чтобы сделать задачу **решаемой (tractable)**, вводится ключевое **допущение**:
    > **Предположим, что предсказание каждого *отдельного* зашумленного токена на позиции $i$ для шага $t-1$ ($x^{(i)}_{t-1}$) зависит *только* от *текущего* зашумленного состояния *всей* последовательности ($x_t$), но *НЕ* зависит от того, что мы предскажем для *других* зашумленных позиций ($j \neq i$) на этом *же* шаге $t-1$.**

*   **Проще говоря:** модель смотрит на *весь* текущий зашумленный текст $x_t$ (включая известные *незашумленные* слова и текущие маски/шумы). На основе этого **полного контекста** $x_t$ она **независимо** предсказывает, что должно стоять на месте *каждой отдельной* маски/шума, чтобы получить $x_{t-1}$.
*   **Аналогия 1 (Кроссворд):** представьте, что $x_t$ — это кроссворд, где некоторые клетки заполнены буквами (незашумленные токены), а некоторые пусты (маски `[MASK]`). Модель смотрит на *весь* кроссворд — и на пересечения слов — и предсказывает, какая буква должна быть в *каждой* пустой клетке ($x^{(i)}_{t-1}$). Предположение факторизации говорит: предсказание буквы для клетки (1,1) зависит от *всех* уже заполненных клеток вокруг нее, но *не* зависит напрямую от того, что вы предскажете *одновременно* для клетки (5,5). Вы предсказываете их независимо, но каждое предсказание опирается на *один и тот же* общий контекст (весь текущий кроссворд $x_t$).
*   **Аналогия 2 (Шахматы):** представьте шахматную доску $x_t$, где некоторые фигуры стоят на своих начальных позициях (незашумленные токены), а некоторые клетки пусты (маски). Модель должна восстановить предыдущее состояние доски $x_{t-1}$, где на пустых клетках могли стоять фигуры. Она смотрит на *всю* текущую доску (расположение оставшихся фигур) и **независимо** решает, какая фигура *наиболее вероятно* стояла на *каждой* пустой клетке перед тем, как ее убрали. Решение для одной клетки зависит от общего положения, но не зависит напрямую от решения для другой клетки *в этот самый момент*.

**Математическое выражение:**

*   Обозначим $\mathcal{M}_t$ как **множество индексов позиций** в последовательности, которые **зашумлены** (замаскированы или искажены) на текущем шаге $t$.
*   Допущение полной факторизации позволяет записать сложное совместное распределение восстановления как **произведение** независимых распределений для **каждой зашумленной позиции**:
    $$
    p_\theta(x_{t-1} \mid x_t) = \prod_{i \in \mathcal{M}_t} p_\theta(x^{(i)}_{t-1} \mid x_t)
    $$
*   **Что это значит:**
    *   $\prod_{i \in \mathcal{M}_t}$: перемножаем вероятности по всем позициям $i$, которые зашумлены на шаге $t$.
    *   $p_\theta(x^{(i)}_{t-1} \mid x_t)$: вероятность того, что на позиции $i$ в *менее* зашумленном состоянии $x_{t-1}$ находится *конкретный* токен (слово, буква) $x^{(i)}_{t-1}$, **при условии, что мы видим *весь* текущий зашумленный текст $x_t$**.
    *   **Ключевой момент:** распределение для позиции $i$ ($p_\theta(x^{(i)}_{t-1} \mid x_t)$) зависит **ТОЛЬКО** от $x_t$ (от всего текущего контекста), но **НЕ** зависит от того, что модель предскажет для $x^{(j)}_{t-1}$ (для другой зашумленной позиции $j$) при расчете *этого же* распределения $p_\theta(x_{t-1} \mid x_t)$. Они считаются **условно независимыми** при фиксированном $x_t$.

**Связь с Masked Language Modeling (MLM) a la BERT:**

*   **Это самая важная аналогия!** посмотрите внимательно на $p_\theta(x^{(i)}_{t-1} \mid x_t)$.
*   **Что это?** это задача предсказания **одного** токена (того, что был на позиции $i$ в $x_{t-1}$) на основе **всего** текущего контекста $x_t$.
*   **Как это выглядит на практике?** на шаге $t$ у нас есть последовательность $x_t$, в которой на позициях $\mathcal{M}_t$ стоят маски `[MASK]` (или другие символы шума). Нейронная сеть ($p_\theta$) принимает на вход $x_t$ и для **каждой** позиции $i$ из $\mathcal{M}_t$ выдает распределение вероятностей ($p_\theta(x^{(i)}_{t-1} \mid x_t)$) по *всему* словарю — какое слово/токен с какой вероятностью должно стоять *вместо этой конкретной маски*, чтобы получить состояние $x_{t-1}$.
*   **Это в точности задача Masked Language Modeling (MLM)!** та самая, на которой обучаются такие модели, как BERT. BERT получает на вход текст с масками и учится предсказывать оригинальные слова под масками, используя контекст *всего* предложения (и $x_t$ в диффузии — это и есть такой контекст с масками).

**Оптимизация (Обучение) с помощью Cross-Entropy:**

*   Как мы обучаем модели типа BERT решать задачу MLM? Мы используем **кросс-энтропийную потерю (cross-entropy loss)**.
*   **Как это работает в диффузии:**
    1.  Во время обучения, для **каждого** шага диффузии $t$ и для **каждой** зашумленной позиции $i \in \mathcal{M}_t$ на этом шаге, мы знаем **оригинальный, правильный токен**, который был на этой позиции *до* наложения шума (это и есть $x^{(i)}_{t-1}$).
    2.  Нейронная сеть ($p_\theta$) для позиции $i$ выдает **предсказанное распределение вероятностей** $p_\theta(x^{(i)}_{t-1} \mid x_t)$ по всем возможным токенам.
    3.  Мы вычисляем **кросс-энтропию** между:
        *   **Идеальным распределением:** вероятность 1.0 для *правильного* токена и 0.0 для всех остальных.
        *   **Предсказанным распределением:** $p_\theta(x^{(i)}_{t-1} \mid x_t)$ от модели.
    4.  Эта кросс-энтропия измеряет, насколько хорошо модель предсказала правильный токен *для этой конкретной позиции $i$*.
    5.  Потери (loss) со *всех* зашумленных позиций $i \in \mathcal{M}_t$ на шаге $t$ **суммируются** (или усредняются). Это и есть общая потеря для шага $t$.
    6.  Градиенты этого общего лосса распространяются назад по нейронной сети, обновляя ее веса $\theta$, чтобы улучшить предсказания.
*   **Итог:** обучение обратного процесса диффузии на *каждом* шаге $t$ **сводится к решению множества независимых задач Masked LM на контексте $x_t$**, и оптимизируется это с помощью привычной **кросс-энтропии** для каждой маски.

**Ключевые выводы:**

1.  **Обратный процесс ($p_\theta$)** — это нейросеть, которая учится "чинить" текст шаг за шагом.
2.  **Факторизация ($p_\theta = \prod p_\theta(...)$)** — это *необходимое упрощение*, делающее обучение возможным. Оно означает: "Предсказывай каждую маску независимо, но используй для каждой предсказания *весь* текущий текст".
3.  **$p_\theta(x^{(i)}_{t-1} \mid x_t)$** — это **ядро процесса**. Это ровно та задача, которую решает BERT (Masked LM): "Что скрывается под этой конкретной маской `[MASK]` в данном контексте $x_t$?".
4.  **Cross-Entropy** — это стандартный и эффективный способ *обучить* нейросеть решать множество таких задач MLM *параллельно* на одном шаге диффузии $t$.

</details>

---

Таким образом, *каждый шаг диффузии оптимизируется с помощью привычной cross-entropy* на правильный токен вместо маски. Однако в отличие от BERT, здесь маскирование применяется многократно и на разных уровнях, поэтому вводятся весовые коэффициенты на каждом шаге. В итоге функция потерь принимает вид **Negative ELBO (NELBO)** — отрицательной вариационной нижней оценки лог-правдоподобия данных. В работе Sahoo et al. (2024) для дискретной диффузии было выведено выражение NELBO через сумму потерь на маскированные позиции:

$$
ELBO = \mathcal{L}_{\text{diff}} = \sum_{t=1}^T w_t \, \mathbb{E}_{x_t \sim q}\left[ -\log p_\theta\left(x_{t-1}^{\mathcal{M}_t} = X^{\mathcal{M}_t} \mid x_t\right) \right],
$$

где:
- $X^{\mathcal{M}_t}$ — истинные токены на позициях, замаскированных в состоянии $x_t$;
- Коэффициенты $w_t$ зависят от выбранного расписания шума (например, $w_t = 1$ для всех $t$ в простейшем случае, либо растут/убывают, отражая важность каждого шага).

Интуитивно модель учится *одновременно предсказывать маски разной "глубины"* — когда замаскировано 10% текста, 20%, … 100%. В пределе $t = T$ она решает задачу «угадай весь текст целиком по контексту = нулю» (что почти невозможно, но этот термин обучает сеть выдавать разумное априорное распределение).

Все описанные модели (Gemini, Mercury, LLaDA) следуют этой парадигме с некоторыми вариациями. Например, **Mercury** через концепцию *score entropy* фактически тоже оптимизирует аналог ELBO, но не напрямую через лог-правдоподобия токенов, а через обучение *восстановления соотношений вероятностей* (то есть вместо предсказания $P(y)$ модель оценивает $\log \frac{P(y)}{P(x)}$ для токенов $y$ и текущего $x$). Было показано, что этот подход эквивалентен новой формулировке задачи score matching в дискретном пространстве и обеспечивает более стабильное обучение, чем прямое предсказание токенов. Тем не менее результат тот же: Mercury обучается восстанавливать маскированную последовательность за несколько шагов, оптимизируя максимальное правдоподобие (через ELBO) и используя виды кросс-энтропийного loss для обновления весов.

Отдельно стоит отметить, что **Eso-LM** ввел новую модель правдоподобия, объединяющую AR и диффузию. Если обозначить через $z_0$ частично сгенерированную MDM-последовательность (с масками), а через $x$ — финальный текст, то полное распределение разложения можно записать как смешанное:

$$
P_\theta(x) = \sum_{z_0} P_\theta(x \mid z_0)\,P_\theta(z_0),
$$

где:
- $P_\theta(z_0)$ — вероятность получить черновик $z_0$ с помощью диффузионной части, а $P_\theta(x \mid z_0)$ — вероятность дописать его до $x$ AR-моделью. Вычислять эту сумму напрямую невозможно, поэтому авторы применили вариационный подход: ввели простое апостериорное распределение (которое маскирует случайные токены у полного $x$, чтобы получить $z_0$) и получили **ELBO для гибридного генератора**.

Интересно, что итоговая функция потерь опять распадается на два слагаемых:
1. NELBO диффузионной части (сумма masked LM потерь по шагам, как выше),
2. Обычная потеря автодополнения на токены, которые остаются на AR-этапе (тоже кросс-энтропия).

Это означает, что Eso-LM можно обучать единой процедурой end-to-end: на каждом примере сначала применить **стохастическое маскирование** (чтобы отделить будущие AR-токены), затем имитировать диффузию для восстановления остальных, и наконец добавить loss AR-модели на оставшиеся токены. Такой подход сохранил обоснованность с точки зрения вероятности (имеется нижняя оценка лог-правдоподобия) и позволил эффективно тренировать единый трансформер на комбинированную задачу.

В целом, математически диффузионные LLM расширяют пространство решений для моделирования $P(X)$. Они подтверждают общий принцип, что **ключевые свойства LLM (масштабируемость, обучение в контексте, следование инструкциям)** обеспечиваются не конкретно авторегрессией, а более фундаментально — мощью генеративного моделирования с максимизацией правдоподобия. Диффузионные модели, оптимизируя ELBO, реализуют те же принципы, только через иную факторизацию. Отличие лишь в том, что AR-модели — частный случай ($T = L$, каждое $x_{t-1}$ содержит один новый токен), а DLM — общий вариант ($T < L$ или даже $T \ll L$, при параллельном обновлении).