# Eso-LM от NVIDIA: как гибрид диффузионного и авторегрессивного подходов меняет NLP
**Оглавление**

1. [Введение](#введение)  
2. [Предпосылки и мотивация](#предпосылки-и-мотивация)  
3. [Основная методология](#основная-методология)  
4. [Дизайн Механизма Внимания](#механизмы-внимания-и-кэширование-ключ-значение)  
5. [Экспериментальные результаты](#экспериментальные-результаты)  
6. [Значимость и влияние](#значимость-и-влияние)

## 1. Введение

Эзотерические языковые модели (Eso-LMs) представляют собой значительный прорыв в генеративном языковом моделировании, впервые успешно объединив парадигмы авторегрессионных (AR) и моделей маскированной диффузии (MDM). В то время как авторегрессионные модели, такие как GPT, превосходны в качестве генерации, но страдают от медленного последовательного вывода, модели маскированной диффузии предлагают возможности параллельной генерации, но традиционно отстают по показателям перплексии и не имеют эффективных механизмов кеширования. Эта работа устраняет эти фундаментальные ограничения, предлагая унифицированную структуру, которая сочетает в себе сильные стороны обоих подходов, минимизируя при этом их соответствующие недостатки.

![Figure_01](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-26/assets/Figure_01.png)

*Рисунок 1: Процесс генерации Eso-LM иллюстрирует двухэтапную процедуру семплирования. Фаза диффузии (оранжевый) постепенно удаляет шум из токенов параллельно, в то время как последовательная фаза (зеленый) заполняет оставшиеся маскированные токены авторегрессивно с богатым обусловливанием как от левого контекста, так и от чистых токенов, обнаруженных во время диффузии.*

## 2. Предпосылки и мотивация

Ландшафт языкового моделирования доминируют авторегрессионные модели, которые генерируют текст последовательно слева направо. Хотя эти модели достигают отличных показателей перплексии, их последовательная природа ограничивает скорость вывода и гибкость. Модели маскированной диффузии появились как альтернатива, предлагая параллельную генерацию токенов и улучшенную управляемость, но они сталкиваются с двумя критическими проблемами: более медленный вывод из-за двунаправленного внимания, которое предотвращает кеширование KV, и заметный разрыв в качестве по сравнению с AR-моделями.

Недавние гибридные подходы, такие как модели дискретного языка с блочным шумоподавлением и диффузией (BD3-LMs), попытались преодолеть этот разрыв, объединив авторегрессионное блочное моделирование с внутриблочной диффузией. Однако эти методы страдают от коллапса мод при низких шагах семплирования и предоставляют лишь частичные преимущества кеширования. Исследование определяет эти ограничения как ключевые барьеры для практического внедрения языковых моделей на основе диффузии.

## 3. Основная методология

### Гибридная Цель Обучения

Eso-LMs представляют новую структуру обучения, которая плавно интерполирует между целями AR и MDM посредством гибридной функции потерь. Ключевое новшество заключается в формулировке вариационной границы:

![Figure_02](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-26/assets/Figure_02.png)

Эта потеря объединяет авторегрессионный член (вычисляемый для изначально маскированных токенов) с членом маскированной диффузии (взвешенное среднее по прогрессивно зашумленным последовательностям). Гиперпараметр $α_0$ контролирует интерполяцию: $α_0=1$ дает чистое поведение MDM, тогда как $α_0=0$ приводит к чистому поведению AR.

### Двухэтапный Процесс Семплирования

Генерация проходит в два различных этапа:

1. **Фаза диффузии:** начиная с полностью маскированной последовательности, модель постепенно удаляет шум из подмножества токенов параллельно, создавая частично маскированную последовательность $z_0$. Ключевая оптимизация обрабатывает только чистые токены и те, которые запланированы для шумоподавления на каждом шаге, значительно сокращая вычислительные затраты.

2. **Последовательная фаза:** оставшиеся маскированные токены заполняются авторегрессивно слева направо, при этом каждый токен обуславливается как своим левым контекстом, так и чистыми токенами, обнаруженными во время фазы диффузии.

## 4. Дизайн Механизма Внимания

В статье представлены два варианта с различными механизмами внимания:

**Eso-LM (A)** использует двунаправленное внимание между чистыми токенами во время обучения диффузии, при этом применяя причинное внимание к маскированным токенам. Во время последовательной фазы он использует пользовательский шаблон внимания, позволяющий маскированным токенам обращать внимание на себя, чистый левый контекст и чистый правый контекст из $z_0$.

**Eso-LM (B)** расширяет принципы причинного внимания, позволяя использовать полное KV-кеширование на обеих фазах. Он обеспечивает причинное внимание между всеми токенами на основе случайных перестановок во время обучения диффузии, поддерживая согласованность между паттернами внимания при обучении и выводе.

## 5. Экспериментальные результаты

### Качество генерации

Eso-LM достигают современного уровня перплексии среди дискретных диффузионных моделей на стандартных бенчмарках. На наборе данных One Billion Words, Eso-LM (A) превосходит предыдущие модели MDLM примерно на 1 PPL даже в чистой конфигурации MDM ($a_0$ = 1). Возможность плавной интерполяции позволяет точно настраивать компромисс между качеством и скоростью: более низкие значения $a_0$ (более похожие на AR) обычно дают лучшую перплексию, приближаясь к производительности чистых авторегрессионных моделей.

### Эффективность вывода

Прорывное достижение состоит в обеспечении KV-кеширования для маскированных диффузионных моделей. Eso-LM (B) демонстрирует выдающееся ускорение по сравнению со стандартными MDM:

- в 14 раз быстрее для длины контекста L=2048
- в 65 раз быстрее для длины контекста L=8192

По сравнению с предыдущими полуавторегрессионными подходами, Eso-LM (B) показывает существенные улучшения:

- в 3,2 раза быстрее, чем BD3-LM (L'=16)
- в 3,8 раза быстрее, чем BD3-LM (L'=4) при L=8192

### Парето-фронтир "скорость-качество"

![Figure_03](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-26/assets/Figure_03.png)

*Рисунок 2: Парето-фронтир, сравнивающий перплексию генерации со средней продолжительностью выборки. Eso-LM устанавливают новый современный уровень по всему спектру "скорость-качество", при этом различные значения $a_0$ обучения предлагают оптимальные компромиссы для разных вычислительных бюджетов.

Eso-LM устанавливают новый современный уровень на Парето-фронтире "скорость-качество". В отличие от BD3-LM, которые страдают от серьезного коллапса мод при низких значениях NFE, Eso-LM поддерживают конкурентоспособную производительность при любых бюджетах выборки, демонстрируя превосходную надежность и стабильность.

###  Технические инновации

**KV-кеширование для диффузионных моделей**

Наиболее значительным техническим вкладом является успешная реализация KV-кеширования для маскированных диффузионных моделей с сохранением их возможностей параллельной генерации. Этот прорыв устраняет фундаментальное ограничение, которое препятствовало практическому применению диффузионных моделей для генерации языка.

**Оптимизированный прямой проход**

Во время диффузионной выборки Eso-LM обрабатывают только подмножество токенов, фактически требуемых на каждом шаге, избегая вычислений над будущими маскированными токенами, не запланированными для денойзинга. Эта оптимизация в сочетании с тщательным дизайном паттерна внимания значительно способствует наблюдаемому ускорению.

###  6. Значимость и влияние

Eso-LM представляют собой сдвиг парадигмы в генеративном языковом моделировании, успешно объединяя два ранее отдельных подхода. Работа демонстрирует, что традиционный компромисс между качеством генерации и эффективностью вывода может быть преодолен с помощью тщательного архитектурного дизайна и методологии обучения.

Введение KV-кеширования для диффузионных моделей имеет глубокие последствия для практического развертывания этих моделей. Продемонстрированные ускорения делают языковые модели на основе диффузии жизнеспособными для приложений реального времени и задач генерации длинного контекста, где они ранее были непрактичны.

Помимо непосредственного прироста производительности, это исследование открывает новые возможности для будущей работы в области гибридных генеративных архитектур. Плавная интерполяция между поведениями AR и MDM обеспечивает гибкую основу для разработки моделей, адаптированных к конкретным требованиям приложений, будь то приоритет качества, скорости или управляемости.
