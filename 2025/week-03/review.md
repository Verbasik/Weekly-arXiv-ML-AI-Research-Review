# Брифинг-документ: rStar-Math

## Математическая революция глубокого мышления: как rStar-Math превращает небольшие языковые модели в мастера математических рассуждений

### Аннотация

В данной статье представлен метод rStar-Math, демонстрирующий способность малых языковых моделей (SLM) достигать конкурентоспособных результатов, сопоставимых и даже превосходящих показатели модели OpenAI o1 в задачах математического рассуждения, без использования дистилляции знаний из более крупных моделей. Ключевой особенностью rStar-Math является применение "глубокого мышления" посредством поиска по дереву Монте-Карло (MCTS), где SLM выступает в роли модели политики, генерируя последовательность шагов решения, а другая SLM оценивает их, действуя как модель вознаграждения за процесс. Представлены три ключевые инновации: метод синтеза данных CoT с расширением кода, новый подход к обучению модели предпочтения процессов (PPM) и стратегия саморазвития. **Главная идея заключается в применении "глубокого мышления" посредством поиска Монте-Карло (MCTS), где SLM выступает в роли модели политики, генерируя последовательность шагов решения, а другая SLM-модель оценивает их, действуя как модель вознаграждения.** Экспериментальные результаты показывают значительное улучшение математических способностей SLM, подтверждая эффективность предложенного подхода.

### 1. Введение

Современные большие языковые модели (LLM) демонстрируют впечатляющие возможности в различных задачах, включая математическое рассуждение. Однако их значительные размеры и вычислительные требования создают препятствия для широкого применения. В связи с этим, возрастает интерес к разработке эффективных методов обучения малых языковых моделей (SLM), способных решать сложные задачи, сохраняя при этом вычислительную эффективность. В области математического рассуждения, традиционные подходы к обучению SLM часто уступают LLM. В данной работе мы представляем rStar-Math, новый метод, позволяющий SLM достигать результатов, сравнимых и даже превосходящих возможности передовых моделей, таких как OpenAI o1, в задачах математического рассуждения. Ключевой идеей rStar-Math является внедрение механизма "глубокого мышления" посредством поиска по дереву Монте-Карло (MCTS) и итеративного самосовершенствования моделей. **Документ представляет rStar-Math — метод, позволяющий небольшим языковым моделям (SLM) достигать результатов, сопоставимых или даже превосходящих результаты OpenAI o1 в задачах математического рассуждения.**

### 2. Методология rStar-Math

Метод rStar-Math основан на трех ключевых инновациях, направленных на эффективное обучение SLM для математического рассуждения:

### Визуальные идеи

![Figure_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-03/assets/Figure_1.png)

> На этом рисунке представлен общий обзор системы rStar-Math. Он иллюстрирует три ключевых нововведения, которые позволяют малым языковым моделям (SLM) овладевать математическими рассуждениями: (a) показывает генерацию пошаговых траекторий рассуждений с помощью поиска по дереву Монте-Карло (MCTS), где каждый шаг проверяется с помощью выполнения кода Python, что гарантирует правильность; (b) изображает построение пар предпочтений на каждом шаге на основе Q-значений, полученных из MCTS, что позволяет обучать модель предпочтений процесса (PPM) без необходимости ручных аннотаций на уровне шагов; (c) показывает итеративный, саморазвивающийся процесс, где политика SLM и PPM многократно обучаются и совершенствуются с использованием все более высококачественных данных, генерируемых системой.

#### 2.1. Саморазвитие (Self-evolution)

rStar-Math использует процесс саморазвития, в котором модель политики (policy SLM), генерирующая шаги решения, и модель предпочтения процессов (PPM), оценивающая эти шаги, итеративно улучшаются, начиная с нуля. В течение нескольких раундов саморазвития, модели обучаются на миллионах синтезированных решений для большого набора математических задач. Этот итеративный процесс позволяет SLM постепенно наращивать свои возможности в математическом рассуждении, достигая уровня современных моделей. Например, в рамках исследования было проведено четыре раунда саморазвития, в ходе которых были сгенерированы миллионы решений для 747 тысяч математических задач. **В течение 4-х раундов саморазвития генерируются миллионы решений для 747 тыс. математических задач. Этот подход позволяет SLM повысить свои возможности в математических рассуждениях до уровня современных моделей.** Процесс синтеза этих решений является итеративным и включает несколько ключевых моментов:

*   **Итеративное улучшение:** В каждом раунде модели (policy SLM и PPM) улучшаются на основе данных, сгенерированных в предыдущем раунде.
*   **Использование поиска по дереву Монте-Карло (MCTS):** Для генерации траекторий рассуждений применяется алгоритм MCTS, разбивающий сложные задачи на более простые одношаговые задачи. MCTS автоматически присваивает Q-значение каждому шагу, основанное на его вкладе в правильный окончательный ответ.
*   **Метод синтеза данных CoT с расширением кода:** На каждом шаге SLM генерирует текстовое описание рассуждения (CoT) и соответствующий код Python.
*   **Модель предпочтения процессов (PPM):** PPM оценивает каждый шаг рассуждения на основе пар предпочтений, полученных из Q-значений MCTS.
*   **Генерация множества траекторий:** Для каждой задачи генерируется множество траекторий рассуждений для обеспечения разнообразия данных.
*   **Фильтрация и отбор:** Траектории, приводящие к правильному ответу, отбираются для обучения модели политики. Использование кода позволяет отфильтровать некачественные шаги.

> **Цитата:**
> *"rStar-Math introduces a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities."*

#### 2.2. Новый метод синтеза данных CoT с расширением кода (Code-augmented CoT)

![Figure 2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-03/assets/Figure_2.png)

> Исследователи потребовали, чтобы модель включала ответы на естественном языке в виде комментариев к коду Python, и только те выходные данные, которые получены с использованием Python, будут использоваться для обучения модели.

Для обучения модели политики используется поиск по дереву Монте-Карло (MCTS) для генерации траекторий рассуждений с пошаговой верификацией. На каждом шаге SLM генерирует не только текстовое описание рассуждения (Chain-of-Thought, CoT), но и соответствующий код Python, реализующий этот шаг. Для обеспечения качества генерируемых данных, сохраняются только те узлы дерева поиска, где выполнение сгенерированного кода Python было успешным. Этот подход позволяет отфильтровать ошибочные промежуточные шаги рассуждения, обеспечивая высокое качество обучающих данных. Таким образом, формируются "траектории рассуждения с расширенным кодом", где каждый шаг подкреплен исполняемым кодом, что повышает надежность процесса обучения. **Для обучения модели политики используется MCTS для генерации траекторий рассуждений с пошаговой верификацией. На каждом шаге SLM генерирует один шаг рассуждения (CoT) и соответствующий код Python. Для проверки качества генерации сохраняются только узлы с успешно выполненным кодом Python, что снижает количество ошибок на промежуточных шагах.** В случаях, когда сгенерированный код Python на каком-либо шаге не выполняется, соответствующий узел в дереве поиска отбрасывается. Такой подход фильтрует ошибочные или некорректные промежуточные шаги рассуждения, сохраняя только траектории, где каждый шаг подтвержден успешным выполнением кода. Хотя это может привести к потере потенциально полезных, но "закодированных" иным способом рассуждений, rStar-Math намеренно делает акцент на исполняемом коде как на критерий качества, особенно важный для математических рассуждений, включающих точные вычисления. Это компромисс между полнотой данных и их качеством, где приоритет отдается точности и верифицируемости каждого шага.

> **Цитата:**
> *"a novel code-augmented CoT data synthesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories with self-annotated MCTS Q-values."*

#### 2.3. Новая модель предпочтения процессов (Process Preference Model, PPM)

В качестве модели вознаграждения используется модель предпочтения процессов (PPM), которая оценивает каждый шаг рассуждения. Обучение PPM строится на основе пар предпочтений (положительных и отрицательных шагов), полученных на основе Q-значений, присваиваемых MCTS. В отличие от моделей вознаграждения за результат (ORM), которые оценивают только конечный ответ, PPM оценивает качество каждого промежуточного шага, что позволяет более эффективно направлять процесс обучения. Использование пар предпочтений позволяет избежать прямого использования зашумленных Q-значений в качестве меток вознаграждения, что повышает стабильность и эффективность обучения. Функция потерь для PPM определяется как:

$$
L_{ppm}(\theta) = -\frac{1}{2 \times N} \sum_{(x, y_{pos}, y_{neg}) \in D} \log \left( \sigma \left( r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle \right) \right)
$$

где:
- $( \theta )$ — параметры модели
- $( N )$ — количество примеров
- $( x )$ — входные данные
- $( y_{pos} )$ и $( y_{neg} )$ — положительный и отрицательный примеры соответственно
- $( D )$ — набор данных
- $( \sigma )$ — сигмоидная функция
- $( r_{\theta} )$ — функция оценки предпочтений модели.

Рассмотрим пример работы функции потерь PPM. Предположим, в процессе MCTS сгенерировано несколько продолжений, включая положительный шаг (y_pos), ведущий к правильному решению с высокой Q-оценкой, и отрицательный шаг (y_neg), не приводящий к успеху с низкой Q-оценкой, в контексте (x) предыдущих шагов. PPM вычисляет оценки для обоих шагов, например, 0.7 для y_pos и -0.3 для y_neg. Разница (1.0) пропускается через сигмоидную функцию (≈ 0.73), и логарифм этого значения (≈ -0.32) вносит вклад в функцию потерь. Если PPM правильно предсказывает предпочтение y_pos, потеря будет небольшой; в противном случае, потеря будет большой, что стимулирует модель к корректировке параметров.

**PPM используется в качестве модели вознаграждения, которая оценивает каждый шаг рассуждения. Обучение PPM строится на основе пар предпочтений (положительных и отрицательных шагов), полученных на основе Q-значений MCTS. Этот подход позволяет избежать использования зашумленных Q-значений напрямую в качестве меток вознаграждения.**

> **Цитата:**
> *"a novel method that trains an SLM acting as a process preference model, i.e., a PPM to implement the desired PRM, that reliably predicts a reward label for each math reasoning step."*

#### 2.4. Использование Монте-Карло (MCTS) для эффективного мышления System 2

Поиск по дереву Монте-Карло (MCTS) играет ключевую роль в реализации "глубокого мышления". MCTS разбивает сложные математические задачи на более простые одношаговые генерации, что снижает сложность задачи для модели политики. MCTS автоматически назначает Q-значение каждому шагу на основе его вклада в получение правильного окончательного ответа. Этот подход устраняет необходимость в трудоемкой пошаговой аннотации человеком для обучения модели вознаграждения. MCTS позволяет модели исследовать различные траектории решения и выбирать наиболее перспективные, имитируя процесс обдуманного мышления (System 2). **MCTS разбивает сложные математические задачи на более простые одношаговые генерации. MCTS автоматически назначает Q-значение каждому шагу на основе его вклада в правильный окончательный ответ. Этот подход устраняет необходимость в пошаговой аннотации человеком для обучения модели вознаграждения.**

> **Цитата:**
> *"MCTS breaks down complex math problems into simpler single-step generation tasks, reducing the difficulty for the policy SLM compared to other System 2 methods... Second, the step-by-step generation in MCTS naturally yields step-level training data for both models."*

### 3. Экспериментальные результаты

![Table_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-03/assets/Table_1.png)

> В этой таблице представлена ​​производительность модели rStar-Math при решении различных задач на математическое мышление. Она показывает точность (pass@1), достигнутую rStar-Math при применении к нескольким малым языковым моделям (SLM) различных размеров. Результаты сравниваются с производительностью моделей OpenAI и другими базовыми показателями, подчеркивая значительные улучшения в возможностях математического мышления, достигнутые rStar-Math благодаря ее подходу глубокого мышления. Таблица демонстрирует способность модели достигать самых современных результатов на стандартных тестах и ​​даже превосходить производительность более крупных и мощных моделей.

Эффективность rStar-Math была оценена на различных математических бенчмарках. **rStar-Math значительно улучшает возможности SLM в математических рассуждениях.** На бенчмарке MATH, rStar-Math значительно повысил точность модели Qwen2.5-Math-7B с 58.8% до 90.0%, а модели Phi3-mini-3.8B с 41.4% до 86.4%. Примечательно, что rStar-Math превзошел результаты OpenAI o1-preview на +4.5% и +0.9% соответственно. **На бенчмарке MATH, rStar-Math повышает точность Qwen2.5-Math-7B с 58.8% до 90.0%, а Phi3-mini-3.8B с 41.4% до 86.4%, превосходя o1-preview на +4.5% и +0.9% соответственно.** На олимпиаде AIME, rStar-Math продемонстрировал способность решать в среднем 53.3% (8 из 15) задач, что соответствует уровню 20% лучших среди старшеклассников с высокими достижениями в математике. **На олимпиаде AIME, rStar-Math решает в среднем 53.3% (8/15) задач, что ставит его в 20% лучших среди старшеклассников, имеющих отличные результаты по математике.** Эти результаты демонстрируют, что rStar-Math позволяет SLM не только достигать уровня LLM, но и превосходить их в сложных задачах математического рассуждения. **rStar-Math превосходит как System 1 (одношаговые), так и System 2 (многошаговые) подходы, включая большие языковые модели и модели, основанные на дистилляции.** Сравнительные анализы показывают, что rStar-Math превосходит ряд моделей System 1, таких как DeepSeek-Coder-V2-Instruct, Mathstral-7B-v0.1, NuminaMath-72B-CoT, LLaMA3.1-8B-Instruct, LLaMA3.1-70B-Instruct, Qwen2.5-Math-72B-Instruct, на различных бенчмарках (MATH, AIME 2024, AMC 2023, Olympiad Bench, College Math, GaokaoEn 2023). rStar-Math также демонстрирует превосходство над System 2 моделью Qwen2.5-Math-72B-Instruct+72B ORM и моделями, использующими подход Best-of-N (Qwen2.5-Math-1.5B-Instruct+72B ORM, Qwen2-Math-7B-Instruct+72B ORM, Qwen2.5-Math-7B-Instruct+72B ORM). Это подчеркивает эффективность "глубокого мышления" System 2, реализуемого в rStar-Math, и ключевую роль PPM. Также было отмечено, что увеличение вычислительной мощности во время тестирования (увеличение количества траекторий поиска) дополнительно улучшает результаты rStar-Math. **Увеличение вычислительной мощности во время теста (увеличение количества траекторий поиска) дополнительно улучшает результаты rStar-Math.** Анализ показал, что PPM является критически важным компонентом для повышения производительности System 2 рассуждения, в то время как модель политики важна, но ее вклад менее значителен по сравнению с PPM при достижении наивысшего уровня производительности. **PPM является решающим фактором для повышения производительности System 2 рассуждения, в то время как модель политики важна, но не так критична, как PPM, когда речь заходит о достижении производительности наивысшего уровня.**

### 4. Обсуждение

Результаты, полученные с помощью rStar-Math, имеют важное значение для развития эффективных и доступных моделей искусственного интеллекта. Метод демонстрирует, что небольшие языковые модели, при использовании инновационных подходов к обучению, могут освоить сложные задачи, традиционно считавшиеся прерогативой больших моделей. Использование MCTS и PPM позволяет эффективно исследовать пространство решений и выбирать наиболее верные траектории рассуждений, имитируя процесс глубокого человеческого мышления. Важно отметить, что rStar-Math превосходит как одношаговые (System 1), так и многошаговые (System 2) подходы, включая модели, основанные на дистилляции знаний. **Метод является общим и может быть применим к различным областям, включая доказательства теорем, кодирование и здравый смысл.** Предложенный метод является общим и потенциально может быть применен к различным областям, где требуется многошаговое рассуждение и верификация, таким как доказательства теорем, кодирование и задачи на здравый смысл. В частности, rStar-Math продемонстрировал потенциал для доказательства математических утверждений, успешно справившись с задачей олимпиадного уровня, применив малую теорему Ферма. Применение rStar-Math к областям кодирования и рассуждений на основе здравого смысла представляется перспективным, однако требует разработки эффективного механизма обратной связи для оценки качества генерируемых траекторий рассуждений. Для кодирования это могут быть обширные тестовые случаи, а для общих рассуждений — человеческая разметка или взаимная проверка с другими LLM. Авторы подчеркивают важность сбора более сложных задач для дальнейшего улучшения rStar-Math, указывая на потенциал для дальнейшего развития метода. **Авторы подчеркивают важность сбора более сложных задач для дальнейшего улучшения rStar-Math.** **rStar-Math демонстрирует, что небольшие языковые модели (SLM) могут освоить сложные математические рассуждения на уровне современных LLM, используя инновационные методы саморазвития, синтеза данных и обучения моделей вознаграждения. Использование MCTS и PPM позволяет эффективно исследовать пространство решений и выбирать наиболее верные траектории рассуждений. Предлагаемый метод может быть применен для обучения и улучшения моделей в других областях, где требуется многошаговое рассуждение и верификация.**

### 5. Заключение

Метод rStar-Math представляет собой значительный прогресс в области математического рассуждения для малых языковых моделей. Благодаря инновационному сочетанию саморазвития, синтеза данных с расширением кода и обучения модели предпочтения процессов, rStar-Math позволяет SLM достигать и превосходить результаты современных LLM в сложных математических задачах. Использование поиска по дереву Монте-Карло для имитации глубокого мышления и пошаговой верификации рассуждений открывает новые перспективы для создания более эффективных и доступных моделей искусственного интеллекта. Предложенный подход имеет потенциал для применения в широком спектре задач, требующих сложных рассуждений, и стимулирует дальнейшие исследования в области обучения эффективных моделей на основе принципов глубокого мышления и самосовершенствования. **В целом, rStar-Math представляет собой значительный прогресс в области математического рассуждения для SLM, предоставляя многообещающий путь для создания более эффективных и доступных моделей ИИ.**

## Глоссарий ключевых терминов

- **SLM (Small Language Model):** Небольшая языковая модель, требующая меньше вычислительных ресурсов, чем большие языковые модели (LLM).
- **LLM (Large Language Model):** Большая языковая модель, требующая значительных вычислительных ресурсов и обычно обладающая большей способностью к рассуждению.
- **MCTS (Monte Carlo Tree Search):** Поиск Монте-Карло – это алгоритм поиска, используемый для принятия решений, особенно в играх. В **rStar-Math**, **MCTS** используется для исследования возможных шагов решения математических задач.
- **CoT (Chain-of-Thought):** Цепочка рассуждений – метод, при котором модель генерирует промежуточные шаги рассуждения перед получением окончательного ответа.
- **PPM (Process Preference Model):** Модель предпочтений процесса – тип **reward** модели, которая оценивает качество каждого шага решения математической задачи на основе **Q-значений**.
- **Q-value:** Оценка качества или вклада конкретного шага в решение задачи. В **rStar-Math**, **Q-значение** присваивается каждому шагу на основе его вклада в получение правильного ответа.
- **Reward Model:** Модель, которая оценивает качество решений или отдельных шагов решения, предоставляя обратную связь для обучения **policy** модели.
- **ORM (Outcome Reward Model):** Модель вознаграждения за результат, которая оценивает только конечный результат решения задачи (правильный/неправильный) и не учитывает промежуточные шаги.
- **PRM (Process Reward Model):** Модель вознаграждения процесса, которая предоставляет обратную связь на уровне шагов решения задачи.
- **Self-Evolution:** Самоэволюция - процесс, при котором **policy SLM** и **PPM** итеративно улучшаются, используя генерируемые данные более высокого качества в каждом раунде.
- **System 1 Thinking:** Быстрое, интуитивное мышление, часто с ошибками.
- **System 2 Thinking:** Медленное, обдуманное мышление, требующее больших вычислительных ресурсов.
- **Pass@1 Accuracy:** Метрика, определяющая процент задач, которые модель решает правильно с первой попытки.
- **Step-by-Step Verified Reasoning Trajectory:** Траектория рассуждения с пошаговой проверкой, где каждый шаг решения проверяется с помощью исполнения кода.
- **Code-Augmented CoT:** Метод генерации траекторий решения, где каждый шаг сопровождается исполняемым кодом, что обеспечивает проверку шагов.
- **Pairwise Ranking Loss:** Функция потерь, используемая для обучения **PPM** на основе пар предпочтений (позитивный шаг vs. негативный шаг).
- **Terminal-guided Annotation:** Метод аннотирования **Q-значений**, который использует только конечный результат (верный или неверный) для оценки промежуточных шагов.
- **PPM-augmented Annotation:** Метод аннотирования **Q-значений**, который использует **PPM** для оценки каждого шага, делая аннотации более точными.
- **Vieta’s Formulas:** Формулы Виета — это математические формулы, которые связывают коэффициенты многочлена с его корнями.
- **AM-GM Inequality:** Неравенство о среднем арифметическом и среднем геометрическом — это неравенство, которое гласит, что среднее арифметическое набора положительных чисел всегда больше или равно их среднему геометрическому.
- **Shoelace Theorem:** Формула площади многоугольника, когда известны координаты его вершин.
- **Fermat’s Little Theorem:** Теорема, утверждающая, что если p — простое число, то для любого целого числа a число a^p-a делится на p.

## Проверь себя: Краткий опрос (с ответами)

### В чём заключается основная идея rStar-Math?
Основная идея rStar-Math заключается в применении "глубокого мышления" с использованием поиска Монте-Карло (MCTS). В этом процессе SLM (Small Language Model) выступает в роли модели политики, генерируя последовательность шагов решения, а другая SLM оценивает эти шаги, действуя как модель вознаграждения за процесс. Это позволяет SLM достигать результатов, сопоставимых с LLM (Large Language Models) в математическом рассуждении.

### Какие три ключевые инновации лежат в основе rStar-Math?
1. **Метод синтеза данных CoT с расширением кода**  
2. **Новый подход к обучению модели предпочтения процессов (PPM)**  
3. **Стратегия саморазвития**

### Что такое Code-augmented CoT и какую роль он играет в rStar-Math?
**Code-augmented CoT** — это метод синтеза данных, при котором SLM генерирует как текстовое описание рассуждения, так и соответствующий код Python. Сохраняются только те узлы, где код успешно выполняется, что позволяет отфильтровать ошибочные шаги. Этот метод играет ключевую роль в повышении качества данных для обучения.

### Какова функция модели предпочтения процессов (PPM)?
**PPM** оценивает каждый шаг рассуждения, основываясь на парах предпочтений, полученных на основе Q-значений. В отличие от моделей, оценивающих только конечный результат, PPM обеспечивает более точную обратную связь на каждом этапе, что способствует улучшению процесса рассуждения.

### Как работает процесс саморазвития в rStar-Math?
Процесс саморазвития заключается в итеративном улучшении модели политики (policy SLM) и модели предпочтения процессов (PPM). В каждом раунде модели обучаются на синтезированных решениях, что приводит к прогрессивному улучшению их способностей.

### Как MCTS используется в rStar-Math?
**MCTS** (Monte Carlo Tree Search) используется для разбиения сложных математических задач на простые одношаговые генерации. Он назначает Q-значения каждому шагу на основе его вклада в правильный ответ, направляя поиск решения и повышая точность рассуждений.

### Какой вывод можно сделать о роли PPM в System 2 мышлении?
**PPM** является решающим фактором для повышения производительности **System 2** рассуждения. Хотя модель политики важна, её вклад не столь критичен, как у PPM при достижении наивысшего уровня производительности.

### Какая главная проблема при обучении моделей математическому рассуждению, которую решает rStar-Math?
rStar-Math решает проблему **нехватки качественных данных** для обучения математическому рассуждению. Для этого используются:
- **Self-evolution** (саморазвитие),
- **Code-augmented CoT** (синтез данных с расширением кода),
- **PPM** (модель предпочтения процессов) для синтеза и оценки данных.

### Каким образом rStar-Math использует Q-значения?
**Q-значения** используются для оценки вклада каждого шага в решение задачи. Они применяются:
- При обучении **PPM**,
- В процессе **MCTS** для выбора наиболее перспективных траекторий рассуждений.  
Q-значения присваиваются автоматически, что исключает необходимость в ручной пошаговой аннотации.