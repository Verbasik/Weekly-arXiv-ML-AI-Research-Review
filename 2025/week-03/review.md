# Брифинг-документ: rStar-Math

## Математическая революция глубокого мышления: как rStar-Math превращает небольшие языковые модели в мастера математических рассуждений

### Аннотация

В данной статье представлен метод rStar-Math, демонстрирующий способность малых языковых моделей (SLM) достигать конкурентоспособных результатов, сопоставимых и даже превосходящих показатели модели OpenAI o1 в задачах математического рассуждения, без использования дистилляции знаний из более крупных моделей. Ключевой особенностью rStar-Math является применение "глубокого мышления" посредством поиска по дереву Монте-Карло (MCTS), где SLM выступает в роли модели политики, генерируя последовательность шагов решения, а другая SLM оценивает их, действуя как модель вознаграждения за процесс. Представлены три ключевые инновации: метод синтеза данных CoT с расширением кода, новый подход к обучению модели предпочтения процессов (PPM) и стратегия саморазвития. Экспериментальные результаты показывают значительное улучшение математических способностей SLM, подтверждая эффективность предложенного подхода.

### 1. Введение

Современные большие языковые модели (LLM) демонстрируют впечатляющие возможности в различных задачах, включая математическое рассуждение. Однако их значительные размеры и вычислительные требования создают препятствия для широкого применения. В связи с этим, возрастает интерес к разработке эффективных методов обучения малых языковых моделей (SLM), способных решать сложные задачи, сохраняя при этом вычислительную эффективность. В области математического рассуждения, традиционные подходы к обучению SLM часто уступают LLM. В данной работе представлен метод rStar-Math, позволяющий SLM достигать результатов, сравнимых и даже превосходящих возможности передовых моделей, таких как OpenAI o1, в задачах математического рассуждения. Ключевой идеей rStar-Math является внедрение механизма "глубокого мышления" посредством поиска по дереву Монте-Карло (MCTS) и итеративного самосовершенствования моделей.

### 2. Методология rStar-Math

Метод rStar-Math основан на трех ключевых инновациях, направленных на эффективное обучение SLM для математического рассуждения:

### Визуальные идеи

![Figure_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-03/assets/Figure_1.png)

> На этом рисунке представлен общий обзор системы rStar-Math. Он иллюстрирует три ключевых нововведения, которые позволяют малым языковым моделям (SLM) овладевать математическими рассуждениями: (a) показывает генерацию пошаговых траекторий рассуждений с помощью поиска по дереву Монте-Карло (MCTS), где каждый шаг проверяется с помощью выполнения кода Python, что гарантирует правильность; (b) изображает построение пар предпочтений на каждом шаге на основе Q-значений, полученных из MCTS, что позволяет обучать модель предпочтений процесса (PPM) без необходимости ручных аннотаций на уровне шагов; (c) показывает итеративный, саморазвивающийся процесс, где политика SLM и PPM многократно обучаются и совершенствуются с использованием все более высококачественных данных, генерируемых системой.

#### 2.1. Саморазвитие (Self-evolution)

rStar-Math использует процесс саморазвития, в котором модель политики (policy SLM), генерирующая шаги решения, и модель предпочтения процессов (PPM), оценивающая эти шаги, итеративно улучшаются, начиная с нуля. В течение нескольких раундов саморазвития, модели обучаются на миллионах синтезированных решений для большого набора математических задач. Этот итеративный процесс позволяет SLM постепенно наращивать свои возможности в математическом рассуждении, достигая уровня современных моделей. Например, в рамках исследования было проведено четыре раунда саморазвития, в ходе которых были сгенерированы миллионы решений для 747 тысяч математических задач. Процесс синтеза этих решений является итеративным и включает несколько ключевых моментов:

*   **Итеративное улучшение:** В каждом раунде модели (policy SLM и PPM) улучшаются на основе данных, сгенерированных в предыдущем раунде.
*   **Использование поиска по дереву Монте-Карло (MCTS):** Для генерации траекторий рассуждений применяется алгоритм MCTS, разбивающий сложные задачи на более простые одношаговые задачи. MCTS автоматически присваивает Q-значение каждому шагу, основанное на его вкладе в правильный окончательный ответ.
*   **Метод синтеза данных CoT с расширением кода:** На каждом шаге SLM генерирует текстовое описание рассуждения (CoT) и соответствующий код Python.
*   **Модель предпочтения процессов (PPM):** PPM оценивает каждый шаг рассуждения на основе пар предпочтений, полученных из Q-значений MCTS.
*   **Генерация множества траекторий:** Для каждой задачи генерируется множество траекторий рассуждений для обеспечения разнообразия данных.
*   **Фильтрация и отбор:** Траектории, приводящие к правильному ответу, отбираются для обучения модели политики. Использование кода позволяет отфильтровать некачественные шаги.

> **Цитата:**
> *"rStar-Math introduces a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities."*

#### 2.2. Новый метод синтеза данных CoT с расширением кода (Code-augmented CoT)

![Figure 2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-03/assets/Figure_2.png)

> Исследователи потребовали, чтобы модель включала ответы на естественном языке в виде комментариев к коду Python, и только те выходные данные, которые получены с использованием Python, будут использоваться для обучения модели.

Для обучения модели политики используется поиск по дереву Монте-Карло (MCTS) для генерации траекторий рассуждений с пошаговой верификацией. На каждом шаге SLM генерирует не только текстовое описание рассуждения (Chain-of-Thought, CoT), но и соответствующий код Python, реализующий этот шаг. Для обеспечения качества генерируемых данных, сохраняются только те узлы дерева поиска, где выполнение сгенерированного кода Python было успешным. Этот подход позволяет отфильтровать ошибочные промежуточные шаги рассуждения, обеспечивая высокое качество обучающих данных. Таким образом, формируются "траектории рассуждения с расширенным кодом", где каждый шаг подкреплен исполняемым кодом, что повышает надежность процесса обучения. В случаях, когда сгенерированный код Python на каком-либо шаге не выполняется, соответствующий узел в дереве поиска отбрасывается. Такой подход фильтрует ошибочные или некорректные промежуточные шаги рассуждения, сохраняя только траектории, где каждый шаг подтвержден успешным выполнением кода. Хотя это может привести к потере потенциально полезных, но "закодированных" иным способом рассуждений, rStar-Math намеренно делает акцент на исполняемом коде как на критерий качества, особенно важный для математических рассуждений, включающих точные вычисления. Это компромисс между полнотой данных и их качеством, где приоритет отдается точности и верифицируемости каждого шага.

> **Цитата:**
> *"a novel code-augmented CoT data synthesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories with self-annotated MCTS Q-values."*

#### 2.3. Новая модель предпочтения процессов (Process Preference Model, PPM)

В качестве модели вознаграждения используется модель предпочтения процессов (PPM), которая оценивает каждый шаг рассуждения. Обучение PPM строится на основе пар предпочтений (положительных и отрицательных шагов), полученных на основе Q-значений, присваиваемых MCTS. В отличие от моделей вознаграждения за результат (ORM), которые оценивают только конечный ответ, PPM оценивает качество каждого промежуточного шага, что позволяет более эффективно направлять процесс обучения. Использование пар предпочтений позволяет избежать прямого использования зашумленных Q-значений в качестве меток вознаграждения, что повышает стабильность и эффективность обучения. Функция потерь для PPM определяется как:

$$
L_{ppm}(\theta) = -\frac{1}{2 \times N} \sum_{(x, y_{pos}, y_{neg}) \in D} \log \left( \sigma \left( r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle \right) \right)
$$

где:
- $( \theta )$ — параметры модели
- $( N )$ — количество примеров
- $( x )$ — входные данные
- $( y_{pos} )$ и $( y_{neg} )$ — положительный и отрицательный примеры соответственно
- $( D )$ — набор данных
- $( \sigma )$ — сигмоидная функция
- $( r_{\theta} )$ — функция оценки предпочтений модели.

**Формирование пар предпочтений:**

Ключевым аспектом PPM является использование *пар предпочтений*. Для каждого входного примера $(x)$ создается пара $(y_{pos}, y_{neg})$, где:

*   $y_{pos}$ представляет собой "положительный" пример шага рассуждения, который считается более предпочтительным (например, шаг, ведущий к состоянию с более высоким Q-значением).
*   $y_{neg}$ представляет собой "отрицательный" пример шага рассуждения, который считается менее предпочтительным (например, шаг, ведущий к состоянию с более низким Q-значением).

Эти пары предпочтений формируют обучающий набор данных $D$.

**Функция оценки предпочтений ($r_{\theta}$):**

Модель PPM использует функцию оценки предпочтений $r_{\theta}$, параметризованную $\theta$. Эта функция принимает на вход входные данные $x$ и шаг рассуждения $y$ и выдает скалярное значение, представляющее "предпочтительность" этого шага в контексте входных данных. Чем выше значение $r_{\theta}\langle x, y \rangle$, тем более предпочтительным считается шаг $y$ для входных данных $x$.

**Функция потерь ($L_{ppm}(\theta)$):**

Функция потерь $L_{ppm}(\theta)$ используется для обучения параметров $\theta$ модели $r_{\theta}$. Цель состоит в том, чтобы настроить параметры $\theta$ таким образом, чтобы модель присваивала более высокие значения предпочтения положительным примерам ($y_{pos}$) по сравнению с отрицательными примерами ($y_{neg}$).

Рассмотрим функцию потерь более детально:

$$
L_{ppm}(\theta) = -\frac{1}{2 \times N} \sum_{(x, y_{pos}, y_{neg}) \in D} \log \left( \sigma \left( r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle \right) \right)
$$

**Разбор компонентов функции потерь:**

1. **$-\frac{1}{2 \times N} \sum_{(x, y_{pos}, y_{neg}) \in D}$**:
    *   **$\sum_{(x, y_{pos}, y_{neg}) \in D}$**:  Суммирование происходит по всем примерам в обучающем наборе данных $D$. Каждый пример состоит из входных данных $x$, положительного примера $y_{pos}$ и отрицательного примера $y_{neg}$.
    *   **$N$**: Общее количество примеров в обучающем наборе данных $D$.
    *   **$\frac{1}{N}$**:  Нормировка, усредняющая потери по всем примерам.
    *   **$\frac{1}{2}$**:  Дополнительный коэффициент масштабирования, который может быть использован для удобства или соответствия определенной реализации.
    *   **$-$**: Отрицательный знак перед суммой указывает на то, что мы хотим *минимизировать* функцию потерь.

2. **$\log \left( \sigma \left( r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle \right) \right)$**:
    *   **$r_{\theta} \langle x, y_{pos} \rangle$**:  Выход функции оценки предпочтений для входных данных $x$ и положительного примера $y_{pos}$. Это значение представляет собой оценку "предпочтительности" шага $y_{pos}$ в контексте $x$.
    *   **$r_{\theta} \langle x, y_{neg} \rangle$**:  Выход функции оценки предпочтений для входных данных $x$ и отрицательного примера $y_{neg}$. Это значение представляет собой оценку "предпочтительности" шага $y_{neg}$ в контексте $x$.
    *   **$r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle$**:  Разница между оценками предпочтений для положительного и отрицательного примеров. Мы хотим, чтобы эта разница была положительной, то есть $r_{\theta} \langle x, y_{pos} \rangle > r_{\theta} \langle x, y_{neg} \rangle$.
    *   **$\sigma(\cdot)$**: Сигмоидная функция. Сигмоидная функция $\sigma(z) = \frac{1}{1 + e^{-z}}$ преобразует любое вещественное число в диапазон $(0, 1)$. В данном контексте она интерпретируется как вероятность того, что положительный пример $y_{pos}$ действительно предпочтительнее отрицательного примера $y_{neg}$. Если разница $r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle$ велика и положительна, то $\sigma(\cdot)$ будет близка к 1. Если разница велика и отрицательна, то $\sigma(\cdot)$ будет близка к 0.
    *   **$\log(\cdot)$**: Натуральный логарифм. Применяется к выходу сигмоидной функции. Поскольку $\sigma(\cdot)$ находится в диапазоне $(0, 1)$, $\log(\sigma(\cdot))$ будет отрицательным числом. Отрицательный знак перед суммой в функции потерь превращает это в положительное значение, которое мы стремимся минимизировать. Использование логарифма связано с концепцией кросс-энтропии в задачах классификации.

**Цель минимизации функции потерь:**

Минимизируя $L_{ppm}(\theta)$, мы настраиваем параметры $\theta$ модели $r_{\theta}$ таким образом, чтобы для каждой пары предпочтений $(y_{pos}, y_{neg})$ модель присваивала более высокое значение предпочтения $y_{pos}$ по сравнению с $y_{neg}$. Другими словами, модель учится согласовываться с предоставленными данными о предпочтениях.

**Преимущества PPM по сравнению с ORM:**

*   **Более эффективное обучение:** Оценивая каждый шаг, PPM предоставляет более плотный сигнал обратной связи для обучения. ORM, оценивая только конечный результат, может столкнуться с проблемой разреженного вознаграждения, особенно в сложных задачах, где успешный результат достигается после множества шагов.
*   **Устойчивость к зашумленным Q-значениям:**  Использование пар предпочтений позволяет избежать прямого использования Q-значений в качестве меток вознаграждения. Q-значения, особенно на ранних этапах обучения MCTS, могут быть шумными и нестабильными. Сравнивая предпочтения, PPM фокусируется на относительной полезности шагов, что делает обучение более робастным.

#### Q-значения в rStar-Math MCTS (Monte Carlo Tree Search)

В rStar-Math MCTS (Monte Carlo Tree Search) каждому шагу присваиваются Q-значения, которые отражают его вклад в достижение правильного окончательного ответа. Эти значения используются для направления выбора узлов MCTS в сторону наиболее перспективных путей решения задачи. Процесс присвоения Q-значений включает несколько этапов:

**1. Генерация траекторий**

- **Построение дерева поиска**: MCTS итеративно строит дерево поиска, начиная с корневого узла, который представляет собой вопрос.
- **Генерация кандидатов**: На каждом шаге модель политики (SLM) генерирует несколько кандидатов (узлов), представляющих промежуточные шаги решения.
- **Извлечение траекторий**: Из дерева поиска извлекаются траектории решения, где каждая траектория представляет собой путь от корня к терминальному узлу (конечный шаг).
- **Присвоение Q-значений**: Каждому шагу $s_i$ в траектории $t = x \oplus s_1 \oplus s_2 \oplus \ldots \oplus s_d$ присваивается Q-значение $Q(s_i)$.

**2. Самоаннотация Q-значений (смотрим на результат)**

Для присвоения Q-значений используются два метода, каждый из которых оценивает качество шага на основе его вклада в получение правильного окончательного ответа:

**Терминально-ориентированная аннотация (оценка по финишу)**

- **Применение**: Используется в первых двух раундах обучения, когда модель PPM еще недостаточно надежна.
- **Обновление Q-значений**: Q-значение шага $s_i$ в $k$-м развертывании $q(s_i)_k$ обновляется на основе Q-значения терминального (финишного) узла $s_d$ $q(s_d)$.
- **Оценка терминальных узлов**: Терминальные узлы оцениваются как $q(s_d) = 1$, если конечный ответ правильный, и $q(s_d) = -1$, если ответ неверный.
- **Формула обновления**: $q(s_i)_k = q(s_i)_{k-1} + q(s_d)_k$, где $q(s_i)_0 = 0$ в первом развертывании.
- **Результат**: Шаги, которые часто ведут к правильному ответу, получают более высокие Q-значения, а шаги, приводящие к неправильным ответам, получают низкие Q-значения.
- **Когда используется:**  В самом начале обучения, когда модель еще ничего не знает и просто пробует все подряд.

**PRM-расширенная аннотация (оценка с подсказкой)**

- **Применение**: Используется, начиная с третьего раунда, когда модель PPM уже обучена.
- **Предсказание Q-значений**: Модель PPM предсказывает начальное Q-значение $q(s_i)_0$ для каждого шага $s_i$, основываясь на частичной траектории $x \oplus s_1 \oplus s_2 \oplus \ldots \oplus s_{i-1} \oplus s_i$. Формально, $q(s_i)_0 = PPM(x \oplus s_1 \oplus s_2 \oplus \ldots \oplus s_{i-1} \oplus s_i)$.
- **Обновление Q-значений**: Это начальное Q-значение обновляется на основе Q-значения терминального узла $q(s_d)$ в процессе обратного распространения MCTS. Обновление происходит по формуле $q(s_i)_k = q(s_i)_{k-1} + q(s_d)_k$, как и в терминальной аннотации.
- **Результат**: PPM-расширенный MCTS помогает модели политики генерировать более качественные шаги, так как он направляет решения в сторону правильных путей.
- **Когда используется:**  Когда модель PPM уже немного обучилась и может делать более осознанные выборы.

**Итог**

В обоих методах, после большого количества развертываний MCTS, шаги, которые последовательно приводят к правильным ответам, получают более высокие Q-значения, а шаги, которые не приводят к правильным ответам, получают низкие Q-значения.

**3. Использование Q-значений в MCTS**

- **Выбор узлов**: Q-значения используются для выбора наиболее перспективных узлов во время фазы выбора MCTS (selection).
- **Формула UCT**: Для выбора узла $s$ используется формула UCT (Upper Confidence bounds for Trees):

  $$
  UCT(s) = Q(s) + c \sqrt{\frac{\ln N_{\text{parent}}(s)}{N(s)}}
  $$

Разберем каждый компонент формулы UCT:

* **$Q(s)$ – Среднее Q-значение узла $s$:**
    * Как ты правильно отметил, $Q(s)$ рассчитывается как среднее всех Q-значений, полученных при посещении узла $s$.
    * Формально:  $Q(s) = \frac{q(s)}{N(s)}$, где:
        * $q(s)$ – это *сумма* всех Q-значений, которые были "возвращены" в узел $s$ после завершения симуляций, начинавшихся с этого узла. Каждый раз, когда симуляция достигает терминального состояния, результат этой симуляции (например, +1 за правильный ответ, -1 за неправильный) распространяется обратно по дереву, обновляя $q(s)$ для каждого узла на пути.
        * $N(s)$ – общее количество раз, когда узел $s$ был посещен.
    * **Интерпретация:** $Q(s)$ представляет собой текущую оценку "ценности" перехода в состояние $s$. Высокое $Q(s)$ говорит о том, что посещение этого узла в среднем приводило к хорошим результатам в прошлом.

* **$c$ – Константа баланса исследования и использования:**
    * Эта константа является гиперпараметром, который нужно настраивать.
    * **Роль:**  Она определяет, насколько MCTS склонен исследовать менее изученные ветви дерева.
    * **Высокое значение $c$:**  Поощряет *исследование* (exploration). Алгоритм будет чаще выбирать узлы, у которых было меньше посещений, даже если их текущее среднее Q-значение не самое высокое. Это помогает находить потенциально хорошие, но еще не до конца изученные пути.
    * **Низкое значение $c$:** Поощряет *использование* (exploitation). Алгоритм будет чаще выбирать узлы с высоким текущим средним Q-значением, полагаясь на уже накопленную информацию.
    * **Выбор $c$:**  Обычно требует экспериментов. Слишком высокое $c$ может привести к неэффективному исследованию неперспективных ветвей, а слишком низкое $c$ может заставить алгоритм застрять на субоптимальных решениях.

* **$N(s)$ – Количество посещений узла $s$:**
    * Это просто счетчик того, сколько раз MCTS проходил через узел $s$.
    * **Роль в формуле UCT:**  Чем меньше посещений у узла, тем больше вторая часть формулы UCT ($\sqrt{\frac{\ln N_{\text{parent}}(s)}{N(s)}}$) будет влиять на выбор. Это стимулирует исследование менее посещаемых узлов.

* **$N_{\text{parent}}(s)$ – Количество посещений родительского узла $s$:**
    * Это количество раз, когда родительский узел узла $s$ был посещен.
    * **Роль в формуле UCT:**  Вместе с $N(s)$ определяет степень "неизведанности" узла $s$ относительно его родителя. Логарифм от количества посещений родителя растет медленнее, чем количество посещений самого узла, что со временем уменьшает вклад исследовательской части формулы для часто посещаемых узлов.

**Баланс исследования и использования на практике:**

В начале работы MCTS, когда большинство узлов посещены мало раз, исследовательская часть формулы UCT играет большую роль, побуждая алгоритм пробовать разные пути. Со временем, по мере накопления информации, средние Q-значения становятся более надежными, и эксплуатационная часть формулы (сами $Q(s)$) начинает доминировать, направляя поиск к наиболее перспективным ветвям.

### Пример работы функции потерь PPM

Теперь давай разберем пример с функцией потерь PPM. PPM (Policy Prediction Model) обучается предсказывать, какие шаги являются хорошими в заданном контексте. Функция потерь используется для измерения того, насколько хорошо PPM делает эти предсказания, и для корректировки параметров модели в процессе обучения.

**Сценарий:**

Предположим, в процессе MCTS, находясь в определенном состоянии (контексте) `x`, модель политики PPM рассматривает два возможных следующих шага:

* **`y_pos`:** Шаг, который, как выяснилось в результате MCTS, ведет к правильному решению и имеет высокую Q-оценку.
* **`y_neg`:** Шаг, который не приводит к успеху и имеет низкую Q-оценку.

**Оценки PPM:**

PPM, основываясь на контексте `x`, выдает оценки вероятности (или "предпочтения") для каждого из этих шагов. В твоем примере:

* PPM оценивает `y_pos` с вероятностью 0.7.
* PPM оценивает `y_neg` с вероятностью -0.3. (Здесь стоит отметить, что PPM может выдавать "сырые" оценки, которые затем могут быть преобразованы в вероятности, например, через softmax. В данном контексте, можно интерпретировать эти значения как относительные "оценки качества" шага).

**Разница и сигмоидная функция:**

Далее, рассматривается разница в оценках между положительным и отрицательным примерами:

* Разница = Оценка(`y_pos`) - Оценка(`y_neg`) = 0.7 - (-0.3) = 1.0

Эта разница показывает, насколько PPM различает хороший и плохой шаг. Затем эта разница пропускается через сигмоидную функцию:

* Сигмоид(1.0) ≈ 0.73

Сигмоидная функция сжимает значения в диапазон от 0 до 1, что можно интерпретировать как "уверенность" модели в том, что `y_pos` лучше, чем `y_neg`.

**Функция потерь (логистическая потеря):**

В данном случае, скорее всего, используется форма логистической функции потерь (binary cross-entropy), которая часто применяется в задачах классификации, где нужно отличить "положительный" пример от "отрицательного". Функция потерь измеряет "расстояние" между предсказанием модели и истинной меткой.

В упрощенном виде, функция потерь стремится минимизировать значение, если модель правильно предсказывает предпочтение, и максимизировать его, если ошибается. В твоем примере, вклад в функцию потерь связан с логарифмом значения, полученного после сигмоиды:

* Логарифм(0.73) ≈ -0.32

**Интерпретация вклада в функцию потерь:**

* **Если PPM правильно предсказывает предпочтение `y_pos`:**  Разница между оценками будет положительной, значение после сигмоиды будет близко к 1, а логарифм этого значения будет близок к 0 (или небольшое отрицательное число). Это означает *небольшую потерю*.
* **Если PPM неправильно предсказывает предпочтение (например, оценка `y_neg` выше, чем `y_pos`):** Разница будет отрицательной, значение после сигмоиды будет близко к 0, а логарифм этого значения будет большим отрицательным числом. В контексте функции потерь, это приведет к *большой потере*.

**Цель обучения PPM:**

Цель обучения PPM состоит в том, чтобы минимизировать общую функцию потерь на большом наборе данных. Когда PPM делает неправильные предсказания (например, не отличает хороший шаг от плохого), функция потерь "штрафует" модель, генерируя большой вклад в общую потерю. Это заставляет алгоритм оптимизации (например, градиентный спуск) корректировать параметры PPM таким образом, чтобы в будущем его предсказания лучше соответствовали наблюдаемым результатам (Q-оценкам, полученным от MCTS).

**В заключение:**

Q-значения в MCTS направляют поиск, помогая алгоритму исследовать перспективные области пространства решений. Функция потерь PPM, в свою очередь, использует информацию о Q-значениях, чтобы обучить модель политики делать более точные предсказания о ценности различных шагов, что, в свою очередь, улучшает эффективность MCTS.

> **Цитата:**
> *"a novel method that trains an SLM acting as a process preference model, i.e., a PPM to implement the desired PRM, that reliably predicts a reward label for each math reasoning step."*

#### 2.4. Использование Монте-Карло (MCTS) для эффективного мышления System 2

Поиск по дереву Монте-Карло (MCTS) играет ключевую роль в реализации "глубокого мышления". MCTS разбивает сложные математические задачи на более простые одношаговые генерации, что снижает сложность задачи для модели политики. MCTS автоматически назначает Q-значение каждому шагу на основе его вклада в получение правильного окончательного ответа. Этот подход устраняет необходимость в трудоемкой пошаговой аннотации человеком для обучения модели вознаграждения. MCTS позволяет модели исследовать различные траектории решения и выбирать наиболее перспективные, имитируя процесс обдуманного мышления (System 2).

> **Цитата:**
> *"MCTS breaks down complex math problems into simpler single-step generation tasks, reducing the difficulty for the policy SLM compared to other System 2 methods... Second, the step-by-step generation in MCTS naturally yields step-level training data for both models."*

### 3. Экспериментальные результаты

![Table_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-03/assets/Table_1.png)

> В этой таблице представлена ​​производительность модели rStar-Math при решении различных задач на математическое мышление. Она показывает точность (pass@1), достигнутую rStar-Math при применении к нескольким малым языковым моделям (SLM) различных размеров. Результаты сравниваются с производительностью моделей OpenAI и другими базовыми показателями, подчеркивая значительные улучшения в возможностях математического мышления, достигнутые rStar-Math благодаря ее подходу глубокого мышления. Таблица демонстрирует способность модели достигать самых современных результатов на стандартных тестах и ​​даже превосходить производительность более крупных и мощных моделей.

Эффективность rStar-Math была оценена на различных математических бенчмарках. **rStar-Math значительно улучшает возможности SLM в математических рассуждениях.** На бенчмарке MATH, rStar-Math значительно повысил точность модели Qwen2.5-Math-7B с 58.8% до 90.0%, а модели Phi3-mini-3.8B с 41.4% до 86.4%. Примечательно, что rStar-Math превзошел результаты OpenAI o1-preview на +4.5% и +0.9% соответственно. На олимпиаде AIME, rStar-Math продемонстрировал способность решать в среднем 53.3% (8 из 15) задач, что соответствует уровню 20% лучших среди старшеклассников с высокими достижениями в математике. Эти результаты демонстрируют, что rStar-Math позволяет SLM не только достигать уровня LLM, но и превосходить их в сложных задачах математического рассуждения. Сравнительные анализы показывают, что rStar-Math превосходит ряд моделей System 1, таких как DeepSeek-Coder-V2-Instruct, Mathstral-7B-v0.1, NuminaMath-72B-CoT, LLaMA3.1-8B-Instruct, LLaMA3.1-70B-Instruct, Qwen2.5-Math-72B-Instruct, на различных бенчмарках (MATH, AIME 2024, AMC 2023, Olympiad Bench, College Math, GaokaoEn 2023). rStar-Math также демонстрирует превосходство над System 2 моделью Qwen2.5-Math-72B-Instruct+72B ORM и моделями, использующими подход Best-of-N (Qwen2.5-Math-1.5B-Instruct+72B ORM, Qwen2-Math-7B-Instruct+72B ORM, Qwen2.5-Math-7B-Instruct+72B ORM). Это подчеркивает эффективность "глубокого мышления" System 2, реализуемого в rStar-Math, и ключевую роль PPM. Также было отмечено, что увеличение вычислительной мощности во время тестирования (увеличение количества траекторий поиска) дополнительно улучшает результаты rStar-Math. Анализ показал, что PPM является критически важным компонентом для повышения производительности System 2 рассуждения, в то время как модель политики важна, но ее вклад менее значителен по сравнению с PPM при достижении наивысшего уровня производительности.

### 4. Обсуждение

Результаты, полученные с помощью rStar-Math, имеют важное значение для развития эффективных и доступных моделей искусственного интеллекта. Метод демонстрирует, что небольшие языковые модели, при использовании инновационных подходов к обучению, могут освоить сложные задачи, традиционно считавшиеся прерогативой больших моделей. Использование MCTS и PPM позволяет эффективно исследовать пространство решений и выбирать наиболее верные траектории рассуждений, имитируя процесс глубокого человеческого мышления. Важно отметить, что rStar-Math превосходит как одношаговые (System 1), так и многошаговые (System 2) подходы, включая модели, основанные на дистилляции знаний. Предложенный метод является общим и потенциально может быть применен к различным областям, где требуется многошаговое рассуждение и верификация, таким как доказательства теорем, кодирование и задачи на здравый смысл. В частности, rStar-Math продемонстрировал потенциал для доказательства математических утверждений, успешно справившись с задачей олимпиадного уровня, применив малую теорему Ферма. Применение rStar-Math к областям кодирования и рассуждений на основе здравого смысла представляется перспективным, однако требует разработки эффективного механизма обратной связи для оценки качества генерируемых траекторий рассуждений. Для кодирования это могут быть обширные тестовые случаи, а для общих рассуждений — человеческая разметка или взаимная проверка с другими LLM. Авторы подчеркивают важность сбора более сложных задач для дальнейшего улучшения rStar-Math, указывая на потенциал для дальнейшего развития метода.

### 5. Заключение

Метод rStar-Math представляет собой значительный прогресс в области математического рассуждения для малых языковых моделей. Благодаря инновационному сочетанию саморазвития, синтеза данных с расширением кода и обучения модели предпочтения процессов, rStar-Math позволяет SLM достигать и превосходить результаты современных LLM в сложных математических задачах. Использование поиска по дереву Монте-Карло для имитации глубокого мышления и пошаговой верификации рассуждений открывает новые перспективы для создания более эффективных и доступных моделей искусственного интеллекта. Предложенный подход имеет потенциал для применения в широком спектре задач, требующих сложных рассуждений, и стимулирует дальнейшие исследования в области обучения эффективных моделей на основе принципов глубокого мышления и самосовершенствования.

## Глоссарий ключевых терминов

- **SLM (Small Language Model):** Небольшая языковая модель, требующая меньше вычислительных ресурсов, чем большие языковые модели (LLM).
- **LLM (Large Language Model):** Большая языковая модель, требующая значительных вычислительных ресурсов и обычно обладающая большей способностью к рассуждению.
- **MCTS (Monte Carlo Tree Search):** Поиск Монте-Карло – это алгоритм поиска, используемый для принятия решений, особенно в играх. В **rStar-Math**, **MCTS** используется для исследования возможных шагов решения математических задач.
- **CoT (Chain-of-Thought):** Цепочка рассуждений – метод, при котором модель генерирует промежуточные шаги рассуждения перед получением окончательного ответа.
- **PPM (Process Preference Model):** Модель предпочтений процесса – тип **reward** модели, которая оценивает качество каждого шага решения математической задачи на основе **Q-значений**.
- **Q-value:** Оценка качества или вклада конкретного шага в решение задачи. В **rStar-Math**, **Q-значение** присваивается каждому шагу на основе его вклада в получение правильного ответа.
- **Reward Model:** Модель, которая оценивает качество решений или отдельных шагов решения, предоставляя обратную связь для обучения **policy** модели.
- **ORM (Outcome Reward Model):** Модель вознаграждения за результат, которая оценивает только конечный результат решения задачи (правильный/неправильный) и не учитывает промежуточные шаги.
- **PRM (Process Reward Model):** Модель вознаграждения процесса, которая предоставляет обратную связь на уровне шагов решения задачи.
- **Self-Evolution:** Самоэволюция - процесс, при котором **policy SLM** и **PPM** итеративно улучшаются, используя генерируемые данные более высокого качества в каждом раунде.
- **System 1 Thinking:** Быстрое, интуитивное мышление, часто с ошибками.
- **System 2 Thinking:** Медленное, обдуманное мышление, требующее больших вычислительных ресурсов.
- **Pass@1 Accuracy:** Метрика, определяющая процент задач, которые модель решает правильно с первой попытки.
- **Step-by-Step Verified Reasoning Trajectory:** Траектория рассуждения с пошаговой проверкой, где каждый шаг решения проверяется с помощью исполнения кода.
- **Code-Augmented CoT:** Метод генерации траекторий решения, где каждый шаг сопровождается исполняемым кодом, что обеспечивает проверку шагов.
- **Pairwise Ranking Loss:** Функция потерь, используемая для обучения **PPM** на основе пар предпочтений (позитивный шаг vs. негативный шаг).
- **Terminal-guided Annotation:** Метод аннотирования **Q-значений**, который использует только конечный результат (верный или неверный) для оценки промежуточных шагов.
- **PPM-augmented Annotation:** Метод аннотирования **Q-значений**, который использует **PPM** для оценки каждого шага, делая аннотации более точными.
- **Vieta’s Formulas:** Формулы Виета — это математические формулы, которые связывают коэффициенты многочлена с его корнями.
- **AM-GM Inequality:** Неравенство о среднем арифметическом и среднем геометрическом — это неравенство, которое гласит, что среднее арифметическое набора положительных чисел всегда больше или равно их среднему геометрическому.
- **Shoelace Theorem:** Формула площади многоугольника, когда известны координаты его вершин.
- **Fermat’s Little Theorem:** Теорема, утверждающая, что если p — простое число, то для любого целого числа a число a^p-a делится на p.

## Проверь себя: Краткий опрос (с ответами)

### В чём заключается основная идея rStar-Math?
Основная идея rStar-Math заключается в применении "глубокого мышления" с использованием поиска Монте-Карло (MCTS). В этом процессе SLM (Small Language Model) выступает в роли модели политики, генерируя последовательность шагов решения, а другая SLM оценивает эти шаги, действуя как модель вознаграждения за процесс. Это позволяет SLM достигать результатов, сопоставимых с LLM (Large Language Models) в математическом рассуждении.

### Какие три ключевые инновации лежат в основе rStar-Math?
1. **Метод синтеза данных CoT с расширением кода**  
2. **Новый подход к обучению модели предпочтения процессов (PPM)**  
3. **Стратегия саморазвития**

### Что такое Code-augmented CoT и какую роль он играет в rStar-Math?
**Code-augmented CoT** — это метод синтеза данных, при котором SLM генерирует как текстовое описание рассуждения, так и соответствующий код Python. Сохраняются только те узлы, где код успешно выполняется, что позволяет отфильтровать ошибочные шаги. Этот метод играет ключевую роль в повышении качества данных для обучения.

### Какова функция модели предпочтения процессов (PPM)?
**PPM** оценивает каждый шаг рассуждения, основываясь на парах предпочтений, полученных на основе Q-значений. В отличие от моделей, оценивающих только конечный результат, PPM обеспечивает более точную обратную связь на каждом этапе, что способствует улучшению процесса рассуждения.

### Как работает процесс саморазвития в rStar-Math?
Процесс саморазвития заключается в итеративном улучшении модели политики (policy SLM) и модели предпочтения процессов (PPM). В каждом раунде модели обучаются на синтезированных решениях, что приводит к прогрессивному улучшению их способностей.

### Как MCTS используется в rStar-Math?
**MCTS** (Monte Carlo Tree Search) используется для разбиения сложных математических задач на простые одношаговые генерации. Он назначает Q-значения каждому шагу на основе его вклада в правильный ответ, направляя поиск решения и повышая точность рассуждений.

### Какой вывод можно сделать о роли PPM в System 2 мышлении?
**PPM** является решающим фактором для повышения производительности **System 2** рассуждения. Хотя модель политики важна, её вклад не столь критичен, как у PPM при достижении наивысшего уровня производительности.

### Какая главная проблема при обучении моделей математическому рассуждению, которую решает rStar-Math?
rStar-Math решает проблему **нехватки качественных данных** для обучения математическому рассуждению. Для этого используются:
- **Self-evolution** (саморазвитие),
- **Code-augmented CoT** (синтез данных с расширением кода),
- **PPM** (модель предпочтения процессов) для синтеза и оценки данных.

### Каким образом rStar-Math использует Q-значения?
**Q-значения** используются для оценки вклада каждого шага в решение задачи. Они применяются:
- При обучении **PPM**,
- В процессе **MCTS** для выбора наиболее перспективных траекторий рассуждений.  
Q-значения присваиваются автоматически, что исключает необходимость в ручной пошаговой аннотации.