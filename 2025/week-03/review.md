# rStar-Math Briefing Document

## The Mathematical Revolution of Deep Reasoning: How rStar-Math Transforms Small Language Models into Masters of Mathematical Reasoning

### Abstract

This paper presents the rStar-Math method, demonstrating the ability of small language models (SLMs) to achieve competitive results comparable to, or even surpassing, those of the OpenAI o1 model in mathematical reasoning tasks, without distilling knowledge from larger models. The key feature of rStar-Math is the application of "deep reasoning" through Monte Carlo Tree Search (MCTS), where an SLM acts as the policy model, generating a sequence of solution steps, and another SLM evaluates them, acting as the process reward model. Three key innovations are presented: the CoT data synthesis method with code expansion, a novel approach to training the Process Preference Model (PPM), and a self-development strategy. Experimental results show significant improvement in the mathematical capabilities of SLMs, confirming the effectiveness of the proposed approach.

### 1. Introduction

Modern large language models (LLMs) demonstrate impressive capabilities across various tasks, including mathematical reasoning. However, their large size and computational requirements create barriers to widespread application. In response, there is growing interest in developing efficient methods for training small language models (SLMs) capable of solving complex tasks while maintaining computational efficiency. In the domain of mathematical reasoning, traditional approaches to training SLMs often lag behind LLMs. This paper presents the rStar-Math method, enabling SLMs to achieve results comparable to, or even surpassing, those of advanced models like OpenAI o1 in mathematical reasoning tasks. The core idea of rStar-Math is the integration of a "deep reasoning" mechanism through Monte Carlo Tree Search (MCTS) and iterative self-improvement of models.

### 2. Methodology rStar-Math

The rStar-Math method is based on three key innovations aimed at effectively training SLMs for mathematical reasoning:

### Visual Ideas

![Figure_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-03/assets/Figure_1.png)

> This figure presents an overview of the rStar-Math system. It illustrates three key innovations that enable small language models (SLMs) to master mathematical reasoning: (a) shows the generation of step-by-step reasoning trajectories using Monte Carlo Tree Search (MCTS), where each step is verified by executing Python code, ensuring correctness; (b) depicts the construction of preference pairs at each step based on Q-values obtained from MCTS, enabling the training of the Process Preference Model (PPM) without the need for manual step-level annotations; (c) shows the iterative, self-developing process where the policy SLM and PPM are repeatedly trained and refined using higher-quality data generated by the system.

#### 2.1. Self-evolution

rStar-Math employs a self-evolution process in which the policy SLM, generating solution steps, and the Process Preference Model (PPM), evaluating these steps, are iteratively improved, starting from scratch. Over several rounds of self-evolution, the models are trained on millions of synthesized solutions for a large set of mathematical problems. This iterative process allows SLMs to gradually build their mathematical reasoning capabilities, reaching the level of modern models. For example, within the study, four rounds of self-evolution were conducted, generating millions of solutions for 747,000 mathematical problems. The process of synthesizing these solutions is iterative and includes several key aspects:

*   **Iterative Improvement:** In each round, the models (policy SLM and PPM) are improved based on data generated in the previous round.
*   **Use of Monte Carlo Tree Search (MCTS):** To generate reasoning trajectories, the MCTS algorithm is employed, breaking down complex problems into simpler single-step problems. MCTS automatically assigns a Q-value to each step based on its contribution to the correct final answer.
*   **Code-augmented CoT Data Synthesis Method:** At each step, the SLM generates a textual description of the reasoning (Chain-of-Thought, CoT) and the corresponding Python code.
*   **Process Preference Model (PPM):** The PPM evaluates each step of the reasoning based on preference pairs derived from MCTS Q-values.
*   **Generation of Multiple Trajectories:** For each problem, multiple reasoning trajectories are generated to ensure data diversity.
*   **Filtering and Selection:** Trajectories leading to the correct answer are selected for training the policy model. Using code allows filtering out subpar steps.

> **Quote:**
> *"rStar-Math introduces a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities."*

#### 2.2. Novel Code-augmented CoT Data Synthesis Method

![Figure 2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-03/assets/Figure_2.png)

> Researchers required the model to include answers in natural language as comments in the Python code, and only outputs obtained using Python would be used for training the model.

To train the policy model, Monte Carlo Tree Search (MCTS) is used to generate reasoning trajectories with step-by-step verification. At each step, the SLM generates not only a textual description of the reasoning (Chain-of-Thought, CoT), but also the corresponding Python code implementing that step. To ensure the quality of the generated data, only those tree nodes where the execution of the generated Python code was successful are retained. This approach allows filtering out erroneous intermediate reasoning steps, ensuring high-quality training data. Thus, "reasoning trajectories with expanded code" are formed, where each step is backed by executable code, enhancing the reliability of the training process. In cases where the generated Python code fails at any step, the corresponding node in the search tree is discarded. This approach filters out erroneous or incorrect intermediate reasoning steps, retaining only trajectories where each step is confirmed by successful code execution. Although this may lead to the loss of potentially useful, but "encoded" in other ways, reasoning, rStar-Math deliberately emphasizes executable code as a quality criterion, especially important for mathematical reasoning involving precise calculations. This is a compromise between data completeness and quality, where priority is given to the accuracy and verifiability of each step.

> **Quote:**
> *"a novel code-augmented CoT data synthesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories with self-annotated MCTS Q-values."*

#### 2.3. New Process Preference Model (Process Preference Model, PPM)

As the reward model, the Process Preference Model (PPM) is used, which evaluates each step of the reasoning. Training the PPM is based on preference pairs (positive and negative steps) derived from Q-values assigned by MCTS. Unlike result-based reward models (ORM), which evaluate only the final answer, the PPM evaluates the quality of each intermediate step, allowing for more effective guidance of the training process. Using preference pairs allows avoiding the direct use of noisy Q-values as reward labels, enhancing the stability and efficiency of training. The loss function for the PPM is defined as:

$$
L_{ppm}(\theta) = -\frac{1}{2 \times N} \sum_{(x, y_{pos}, y_{neg}) \in D} \log \left( \sigma \left( r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle \right) \right)
$$

where:
- $( \theta )$ — model parameters
- $( N )$ — number of examples
- $( x )$ — input data
- $( y_{pos} )$ and $( y_{neg} )$ — positive and negative examples, respectively
- $( D )$ — dataset
- $( \sigma )$ — sigmoid function
- $( r_{\theta} )$ — preference evaluation function of the model.

**Preference Pair Formation:**

A key aspect of the PPM is the use of *preference pairs*. For each input example $(x)$, a pair $(y_{pos}, y_{neg})$ is created, where:

*   $y_{pos}$ represents a "positive" example of a reasoning step considered more preferable (e.g., a step leading to a state with a higher Q-value).
*   $y_{neg}$ represents a "negative" example of a reasoning step considered less preferable (e.g., a step leading to a state with a lower Q-value).

These preference pairs form the training dataset $D$.

**Preference Evaluation Function ($r_{\theta}$):**

The PPM uses a preference evaluation function $r_{\theta}$, parameterized by $\theta$. This function takes as input the input data $x$ and the reasoning step $y$ and outputs a scalar value representing the "preference" of this step in the context of the input data. The higher the value $r_{\theta}\langle x, y \rangle$, the more preferable the step $y$ is considered for the input data $x$.

**Loss Function ($L_{ppm}(\theta)$):**

The loss function $L_{ppm}(\theta)$ is used to train the parameters $\theta$ of the model $r_{\theta}$. The goal is to tune the parameters $\theta$ such that the model assigns higher preference values to positive examples ($y_{pos}$) compared to negative examples ($y_{neg}$).

Let's examine the loss function in more detail:

$$
L_{ppm}(\theta) = -\frac{1}{2 \times N} \sum_{(x, y_{pos}, y_{neg}) \in D} \log \left( \sigma \left( r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle \right) \right)
$$

**Breakdown of Loss Function Components:**

1. **$-\frac{1}{2 \times N} \sum_{(x, y_{pos}, y_{neg}) \in D}$**:
    *   **$\sum_{(x, y_{pos}, y_{neg}) \in D}$**: Summation occurs over all examples in the training dataset $D$. Each example consists of input data $x$, a positive example $y_{pos}$, and a negative example $y_{neg}$.
    *   **$N$**: Total number of examples in the training dataset $D$.
    *   **$\frac{1}{N}$**: Normalization, averaging the losses across all examples.
    *   **$\frac{1}{2}$**: Additional scaling factor, which may be used for convenience or to match a specific implementation.
    *   **$-$**: The negative sign before the summation indicates that we want to *minimize* the loss function.

2. **$\log \left( \sigma \left( r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle \right) \right)$**:
    *   **$r_{\theta} \langle x, y_{pos} \rangle$**: Output of the preference evaluation function for input data $x$ and the positive example $y_{pos}$. This value represents the evaluation of the "preference" of the step $y_{pos}$ in the context of $x$.
    *   **$r_{\theta} \langle x, y_{neg} \rangle$**: Output of the preference evaluation function for input data $x$ and the negative example $y_{neg}$. This value represents the evaluation of the "preference" of the step $y_{neg}$ in the context of $x$.
    *   **$r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle$**: Difference between the preference evaluations for the positive and negative examples. We want this difference to be positive, i.e., $r_{\theta} \langle x, y_{pos} \rangle > r_{\theta} \langle x, y_{neg} \rangle$.
    *   **$\sigma(\cdot)$**: Sigmoid function. The sigmoid function $\sigma(z) = \frac{1}{1 + e^{-z}}$ transforms any real number into the range $(0, 1)$. In this context, it is interpreted as the probability that the positive example $y_{pos}$ is indeed preferable to the negative example $y_{neg}$. If the difference $r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle$ is large and positive, then $\sigma(\cdot)$ will be close to 1. If the difference is large and negative, then $\sigma(\cdot)$ will be close to 0.
    *   **$\log(\cdot)$**: Natural logarithm. Applied to the output of the sigmoid function. Since $\sigma(\cdot)$ is in the range $(0, 1)$, $\log(\sigma(\cdot))$ will be a negative number. The negative sign before the summation in the loss function turns this into a positive value that we aim to minimize. The use of the logarithm is related to the concept of cross-entropy in classification tasks.

3. **$r_{\theta}\langle x, y \rangle$**:
    *   **Input Data:**
        *   The input to the model consists of textual data representing the **problem statement** $x$ and the **sequence of reasoning steps** $s$.
        *   At the first step, only the problem statement $x$ is input. At subsequent steps, the concatenation of the problem statement ($x$) and the sequence of previous solution steps up to the current step $s_i$ is input: $x \oplus s_1 \oplus s_2 \oplus ... \oplus s_{i-1} \oplus s_i$.
        *   **Important**: These textual data include not only the problem text but also the **sequence of reasoning steps in natural language and Python code, including comments**.
    *   **Representation as Vectors:**
        *   The textual data **are not input directly** into the neural network. Instead, they are **transformed into vector representations** using an embedding mechanism, as in other language models.
        *   These vector representations are **numerical arrays** that capture the semantic and syntactic information contained within the text.
    *   **Neural Network:**
        *   The vector representations of the input data are fed into a **neural network** that implements the function $r_{\theta}$.
        *   This network consists of several layers performing **nonlinear transformations** on the vector representations.
    *   **Scalar Output:**
        *   At the output of the neural network, a **single real number** is obtained — a scalar value in the range from -1 to 1.
        *   This number represents the **"preference" evaluation** of the current reasoning step $s_i$ for the problem $x$. The higher the value, the more preferable this step is.
    *   **Model Training:**
        *   The PPM model **is not trained on "absolute" preference values**. Instead, it is trained to distinguish between **"positive" and "negative" steps**.
        *   For each reasoning step, pairs are created from "positive" steps with higher Q-values and "negative" steps with lower Q-values.
        *   The PPM model is trained to predict **higher values for positive steps** and **lower values for negative steps**, using a pairwise ranking function.

**Key Points:**

*   The function $r_{\theta}(x, y)$ does not compute a dot product, but rather represents a **direct pass** through the network layers that processes the **textual input representations**.
*   At the output of this network, a **single real number** from -1 to 1 is obtained, which evaluates the "preference" of the step.
*   The model is trained not on absolute values, but on the **difference between "positive" and "negative" steps**.

Thus, the function $r_{\theta}(x, y)$ is a **neural network that takes textual representations of the problem and solution steps, transforms them into vectors, and outputs a scalar preference evaluation at the output.**


# rStar-Math Briefing Document

## The Mathematical Revolution of Deep Reasoning: How rStar-Math Transforms Small Language Models into Masters of Mathematical Reasoning

### Abstract

This paper presents the rStar-Math method, demonstrating the ability of small language models (SLMs) to achieve competitive results comparable to, or even surpassing, those of the OpenAI o1 model in mathematical reasoning tasks, without distilling knowledge from larger models. The key feature of rStar-Math is the application of "deep reasoning" through Monte Carlo Tree Search (MCTS), where an SLM acts as the policy model, generating a sequence of solution steps, and another SLM evaluates them, acting as the process reward model. Three key innovations are presented: the CoT data synthesis method with code expansion, a novel approach to training the Process Preference Model (PPM), and a self-development strategy. Experimental results show significant improvement in the mathematical capabilities of SLMs, confirming the effectiveness of the proposed approach.

### 1. Introduction

Modern large language models (LLMs) demonstrate impressive capabilities across various tasks, including mathematical reasoning. However, their large size and computational requirements create barriers to widespread application. In response, there is growing interest in developing efficient methods for training small language models (SLMs) capable of solving complex tasks while maintaining computational efficiency. In the domain of mathematical reasoning, traditional approaches to training SLMs often lag behind LLMs. This paper presents the rStar-Math method, enabling SLMs to achieve results comparable to, or even surpassing, those of advanced models like OpenAI o1 in mathematical reasoning tasks. The core idea of rStar-Math is the integration of a "deep reasoning" mechanism through Monte Carlo Tree Search (MCTS) and iterative self-improvement of models.

### 2. Methodology rStar-Math

The rStar-Math method is based on three key innovations aimed at effectively training SLMs for mathematical reasoning:

### Visual Ideas

![Figure_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-03/assets/Figure_1.png)

> This figure presents an overview of the rStar-Math system. It illustrates three key innovations that enable small language models (SLMs) to master mathematical reasoning: (a) shows the generation of step-by-step reasoning trajectories using Monte Carlo Tree Search (MCTS), where each step is verified by executing Python code, ensuring correctness; (b) depicts the construction of preference pairs at each step based on Q-values obtained from MCTS, enabling the training of the Process Preference Model (PPM) without the need for manual step-level annotations; (c) shows the iterative, self-developing process where the policy SLM and PPM are repeatedly trained and refined using higher-quality data generated by the system.

#### 2.1. Self-evolution

rStar-Math employs a self-evolution process in which the policy SLM, generating solution steps, and the Process Preference Model (PPM), evaluating these steps, are iteratively improved, starting from scratch. Over several rounds of self-evolution, the models are trained on millions of synthesized solutions for a large set of mathematical problems. This iterative process allows SLMs to gradually build their mathematical reasoning capabilities, reaching the level of modern models. For example, within the study, four rounds of self-evolution were conducted, generating millions of solutions for 747,000 mathematical problems. The process of synthesizing these solutions is iterative and includes several key aspects:

*   **Iterative Improvement:** In each round, the models (policy SLM and PPM) are improved based on data generated in the previous round.
*   **Use of Monte Carlo Tree Search (MCTS):** To generate reasoning trajectories, the MCTS algorithm is employed, breaking down complex problems into simpler single-step problems. MCTS automatically assigns a Q-value to each step based on its contribution to the correct final answer.
*   **Code-augmented CoT Data Synthesis Method:** At each step, the SLM generates a textual description of the reasoning (Chain-of-Thought, CoT) and the corresponding Python code.
*   **Process Preference Model (PPM):** The PPM evaluates each step of the reasoning based on preference pairs derived from MCTS Q-values.
*   **Generation of Multiple Trajectories:** For each problem, multiple reasoning trajectories are generated to ensure data diversity.
*   **Filtering and Selection:** Trajectories leading to the correct answer are selected for training the policy model. Using code allows filtering out subpar steps.

> **Quote:**
> *"rStar-Math introduces a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities."*

#### 2.2. Novel Code-augmented CoT Data Synthesis Method

![Figure 2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-03/assets/Figure_2.png)

> Researchers required the model to include answers in natural language as comments in the Python code, and only outputs obtained using Python would be used for training the model.

To train the policy model, Monte Carlo Tree Search (MCTS) is used to generate reasoning trajectories with step-by-step verification. At each step, the SLM generates not only a textual description of the reasoning (Chain-of-Thought, CoT), but also the corresponding Python code implementing that step. To ensure the quality of the generated data, only those tree nodes where the execution of the generated Python code was successful are retained. This approach allows filtering out erroneous intermediate reasoning steps, ensuring high-quality training data. Thus, "reasoning trajectories with expanded code" are formed, where each step is backed by executable code, enhancing the reliability of the training process. In cases where the generated Python code fails at any step, the corresponding node in the search tree is discarded. This approach filters out erroneous or incorrect intermediate reasoning steps, retaining only trajectories where each step is confirmed by successful code execution. Although this may lead to the loss of potentially useful, but "encoded" in other ways, reasoning, rStar-Math deliberately emphasizes executable code as a quality criterion, especially important for mathematical reasoning involving precise calculations. This is a compromise between data completeness and quality, where priority is given to the accuracy and verifiability of each step.

> **Quote:**
> *"a novel code-augmented CoT data synthesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories with self-annotated MCTS Q-values."*

#### 2.3. New Process Preference Model (Process Preference Model, PPM)

As the reward model, the Process Preference Model (PPM) is used, which evaluates each step of the reasoning. Training the PPM is based on preference pairs (positive and negative steps) derived from Q-values assigned by MCTS. Unlike result-based reward models (ORM), which evaluate only the final answer, PPM evaluates the quality of each intermediate step, allowing for more effective guidance of the training process. Using preference pairs allows avoiding the direct use of noisy Q-values as reward labels, enhancing the stability and efficiency of training. The loss function for the PPM is defined as:

$$
L_{ppm}(\theta) = -\frac{1}{2 \times N} \sum_{(x, y_{pos}, y_{neg}) \in D} \log \left( \sigma \left( r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle \right) \right)
$$

where:
- $( \theta )$ — model parameters
- $( N )$ — number of examples
- $( x )$ — input data
- $( y_{pos} )$ and $( y_{neg} )$ — positive and negative examples, respectively
- $( D )$ — dataset
- $( \sigma )$ — sigmoid function
- $( r_{\theta} )$ — preference evaluation function of the model.

**Preference Pair Formation:**

A key aspect of the PPM is the use of *preference pairs*. For each input example $(x)$, a pair $(y_{pos}, y_{neg})$ is created, where:

*   $y_{pos}$ represents a "positive" example of a reasoning step considered more preferable (e.g., a step leading to a state with a higher Q-value).
*   $y_{neg}$ represents a "negative" example of a reasoning step considered less preferable (e.g., a step leading to a state with a lower Q-value).

These preference pairs form the training dataset $D$.

**Preference Evaluation Function ($r_{\theta}$):**

The PPM uses a preference evaluation function $r_{\theta}$, parameterized by $\theta$. This function takes as input the input data $x$ and the reasoning step $y$ and outputs a scalar value representing the "preference" of this step in the context of the input data. The higher the value $r_{\theta}\langle x, y \rangle$, the more preferable the step $y$ is considered for the input data $x$.

**Loss Function ($L_{ppm}(\theta)$):**

The loss function $L_{ppm}(\theta)$ is used to train the parameters $\theta$ of the model $r_{\theta}$. The goal is to tune the parameters $\theta$ such that the model assigns higher preference values to positive examples ($y_{pos}$) compared to negative examples ($y_{neg}$).

Let's examine the loss function in more detail:

$$
L_{ppm}(\theta) = -\frac{1}{2 \times N} \sum_{(x, y_{pos}, y_{neg}) \in D} \log \left( \sigma \left( r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle \right) \right)
$$

**Breakdown of Loss Function Components:**

1. **$-\frac{1}{2 \times N} \sum_{(x, y_{pos}, y_{neg}) \in D}$**:
    *   **$\sum_{(x, y_{pos}, y_{neg}) \in D}$**: Summation occurs over all examples in the training dataset $D$. Each example consists of input data $x$, a positive example $y_{pos}$, and a negative example $y_{neg}$.
    *   **$N$**: Total number of examples in the training dataset $D$.
    *   **$\frac{1}{N}$**: Normalization, averaging the losses across all examples.
    *   **$\frac{1}{2}$**: Additional scaling factor, which may be used for convenience or to match a specific implementation.
    *   **$-$**: Negative sign before the summation indicates that we want to *minimize* the loss function.

2. **$\log \left( \sigma \left( r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle \right) \right)$**:
    *   **$r_{\theta} \langle x, y_{pos} \rangle$**: Output of the preference evaluation function for input data $x$ and the positive example $y_{pos}$. This value represents the evaluation of the "preference" of the step $y_{pos}$ in the context of $x$.
    *   **$r_{\theta} \langle x, y_{neg} \rangle$**: Output of the preference evaluation function for input data $x$ and the negative example $y_{neg}$. This value represents the evaluation of the "preference" of the step $y_{neg}$ in the context of $x$.
    *   **$r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle$**: Difference between the preference evaluations for the positive and negative examples. We want this difference to be positive, i.e., $r_{\theta} \langle x, y_{pos} \rangle > r_{\theta} \langle x, y_{neg} \rangle$.
    *   **$\sigma(\cdot)$**: Sigmoid function. The sigmoid function $\sigma(z) = \frac{1}{1 + e^{-z}}$ transforms any real number into the range $(0, 1)$. In this context, it is interpreted as the probability that the positive example $y_{pos}$ is indeed preferable to the negative example $y_{neg}$. If the difference $r_{\theta} \langle x, y_{pos} \rangle - r_{\theta} \langle x, y_{neg} \rangle$ is large and positive, then $\sigma(\cdot)$ will be close to 1. If the difference is large and negative, then $\sigma(\cdot)$ will be close to 0.
    *   **$\log(\cdot)$**: Natural logarithm. Applied to the output of the sigmoid function. Since $\sigma(\cdot)$ is in the range $(0, 1)$, $\log(\sigma(\cdot))$ will be a negative number. The negative sign before the summation in the loss function turns this into a positive value that we aim to minimize. The use of the logarithm is related to the concept of cross-entropy in classification tasks.

3. **$r_{\theta}\langle x, y \rangle$**:
    *   **Input Data:**
        *   The input to the model consists of textual data representing the **problem statement** $x$ and the **sequence of reasoning steps** $s$.
        *   At the first step, only the problem statement $x$ is input. At subsequent steps, the concatenation of the problem statement ($x$) and the sequence of previous solution steps up to the current step $s_i$ is input: $x \oplus s_1 \oplus s_2 \oplus ... \oplus s_{i-1} \oplus s_i$.
        *   **Important**: These textual data include not only the problem text but also the **sequence of reasoning steps in natural language and Python code, including comments**.
    *   **Representation as Vectors:**
        *   The textual data **are not input directly** into the neural network. Instead, they are **transformed into vector representations** using an embedding mechanism, as in other language models.
        *   These vector representations are **numerical arrays** that capture the semantic and syntactic information contained within the text.
    *   **Neural Network:**
        *   The vector representations of the input data are fed into a **neural network** that implements the function $r_{\theta}$.
        *   This network consists of several layers performing **nonlinear transformations** on the vector representations.
    *   **Scalar Output:**
        *   At the output of the neural network, a **single real number** — a scalar value in the range from -1 to 1 — is obtained.
        *   This number represents the **"preference" evaluation** of the current reasoning step $s_i$ for the problem $x$. The higher the value, the more preferable this step is.
    *   **Model Training:**
        *   The PPM model **is not trained on "absolute" preference values**. Instead, it is trained to distinguish between **"positive" and "negative" steps**.
        *   For each reasoning step, pairs are created from "positive" steps with higher Q-values and "negative" steps with lower Q-values.
        *   The PPM model is trained to predict **higher values for positive steps** and **lower values for negative steps**, using a pairwise ranking function.

**Key Points:**

*   The function $r_{\theta}(x, y)$ does not compute a dot product, but rather represents a **direct pass** through the network layers that processes the **textual input representations**.
*   At the output of this network, a **single real number** from -1 to 1 is obtained, which evaluates the "preference" of the step.
*   The model is trained not on absolute values, but on the **difference between "positive" and "negative" steps**.

Thus, the function $r_{\theta}(x, y)$ is a **neural network that takes textual representations of the problem and solution steps, transforms them into vectors, and outputs a scalar preference evaluation at the output.**

**Objective of Minimizing the Loss Function:**

By minimizing $L_{ppm}(\theta)$, we tune the parameters $\theta$ of the model $r_{\theta}$ such that for each preference pair $(y_{pos}, y_{neg})$, the model assigns a higher preference value to $y_{pos}$ compared to $y_{neg}$. In other words, the model learns to align with the provided preference data.

**Advantages of PPM over ORM:**

*   **More Effective Training:** By evaluating each step, the PPM provides a denser feedback signal for training. ORM, evaluating only the final answer, may face a sparse reward problem, especially in complex tasks where a successful outcome is reached after multiple steps.
*   **Robustness to Noisy Q-values:** Using preference pairs allows avoiding the direct use of Q-values as reward labels. Q-values, especially during the early stages of MCTS training, can be noisy and unstable. By comparing preferences, the PPM focuses on the relative usefulness of steps, making training more robust.

#### Q-values in rStar-Math MCTS (Monte Carlo Tree Search)

In rStar-Math MCTS (Monte Carlo Tree Search), each step is assigned Q-values reflecting its contribution to achieving the correct final answer. These values are used to guide node selection in MCTS towards more promising solution paths. The process of assigning Q-values involves several stages:

**1. Generating Trajectories**

- **Building the Search Tree**: MCTS iteratively builds a search tree, starting from the root node representing the question.
- **Generating Candidates**: At each step, the policy model (SLM) generates several candidates (nodes) representing intermediate solution steps.
- **Extracting Solution Trajectories**: From the search tree, solution trajectories are extracted, where each trajectory represents a path from the root to a terminal node (final step).
- **Assigning Q-values**: Each step $s_i$ in trajectory $t = x \oplus s_1 \oplus s_2 \oplus \ldots \oplus s_d$ is assigned a Q-value $Q(s_i)$.

**2. Self-Annotating Q-values (Looking at the Result)**

Two methods are used to assign Q-values, each evaluating the step's quality based on its contribution to obtaining the correct final answer:

**Terminal-Oriented Annotation (Finish-line Evaluation)**

- **Application**: Used in the first two training rounds when the PPM model is not yet reliable enough.
- **Updating Q-values**: The Q-value of step $s_i$ in $k$-th rollout $q(s_i)_k$ is updated based on the Q-value of the terminal (finish) node $s_d$ $q(s_d)$.
- **Evaluating Terminal Nodes**: Terminal nodes are evaluated as $q(s_d) = 1$ if the final answer is correct, and $q(s_d) = -1$ if the answer is incorrect.
- **Update Formula**: $q(s_i)_k = q(s_i)_{k-1} + q(s_d)_k$, where $q(s_i)_0 = 0$ in the first rollout.
- **Result**: Steps that frequently lead to the correct answer receive higher Q-values, while steps leading to incorrect answers receive low Q-values.
- **When Used**: At the very beginning of training, when the model knows nothing and is just trying everything.

**PPM-Enhanced Annotation (Hint-based Evaluation)**

- **Application**: Used starting from the third round, when the PPM model is already trained.
- **Predicting Q-values**: The PPM model predicts the initial Q-value $q(s_i)_0$ for each step $s_i$, based on the partial trajectory $x \oplus s_1 \oplus s_2 \oplus \ldots \oplus s_{i-1} \oplus s_i$. Formally, $q(s_i)_0 = PPM(x \oplus s_1 \oplus s_2 \oplus \ldots \oplus s_{i-1} \oplus s_i)$.
- **Updating Q-values**: This initial Q-value is updated based on the Q-value of the terminal node $q(s_d)$ during the backpropagation phase of MCTS. The update occurs according to the formula $q(s_i)_k = q(s_i)_{k-1} + q(s_d)_k$, as in terminal annotation.
- **Result**: PPM-enhanced MCTS helps the policy model generate higher-quality steps, as it guides solutions towards correct paths.
- **When Used**: When the PPM model has learned a bit and can make more informed choices.

**Summary**

In both methods, after a large number of MCTS rollouts, steps that consistently lead to correct answers receive higher Q-values, while steps that do not lead to correct answers receive low Q-values.

**3. Using Q-values in MCTS**

- **Node Selection**: Q-values are used to select the most promising nodes during the selection phase of MCTS.
- **UCT Formula**: The node $s$ is selected using the UCT (Upper Confidence bounds for Trees) formula:

  $$
  UCT(s) = Q(s) + c \sqrt{\frac{\ln N_{\text{parent}}(s)}{N(s)}}
  $$

Let's break down each component of the UCT formula:

* **$Q(s)$ – Average Q-value of node $s$:**
    * As you correctly noted, $Q(s)$ is calculated as the average of all Q-values obtained when visiting node $s$.
    * Formally: $Q(s) = \frac{q(s)}{N(s)}$, where:
        * $q(s)$ – this is the *sum* of all Q-values that were "returned" to node $s$ after completing simulations starting from this node. Each time a simulation reaches a terminal state, the result of this simulation (e.g., +1 for a correct answer, -1 for an incorrect one) is propagated back up the tree, updating $q(s)$ for each node on the path.
        * $N(s)$ – the total number of times node $s$ has been visited.
    * **Interpretation:** $Q(s)$ represents the current "value" or "quality" of transitioning to state $s$. A high $Q(s)$ indicates that visiting this node has, on average, led to good results in the past.

* **$c$ – Exploration-Exploitation Balance Constant:**
    * This is a hyperparameter that needs to be tuned.
    * **Role:** It determines how much MCTS is inclined to explore less visited branches of the tree.
    * **High $c$ value:** Encourages *exploration*. The algorithm will more frequently select nodes that have been visited fewer times, even if their current average Q-value is not the highest. This helps find potentially good but not yet fully explored paths.
    * **Low $c$ value:** Encourages *exploitation*. The algorithm will more frequently select nodes with high current average Q-values, relying on already accumulated information.
    * **Choosing $c$:** Usually requires experimentation. Too high a $c$ can lead to inefficient exploration of unpromising branches, while too low a $c$ can cause the algorithm to get stuck on suboptimal solutions.

* **$N(s)$ – Number of Visits to Node $s$:**
    * This is simply a counter of how many times MCTS has passed through node $s$.
    * **Role in the UCT formula:** The less a node has been visited, the more the second part of the UCT formula ($\sqrt{\frac{\ln N_{\text{parent}}(s)}{N(s)}}$) will influence the selection. This encourages the exploration of less-visited nodes.

* **$N_{\text{parent}}(s)$ – Number of Visits to Parent Node $s$:**
    * This is the number of times the parent node of node $s$ has been visited.
    * **Role in the UCT formula:** Together with $N(s)$, it determines the degree of "unexploredness" of node $s$ relative to its parent. The logarithm of the number of parent visits grows more slowly than the number of visits to the node itself, which over time reduces the influence of the exploration part of the formula for frequently visited nodes.

**Balancing Exploration and Exploitation in Practice:**

At the beginning of MCTS operation, when most nodes have been visited few times, the exploration part of the UCT formula plays a larger role, prompting the algorithm to try different paths. Over time, as information accumulates, the average Q-values become more reliable, and the exploitation part of the formula (the $Q(s)$ itself) begins to dominate, guiding the search towards the most promising branches.

### Example of PPM Loss Function

Now let's examine an example with the PPM loss function. The PPM (Policy Prediction Model) is trained to predict which steps are good in a given context. The loss function is used to measure how well the PPM makes these predictions and to adjust the model parameters during training.

**Scenario:**

Suppose, during MCTS, in a certain state (context) `x`, the PPM considers two possible next steps:

* **`y_pos`:** A step that, as revealed by MCTS, leads to a correct solution and has a high Q-score.
* **`y_neg`:** A step that does not lead to success and has a low Q-score.

**PPM Evaluations:**

Based on the context `x`, the PPM assigns evaluations (or "preferences") to each of these steps. For example:

* The PPM evaluates `y_pos` with a probability of 0.7.
* The PPM evaluates `y_neg` with a probability of -0.3. (Note that the PPM may output "raw" scores that are then converted into probabilities, for example, via softmax. In this context, these values can be interpreted as relative "quality scores" of the steps.)

**Difference and Sigmoid Function:**

Next, the difference between the evaluations of the positive and negative examples is considered:

* Difference = Evaluation(`y_pos`) - Evaluation(`y_neg`) = 0.7 - (-0.3) = 1.0

This difference shows how much the PPM distinguishes the good step from the bad one. Then, this difference is passed through the sigmoid function:

* Sigmoid(1.0) ≈ 0.73

The sigmoid function compresses values into the range from 0 to 1, which can be interpreted as the "confidence" of the model that `y_pos` is better than `y_neg`.

**PPM Loss Function (Logistic Loss):**

In this case, a form of logistic loss function is used, which is often applied in classification tasks where one needs to distinguish a "positive" example from a "negative" one. The loss function measures the "distance" between the model's prediction and the true label.

In a simplified form, the loss function aims to minimize the value when the model correctly predicts the preference and maximizes it when it makes a mistake. In our example, the contribution to the loss function is related to the logarithm of the value obtained after the sigmoid:

* Logarithm(0.73) ≈ -0.32

**Interpretation of Loss Contribution:**

* **If the PPM correctly predicts the preference of `y_pos`:** The difference between the evaluations will be positive, the value after the sigmoid will be close to 1, and the logarithm of this value will be close to 0 (or a small negative number). This means a *small loss*.
* **If the PPM incorrectly predicts the preference (e.g., the evaluation of `y_neg` is higher than `y_pos`):** The difference will be negative, the value after the sigmoid will be close to 0, and the logarithm of this value will be a large negative number. In the context of the loss function, this leads to a *large loss*.

**PPM Training Goal:**

The goal of training the PPM is to minimize the total loss on a large dataset. When the PPM makes incorrect predictions (e.g., fails to distinguish a good step from a bad one), the loss function "penalizes" the model, generating a large contribution to the total loss. This causes the optimization algorithm (e.g., gradient descent) to adjust the PPM parameters so that in the future, its predictions better match the observed results (Q-scores obtained from MCTS).

**In Conclusion:**

Q-values in MCTS guide the search, helping the algorithm explore promising areas of the solution space. The PPM loss function, in turn, uses information from Q-values to train the policy model to make more accurate predictions about the value of different steps, which in turn improves the effectiveness of MCTS.

> **Quote:**
> *"a novel method that trains an SLM acting as a process preference model, i.e., a PPM to implement the desired PRM, that reliably predicts a reward label for each math reasoning step."*

#### 2.4. Using Monte Carlo (MCTS) for Effective System 2 Thinking

Monte Carlo Tree Search (MCTS) plays a key role in implementing "deep thinking." MCTS breaks down complex mathematical problems into simpler single-step generation tasks, reducing the difficulty for the policy SLM compared to other System 2 methods. MCTS automatically assigns Q-values to each step based on its contribution to obtaining the correct final answer. This approach eliminates the need for labor-intensive, step-by-step human annotation for training the reward model. MCTS allows the model to explore various solution trajectories and select the most promising ones, simulating the process of deliberate thinking (System 2).

> **Quote:**
> *"MCTS breaks down complex math problems into simpler single-step generation tasks, reducing the difficulty for the policy SLM compared to other System 2 methods... Second, the step-by-step generation in MCTS naturally yields step-level training data for both models."*

### 3. Experimental Results

![Table_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-03/assets/Table_1.png)

> This table presents the performance of the rStar-Math model on various mathematical reasoning tasks. It shows the accuracy (pass@1) achieved by rStar-Math when applied to several small language models (SLM) of different sizes. The results are compared with the performance of OpenAI models and other baseline metrics, highlighting the significant improvements in mathematical reasoning capabilities achieved by rStar-Math through its deep thinking approach. The table demonstrates the model's ability to achieve state-of-the-art results on standard benchmarks and even surpass the performance of larger and more powerful models.

The effectiveness of rStar-Math was evaluated on various mathematical benchmarks. **rStar-Math significantly improves the mathematical reasoning capabilities of SLMs.** On the MATH benchmark, rStar-Math significantly increased the accuracy of the Qwen2.5-Math-7B model from 58.8% to 90.0%, and the Phi3-mini-3.8B model from 41.4% to 86.4%. Notably, rStar-Math surpassed the results of OpenAI o1-preview by +4.5% and +0.9% respectively. On the AIME Olympiad, rStar-Math demonstrated the ability to solve on average 53.3% (8 out of 15) of the problems, corresponding to the level of the top 20% of high-achieving high school students in mathematics. These results demonstrate that rStar-Math enables SLMs not only to reach the level of LLMs but also to surpass them in complex mathematical reasoning tasks. Comparative analyses show that rStar-Math outperforms several System 1 models, such as DeepSeek-Coder-V2-Instruct, Mathstral-7B-v0.1, NuminaMath-72B-CoT, LLaMA3.1-8B-Instruct, LLaMA3.1-70B-Instruct, Qwen2.5-Math-72B-Instruct, on various benchmarks (MATH, AIME 2024, AMC 2023, Olympiad Bench, College Math, GaokaoEn 2023). rStar-Math also demonstrates superiority over the System 2 model Qwen2.5-Math-72B-Instruct+72B ORM and models using the Best-of-N approach (Qwen2.5-Math-1.5B-Instruct+72B ORM, Qwen2-Math-7B-Instruct+72B ORM, Qwen2.5-Math-7B-Instruct+72B ORM). This underscores the effectiveness of the "deep thinking" System 2 implemented in rStar-Math and the critical role of the PPM. It was also noted that increasing computational power during testing (increasing the number of search trajectories) further improves rStar-Math results. Analysis showed that the PPM is a critically important component for enhancing System 2 reasoning performance, while the policy model is important but contributes less significantly to achieving the highest level of performance compared to the PPM.

### 4. Discussion

The results obtained with rStar-Math are significant for the development of efficient and accessible artificial intelligence models. The method demonstrates that small language models, when using innovative training approaches, can master complex tasks traditionally considered the prerogative of large models. The use of MCTS and PPM allows for effective exploration of the solution space and selection of the most promising reasoning trajectories, simulating the process of deep human thinking. It is important to note that rStar-Math surpasses both single-step (System 1) and multi-step (System 2) approaches, including models based on knowledge distillation. The proposed method is general and potentially applicable to various domains requiring multi-step reasoning and verification, such as theorem proving, coding, and common sense reasoning. In particular, rStar-Math demonstrated potential for proving mathematical statements, successfully handling an Olympiad-level problem by applying Fermat's Little Theorem. Applying rStar-Math to coding and common sense reasoning domains appears promising, but requires developing an effective feedback mechanism for evaluating the quality of generated reasoning trajectories. For coding, this could be extensive test cases, and for general reasoning, human annotation or mutual verification with other LLMs. The authors emphasize the importance of collecting more complex tasks for further improvement of rStar-Math, pointing to the potential for further development of the method.

### 5. Conclusion

The rStar-Math method represents a significant advancement in mathematical reasoning for small language models. Through an innovative combination of self-evolution, code-augmented CoT data synthesis, and training the Process Preference Model (PPM), rStar-Math enables SLMs to achieve and surpass the results of modern LLMs in complex mathematical problems. The use of Monte Carlo Tree Search (MCTS) to simulate deep thinking and step-by-step verification opens new perspectives for creating more efficient and accessible artificial intelligence models. The proposed approach has the potential for application in a wide range of tasks requiring complex reasoning and holds promise for stimulating further research in the field of training efficient models based on principles of deep thinking and self-improvement.

## Glossary of Key Terms

- **SLM (Small Language Model):** A small language model requiring fewer computational resources than large language models (LLM).
- **LLM (Large Language Model):** A large language model requiring significant computational resources and typically possessing greater reasoning capabilities.
- **MCTS (Monte Carlo Tree Search):** Monte Carlo Tree Search is a search algorithm used for decision-making, especially in games. In **rStar-Math**, **MCTS** is used to explore possible steps for solving mathematical problems.
- **CoT (Chain-of-Thought):** Chain-of-Thought is a method where the model generates intermediate reasoning steps before arriving at the final answer.
- **PPM (Process Preference Model):** Process Preference Model is a type of **reward** model that evaluates the quality of each step in solving a mathematical problem based on **Q-values**.
- **Q-value:** A measure of the quality or contribution of a specific step to solving the problem. In **rStar-Math**, a **Q-value** is assigned to each step based on its contribution to obtaining the correct answer.
- **Reward Model:** A model that evaluates the quality of solutions or individual solution steps, providing feedback for training the **policy** model.
- **ORM (Outcome Reward Model):** Outcome Reward Model is a model that evaluates only the final result of a problem solution (correct or incorrect) and does not consider intermediate steps.
- **PRM (Process Reward Model):** Process Reward Model provides feedback at the level of solution steps.
- **Self-Evolution:** Self-evolution is the process where the **policy SLM** and **PPM** are iteratively improved using higher-quality generated data in each round.
- **System 1 Thinking:** Fast, intuitive thinking, often error-prone.
- **System 2 Thinking:** Slow, deliberate thinking requiring significant computational resources.
- **Pass@1 Accuracy:** A metric defining the percentage of problems the model solves correctly on the first attempt.
- **Step-by-Step Verified Reasoning Trajectory:** A reasoning trajectory with step-by-step verification, where each solution step is verified using code execution.
- **Code-Augmented CoT:** A method of generating solution trajectories where each step is accompanied by executable code, ensuring step verification.
- **Pairwise Ranking Loss:** A loss function used for training the **PPM** based on preference pairs (positive step vs. negative step).
- **Terminal-guided Annotation:** A method of annotating **Q-values** that uses only the final result (correct or incorrect) to evaluate intermediate steps.
- **PPM-augmented Annotation:** A method of annotating **Q-values** that uses the **PPM** to evaluate each step, making annotations more accurate.

## Test Yourself: Quick Quiz (with Answers)

### What is the main idea of rStar-Math?
The main idea of rStar-Math is to apply "deep thinking" using Monte Carlo Tree Search (MCTS). In this process, an SLM acts as the policy model, generating a sequence of solution steps, while another SLM acts as the process reward model, evaluating these steps. This allows SLMs to achieve results comparable to LLMs in mathematical reasoning.

### What are the three key innovations underlying rStar-Math?
1. **Code-augmented CoT data synthesis method**
2. **New approach to training the Process Preference Model (PPM)**
3. **Self-development strategy**

### What is Code-augmented CoT and what role does it play in rStar-Math?
**Code-augmented CoT** is a data synthesis method where the SLM generates both a textual description of the reasoning and the corresponding Python code. Only those nodes where the code executes successfully are retained, allowing filtering out erroneous steps. This method plays a key role in improving the quality of data for training.

### What is the function of the Process Preference Model (PPM)?
The **PPM** evaluates each reasoning step based on preference pairs derived from Q-values. Unlike models that evaluate only the final result, the PPM provides more precise feedback at each stage, aiding the improvement of the reasoning process.

### How does the self-development process work in rStar-Math?
Self-development involves the iterative improvement of the policy SLM and the Process Preference Model (PPM). In each round, the models are trained on synthesized solutions, leading to progressive enhancement of their capabilities.

### How is MCTS used in rStar-Math?
**MCTS** (Monte Carlo Tree Search) is used to break down complex mathematical problems into simple single-step generation tasks. It assigns Q-values to each step based on its contribution to the correct answer, guiding the search process and improving the accuracy of reasoning.

### What conclusion can be drawn about the role of PPM in System 2 thinking?
The **PPM** is a critical factor in enhancing the performance of **System 2** reasoning. While the policy model is important, its contribution is not as critical as the PPM's when achieving the highest level of performance.

### What is the main problem in training models for mathematical reasoning that rStar-Math addresses?
rStar-Math addresses the problem of **lack of quality data** for training mathematical reasoning. This is achieved through:
- **Self-evolution** (self-development),
- **Code-augmented CoT** (data synthesis with code expansion),
- **PPM** (model for synthesizing and evaluating data).

### How does rStar-Math use Q-values?
**Q-values** are used to evaluate the contribution of each step to solving the problem. They are applied in:
- Training the **PPM**,
- The **MCTS** process for selecting the most promising reasoning trajectories.
Q-values are assigned automatically, eliminating the need for manual step-by-step annotation.