# Distillation Scaling Laws ü§ñ

[![arXiv](https://img.shields.io/badge/arXiv-2501.12948-b31b1b.svg  )](https://arxiv.org/abs/2502.08606  )

üìå What's in this issue:

1Ô∏è‚É£ Guide: What is Distillation?
- We'll break down how distillation works‚Äîthe method that enables "transferring" knowledge from a large teacher model to a compact student model. Why is it so popular, and how does it help deploy models locally?

2Ô∏è‚É£ Comparative Analysis: Distillation vs Quantization
- Which is more effective: compressing a model via distillation or via quantization? Pros, cons, and pitfalls of each approach.

3Ô∏è‚É£ Review of Apple's Paper: Researchers investigated distillation scaling laws and made intriguing discoveries:
- Why can an overly powerful teacher degrade student performance?
- How does distillation obey scaling laws?
- What equation optimizes the distillation process?
- Why is distillation more effective than training from scratch‚Äîbut only under specific conditions?

üîç Why is this important?
- Distillation is a powerful tool, but its effective use requires understanding its nuances. In this issue, we'll explore how to avoid pitfalls and achieve maximum efficiency.

---

<div align="center">

**Explore with us üöÄ**

‚≠ê Star this repository if you found it helpful

</div>