{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Qwen3 MoE ‚Äî –£—á–µ–±–Ω—ã–π Walkthrough\n",
    "\n",
    "–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å! –≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ –ø—Ä–æ–≤–µ–¥—ë—Ç –≤–∞—Å —á–µ—Ä–µ–∑ —Å–±–æ—Ä–∫—É Qwen3 MoE: –æ—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –¥–æ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –º–∏–Ω–∏‚Äë–¥–µ–º–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
    "\n",
    "**–ß—Ç–æ –≤—ã —Å–¥–µ–ª–∞–µ—Ç–µ:**\n",
    "- üß© –°–æ–±–µ—Ä—ë—Ç–µ –º–æ–¥—É–ª–∏: RMSNorm ‚Üí RoPE ‚Üí SwiGLU ‚Üí GQA ‚Üí Transformer ‚Üí MoE\n",
    "- üß™ –ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —Å–µ–∫—Ü–∏–∏ ‚Äî –±—ã—Å—Ç—Ä—ã–π —á–µ–∫ —Ñ–æ—Ä–º –∏ —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "- üöÄ –§–∏–Ω–∞–ª: `Qwen3MoEModel` + –∫–æ—Ä–æ—Ç–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (greedy/sampling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è –û–∫—Ä—É–∂–µ–Ω–∏–µ –∏ Reproducibility\n",
    "–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –∏–º–ø–æ—Ä—Ç—ã, –≤—ã–±–µ—Ä–∏—Ç–µ `device`, –∑–∞—Ñ–∏–∫—Å–∏—Ä—É–π—Ç–µ —Å–∏–¥—ã –∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA.\n",
    "\n",
    "**üß™ Quick Check (–∫–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞):**\n",
    "\n",
    "```python\n",
    "import torch, random, numpy as np\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "set_seed(42); print('CUDA:', torch.cuda.is_available())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß≠ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b39b623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_position_embeddings = 2048\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Qwen3 MoE Model Configuration\n",
    "\n",
    "–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—Å–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ.\n",
    "\"\"\"\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Qwen3Config:\n",
    "    \"\"\"\n",
    "    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Qwen3 MoE –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:\n",
    "    ------------\n",
    "    - Vocabulary: GPT-2 tokenizer (50257 —Ç–æ–∫–µ–Ω–æ–≤)\n",
    "    - Embedding: 1024-dim continuous vectors\n",
    "    - Transformer: 12 MoE blocks —Å GQA + RoPE + SwiGLU\n",
    "    - MoE: 8 —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, 2 –∞–∫—Ç–∏–≤–Ω—ã—Ö per token (25% –∞–∫—Ç–∏–≤–∞—Ü–∏—è)\n",
    "    - Output: Language modeling head (1024 ‚Üí 50257)\n",
    "\n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "    ----------\n",
    "    Model Architecture:\n",
    "        vocab_size: –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (GPT-2 = 50257)\n",
    "        hidden_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è (embedding dimension)\n",
    "        num_layers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ MoE Transformer –±–ª–æ–∫–æ–≤\n",
    "        intermediate_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å FFN –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞\n",
    "\n",
    "    Attention:\n",
    "        num_attention_heads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ Query –≥–æ–ª–æ–≤ (–¥–ª—è GQA)\n",
    "        num_key_value_heads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ Key/Value –≥–æ–ª–æ–≤ (GQA –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞)\n",
    "        max_position_embeddings: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        rope_theta: –ë–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–ª—è RoPE (10000.0 —Å—Ç–∞–Ω–¥–∞—Ä—Ç)\n",
    "\n",
    "    MoE Specific:\n",
    "        num_experts: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–º MoE —Å–ª–æ–µ\n",
    "        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ per token\n",
    "        balance_loss_coef: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è load balancing loss (–æ–±—ã—á–Ω–æ 0.01)\n",
    "\n",
    "    Regularization:\n",
    "        dropout: Dropout rate –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (0.0 = –æ—Ç–∫–ª—é—á–µ–Ω)\n",
    "\n",
    "    Training:\n",
    "        initializer_range: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤\n",
    "\n",
    "    –ü—Ä–∏–º–µ—Ä—ã:\n",
    "    --------\n",
    "    >>> # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (0.6B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)\n",
    "    >>> config = Qwen3Config()\n",
    "    >>> print(f\"Model size: ~{config.vocab_size * config.hidden_size / 1e9:.2f}B parameters\")\n",
    "\n",
    "    >>> # –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è\n",
    "    >>> config = Qwen3Config(\n",
    "    ...     hidden_size=2048,\n",
    "    ...     num_layers=24,\n",
    "    ...     num_experts=16\n",
    "    ... )\n",
    "    \"\"\"\n",
    "\n",
    "    # Model Architecture\n",
    "    vocab_size: int = 50257  # GPT-2 tokenizer\n",
    "    hidden_size: int = 1024\n",
    "    num_layers: int = 12\n",
    "    intermediate_size: int = 2048  # 2 * hidden_size –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞\n",
    "\n",
    "    # Attention Configuration\n",
    "    num_attention_heads: int = 16  # Query heads\n",
    "    num_key_value_heads: int = 4   # KV heads (GQA: 4x –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞)\n",
    "    max_position_embeddings: int = 2048\n",
    "    rope_theta: float = 10000.0\n",
    "\n",
    "    # MoE Configuration\n",
    "    num_experts: int = 8\n",
    "    top_k: int = 2\n",
    "    balance_loss_coef: float = 0.01\n",
    "\n",
    "    # Regularization\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # Training\n",
    "    initializer_range: float = 0.02\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.\"\"\"\n",
    "        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "        assert self.vocab_size > 0, \"vocab_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º\"\n",
    "        assert self.hidden_size > 0, \"hidden_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º\"\n",
    "        assert self.num_layers > 0, \"num_layers –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º\"\n",
    "        assert self.intermediate_size > 0, \"intermediate_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º\"\n",
    "\n",
    "        # Attention –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "        assert self.num_attention_heads > 0, \"num_attention_heads –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º\"\n",
    "        assert self.num_key_value_heads > 0, \"num_key_value_heads –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º\"\n",
    "        assert (\n",
    "            self.num_attention_heads % self.num_key_value_heads == 0\n",
    "        ), \"num_attention_heads –¥–æ–ª–∂–µ–Ω –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ num_key_value_heads\"\n",
    "        assert (\n",
    "            self.hidden_size % self.num_attention_heads == 0\n",
    "        ), \"hidden_size –¥–æ–ª–∂–µ–Ω –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ num_attention_heads\"\n",
    "\n",
    "        # MoE –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "        assert self.num_experts > 0, \"num_experts –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º\"\n",
    "        assert self.top_k > 0, \"top_k –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º\"\n",
    "        assert self.top_k <= self.num_experts, \"top_k –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ num_experts\"\n",
    "\n",
    "        # Regularization –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "        assert 0.0 <= self.dropout <= 1.0, \"dropout –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1]\"\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ —Å–ª–æ–≤–∞—Ä—å.\"\"\"\n",
    "        return {\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"hidden_size\": self.hidden_size,\n",
    "            \"num_layers\": self.num_layers,\n",
    "            \"intermediate_size\": self.intermediate_size,\n",
    "            \"num_attention_heads\": self.num_attention_heads,\n",
    "            \"num_key_value_heads\": self.num_key_value_heads,\n",
    "            \"max_position_embeddings\": self.max_position_embeddings,\n",
    "            \"rope_theta\": self.rope_theta,\n",
    "            \"num_experts\": self.num_experts,\n",
    "            \"top_k\": self.top_k,\n",
    "            \"balance_loss_coef\": self.balance_loss_coef,\n",
    "            \"dropout\": self.dropout,\n",
    "            \"initializer_range\": self.initializer_range,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84214c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞\n",
    "from typing import Optional\n",
    "\n",
    "# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Root Mean Square Layer Normalization ‚Äî —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ LayerNorm.\n",
    "        –§–æ—Ä–º—É–ª–∞: RMSNorm(x) = x / sqrt(mean(x¬≤) + eps) * weight\n",
    "\n",
    "        –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç LayerNorm, RMSNorm –Ω–µ —Ü–µ–Ω—Ç—Ä–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ (–Ω–µ –≤—ã—á–∏—Ç–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ),\n",
    "        —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ª—É—á—à—É—é —á–∏—Å–ª–µ–Ω–Ω—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
    "        –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.\n",
    "\n",
    "        –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:\n",
    "        - –ú–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (–Ω–µ—Ç —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è)\n",
    "        - –õ—É—á—à–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö\n",
    "        - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö (LLaMA, Qwen, –∏ –¥—Ä.)\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        normalized_shape: int –∏–ª–∏ tuple —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
    "                          –û–±—ã—á–Ω–æ —Ä–∞–≤–µ–Ω hidden_size –º–æ–¥–µ–ª–∏.\n",
    "        eps: –ú–∞–ª–∞—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏, –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –Ω–æ–ª—å.\n",
    "                          –î–æ–±–∞–≤–ª—è–µ—Ç—Å—è –ø–æ–¥ –∫–æ—Ä–µ–Ω—å: sqrt(mean(x¬≤) + eps).\n",
    "        elementwise_affine: –ï—Å–ª–∏ True, –¥–æ–±–∞–≤–ª—è–µ—Ç –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã weight.\n",
    "                            –ï—Å–ª–∏ False, –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> import torch\n",
    "        >>> rms_norm = RMSNorm(512)\n",
    "        >>> x = torch.randn(10, 20, 512)\n",
    "        >>> output = rms_norm(x)\n",
    "        >>> output.shape\n",
    "        torch.Size([10, 20, 512])\n",
    "\n",
    "        >>> # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: RMS –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–ª–∏–∑–æ–∫ –∫ 1\n",
    "        >>> rms_value = torch.sqrt(torch.mean(output**2, dim=-1))\n",
    "        >>> print(f\"RMS after normalization: {rms_value.mean():.4f}\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        normalized_shape: int,\n",
    "        eps: float = 1e-6,\n",
    "        elementwise_affine: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ normalized_shape –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ forward\n",
    "        # TODO: –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ eps –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "        # TODO: –ï—Å–ª–∏ elementwise_affine=True, —Å–æ–∑–¥–∞–π—Ç–µ Parameter weight —Å —Ñ–æ—Ä–º–æ–π (normalized_shape)\n",
    "        # TODO: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ weight –µ–¥–∏–Ω–∏—Ü–∞–º–∏: torch.ones()\n",
    "        # TODO: –ï—Å–ª–∏ elementwise_affine=False, –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ weight –∫–∞–∫ None\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É weight –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –µ–¥–∏–Ω–∏—Ü–∞–º–∏, –∞ –Ω–µ –Ω—É–ª—è–º–∏?\n",
    "        # - –ß—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç, –µ—Å–ª–∏ eps —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π?\n",
    "        # - –ó–∞—á–µ–º –Ω—É–∂–µ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä elementwise_affine?\n",
    "        # pass\n",
    "\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "\n",
    "        # –°–æ–∑–¥–∞–µ–º –æ–±—É—á–∞–µ–º—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è (g –≤ —Ñ–æ—Ä–º—É–ª–µ RMSNorm)\n",
    "        if elementwise_affine == True:\n",
    "            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º weight –µ–¥–∏–Ω–∏—Ü–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "            # weight —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –≤–µ–∫—Ç–æ—Ä—É g –≤ —Ñ–æ—Ä–º—É–ª–µ: g ‚äô (x/RMS(x))\n",
    "            self.weight = nn.Parameter(\n",
    "                torch.ones(normalized_shape)\n",
    "            )\n",
    "        else:\n",
    "            # –ï—Å–ª–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è, —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –ø—É—Å—Ç–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä\n",
    "            # –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏ PyTorch (state_dict –∏ –¥—Ä.)\n",
    "            self.register_parameter('weight', None)\n",
    "                    \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –†–µ–∞–ª–∏–∑—É–µ—Ç —Ñ–æ—Ä–º—É–ª—É RMSNorm: x / sqrt(mean(x¬≤) + eps) * weight.\n",
    "            –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –ø–æ –µ–≥–æ —Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é,\n",
    "            –±–µ–∑ —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è (–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç LayerNorm).\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (..., normalized_shape)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä —Ç–æ–π –∂–µ —Ñ–æ—Ä–º—ã\n",
    "\n",
    "        Raises:\n",
    "        ---------------\n",
    "            RuntimeError: –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω—è—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å x –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å normalized_shape\n",
    "        \"\"\"\n",
    "        # TODO: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å x —Ä–∞–≤–Ω–∞ self.normalized_shape\n",
    "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ –∫–≤–∞–¥—Ä–∞—Ç—ã —ç–ª–µ–º–µ–Ω—Ç–æ–≤: x_squared = x * x –∏–ª–∏ x.pow(2)\n",
    "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ —Å—Ä–µ–¥–Ω–µ–µ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –æ—Å–∏: torch.mean\n",
    "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ RMS: torch.sqrt\n",
    "        # TODO: –ù–æ—Ä–º–∞–ª–∏–∑—É–π—Ç–µ –ø–æ —Ñ–æ—Ä–º—É–ª–µ: RMSNorm(x) = x / sqrt(mean(x¬≤) + eps)\n",
    "        # TODO: –ï—Å–ª–∏ –µ—Å—Ç—å weight, –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ: output = normalized * self.weight\n",
    "        # TODO: –í–µ—Ä–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –≤–∞–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å keepdim=True –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ —Å—Ä–µ–¥–Ω–µ–≥–æ?\n",
    "        # - –ö–∞–∫ –ø–æ–≤–µ–¥–µ—Ç —Å–µ–±—è —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞ —Ç–µ–Ω–∑–æ—Ä–∞—Ö —Ä–∞–∑–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏?\n",
    "        # - –ß—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ –ø—Ä–∏ –æ–±—Ä–∞—Ç–Ω–æ–º –ø—Ä–æ—Ö–æ–¥–µ?\n",
    "        # - –ö–∞–∫ RMSNorm –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π?\n",
    "        # pass\n",
    "\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞\n",
    "        if x.shape[-1] != self.normalized_shape:\n",
    "            raise RuntimeError(\n",
    "                f\"–ü–æ—Å–ª–µ–¥–Ω—è—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å x –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–∞–≤–Ω–∞ {self.normalized_shape}\"\n",
    "            )\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º –∫–≤–∞–¥—Ä–∞—Ç—ã —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n",
    "        x_sqr = x * x\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –æ—Å–∏\n",
    "        # keepdim=True —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –≤–µ—â–∞–Ω–∏—è –ø—Ä–∏ –¥–µ–ª–µ–Ω–∏–∏\n",
    "        mean_sqr = torch.mean(x_sqr, dim=-1, keepdim=True)\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º RMS (–∫–æ—Ä–µ–Ω—å –∏–∑ —Å—Ä–µ–¥–Ω–µ–≥–æ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤) —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º eps –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "        rms = torch.sqrt(mean_sqr + self.eps)\n",
    "\n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä, –¥–µ–ª—è –Ω–∞ RMS\n",
    "        x_norm = x / rms\n",
    "\n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ, –µ—Å–ª–∏ –µ—Å—Ç—å weight\n",
    "        if self.weight is not None:\n",
    "            # –ü–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞–µ–º—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä weight\n",
    "            return x_norm * self.weight\n",
    "        \n",
    "        return x_norm\n",
    "        \n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        \"\"\"–°—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥—É–ª—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏.\"\"\"\n",
    "        return f'normalized_shape={self.normalized_shape}, eps={self.eps}, elementwise_affine={self.weight is not None}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß≠ –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Rotary Position Embedding (RoPE) ‚Äî –º–µ—Ç–æ–¥ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è,\n",
    "        –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –≤—Ä–∞—â–µ–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –ø–ª–æ—Å–∫–æ—Å—Ç–∏.\n",
    "        \n",
    "        –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, RoPE –∫–æ–¥–∏—Ä—É–µ—Ç\n",
    "        –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –¥–ª—è\n",
    "        –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.\n",
    "        \n",
    "        –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:\n",
    "        - –ò–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ —Å–¥–≤–∏–≥—É (shift invariance)\n",
    "        - –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π\n",
    "        - –•–æ—Ä–æ—à–∞—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è –Ω–∞ –¥–ª–∏–Ω—ã, –ø—Ä–µ–≤—ã—à–∞—é—â–∏–µ –æ–±—É—á–∞—é—â–∏–µ\n",
    "        - –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –ª–∏–Ω–µ–π–Ω—ã–º–∏ attention –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏\n",
    "        \n",
    "    Args:\n",
    "    ---------------\n",
    "        dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —á–µ—Ç–Ω–æ–π –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è)\n",
    "        base: –ë–∞–∑–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç (–æ–±—ã—á–Ω–æ 10000.0)\n",
    "        max_position: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è (–∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è)\n",
    "        scale: –ú–∞—Å—à—Ç–∞–±–∏—Ä—É—é—â–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è —á–∞—Å—Ç–æ—Ç (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è RoPE scaling)\n",
    "        \n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> import torch\n",
    "        >>> rope = RoPE(dim=128)\n",
    "        >>> q = torch.randn(2, 4, 128)  # [batch_size, seq_len, dim]\n",
    "        >>> k = torch.randn(2, 4, 128)  # [batch_size, seq_len, dim]\n",
    "        >>> q_pos, k_pos = rope(q, k)\n",
    "        >>> q_pos.shape, k_pos.shape\n",
    "        (torch.Size([2, 4, 128]), torch.Size([2, 4, 128]))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        base: float = 10000.0,\n",
    "        max_position: int = 2048,\n",
    "        scale: float = 1.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ dim —á–µ—Ç–Ω–æ–µ (–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è)\n",
    "        # TODO: –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (dim, base, max_position, scale)\n",
    "        # TODO: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç–µ sin/cos —Ç–∞–±–ª–∏—Ü—É —á–∞—Å—Ç–æ—Ç –¥–ª—è –ø–æ–∑–∏—Ü–∏–π [0, max_position)\n",
    "        #       - –°–æ–∑–¥–∞–π—Ç–µ —Ç–µ–Ω–∑–æ—Ä –ø–æ–∑–∏—Ü–∏–π;\n",
    "        #       - –°–æ–∑–¥–∞–π—Ç–µ —Ç–µ–Ω–∑–æ—Ä —á–∞—Å—Ç–æ—Ç;\n",
    "        #       - –í—ã—á–∏—Å–ª–∏—Ç–µ —É–≥–ª—ã –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ –∏ —á–∞—Å—Ç–æ—Ç—ã;\n",
    "        #       - –í—ã—á–∏—Å–ª–∏—Ç–µ sin –∏ cos;\n",
    "        #       - –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –∫—ç—à –∫–∞–∫ –±—É—Ñ–µ—Ä—ã (–Ω–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã).\n",
    "        \n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É RoPE –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π?\n",
    "        # - –ö–∞–∫ –≤—ã–±–æ—Ä base –≤–ª–∏—è–µ—Ç –Ω–∞ —á–∞—Å—Ç–æ—Ç–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è?\n",
    "        # - –ü–æ—á–µ–º—É –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç scale < 1.0?\n",
    "        # pass\n",
    "\n",
    "        if dim % 2 != 0:\n",
    "            raise ValueError('–î–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã –ø–∞—Ä–∞–º–µ—Ç—Ä dim –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–µ—Ç–Ω—ã–º')\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–µ—Ç–æ–¥–∞—Ö\n",
    "        self.dim = dim                    # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —á–µ—Ç–Ω–æ–π)\n",
    "        self.base = base                  # –ë–∞–∑–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç (–æ–±—ã—á–Ω–æ 10000.0)\n",
    "        self.max_position = max_position  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "        self.scale = scale                # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É—é—â–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤\n",
    "\n",
    "        # –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä –ø–æ–∑–∏—Ü–∏–π [0, 1, 2, ..., max_position-1]\n",
    "        position = torch.arange(start = 0, end = max_position, step =1).float()\n",
    "\n",
    "        # –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä —Å —á–µ—Ç–Ω—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏ [0, 2, 4, ...] –¥–ª—è –∞–¥—Ä–µ—Å–∞—Ü–∏–∏ –ø–∞—Ä –∏–∑–º–µ—Ä–µ–Ω–∏–π\n",
    "        # –ö–∞–∂–¥–∞—è –ø–∞—Ä–∞ (2i, 2i+1) –±—É–¥–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è –∫–∞–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —á–∏—Å–ª–æ\n",
    "        idx = torch.arange(start=0, end=dim, step=2).float()\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º —á–∞—Å—Ç–æ—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –∏–∑–º–µ—Ä–µ–Ω–∏–π –ø–æ —Ñ–æ—Ä–º—É–ª–µ: œâ_d = base^(-2d/D)\n",
    "        # - –ù–∏–∑–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã (–Ω–∞—á–∞–ª–æ —Ç–µ–Ω–∑–æ—Ä–∞) –º–µ–Ω—è—é—Ç—Å—è –º–µ–¥–ª–µ–Ω–Ω–æ —Å –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–∏\n",
    "        # - –í—ã—Å–æ–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã (–∫–æ–Ω–µ—Ü —Ç–µ–Ω–∑–æ—Ä–∞) –º–µ–Ω—è—é—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ\n",
    "        # - scale < 1.0 –∑–∞–º–µ–¥–ª—è–µ—Ç –≤—Ä–∞—â–µ–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–π —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ –Ω–∞ –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã\n",
    "        freqs = base ** (-idx / dim)\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º —É–≥–ª—ã –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–æ–∑–∏—Ü–∏–∏ –∏ —á–∞—Å—Ç–æ—Ç—ã\n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º scale –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤\n",
    "        # –§–æ—Ä–º–∞: (max_position, dim/2)\n",
    "        angles = position.unsqueeze(1) * freqs.unsqueeze(0) / scale\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º sin –∏ cos –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–≥–ª–∞\n",
    "        cos = torch.cos(angles)  # (max_position, dim/2)\n",
    "        sin = torch.sin(angles)  # (max_position, dim/2)\n",
    "\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º sin –∏ cos –∫–∞–∫ –±—É—Ñ–µ—Ä—ã (–Ω–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º register_buffer –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å CUDA –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏\n",
    "        self.register_buffer('sin_cached', sin)\n",
    "        self.register_buffer('cos_cached', cos)\n",
    "\n",
    "        \n",
    "    def _compute_rope_embeddings(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        positions: torch.Tensor,\n",
    "        is_query: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–æ—Ç–∞—Ü–∏–æ–Ω–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ç–µ–Ω–∑–æ—Ä—É.\n",
    "        \n",
    "        Args:\n",
    "        ---------------\n",
    "            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (..., seq_len, dim)\n",
    "            positions: –¢–µ–Ω–∑–æ—Ä –ø–æ–∑–∏—Ü–∏–π —Ñ–æ—Ä–º—ã (..., seq_len)\n",
    "            is_query: –§–ª–∞–≥, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –≤—Ö–æ–¥ query (True) –∏–ª–∏ key (False)\n",
    "            \n",
    "        Returns:\n",
    "        ---------------\n",
    "            –¢–µ–Ω–∑–æ—Ä —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ç–æ–π –∂–µ —Ñ–æ—Ä–º—ã, —á—Ç–æ –∏ x\n",
    "        \"\"\"\n",
    "        # TODO: –ü–æ–ª—É—á–∏—Ç–µ —Ñ–æ—Ä–º—É –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ x\n",
    "        # TODO: –ò–∑–≤–ª–µ–∫–∏—Ç–µ sin –∏ cos –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π –∏–∑ –∫—ç—à–∞ –∏–ª–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ –∏—Ö –Ω–∞ –ª–µ—Ç—É\n",
    "        # TODO: –ï—Å–ª–∏ positions –≤—ã—Ö–æ–¥—è—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã max_position, –≤—ã—á–∏—Å–ª–∏—Ç–µ sin –∏ cos –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ –≤—Ä–∞—â–µ–Ω–∏–µ –∫ –∫–∞–∂–¥–æ–π –ø–∞—Ä–µ —Å–æ—Å–µ–¥–Ω–∏—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π (dim[i], dim[i+1])\n",
    "        #       - –î–ª—è —á–µ—Ç–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ i: x[..., i] = x[..., i] * cos - x[..., i+1] * sin\n",
    "        #       - –î–ª—è –Ω–µ—á–µ—Ç–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ i: x[..., i] = x[..., i] * sin + x[..., i-1] * cos\n",
    "        # TODO: –ï—Å–ª–∏ is_query=False (–¥–ª—è –∫–ª—é—á–µ–π), –∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–π—Ç–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–∞—â–µ–Ω–∏—è\n",
    "        # TODO: –í–µ—Ä–Ω–∏—Ç–µ —Ç–µ–Ω–∑–æ—Ä —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º\n",
    "        \n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –¥–ª—è query –∏ key –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–∞–∑–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤—Ä–∞—â–µ–Ω–∏—è?\n",
    "        # - –ö–∞–∫ RoPE –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ?\n",
    "        # - –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ max_position?\n",
    "        # pass\n",
    "\n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Ñ–æ—Ä–º—É –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞\n",
    "        x_shape = x.shape\n",
    "\n",
    "        if positions.max() < self.max_position:\n",
    "            sin = self.sin_cached[positions]\n",
    "            cos = self.cos_cached[positions]\n",
    "        else:\n",
    "            # –¢–∞–∫ –∂–µ –∫–∞–∫ –≤ __init__\n",
    "            idx = torch.arange(start=0, end=self.dim, step=2).float()\n",
    "            freqs = self.base ** (-idx / self.dim)\n",
    "            angles = positions.unsqueeze(1) * freqs.unsqueeze(0) / self.scale\n",
    "\n",
    "            cos = torch.cos(angles)\n",
    "            sin = torch.sin(angles)\n",
    "\n",
    "        # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ç–æ–π –∂–µ —Ñ–æ—Ä–º—ã, —á—Ç–æ –∏ –≤—Ö–æ–¥–Ω–æ–π\n",
    "        x_out = torch.zeros_like(x)\n",
    "\n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—Ä–∞—â–µ–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –≤—Ö–æ–¥–∞ (query –∏–ª–∏ key)\n",
    "        if is_query:\n",
    "            # –î–ª—è query - –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–∞—â–µ–Ω–∏—è\n",
    "            x_out[..., 0::2] = x[..., 0::2] * cos - x[..., 1::2] * sin\n",
    "            x_out[..., 1::2] = x[..., 1::2] * cos + x[..., 0::2] * sin\n",
    "        else:\n",
    "            # –î–ª—è key - –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–∞—â–µ–Ω–∏—è\n",
    "            x_out[..., 0::2] = x[..., 0::2] * cos + x[..., 1::2] * sin\n",
    "            x_out[..., 1::2] = x[..., 1::2] * cos - x[..., 0::2] * sin\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        positions: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä–∏–º–µ–Ω—è–µ—Ç RoPE –∫ query –∏ key —Ç–µ–Ω–∑–æ—Ä–∞–º.\n",
    "        \n",
    "        Args:\n",
    "        ---------------\n",
    "            query: Query —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (..., seq_len, dim)\n",
    "            key: Key —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (..., seq_len, dim)\n",
    "            positions: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä –ø–æ–∑–∏—Ü–∏–π. –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è torch.arange(seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            –ö–æ—Ä—Ç–µ–∂ (query_pos, key_pos) —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º\n",
    "        \"\"\"\n",
    "        # TODO: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ query –∏ key —Ä–∞–≤–Ω–æ self.dim\n",
    "        # TODO: –ü–æ–ª—É—á–∏—Ç–µ seq_len –∏–∑ —Ñ–æ—Ä–º—ã query\n",
    "        # TODO: –ï—Å–ª–∏ positions –Ω–µ —É–∫–∞–∑–∞–Ω—ã, —Å–æ–∑–¥–∞–π—Ç–µ –∏—Ö —á–µ—Ä–µ–∑ torch\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ _compute_rope_embeddings –∫ query\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ _compute_rope_embeddings –∫ key\n",
    "        # TODO: –í–µ—Ä–Ω–∏—Ç–µ –∫–æ—Ä—Ç–µ–∂ (query_pos, key_pos)\n",
    "        \n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ö–∞–∫ RoPE –≤–ª–∏—è–µ—Ç –Ω–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É query –∏ key –≤ attention?\n",
    "        # - –ü–æ—á–µ–º—É –≤–∞–∂–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å RoPE –∫ –æ–±–æ–∏–º —Ç–µ–Ω–∑–æ—Ä–∞–º - query –∏ key?\n",
    "        # - –ö–∞–∫ –º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è RoPE –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π?\n",
    "        # pass\n",
    "\n",
    "        if query.shape[-1] != self.dim or key.shape[-1] != self.dim:\n",
    "            raise ValueError(f'–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å query –∏ key –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–∞–≤–Ω–∞ {self.dim}')\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–æ—Ä–º—ã query –∏ key —Å–æ–≤–ø–∞–¥–∞—é—Ç –ø–æ seq_len\n",
    "        if query.shape[-2] != key.shape[-2]:\n",
    "            raise ValueError(f'–î–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π query –∏ key –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å')\n",
    "        \n",
    "        seq_len = query.shape[-2]\n",
    "\n",
    "        if positions is not None:\n",
    "            query_rope = self._compute_rope_embeddings(query, positions, is_query=True)\n",
    "            key_rope   = self._compute_rope_embeddings(key,   positions, is_query=False)\n",
    "\n",
    "            return query_rope, key_rope\n",
    "\n",
    "        else:\n",
    "            # –°–æ–∑–¥–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ –æ—Ç 0 –¥–æ seq_len-1 –Ω–∞ —Ç–æ–º –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ, —á—Ç–æ –∏ query\n",
    "            positions = torch.arange(seq_len, device=query.device)\n",
    "            # –ü—Ä–∏–º–µ–Ω—è–µ–º RoPE –∫ query –∏ key\n",
    "            query_rope = self._compute_rope_embeddings(query, positions, is_query=True)\n",
    "            key_rope = self._compute_rope_embeddings(key, positions, is_query=False)\n",
    "            \n",
    "            return query_rope, key_rope\n",
    "\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        \"\"\"–°—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥—É–ª—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏.\"\"\"\n",
    "        return f'dim={self.dim}, base={self.base}, max_position={self.max_position}, scale={self.scale}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° –ê–∫—Ç–∏–≤–∞—Ü–∏–∏: SwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14881fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞\n",
    "from typing import Optional\n",
    "\n",
    "# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Swish –∞–∫—Ç–∏–≤–∞—Ü–∏—è: x * sigmoid(x)\n",
    "        \n",
    "        –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –≤ —Å—Ç–∞—Ç—å–µ \"Searching for Activation Functions\" (Ramachandran et al., 2017).\n",
    "        –¢–∞–∫–∂–µ –∏–∑–≤–µ—Å—Ç–Ω–∞ –∫–∞–∫ SiLU (Sigmoid Linear Unit) –≤ PyTorch.\n",
    "        \n",
    "        –§–æ—Ä–º—É–ª–∞: Swish(x) = x * sigmoid(x)\n",
    "        \n",
    "        –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:\n",
    "        - –ì–ª–∞–¥–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è (–≤—Å–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç)\n",
    "        - –ù–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ —Å–≤–µ—Ä—Ö—É (–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç sigmoid)\n",
    "        - –ò–º–µ–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å, –±–ª–∏–∑–∫—É—é –∫ ReLU –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "        - –ò–º–µ–µ—Ç –Ω–µ–±–æ–ª—å—à–æ–µ –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "        \n",
    "    Args:\n",
    "    ---------------\n",
    "        beta: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: x * sigmoid(beta * x)\n",
    "              –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é beta=1.0 (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Swish)\n",
    "        \n",
    "    Returns:\n",
    "    ---------------\n",
    "        –¢–µ–Ω–∑–æ—Ä —Ç–æ–π –∂–µ —Ñ–æ—Ä–º—ã, —á—Ç–æ –∏ –≤—Ö–æ–¥, —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω–æ–π Swish –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π\n",
    "        \n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> import torch\n",
    "        >>> swish = Swish()\n",
    "        >>> x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "        >>> swish(x)\n",
    "        tensor([-0.2384, -0.2689, 0.0000, 0.7311, 1.7616])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, beta: float = 1.0):\n",
    "        super().__init__()\n",
    "        # TODO: –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ beta –ø–∞—Ä–∞–º–µ—Ç—Ä\n",
    "        # pass\n",
    "\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä–∏–º–µ–Ω—è–µ—Ç Swish –∞–∫—Ç–∏–≤–∞—Ü–∏—é –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ç–µ–Ω–∑–æ—Ä—É.\n",
    "        \n",
    "        Args:\n",
    "        ---------------\n",
    "            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –ª—é–±–æ–π —Ñ–æ—Ä–º—ã\n",
    "            \n",
    "        Returns:\n",
    "        ---------------\n",
    "            –¢–µ–Ω–∑–æ—Ä —Ç–æ–π –∂–µ —Ñ–æ—Ä–º—ã —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω–æ–π Swish –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π\n",
    "        \"\"\"\n",
    "        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ Swish –∞–∫—Ç–∏–≤–∞—Ü–∏—é: x * sigmoid(beta * x)\n",
    "        # pass\n",
    "\n",
    "        # –°–∏–≥–º–æ–∏–¥–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è —Ç–µ–Ω–∑–æ—Ä–∞ –æ—Ç 0 –¥–æ 1, \n",
    "        # –∑–∞—Ç–µ–º —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä, —Ç–µ–º —Å–∞–º—ã–º —Å–≥–ª–∞–∂–∏–≤–∞—è –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "        return x * torch.sigmoid(self.beta * x)\n",
    "\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        SwiGLU (Swish-Gated Linear Unit) - –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è,\n",
    "        –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –≤–∫–ª—é—á–∞—è Qwen3.\n",
    "        \n",
    "        –°–æ—á–µ—Ç–∞–µ—Ç Swish –∞–∫—Ç–∏–≤–∞—Ü–∏—é –∏ –º–µ—Ö–∞–Ω–∏–∑–º –≥–µ–π—Ç–∏–Ω–≥–∞ (GLU).\n",
    "        \n",
    "        –§–æ—Ä–º—É–ª–∞:\n",
    "        SwiGLU(x, W1, W2, b1, b2) = Swish(W1*x + b1) ‚äô (W2*x + b2)\n",
    "        \n",
    "        –≥–¥–µ:\n",
    "        - W1, W2 - –≤–µ—Å–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã\n",
    "        - b1, b2 - –≤–µ–∫—Ç–æ—Ä—ã —Å–º–µ—â–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ)\n",
    "        - ‚äô - –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ\n",
    "        \n",
    "        –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:\n",
    "        - –õ—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å ReLU/GELU –≤ –≥–ª—É–±–æ–∫–∏—Ö –º–æ–¥–µ–ª—è—Ö\n",
    "        - –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≥–µ–π—Ç–∏–Ω–≥–∞ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –ø–æ—Ç–æ–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
    "        - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM (Qwen, PaLM, LLaMA)\n",
    "        \n",
    "    Args:\n",
    "    ---------------\n",
    "        input_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞\n",
    "        output_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞\n",
    "        intermediate_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü W1 –∏ W2 (–∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ 4*input_dim)\n",
    "        bias: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —Å–º–µ—â–µ–Ω–∏–µ –≤ –ª–∏–Ω–µ–π–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)\n",
    "        \n",
    "    Returns:\n",
    "    ---------------\n",
    "        –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (..., output_dim) - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è SwiGLU\n",
    "        \n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> import torch\n",
    "        >>> swiglu = SwiGLU(512, 512)\n",
    "        >>> x = torch.randn(2, 10, 512)\n",
    "        >>> output = swiglu(x)\n",
    "        >>> output.shape\n",
    "        torch.Size([2, 10, 512])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        intermediate_dim: Optional[int] = None,\n",
    "        bias: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: –ï—Å–ª–∏ intermediate_dim –Ω–µ —É–∫–∞–∑–∞–Ω, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–≥–æ –∫–∞–∫ 4*output_dim\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π gate_proj –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏ –≤—Ö–æ–¥–∞ –≤ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π value_proj –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏ –≤—Ö–æ–¥–∞ –≤ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ —ç–∫–∑–µ–º–ø–ª—è—Ä Swish –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
    "        \n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç 4 –¥–ª—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏?\n",
    "        # - –ö–∞–∫–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –¥–∞–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –≥–µ–π—Ç–∏–Ω–≥–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–æ—Å—Ç–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π?\n",
    "        # - –ü–æ—á–µ–º—É SwiGLU –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –≥–ª—É–±–æ–∫–∏—Ö –º–æ–¥–µ–ª—è—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å ReLU/GELU?\n",
    "        # pass\n",
    "\n",
    "        self.input_dim  = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.bias = bias\n",
    "\n",
    "        self.swish = Swish()\n",
    "        \n",
    "        # –ï—Å–ª–∏ intermediate_dim –Ω–µ —É–∫–∞–∑–∞–Ω, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –µ–≥–æ –∫–∞–∫ 4*input_dim\n",
    "        if intermediate_dim is None:\n",
    "            self.intermediate_dim = 4 * input_dim\n",
    "        else:\n",
    "            self.intermediate_dim = intermediate_dim\n",
    "\n",
    "        # nn.Linear —Å–æ–∑–¥–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –≤–µ—Å–æ–≤ —Ä–∞–∑–º–µ—Ä–∞ [intermediate_dim, input_dim] –∏ –≤–µ–∫—Ç–æ—Ä —Å–º–µ—â–µ–Ω–∏—è [intermediate_dim]\n",
    "        # –≠—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–æ–µ–∫—Ü–∏–∏ W1*x + b1 –≤ —Ñ–æ—Ä–º—É–ª–µ SwiGLU (—Ä–∞—Å—à–∏—Ä—è–µ–º)\n",
    "        self.gate_proj = nn.Linear(self.input_dim, self.intermediate_dim, bias=self.bias)\n",
    "        # –≠—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–æ–µ–∫—Ü–∏–∏ W2*x + b2 –≤ —Ñ–æ—Ä–º—É–ª–µ SwiGLU (—Ä–∞—Å—à–∏—Ä—è–µ–º)\n",
    "        self.value_proj = nn.Linear(self.input_dim, self.intermediate_dim, bias=self.bias)\n",
    "        # –ü—Ä–æ–µ–∫—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –≤—ã—Ö–æ–¥–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (—Å–∂–∏–º–∞–µ–º)\n",
    "        self.output_proj = nn.Linear(self.intermediate_dim, self.output_dim, bias=self.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä–∏–º–µ–Ω—è–µ—Ç SwiGLU –∞–∫—Ç–∏–≤–∞—Ü–∏—é –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ç–µ–Ω–∑–æ—Ä—É.\n",
    "        \n",
    "        Args:\n",
    "        ---------------\n",
    "            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (..., input_dim)\n",
    "            \n",
    "        Returns:\n",
    "        ---------------\n",
    "            –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (..., output_dim) - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è SwiGLU\n",
    "        \"\"\"\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ gate_proj –∫ –≤—Ö–æ–¥—É\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ Swish –∞–∫—Ç–∏–≤–∞—Ü–∏—é –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É gate_proj\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ value_proj –∫ –≤—Ö–æ–¥—É\n",
    "        # TODO: –ü–µ—Ä–µ–º–Ω–æ–∂—å—Ç–µ –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Swish(gate_proj(x)) –∏ value_proj(x)\n",
    "        # TODO: –í–µ—Ä–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "        \n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ö–∞–∫ –º–µ—Ö–∞–Ω–∏–∑–º –≥–µ–π—Ç–∏–Ω–≥–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø—Ä–∏ –æ–±—Ä–∞—Ç–Ω–æ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–∏?\n",
    "        # - –ü–æ—á–µ–º—É –≤–∞–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ –¥–ª—è gate –∏ value?\n",
    "        # - –ö–∞–∫ SwiGLU —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç –æ–±—É—á–µ–Ω–∏—é –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏—Ö –º–æ–¥–µ–ª–µ–π?\n",
    "        # pass\n",
    "\n",
    "        # –°–æ–∑–¥–∞–µ–º –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π, W1*x + b1\n",
    "        gate_proj = self.gate_proj(x)\n",
    "        # –°–æ–∑–¥–∞–µ–º –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π, W2*x + b2\n",
    "        value_proj = self.value_proj(x)\n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º Swish –∞–∫—Ç–∏–≤–∞—Ü–∏—é –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É gate_proj\n",
    "        swish = self.swish.forward(gate_proj)\n",
    "\n",
    "        # –ü–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–º–µ—Ö–∞–Ω–∏–∑–º –≥–µ–π—Ç–∏–Ω–≥–∞)\n",
    "        swiglu_intermediate = swish * value_proj\n",
    "        \n",
    "        # –ü—Ä–æ–µ–∫—Ü–∏—è –≤ –≤—ã—Ö–æ–¥–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (—Å–∂–∏–º–∞–µ–º: input == output)\n",
    "        output = self.output_proj(swiglu_intermediate)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëÄ –í–Ω–∏–º–∞–Ω–∏–µ: Grouped‚ÄëQuery Attention (GQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# –õ–æ–∫–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "sys.path.insert(0, parent_dir)\n",
    "from positional_encoding.rope import RoPE\n",
    "\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Grouped-Query Attention (GQA) - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è Multi-Head Attention,\n",
    "        –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –≤–∫–ª—é—á–∞—è Qwen3.\n",
    "        \n",
    "        –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç Multi-Head Attention, –≥–¥–µ –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –∏–º–µ–µ—Ç —Å–≤–æ–∏ –ø—Ä–æ–µ–∫—Ü–∏–∏\n",
    "        –¥–ª—è query, key –∏ value, –≤ GQA –∑–∞–ø—Ä–æ—Å—ã (queries) –≥—Ä—É–ø–ø–∏—Ä—É—é—Ç—Å—è, –∞ –∫–ª—é—á–∏ (keys)\n",
    "        –∏ –∑–Ω–∞—á–µ–Ω–∏—è (values) –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≥–æ–ª–æ–≤–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "        \n",
    "        –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –∏ –ø–∞–º—è—Ç—å, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º\n",
    "        –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—É—é –º–æ—â–Ω–æ—Å—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "        \n",
    "        –§–æ—Ä–º—É–ª–∞:\n",
    "        GQA(Q, K, V) = Softmax(QK^T/‚àöd_k)V\n",
    "        \n",
    "        –≥–¥–µ:\n",
    "        - Q —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ G –≥—Ä—É–ø–ø (–º–µ–Ω—å—à–µ, —á–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –¥–ª—è –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π)\n",
    "        - K –∏ V –∏–º–µ—é—Ç H –≥–æ–ª–æ–≤ (H ‚â• G)\n",
    "        - –ö–∞–∂–¥–∞—è –≥—Ä—É–ø–ø–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ–≤ –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "        \n",
    "        –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:\n",
    "        - –°–Ω–∏–∂–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏\n",
    "        - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ–π –º–æ—â–Ω–æ—Å—Ç–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        - –£–ª—É—á—à–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π\n",
    "        \n",
    "    Args:\n",
    "    ---------------\n",
    "        hidden_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è == d_model (example: LLaMA 70B hidden_size = d_model = 8192)\n",
    "        num_query_groups: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä—É–ø–ø –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "        num_attention_heads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (–¥–ª—è –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π)\n",
    "        head_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        dropout: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥—Ä–æ–ø–∞—É—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.0)\n",
    "        bias: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —Å–º–µ—â–µ–Ω–∏–µ –≤ –ª–∏–Ω–µ–π–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)\n",
    "        use_rope: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ RoPE –¥–ª—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)\n",
    "        rope_theta: –ë–∞–∑–∞ –¥–ª—è RoPE (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10000.0)\n",
    "        rope_scaling: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è RoPE (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.0)\n",
    "        max_position: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è RoPE (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2048)\n",
    "        \n",
    "    Returns:\n",
    "    ---------------\n",
    "        –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size) - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è GQA\n",
    "        \n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> import torch\n",
    "        >>> gqa = GroupedQueryAttention(\n",
    "        ...     hidden_size=512,\n",
    "        ...     num_query_groups=8,\n",
    "        ...     num_attention_heads=16,\n",
    "        ...     head_dim=64\n",
    "        ... )\n",
    "        >>> x = torch.randn(2, 10, 512)\n",
    "        >>> output = gqa(x)\n",
    "        >>> output.shape\n",
    "        torch.Size([2, 10, 512])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_query_groups: int,\n",
    "        num_attention_heads: int,\n",
    "        head_dim: Optional[int] = None,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = True,\n",
    "        use_rope: bool = True,\n",
    "        rope_theta: float = 10000.0,\n",
    "        rope_scaling: float = 1.0,\n",
    "        max_position: int = 2048\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ num_attention_heads –¥–µ–ª–∏—Ç—Å—è –Ω–∞ num_query_groups\n",
    "        # TODO: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ hidden_size –¥–µ–ª–∏—Ç—Å—è –Ω–∞ num_attention_heads\n",
    "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ head_dim, –µ—Å–ª–∏ –æ–Ω –Ω–µ —É–∫–∞–∑–∞–Ω\n",
    "        # TODO: –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç—ã –∫–ª–∞—Å—Å–∞\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ –¥–ª—è query, key –∏ value\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ –ø—Ä–æ–µ–∫—Ü–∏—é –¥–ª—è –≤—ã—Ö–æ–¥–∞\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ dropout —Å–ª–æ–π\n",
    "        # TODO: –ï—Å–ª–∏ use_rope=True, —Å–æ–∑–¥–∞–π—Ç–µ RoPE –º–æ–¥—É–ª—å\n",
    "        \n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É GQA —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ, —á–µ–º –æ–±—ã—á–Ω—ã–π Multi-Head Attention?\n",
    "        # - –ö–∞–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä—É–ø–ø –∑–∞–ø—Ä–æ—Å–æ–≤ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏?\n",
    "        # - –ö–∞–∫–∏–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–∞–µ—Ç —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π?\n",
    "        # - –ö–∞–∫ RoPE –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å GQA?\n",
    "        # pass\n",
    "\n",
    "        assert num_attention_heads % num_query_groups == 0, \"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –¥–æ–ª–∂–Ω–æ –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä—É–ø–ø –∑–∞–ø—Ä–æ—Å–æ–≤\"\n",
    "        assert hidden_size % num_attention_heads == 0, \"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–æ–ª–∂–Ω–∞ –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\"\n",
    "        \n",
    "        # –î–µ–ª–∏–º —Å–∫—Ä—ã—Ç—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å d_model (hidden_size) –Ω–∞ —á–∏—Å–ª–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è h (num_attention_heads)\n",
    "        # –ö–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫—É—Å–æ–∫ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ d_head = d_model / h, –∞ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è h –≥–æ–ª–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å\n",
    "        head_dim = hidden_size // num_attention_heads if head_dim is None else head_dim\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_query_groups = num_query_groups\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.use_rope = use_rope\n",
    "        self.rope_theta = rope_theta\n",
    "        self.rope_scaling = rope_scaling\n",
    "        self.max_position = max_position\n",
    "\n",
    "        # –ü—Ä–æ–µ–∫—Ü–∏–∏ –¥–ª—è query, key –∏ value\n",
    "        # –í GQA: query –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä—É–ø–ø—ã, –∞ key/value –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≤—Å–µ –≥–æ–ª–æ–≤—ã\n",
    "        self.query_proj = nn.Linear(in_features = hidden_size,\n",
    "                                    out_features = num_query_groups * head_dim,\n",
    "                                    bias=bias)\n",
    "\n",
    "        self.key_proj   = nn.Linear(in_features = hidden_size,\n",
    "                                    out_features = num_attention_heads * head_dim,\n",
    "                                    bias=bias)\n",
    "\n",
    "        self.value_proj = nn.Linear(in_features = hidden_size,\n",
    "                                    out_features = num_attention_heads * head_dim,\n",
    "                                    bias=bias)\n",
    "        \n",
    "        self.output_proj = nn.Linear(in_features = num_query_groups * head_dim,\n",
    "                                    out_features = hidden_size,\n",
    "                                    bias=bias)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "        self.rope = RoPE(dim=head_dim) if use_rope else None\n",
    "\n",
    "        \n",
    "    def _split_heads(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        num_heads: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –†–∞–∑–¥–µ–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–Ω—é—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Ç–µ–Ω–∑–æ—Ä–∞ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "        \n",
    "        Args:\n",
    "        ---------------\n",
    "            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
    "            num_heads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "            \n",
    "        Returns:\n",
    "        ---------------\n",
    "            –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, num_heads, seq_len, head_dim)\n",
    "        \"\"\"\n",
    "        # TODO: –ü–æ–ª—É—á–∏—Ç–µ –Ω–æ–≤—É—é —Ñ–æ—Ä–º—É —Ç–µ–Ω–∑–æ—Ä–∞\n",
    "        # TODO: –ò–∑–º–µ–Ω–∏—Ç–µ —Ñ–æ—Ä–º—É —Ç–µ–Ω–∑–æ—Ä–∞ –∏ —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–π—Ç–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏\n",
    "        # TODO: –í–µ—Ä–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "        # pass\n",
    "\n",
    "        # batch_size - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ\n",
    "        # seq_len - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        # total_dim - –æ–±—â–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (num_heads * head_dim)\n",
    "        batch_size, seq_len, total_dim = x.shape\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º head_dim –Ω–∞ –æ—Å–Ω–æ–≤–µ total_dim –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≥–æ–ª–æ–≤\n",
    "        head_dim = total_dim // num_heads\n",
    "\n",
    "        # .view() –ø–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä –≤ –Ω–æ–≤—É—é —Ñ–æ—Ä–º—É, –Ω–µ –∏–∑–º–µ–Ω—è—è –¥–∞–Ω–Ω—ã–µ.\n",
    "        # –ú–µ–Ω—è–µ–º —Ñ–æ—Ä–º—É —Å (batch, seq, total_dim) –Ω–∞ (batch, seq, num_heads, head_dim)\n",
    "        x = x.view(batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "        # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ (batch, num_heads, seq, head_dim) —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _repeat_kv_heads(self, kv: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü–æ–≤—Ç–æ—Ä—è–µ—Ç key/value –≥–æ–ª–æ–≤—ã –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≥—Ä—É–ø–ø query.\n",
    "            –≠—Ç–æ –∫–ª—é—á–µ–≤–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å GQA - –∫–∞–∂–¥–∞—è –≥—Ä—É–ø–ø–∞ query –∏—Å–ø–æ–ª—å–∑—É–µ—Ç\n",
    "            –Ω–µ—Å–∫–æ–ª—å–∫–æ key/value –≥–æ–ª–æ–≤.\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            kv: –¢–µ–Ω–∑–æ—Ä key –∏–ª–∏ value —Ñ–æ—Ä–º—ã (batch_size, num_attention_heads, seq_len, head_dim)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, num_query_groups, seq_len, head_dim)\n",
    "            –≥–¥–µ key/value –≥–æ–ª–æ–≤—ã —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã query\n",
    "        \"\"\"\n",
    "        # TODO: –ü–æ–ª—É—á–∏—Ç–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞\n",
    "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ key/value –≥–æ–ª–æ–≤ –Ω–∞ –æ–¥–Ω—É –≥—Ä—É–ø–ø—É query\n",
    "        # TODO: –ò–∑–º–µ–Ω–∏—Ç–µ —Ñ–æ—Ä–º—É —Ç–µ–Ω–∑–æ—Ä–∞ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –≥–æ–ª–æ–≤\n",
    "        # TODO: –£—Å—Ä–µ–¥–Ω–∏—Ç–µ –≥–æ–ª–æ–≤—ã –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã\n",
    "        # TODO: –í–µ—Ä–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –º—ã –≥—Ä—É–ø–ø–∏—Ä—É–µ–º key/value –≥–æ–ª–æ–≤—ã, –∞ –Ω–µ –¥—É–±–ª–∏—Ä—É–µ–º –∏—Ö?\n",
    "        # - –ö–∞–∫ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏?\n",
    "        # - –ö–∞–∫–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—é –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (concatenation, max pooling)?\n",
    "        # - –ö–∞–∫ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ num_attention_heads –∫ num_query_groups –≤–ª–∏—è–µ—Ç –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å?\n",
    "\n",
    "        batch_size, num_kv_heads, seq_len, head_dim = kv.shape\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º, —Å–∫–æ–ª—å–∫–æ key/value –≥–æ–ª–æ–≤ –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ –æ–¥–Ω—É –≥—Ä—É–ø–ø—É query\n",
    "        heads_per_group = num_kv_heads // self.num_query_groups\n",
    "\n",
    "        # –ò–∑–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º—É –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –≥–æ–ª–æ–≤: (batch, heads, seq, dim) -> (batch, groups, heads_per_group, seq, dim)\n",
    "        kv = kv.view(batch_size, self.num_query_groups, heads_per_group, seq_len, head_dim)\n",
    "\n",
    "        # –£—Å—Ä–µ–¥–Ω—è–µ–º –≥–æ–ª–æ–≤—ã –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –ø–æ –æ—Å–∏ heads_per_group (–∏–Ω–¥–µ–∫—Å 2)\n",
    "        kv = kv.mean(dim=2)  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: (batch_size, num_query_groups, seq_len, head_dim)\n",
    "\n",
    "        return kv\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False\n",
    "    ) -> Union[\n",
    "        Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]], Optional[torch.Tensor]],\n",
    "        torch.Tensor\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä–∏–º–µ–Ω—è–µ—Ç Grouped-Query Attention –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ç–µ–Ω–∑–æ—Ä—É.\n",
    "        \n",
    "        Args:\n",
    "        ---------------\n",
    "            hidden_states: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: –ú–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "            position_ids: –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è RoPE (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "            past_key_value: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª—é—á–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "            output_attentions: –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "            use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "            \n",
    "        Returns:\n",
    "        ---------------\n",
    "            –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size) - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è GQA\n",
    "            –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª—é—á–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è, –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        \"\"\"\n",
    "        # TODO: –ü–æ–ª—É—á–∏—Ç–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ –¥–ª—è query, key –∏ value\n",
    "        # TODO: –†–∞–∑–¥–µ–ª–∏—Ç–µ query –Ω–∞ –≥—Ä—É–ø–ø—ã, –∞ key –∏ value –Ω–∞ –≥–æ–ª–æ–≤—ã\n",
    "        # TODO: –ï—Å–ª–∏ use_rope=True, –ø—Ä–∏–º–µ–Ω–∏—Ç–µ RoPE –∫ query –∏ key\n",
    "        # TODO: –ï—Å–ª–∏ past_key_value –Ω–µ None, –æ–±—ä–µ–¥–∏–Ω–∏—Ç–µ —Å —Ç–µ–∫—É—â–∏–º–∏ key –∏ value\n",
    "        # TODO: –ï—Å–ª–∏ use_cache=True, –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –Ω–æ–≤—ã–π past_key_value\n",
    "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ query –∏ key\n",
    "        # TODO: –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–π—Ç–µ —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ\n",
    "        # TODO: –ï—Å–ª–∏ attention_mask –Ω–µ None, –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –º–∞—Å–∫—É\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ softmax –∫ –≤–µ—Å–∞–º –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ dropout –∫ –≤–µ—Å–∞–º –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ –≤–∑–≤–µ—à–µ–Ω–Ω—É—é —Å—É–º–º—É –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "        # TODO: –û–±—ä–µ–¥–∏–Ω–∏—Ç–µ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ –≤—ã—Ö–æ–¥–Ω—É—é –ø—Ä–æ–µ–∫—Ü–∏—é\n",
    "        # TODO: –í–µ—Ä–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –≤—ã—Ö–æ–¥—ã\n",
    "        \n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ö–∞–∫ attention_mask –≤–ª–∏—è–µ—Ç –Ω–∞ –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è?\n",
    "        # - –ö–∞–∫ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π —É—Å–∫–æ—Ä—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é?\n",
    "        # - –ö–∞–∫–∏–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RoPE –≤ GQA?\n",
    "        # pass\n",
    "\n",
    "        # –¢–µ–Ω–∑–æ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "\n",
    "        # –ü—Ä–æ–µ–∫—Ü–∏–∏ –¥–ª—è query, key –∏ value\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∞–±—Ä–∏–∫—É –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ–µ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ü–∏–π\n",
    "        query = self.query_proj(hidden_states)\n",
    "        key   = self.key_proj(hidden_states)\n",
    "        value = self.value_proj(hidden_states)\n",
    "\n",
    "        # –†–∞–∑–¥–µ–ª—è–µ–º query –Ω–∞ –≥—Ä—É–ø–ø—ã, –∞ key –∏ value –Ω–∞ –≥–æ–ª–æ–≤—ã\n",
    "        query = self._split_heads(query, self.num_query_groups)\n",
    "        key   = self._split_heads(key,   self.num_attention_heads)\n",
    "        value = self._split_heads(value, self.num_attention_heads)\n",
    "\n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º RoPE –¥–æ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏, –∫–æ–≥–¥–∞ —Ç–µ–Ω–∑–æ—Ä—ã –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ\n",
    "        if self.use_rope:\n",
    "            if position_ids is None:\n",
    "                # –°–æ–∑–¥–∞–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è RoPE: [0, 1, 2, ..., seq_len-1]\n",
    "                position_ids = torch.arange(seq_len, dtype=torch.long, device=hidden_states.device)\n",
    "\n",
    "                # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è 1D ‚Üí 2D ‚Üí Batch-compatible:\n",
    "                # 1. unsqueeze(0): [0,1,2,3] ‚Üí [[0,1,2,3]] (–¥–æ–±–∞–≤–ª—è–µ–º batch —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å)\n",
    "                # 2. expand(batch_size, -1): [[0,1,2,3]] ‚Üí [[0,1,2,3], [0,1,2,3], ...]\n",
    "                # –†–µ–∑—É–ª—å—Ç–∞—Ç: –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –≤ batch –ø–æ–ª—É—á–∞–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø–æ–∑–∏—Ü–∏–∏\n",
    "                # expand() —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω - —Å–æ–∑–¥–∞–µ—Ç view –±–µ–∑ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
    "                position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "            # RoPE –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤–µ –æ—Ç–¥–µ–ª—å–Ω–æ\n",
    "            # –°–Ω–∞—á–∞–ª–∞ –∏–∑–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è RoPE\n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º reshape() –≤–º–µ—Å—Ç–æ view() —Ç–∞–∫ –∫–∞–∫ –ø–æ—Å–ª–µ transpose() —Ç–µ–Ω–∑–æ—Ä –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ-contiguous\n",
    "            query_for_rope = query.reshape(batch_size * self.num_query_groups, seq_len, self.head_dim)\n",
    "            key_for_rope   = key.reshape(batch_size * self.num_attention_heads, seq_len, self.head_dim)\n",
    "\n",
    "            # –†–∞—Å—à–∏—Ä—è–µ–º position_ids –¥–ª—è –≤—Å–µ—Ö –≥–æ–ª–æ–≤\n",
    "            pos_query = position_ids.unsqueeze(1).expand(-1, self.num_query_groups, -1).contiguous().view(-1, seq_len)\n",
    "            pos_key = position_ids.unsqueeze(1).expand(-1, self.num_attention_heads, -1).contiguous().view(-1, seq_len)\n",
    "\n",
    "            # –ü—Ä–∏–º–µ–Ω—è–µ–º RoPE\n",
    "            query_rope, _ = self.rope(query_for_rope, query_for_rope, pos_query)\n",
    "            _, key_rope = self.rope(key_for_rope, key_for_rope, pos_key)\n",
    "\n",
    "            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É\n",
    "            query = query_rope.view(batch_size, self.num_query_groups, seq_len, self.head_dim)\n",
    "            key = key_rope.view(batch_size, self.num_attention_heads, seq_len, self.head_dim)\n",
    "\n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É key/value –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è query –≥—Ä—É–ø–ø–∞–º\n",
    "        key   = self._repeat_kv_heads(key)\n",
    "        value = self._repeat_kv_heads(value)\n",
    "\n",
    "        # –í—Å–µ —Ç–µ–Ω–∑–æ—Ä—ã —É–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ (batch, heads, seq, dim) –ø–æ—Å–ª–µ _split_heads –∏ _repeat_kv_heads\n",
    "        # query: (batch_size, num_query_groups, seq_len, head_dim)\n",
    "        # key:   (batch_size, num_query_groups, seq_len, head_dim)\n",
    "        # value: (batch_size, num_query_groups, seq_len, head_dim)\n",
    "\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å past_key_value, –µ—Å–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–æ\n",
    "        # –§–æ—Ä–º–∞—Ç: (batch, heads, seq, dim), –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º –ø–æ seq_len (dim=2)\n",
    "        if past_key_value is not None:\n",
    "            key = torch.cat([past_key_value[0], key], dim=2)\n",
    "            value = torch.cat([past_key_value[1], value], dim=2)\n",
    "\n",
    "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –Ω–æ–≤—ã–π past_key_value\n",
    "        if use_cache:\n",
    "            past_key_value = (key, value)\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ query –∏ key –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –æ–¥–∏–Ω —Ä–∞–∑\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        if attention_mask is not None:\n",
    "            scores = scores + attention_mask\n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º softmax –∫ –≤–µ—Å–∞–º –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º dropout –∫ –≤–µ—Å–∞–º –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        weights = self.dropout(weights)\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—É—é —Å—É–º–º—É –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "        context = torch.matmul(weights, value)\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è: (batch, heads, seq, dim) -> (batch, seq, heads, dim)\n",
    "        context = context.transpose(1, 2).contiguous()\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≥–æ–ª–æ–≤—ã: (batch, seq, heads, dim) -> (batch, seq, heads*dim)\n",
    "        context = context.view(batch_size, seq_len, self.num_query_groups * self.head_dim)\n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –ø—Ä–æ–µ–∫—Ü–∏—é\n",
    "        output = self.output_proj(context)\n",
    "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–ª–∞–≥–æ–≤\n",
    "        if use_cache and output_attentions:\n",
    "            return output, past_key_value, weights\n",
    "        elif use_cache:\n",
    "            return output, past_key_value, None\n",
    "        elif output_attentions:\n",
    "            return output, None, weights\n",
    "        else:\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß± –ë–∞–∑–æ–≤—ã–π Transformer‚Äë–±–ª–æ–∫ (–±–µ–∑ MoE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f187873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# –õ–æ–∫–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "sys.path.insert(0, parent_dir)\n",
    "from normalization.rmsnorm import RMSNorm\n",
    "from attention.gqa import GroupedQueryAttention\n",
    "from activations.swiglu import SwiGLU\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –ë–ª–æ–∫ Transformer –¥–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Qwen3 MoE. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Pre-Norm –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É\n",
    "        —Å Grouped-Query Attention –∏ SwiGLU –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π.\n",
    "\n",
    "        –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –±–ª–æ–∫–∞:\n",
    "        Input ‚Üí RMSNorm ‚Üí GQA ‚Üí Residual ‚Üí RMSNorm ‚Üí SwiGLU ‚Üí Residual ‚Üí Output\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        hidden_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è (d_model)\n",
    "        num_query_groups: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä—É–ø–ø –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è GQA\n",
    "        num_attention_heads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è key/value\n",
    "        intermediate_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è –≤ SwiGLU (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 4 * hidden_size)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_query_groups: int,\n",
    "        num_attention_heads: int,\n",
    "        intermediate_size: Optional[int] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # TODO: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ intermediate_size –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω (4 * hidden_size)\n",
    "        # TODO: –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç—ã\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ self.attention_norm = RMSNorm(hidden_size)\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ self.attention = GroupedQueryAttention(...)\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ self.ffn_norm = RMSNorm(hidden_size)\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ self.feed_forward = SwiGLU(...)\n",
    "\n",
    "        # --- –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ -------------------------------------------------\n",
    "        assert (\n",
    "            isinstance(hidden_size, int) and hidden_size > 0\n",
    "        ), \"hidden_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\"\n",
    "        assert (\n",
    "            isinstance(num_query_groups, int) and num_query_groups > 0\n",
    "        ), \"num_query_groups –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\"\n",
    "        assert (\n",
    "            isinstance(num_attention_heads, int) and num_attention_heads > 0\n",
    "        ), \"num_attention_heads –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\"\n",
    "\n",
    "        # –ö–ª—é—á–µ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è GQA –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:\n",
    "        assert (\n",
    "            num_attention_heads % num_query_groups == 0\n",
    "        ), (\n",
    "            \"num_attention_heads –¥–æ–ª–∂–µ–Ω –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ num_query_groups –¥–ª—è \"\n",
    "            \"–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã GQA\"\n",
    "        )\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–µ–ª–∏–º–æ—Å—Ç–∏ hidden_size –Ω–∞ —á–∏—Å–ª–æ –≥–æ–ª–æ–≤\n",
    "        # –¢–µ–Ω–∑–æ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–æ–ª–∂–µ–Ω —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ —á–∏—Å–ª–æ –≥–æ–ª–æ–≤\n",
    "        assert (\n",
    "            hidden_size % num_attention_heads == 0\n",
    "        ), \"hidden_size –¥–æ–ª–∂–µ–Ω –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ num_attention_heads\"\n",
    "\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ intermediate_size –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω\n",
    "        if intermediate_size is not None:\n",
    "            assert (\n",
    "                isinstance(intermediate_size, int) and intermediate_size > 0\n",
    "            ), \"intermediate_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\"\n",
    "\n",
    "        # --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤ ----------------------------------------------\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_query_groups = num_query_groups\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = (\n",
    "            intermediate_size if intermediate_size is not None else 4 * hidden_size\n",
    "        )\n",
    "\n",
    "        # --- –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –ø–æ–¥–±–ª–æ–∫–æ–≤ ----------------------------------\n",
    "        self.attention_norm = RMSNorm(hidden_size)\n",
    "        self.attention = GroupedQueryAttention(\n",
    "            hidden_size=hidden_size,\n",
    "            num_query_groups=num_query_groups,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "        )\n",
    "        self.ffn_norm = RMSNorm(hidden_size)\n",
    "        self.feed_forward = SwiGLU(\n",
    "            input_dim=self.hidden_size,\n",
    "            output_dim=self.hidden_size,\n",
    "            intermediate_dim=self.intermediate_size,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False\n",
    "    ) -> Union[torch.Tensor, Tuple]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä–∏–º–µ–Ω—è–µ—Ç Transformer –±–ª–æ–∫ –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ç–µ–Ω–∑–æ—Ä—É.\n",
    "            Input ‚Üí RMSNorm ‚Üí GQA ‚Üí Residual ‚Üí RMSNorm ‚Üí SwiGLU ‚Üí Residual ‚Üí Output\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            hidden_states: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size) - –≤—ã—Ö–æ–¥ Transformer –±–ª–æ–∫–∞\n",
    "        \"\"\"\n",
    "        # TODO: –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ residual connection\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ attention_norm\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ self.attention\n",
    "        # TODO: –î–æ–±–∞–≤—å—Ç–µ –ø–µ—Ä–≤—ã–π residual connection\n",
    "        # TODO: –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ residual connection\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ ffn_norm\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ self.feed_forward\n",
    "        # TODO: –î–æ–±–∞–≤—å—Ç–µ –≤—Ç–æ—Ä–æ–π residual connection\n",
    "        # TODO: –í–µ—Ä–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –î–û attention/ffn, –∞ –Ω–µ –ø–æ—Å–ª–µ?\n",
    "        # - –ö–∞–∫ residual connections –ø–æ–º–æ–≥–∞—é—Ç –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π?\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # –ü–ï–†–í–´–ô RESIDUAL BLOCK: Self-Attention (GQA)\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ö–æ–¥ –¥–ª—è –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —Å–≤—è–∑–∏ (residual).\n",
    "        residual_1 = hidden_states\n",
    "\n",
    "        # –ü—Ä–µ–¥–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "        normed = self.attention_norm(hidden_states)\n",
    "\n",
    "        # –í—ã–∑—ã–≤–∞–µ–º –º–æ–¥—É–ª—å –≥—Ä—É–ø–ø–æ–≤–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è. –û–Ω –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å:\n",
    "        # - —Ç–æ–ª—å–∫–æ –≤—ã—Ö–æ–¥ (Tensor), –ª–∏–±–æ\n",
    "        # - –∫–æ—Ä—Ç–µ–∂ (att_output, present_key_value, attn_weights).\n",
    "        att_output = self.attention(\n",
    "            hidden_states=normed,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "\n",
    "        if isinstance(att_output, tuple):\n",
    "            att_output, present_key_value, attn_weights = att_output\n",
    "        else:\n",
    "            present_key_value = None\n",
    "            attn_weights = None\n",
    "\n",
    "        # –ü–µ—Ä–≤–∞—è residual-—Å–≤—è–∑—å: —Å–∫–ª–∞–¥—ã–≤–∞–µ–º –≤—Ö–æ–¥ –∏ –≤—ã—Ö–æ–¥ –ø–æ–¥–±–ª–æ–∫–∞.\n",
    "        hidden_states = att_output + residual_1\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # –í–¢–û–†–û–ô RESIDUAL BLOCK: Feed-Forward (SwiGLU)\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "        residual_2 = hidden_states\n",
    "\n",
    "        # –ü—Ä–µ–¥–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–¥ FFN –ø–æ —Ç–µ–º –∂–µ –ø—Ä–∏—á–∏–Ω–∞–º, —á—Ç–æ –∏ –ø–µ—Ä–µ–¥ –≤–Ω–∏–º–∞–Ω–∏–µ–º.\n",
    "        normed = self.ffn_norm(hidden_states)\n",
    "\n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–µ–ª–∏–Ω–µ–π–Ω—É—é –ø—Ä–æ–µ–∫—Ü–∏—é SwiGLU —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏\n",
    "        # –¥–æ intermediate_size –∏ –æ–±—Ä–∞—Ç–Ω–æ–π –ø—Ä–æ–µ–∫—Ü–∏–µ–π –∫ hidden_size.\n",
    "        ffn_output = self.feed_forward(normed)\n",
    "\n",
    "        # –í—Ç–æ—Ä–∞—è residual-—Å–≤—è–∑—å.\n",
    "        hidden_states = ffn_output + residual_2\n",
    "\n",
    "        if use_cache or output_attentions:\n",
    "            return hidden_states, present_key_value, attn_weights\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ£Ô∏è MoE: Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80694326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MoERouter(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        MoE Router (Mixture-of-Experts Router) –¥–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Qwen3.\n",
    "\n",
    "        –†–æ—É—Ç–µ—Ä —Ä–µ—à–∞–µ—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞:\n",
    "        1. –ö–∞–∫–∏–µ K —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏–∑ N –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å (Top-K selection)\n",
    "        2. –° –∫–∞–∫–∏–º–∏ –≤–µ—Å–∞–º–∏ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –≤—ã—Ö–æ–¥—ã (gating weights)\n",
    "        3. –ö–∞–∫ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–≥—Ä—É–∑–∫—É –º–µ–∂–¥—É —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ (load balancing)\n",
    "\n",
    "        –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:\n",
    "        Input Token ‚Üí Linear Projection ‚Üí Softmax ‚Üí Top-K Selection ‚Üí Gating Weights\n",
    "\n",
    "        –î–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ (0.6B): N=8 —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, K=2 –∞–∫—Ç–∏–≤–Ω—ã—Ö per token\n",
    "        –î–ª—è —Å–ø—Ä–∞–≤–∫–∏, Qwen3-30B –∏—Å–ø–æ–ª—å–∑—É–µ—Ç: N=128 —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, K=8 –∞–∫—Ç–∏–≤–Ω—ã—Ö per token\n",
    "\n",
    "    Mathematical Formulation:\n",
    "    ---------------\n",
    "        1. Gating scores: g = Softmax(W_g * x)\n",
    "           –≥–¥–µ W_g - –æ–±—É—á–∞–µ–º–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–∞ (hidden_size, num_experts)\n",
    "\n",
    "        2. Top-K selection: indices, weights = TopK(g, k=top_k)\n",
    "           –í—ã–±–∏—Ä–∞–µ–º K —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ –≤–µ—Å–∞–º–∏\n",
    "\n",
    "        3. Renormalization: weights = Softmax(weights)\n",
    "           –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (—Å—É–º–º–∞ = 1)\n",
    "\n",
    "        4. Load balancing loss: L_balance = Œ± * mean(f * P)\n",
    "           –≥–¥–µ f - —á–∞—Å—Ç–æ—Ç–∞ –≤—ã–±–æ—Ä–∞ —ç–∫—Å–ø–µ—Ä—Ç–∞, P - —Å—Ä–µ–¥–Ω–∏–π –≤–µ—Å —ç–∫—Å–ø–µ—Ä—Ç–∞\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        hidden_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "        num_experts: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (N)\n",
    "        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ per token (K)\n",
    "        capacity_factor: –§–∞–∫—Ç–æ—Ä –µ–º–∫–æ—Å—Ç–∏ –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ per expert (default: 1.25)\n",
    "        balance_loss_coef: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è load balancing loss (default: 0.01)\n",
    "\n",
    "    Returns (from forward):\n",
    "    ---------------\n",
    "        routing_weights: –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, top_k)\n",
    "                        –í–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ K –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "        selected_experts: –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, top_k) dtype=long\n",
    "                         –ò–Ω–¥–µ–∫—Å—ã –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ [0, num_experts)\n",
    "        balance_loss: –°–∫–∞–ª—è—Ä - loss –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞–≥—Ä—É–∑–∫–∏ –º–µ–∂–¥—É —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏\n",
    "\n",
    "    Example:\n",
    "    ---------------\n",
    "        >>> # –î–ª—è –º–æ–¥–µ–ª–∏ 0.6B\n",
    "        >>> router = MoERouter(hidden_size=512, num_experts=8, top_k=2)\n",
    "        >>> x = torch.randn(2, 10, 512)  # (batch=2, seq=10, hidden=512)\n",
    "        >>> weights, experts, loss = router(x)\n",
    "        >>> weights.shape  # torch.Size([2, 10, 2])\n",
    "        >>> experts.shape  # torch.Size([2, 10, 2])\n",
    "        >>> loss.item()    # –°–∫–∞–ª—è—Ä loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_experts: int,\n",
    "        top_k: int = 2,\n",
    "        capacity_factor: float = 1.25,\n",
    "        balance_loss_coef: float = 0.01\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (hidden_size, num_experts, top_k)\n",
    "        # TODO: –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ top_k <= num_experts\n",
    "        # TODO: –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç—ã –∫–ª–∞—Å—Å–∞\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ self.gate - –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "        #       –†–∞–∑–º–µ—Ä—ã: (hidden_size) -> (num_experts)\n",
    "        # TODO: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –≤–µ—Å–∞ gate –Ω–µ–±–æ–ª—å—à–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã top_k –±—ã–ª –º–µ–Ω—å—à–µ num_experts?\n",
    "        # - –ö–∞–∫ capacity_factor –≤–ª–∏—è–µ—Ç –Ω–∞ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –Ω–∞–≥—Ä—É–∑–∫–∏?\n",
    "        # - –ó–∞—á–µ–º –Ω—É–∂–Ω–∞ –Ω–µ–±–æ–ª—å—à–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ gate?\n",
    "        # - –ö–∞–∫–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã Softmax –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è gating?\n",
    "        # pass\n",
    "\n",
    "        # --- –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ -------------------------------------------------\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º assert –¥–ª—è —Ä–∞–Ω–Ω–µ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.\n",
    "        assert isinstance(hidden_size, int) and hidden_size > 0, (\n",
    "            \"hidden_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\"\n",
    "        )\n",
    "        assert isinstance(num_experts, int) and num_experts > 0, (\n",
    "            \"num_experts –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\"\n",
    "        )\n",
    "        assert isinstance(top_k, int) and top_k > 0, (\n",
    "            \"top_k –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\"\n",
    "        )\n",
    "        assert top_k <= num_experts, (\n",
    "            \"num_experts –¥–æ–ª–∂–µ–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–æ top_k\"\n",
    "        )\n",
    "        assert isinstance(capacity_factor, float) and capacity_factor > 0, (\n",
    "            \"capacity_factor –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —á–∏—Å–ª–æ–º\"\n",
    "        )\n",
    "        assert isinstance(balance_loss_coef, float) and balance_loss_coef >= 0, (\n",
    "            \"balance_loss_coef –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º —á–∏—Å–ª–æ–º\"\n",
    "        )\n",
    "\n",
    "        # --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤ ----------------------------------------------\n",
    "        # –•—Ä–∞–Ω–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç—ã —ç–∫–∑–µ–º–ø–ª—è—Ä–∞, —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö\n",
    "        # –ø—Ä–∏ –¥–∞–ª—å–Ω–µ–π—à–µ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ –∏ —Ä–∞—Å—á—ë—Ç–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–π.\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.balance_loss_coef = balance_loss_coef\n",
    "\n",
    "        # –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π-gate –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –ª–æ–≥–∏—Ç—ã –ø–æ —ç–∫—Å–ø–µ—Ä—Ç–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "        # –ø–æ—Å–ª–µ–¥—É—é—â–∏–π softmax (–∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –≤ forward) –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –∏—Ö –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏.\n",
    "        self.gate = nn.Linear(hidden_size, num_experts)\n",
    "\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: –Ω–µ–±–æ–ª—å—à–æ–π –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —à—É–º —É—Å–∫–æ—Ä—è–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å.\n",
    "        self.gate.weight.data.normal_(0, 0.01)\n",
    "\n",
    "        # –ù—É–ª–µ–≤–æ–π —Å–¥–≤–∏–≥ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Å–º–µ—â–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ —ç–∫—Å–ø–µ—Ä—Ç–∞–º –Ω–∞ —Å—Ç–∞—Ä—Ç–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "        # –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ bias ‚Äî –Ω–∞ —Å–ª—É—á–∞–π future-refactor.\n",
    "        if self.gate.bias is not None:\n",
    "            self.gate.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        training: bool = True\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä–∏–º–µ–Ω—è–µ—Ç MoE routing –∫ –≤—Ö–æ–¥–Ω—ã–º —Å–∫—Ä—ã—Ç—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏—è–º.\n",
    "\n",
    "            –ü—Ä–æ—Ü–µ—Å—Å:\n",
    "            1. –ü—Ä–æ–µ–∫—Ü–∏—è –≤—Ö–æ–¥–∞ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "            2. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ gating scores —á–µ—Ä–µ–∑ Softmax\n",
    "            3. Top-K selection - –≤—ã–±–æ—Ä K –ª—É—á—à–∏—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "            4. Renormalization –≤–µ—Å–æ–≤ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "            5. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ load balancing loss (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ training=True)\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            hidden_states: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
    "            training: –§–ª–∞–≥ —Ä–µ–∂–∏–º–∞ –æ–±—É—á–µ–Ω–∏—è (–¥–ª—è load balancing loss)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            routing_weights: –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, top_k)\n",
    "                           –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "            selected_experts: –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, top_k)\n",
    "                            –ò–Ω–¥–µ–∫—Å—ã –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "            balance_loss: –°–∫–∞–ª—è—Ä - load balancing loss (0.0 –µ—Å–ª–∏ training=False)\n",
    "        \"\"\"\n",
    "        # TODO: –ü–æ–ª—É—á–∏—Ç–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ (batch_size, seq_len, hidden_size)\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ self.gate –∫ hidden_states –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ª–æ–≥–∏—Ç–æ–≤\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ Softmax –ø–æ –æ—Å–∏ num_experts –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è gating_scores\n",
    "        #       –≠—Ç–æ –¥–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–æ –≤—Å–µ–º —ç–∫—Å–ø–µ—Ä—Ç–∞–º\n",
    "        # TODO: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ torch.topk –¥–ª—è –≤—ã–±–æ—Ä–∞ top_k —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "        #       –ü–æ–ª—É—á–∏—Ç–µ: routing_weights (–≤–µ—Å–∞), selected_experts (–∏–Ω–¥–µ–∫—Å—ã)\n",
    "        # TODO: –†–µ-–Ω–æ—Ä–º–∞–ª–∏–∑—É–π—Ç–µ routing_weights —á–µ—Ä–µ–∑ Softmax\n",
    "        #       –í–∞–∂–Ω–æ: –≤–µ—Å–∞ K –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–æ–ª–∂–Ω—ã —Å—É–º–º–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ 1\n",
    "        # TODO: –ï—Å–ª–∏ training=True, –≤—ã—á–∏—Å–ª–∏—Ç–µ load balancing loss\n",
    "        #       –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ _compute_balance_loss\n",
    "        # TODO: –í–µ—Ä–Ω–∏—Ç–µ (routing_weights, selected_experts, balance_loss)\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –Ω—É–∂–Ω–∞ —Ä–µ-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ—Å–ª–µ Top-K selection?\n",
    "        # - –ß—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç, –µ—Å–ª–∏ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –≤—ã–±–µ—Ä—É—Ç –æ–¥–Ω–∏—Ö –∏ —Ç–µ—Ö –∂–µ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤?\n",
    "        # - –ö–∞–∫ Top-K selection –≤–ª–∏—è–µ—Ç –Ω–∞ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å?\n",
    "        # - –ü–æ—á–µ–º—É balance_loss –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ training=True?\n",
    "        # pass\n",
    "\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "\n",
    "        # Linear projection (W¬∑x + b)\n",
    "        # –ü—Ä–æ–µ–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: (B, S, H) ‚Üí (B, S, N)\n",
    "        # –õ–æ–≥–∏—Ç—ã –¥–ª—è –≤—Å–µ—Ö N —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 8 –¥–ª—è –º–æ–¥–µ–ª–∏ 0.6B)\n",
    "        logits = self.gate(hidden_states)\n",
    "\n",
    "        # Softmax –ø–æ –æ—Å–∏ num_experts –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è gating_scores\n",
    "        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–æ –≤—Å–µ–º —ç–∫—Å–ø–µ—Ä—Ç–∞–º\n",
    "        gating_scores = F.softmax(\n",
    "            input = logits,\n",
    "            dim = -1\n",
    "        )\n",
    "        \n",
    "        # Top-K selection - –≤—ã–±–æ—Ä K –ª—É—á—à–∏—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "        # –ü–æ–ª—É—á–µ–Ω–∏–µ routing_weights (–≤–µ—Å–∞) –∏ selected_experts (–∏–Ω–¥–µ–∫—Å—ã)\n",
    "        routing_weights, selected_experts = torch.topk(\n",
    "            input = gating_scores,\n",
    "            k = self.top_k,\n",
    "            dim = -1\n",
    "        )\n",
    "\n",
    "        # Renormalization –≤–µ—Å–æ–≤ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ K –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (—Å—É–º–º–∞ = 1)\n",
    "        routing_weights = F.softmax(\n",
    "            input = routing_weights,\n",
    "            dim = -1\n",
    "        )\n",
    "\n",
    "        if training:\n",
    "            load_balance_loss = self._compute_balance_loss(\n",
    "                gating_scores = gating_scores,\n",
    "                selected_experts = selected_experts\n",
    "            )\n",
    "        else:\n",
    "            load_balance_loss = torch.tensor(0.0, device=gating_scores.device)\n",
    "\n",
    "        return routing_weights, selected_experts, load_balance_loss\n",
    "\n",
    "\n",
    "    def _compute_balance_loss(\n",
    "        self,\n",
    "        gating_scores: torch.Tensor,\n",
    "        selected_experts: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –í—ã—á–∏—Å–ª—è–µ—Ç load balancing loss –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏.\n",
    "\n",
    "            –¶–µ–ª—å: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å —Å–∏—Ç—É–∞—Ü–∏—é, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –º–∞–ª—É—é —á–∞—Å—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–æ–≤.\n",
    "\n",
    "            –§–æ—Ä–º—É–ª–∞: L_balance = Œ± * num_experts * Œ£(f_i * P_i)\n",
    "            –≥–¥–µ:\n",
    "            - f_i - fraction of tokens routed to expert i\n",
    "            - P_i - mean gating score for expert i\n",
    "            - Œ± - balance_loss_coef\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            gating_scores: –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, num_experts)\n",
    "                          Softmax scores –¥–ª—è –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "            selected_experts: –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, top_k)\n",
    "                            –ò–Ω–¥–µ–∫—Å—ã –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            balance_loss: –°–∫–∞–ª—è—Ä —Ç–µ–Ω–∑–æ—Ä - loss –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏\n",
    "        \"\"\"\n",
    "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ frequency (f_i) - —Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤—ã–±—Ä–∞–ª–∏ –∫–∞–∂–¥–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞\n",
    "        #       –ü–æ–¥—Å–∫–∞–∑–∫–∞: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ torch.bincount –∏–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ one-hot –∏ —É—Å—Ä–µ–¥–Ω–∏—Ç–µ\n",
    "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ mean gating probability (P_i) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞\n",
    "        #       –ü–æ–¥—Å–∫–∞–∑–∫–∞: —É—Å—Ä–µ–¥–Ω–∏—Ç–µ gating_scores –ø–æ batch –∏ sequence dimensions\n",
    "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ loss = balance_loss_coef * num_experts * sum(f_i * P_i)\n",
    "        # TODO: –í–µ—Ä–Ω–∏—Ç–µ balance_loss\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –º—ã —É–º–Ω–æ–∂–∞–µ–º –Ω–∞ num_experts –≤ —Ñ–æ—Ä–º—É–ª–µ?\n",
    "        # - –ö–∞–∫ —ç—Ç–æ—Ç loss –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏?\n",
    "        # - –ß—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç, –µ—Å–ª–∏ balance_loss_coef —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π?\n",
    "        # - –ö–∞–∫–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç?\n",
    "        # pass\n",
    "\n",
    "        if gating_scores.dim() != 3:\n",
    "            raise ValueError(\"gating_scores –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ñ–æ—Ä–º—É (B, S, N).\")\n",
    "        if selected_experts.dim() != 3:\n",
    "            raise ValueError(\"selected_experts –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ñ–æ—Ä–º—É (B, S, K).\")\n",
    "        if gating_scores.size(-1) != self.num_experts:\n",
    "            raise ValueError(\"–ü–æ—Å–ª–µ–¥–Ω—è—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å gating_scores –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å N.\")\n",
    "\n",
    "        # # .view() –ø–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä —É–∂–µ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ –Ω–æ–≤—É—é —Ñ–æ—Ä–º—É, –Ω–µ –∏–∑–º–µ–Ω—è—è –¥–∞–Ω–Ω—ã–µ.\n",
    "        # –ë—ã–ª–æ: (batch_size, seq_len, top_k) = (2, 10, 8)\n",
    "        # –°—Ç–∞–ª–æ: (160,) ‚Äî –≤—Å–µ –∏–Ω–¥–µ–∫—Å—ã –≤ –æ–¥–Ω–æ–º –º–∞—Å—Å–∏–≤–µ, –º—ã –ø–æ–ª—É—á–∞–µ–º –æ–¥–∏–Ω –¥–ª–∏–Ω–Ω—ã–π –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä\n",
    "        flattened_experts = selected_experts.view(-1)\n",
    "\n",
    "        # expert_counts[i] = —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —ç–∫—Å–ø–µ—Ä—Ç i –±—ã–ª –≤—ã–±—Ä–∞–Ω\n",
    "        expert_counts = torch.bincount(\n",
    "            flattened_experts,\n",
    "            minlength=self.num_experts  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä –¥–ª–∏–Ω—ã num_experts (–Ω–∞–ø—Ä–∏–º–µ—Ä, 8)\n",
    "        )\n",
    "\n",
    "        # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–±–æ—Ä–æ–≤ = batch_size * seq_len * top_k\n",
    "        batch_size, seq_len, top_k = selected_experts.shape\n",
    "        total_selections = batch_size * seq_len * top_k\n",
    "\n",
    "        # f_i = (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑, –∫–æ–≥–¥–∞ —ç–∫—Å–ø–µ—Ä—Ç i –±—ã–ª –≤—ã–±—Ä–∞–Ω) / (–æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–±–æ—Ä–æ–≤)\n",
    "        f_i = expert_counts.float() / total_selections\n",
    "\n",
    "        # –ó–∞—á–µ–º –≤—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è gating_scores, –∫–æ–≥–¥–∞ —ç—Ç–æ —É–∂–µ —Ç–µ–Ω–∑–æ—Ä –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –ø–æ—Å–ª–µ softmax?\n",
    "        # –ü–æ—Ç–æ–º—É —á—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å —Å—Ä–µ–¥–Ω—é—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ –∫–∞–∂–¥–æ–º —ç–∫—Å–ø–µ—Ä—Ç–µ –ø–æ –≤—Å–µ–º —Ç–æ–∫–µ–Ω–∞–º. –≠—Ç–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —á–∞—Å—Ç–æ—Ç—ã –≤—ã–±–æ—Ä–∞:\n",
    "        #   - f_i = –∫–∞–∫ —á–∞—Å—Ç–æ —ç–∫—Å–ø–µ—Ä—Ç –ø–æ–ø–∞–¥–∞–µ—Ç –≤ Top-K (0 –∏–ª–∏ 1 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞)\n",
    "        #   - P_i = –∫–∞–∫—É—é —Å—Ä–µ–¥–Ω—é—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª—å –Ω–∞–∑–Ω–∞—á–∞–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç—É (–¥–æ Top-K)\n",
    "        # –ü—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ f_i * P_i –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ, –∫–æ–≥–¥–∞ —ç–∫—Å–ø–µ—Ä—Ç –∏ —á–∞—Å—Ç–æ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è, –∏ –º–æ–¥–µ–ª—å –≤ –Ω—ë–º —É–≤–µ—Ä–µ–Ω–∞ ‚Üí —ç—Ç–æ –¥–∏—Å–±–∞–ª–∞–Ω—Å ‚Üí –≤—ã—Å–æ–∫–∏–π loss ‚Üí –≥—Ä–∞–¥–∏–µ–Ω—Ç —à—Ç—Ä–∞—Ñ—É–µ—Ç.\n",
    "        P_i = gating_scores.mean(dim=(0, 1))\n",
    "\n",
    "        balance_loss = self.balance_loss_coef * self.num_experts * (f_i * P_i).sum()\n",
    "\n",
    "        return balance_loss\n",
    "\n",
    "\n",
    "    def expert_capacity(self, num_tokens: int) -> int:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –í—ã—á–∏—Å–ª—è–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –µ–º–∫–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞.\n",
    "\n",
    "            Capacity = (num_tokens / num_experts) * capacity_factor * top_k\n",
    "\n",
    "            –≠—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω —ç–∫—Å–ø–µ—Ä—Ç,\n",
    "            –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—è –ø–µ—Ä–µ–≥—Ä—É–∑–∫—É –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤.\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            num_tokens: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (batch_size * seq_len)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            capacity: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ per expert\n",
    "        \"\"\"\n",
    "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ –±–∞–∑–æ–≤—É—é capacity = num_tokens / num_experts\n",
    "        # TODO: –£–º–Ω–æ–∂—å—Ç–µ –Ω–∞ capacity_factor –¥–ª—è –∑–∞–ø–∞—Å–∞\n",
    "        # TODO: –£–º–Ω–æ–∂—å—Ç–µ –Ω–∞ top_k (–∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –∏–¥–µ—Ç –∫ K —ç–∫—Å–ø–µ—Ä—Ç–∞–º)\n",
    "        # TODO: –û–∫—Ä—É–≥–ª–∏—Ç–µ –¥–æ —Ü–µ–ª–æ–≥–æ —á–∏—Å–ª–∞ (ceil)\n",
    "        # TODO: –í–µ—Ä–Ω–∏—Ç–µ capacity\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ó–∞—á–µ–º –Ω—É–∂–µ–Ω capacity_factor > 1.0?\n",
    "        # - –ß—Ç–æ –¥–µ–ª–∞—Ç—å —Å —Ç–æ–∫–µ–Ω–∞–º–∏, –ø—Ä–µ–≤—ã—à–∞—é—â–∏–º–∏ capacity?\n",
    "        # - –ö–∞–∫ capacity –≤–ª–∏—è–µ—Ç –Ω–∞ memory footprint?\n",
    "        # pass\n",
    "\n",
    "        capacity = math.ceil((num_tokens / self.num_experts) * self.capacity_factor * self.top_k)\n",
    "\n",
    "        return capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßë‚Äçüè´ MoE: Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593fea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞\n",
    "from typing import Optional\n",
    "\n",
    "# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# –õ–æ–∫–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã\n",
    "from experiments.domain.activations.swiglu import SwiGLU\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Expert Network –¥–ª—è Mixture-of-Experts –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.\n",
    "\n",
    "        –ö–∞–∂–¥—ã–π —ç–∫—Å–ø–µ—Ä—Ç - —ç—Ç–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º–∞—è feed-forward —Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç\n",
    "        —Ç–æ–∫–µ–Ω—ã, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –∫ –Ω–µ–π Router'–æ–º.\n",
    "\n",
    "        –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:\n",
    "        Input (hidden_size) ‚Üí SwiGLU FFN ‚Üí Output (hidden_size)\n",
    "\n",
    "        –í–Ω—É—Ç—Ä–∏ SwiGLU:\n",
    "        hidden_size ‚Üí intermediate_size (—Å gating) ‚Üí hidden_size\n",
    "\n",
    "        –î–ª—è –º–æ–¥–µ–ª–∏ 0.6B:\n",
    "        - hidden_size = 512\n",
    "        - intermediate_size = 2048 (–æ–±—ã—á–Ω–æ 4 * hidden_size)\n",
    "        - num_experts = 8 (–∫–∞–∂–¥—ã–π —Å –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–º–∏ –≤–µ—Å–∞–º–∏)\n",
    "\n",
    "    Mathematical Flow:\n",
    "    ---------------\n",
    "        x ‚àà ‚Ñù^(batch√óseq√óhidden)\n",
    "            ‚Üì\n",
    "        SwiGLU(x) = Swish(W1¬∑x) ‚äô (W2¬∑x)  [intermediate_dim]\n",
    "            ‚Üì\n",
    "        W3¬∑SwiGLU(x) + b3\n",
    "            ‚Üì\n",
    "        output ‚àà ‚Ñù^(batch√óseq√óhidden)\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        hidden_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞ –∏ –≤—ã—Ö–æ–¥–∞ (–¥–æ–ª–∂–Ω–∞ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å hidden_size –º–æ–¥–µ–ª–∏)\n",
    "        intermediate_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è (–æ–±—ã—á–Ω–æ 4 * hidden_size)\n",
    "        dropout: Dropout –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (default: 0.0)\n",
    "\n",
    "    Returns (from forward):\n",
    "    ---------------\n",
    "        output: –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
    "                –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–µ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "\n",
    "    Example:\n",
    "    ---------------\n",
    "        >>> # –°–æ–∑–¥–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ 0.6B\n",
    "        >>> expert = Expert(hidden_size=512, intermediate_size=2048)\n",
    "        >>> x = torch.randn(2, 10, 512)  # (batch=2, seq=10, hidden=512)\n",
    "        >>> output = expert(x)\n",
    "        >>> output.shape  # torch.Size([2, 10, 512])\n",
    "\n",
    "        >>> # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "        >>> num_experts = 8\n",
    "        >>> experts = nn.ModuleList([\n",
    "        ...     Expert(hidden_size=512, intermediate_size=2048)\n",
    "        ...     for _ in range(num_experts)\n",
    "        ... ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "        #       - hidden_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\n",
    "        #       - intermediate_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\n",
    "        #       - dropout –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0.0, 1.0)\n",
    "        # TODO: –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç—ã –∫–ª–∞—Å—Å–∞\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ self.ffn - —ç–∫–∑–µ–º–ø–ª—è—Ä SwiGLU\n",
    "        #       –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: input_dim=hidden_size, output_dim=hidden_size,\n",
    "        #                  intermediate_dim=intermediate_size\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ self.dropout - —Å–ª–æ–π Dropout —Å –∑–∞–¥–∞–Ω–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é\n",
    "        #       (–¥–∞–∂–µ –µ—Å–ª–∏ dropout=0.0, —Å–æ–∑–¥–∞–π—Ç–µ —Å–ª–æ–π –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è)\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É intermediate_size –æ–±—ã—á–Ω–æ –≤ 4 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ hidden_size?\n",
    "        # - –ó–∞—á–µ–º –Ω—É–∂–µ–Ω dropout –≤ —ç–∫—Å–ø–µ—Ä—Ç–∞—Ö?\n",
    "        # - –ö–∞–∫ SwiGLU –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ–±—ã—á–Ω–æ–≥–æ ReLU FFN?\n",
    "        # - –ü–æ—á–µ–º—É –∫–∞–∂–¥—ã–π —ç–∫—Å–ø–µ—Ä—Ç –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É?\n",
    "        # pass\n",
    "\n",
    "        # --- –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ -------------------------------------------------\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º assert –¥–ª—è —Ä–∞–Ω–Ω–µ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.\n",
    "        assert isinstance(hidden_size, int) and hidden_size > 0, (\n",
    "            \"hidden_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\"\n",
    "        )\n",
    "        assert isinstance(intermediate_size, int) and intermediate_size > 0, (\n",
    "            \"intermediate_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\"\n",
    "        )\n",
    "        assert isinstance(dropout, float) and 0.0 <= dropout < 1.0, (\n",
    "            \"dropout –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0.0, 1.0)\"\n",
    "        )\n",
    "\n",
    "        # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ------------------------------------------------\n",
    "        # –•—Ä–∞–Ω–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç—ã —ç–∫–∑–µ–º–ø–ª—è—Ä–∞\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.dropout_prob = dropout\n",
    "\n",
    "        # --- –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–µ–≤ ------------------------------------------------------\n",
    "        # SwiGLU feed-forward —Å–µ—Ç—å\n",
    "        self.ffn = SwiGLU(\n",
    "            input_dim=self.hidden_size,\n",
    "            output_dim=self.hidden_size,\n",
    "            intermediate_dim=self.intermediate_size\n",
    "        )\n",
    "\n",
    "        # Dropout —Å–ª–æ–π –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
    "        self.dropout = nn.Dropout(p=self.dropout_prob)\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–∞ –∫ –≤—Ö–æ–¥–Ω—ã–º —Å–∫—Ä—ã—Ç—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏—è–º.\n",
    "\n",
    "            –ü—Ä–æ—Ü–µ—Å—Å:\n",
    "            1. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SwiGLU feed-forward —Å–µ—Ç–∏\n",
    "            2. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            hidden_states: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
    "                          –°–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –∫ —ç—Ç–æ–º—É —ç–∫—Å–ø–µ—Ä—Ç—É\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            output: –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
    "                   –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–µ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "        \"\"\"\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ self.ffn –∫ hidden_states\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ self.dropout –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É\n",
    "        # TODO: –í–µ—Ä–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ù—É–∂–µ–Ω –ª–∏ residual connection –≤–Ω—É—Ç—Ä–∏ —ç–∫—Å–ø–µ—Ä—Ç–∞?\n",
    "        # - –ö–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è dropout - —Ç–æ–ª—å–∫–æ –ø—Ä–∏ training –∏–ª–∏ –≤—Å–µ–≥–¥–∞?\n",
    "        # - –ö–∞–∫ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–∞ –∏ –≤—ã—Ö–æ–¥–∞ —Å–≤—è–∑–∞–Ω—ã?\n",
    "        # pass\n",
    "\n",
    "        # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ SwiGLU FFN\n",
    "        x = self.ffn(hidden_states)\n",
    "        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß∞ SimpleMoELayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc0138",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SimpleMoELayer - —ç—Ç–æ —É—á–µ–±–Ω–∞—è –≤–µ—Ä—Å–∏—è MoE, –∫–æ—Ç–æ—Ä–∞—è —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –ª–æ–≥–∏–∫–∏, –∞ –Ω–µ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ò—Å–ø–æ–ª—å–∑—É—è –ø—Ä–æ—Å—Ç–æ–π —Ü–∏–∫–ª –ø–æ —Ç–æ–∫–µ–Ω–∞–º, –º—ã –∏–∑–±–µ–≥–∞–µ–º —Å–ª–æ–∂–Ω—ã—Ö\n",
    "—Ç–µ–Ω–∑–æ—Ä–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏, –¥–µ–ª–∞—è –∫–æ–¥ –ø–æ–Ω—è—Ç–Ω—ã–º –∏ –ª–µ–≥–∫–æ –æ—Ç–ª–∞–∂–∏–≤–∞–µ–º—ã–º. –≠—Ç–æ –∏–¥–µ–∞–ª—å–Ω—ã–π first step –ø–µ—Ä–µ–¥ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# –õ–æ–∫–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã\n",
    "from experiments.domain.moe.router import MoERouter\n",
    "from experiments.domain.moe.expert import Expert\n",
    "\n",
    "\n",
    "class SimpleMoELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –ü—Ä–æ—Å—Ç–∞—è (–Ω–∞–∏–≤–Ω–∞—è) —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è MoE Layer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
    "\n",
    "        –≠—Ç–∞ –≤–µ—Ä—Å–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—ã–µ —Ü–∏–∫–ª—ã –≤–º–µ—Å—Ç–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π.\n",
    "        –ò–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ª–æ–≥–∏–∫–∏ MoE –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ—Ö–æ–¥–æ–º –∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏.\n",
    "\n",
    "        –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:\n",
    "        Input ‚Üí Router (–≤—ã–±–æ—Ä —ç–∫—Å–ø–µ—Ä—Ç–æ–≤) ‚Üí Dispatch ‚Üí Experts ‚Üí Combine ‚Üí Residual ‚Üí Output\n",
    "\n",
    "        Pipeline:\n",
    "        1. Router: –≤—ã–±–∏—Ä–∞–µ—Ç top_k —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "        2. Dispatch: —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–æ–∫–µ–Ω—ã –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —ç–∫—Å–ø–µ—Ä—Ç–∞–º\n",
    "        3. Process: –∫–∞–∂–¥—ã–π —ç–∫—Å–ø–µ—Ä—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–≤–æ–∏ —Ç–æ–∫–µ–Ω—ã\n",
    "        4. Combine: —Å–æ–±–∏—Ä–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –≤–µ—Å–∞–º–∏ –æ—Ç Router\n",
    "        5. Residual: –¥–æ–±–∞–≤–ª—è–µ—Ç –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É\n",
    "\n",
    "        –î–ª—è –º–æ–¥–µ–ª–∏ 0.6B:\n",
    "        - num_experts = 8\n",
    "        - top_k = 2 (–∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω ‚Üí 2 —ç–∫—Å–ø–µ—Ä—Ç–∞)\n",
    "        - hidden_size = 512\n",
    "        - intermediate_size = 2048\n",
    "\n",
    "    Mathematical Flow:\n",
    "    ---------------\n",
    "        x ‚àà ‚Ñù^(B√óS√óH)\n",
    "            ‚Üì\n",
    "        Router: (weights, experts_idx, loss) = Router(x)\n",
    "            weights ‚àà ‚Ñù^(B√óS√óK)      # –í–µ—Å–∞ –¥–ª—è K —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "            experts_idx ‚àà ‚Ñ§^(B√óS√óK)  # –ò–Ω–¥–µ–∫—Å—ã —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ [0, N)\n",
    "            ‚Üì\n",
    "        For each token t in (B√óS):\n",
    "            output[t] = Œ£(k=1 to K) weights[t,k] * Expert[experts_idx[t,k]](x[t])\n",
    "            ‚Üì\n",
    "        output = output + x  # Residual connection\n",
    "            ‚Üì\n",
    "        return output, loss\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        hidden_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞/–≤—ã—Ö–æ–¥–∞ (–¥–æ–ª–∂–Ω–∞ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –º–æ–¥–µ–ª—å—é)\n",
    "        num_experts: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (8 –¥–ª—è –º–æ–¥–µ–ª–∏ 0.6B)\n",
    "        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ per token (2 –¥–ª—è –º–æ–¥–µ–ª–∏ 0.6B)\n",
    "        intermediate_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (–æ–±—ã—á–Ω–æ 4*hidden_size)\n",
    "        expert_dropout: Dropout –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (default: 0.0)\n",
    "        capacity_factor: –§–∞–∫—Ç–æ—Ä –µ–º–∫–æ—Å—Ç–∏ –¥–ª—è Router (default: 1.25)\n",
    "        balance_loss_coef: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è load balancing loss (default: 0.01)\n",
    "\n",
    "    Returns (from forward):\n",
    "    ---------------\n",
    "        output: –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
    "                –í—ã—Ö–æ–¥–Ω—ã–µ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ—Å–ª–µ MoE –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "        balance_loss: –°–∫–∞–ª—è—Ä - load balancing loss –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "    Example:\n",
    "    ---------------\n",
    "        >>> # –°–æ–∑–¥–∞–Ω–∏–µ MoE Layer –¥–ª—è –º–æ–¥–µ–ª–∏ 0.6B\n",
    "        >>> moe = SimpleMoELayer(\n",
    "        ...     hidden_size=512,\n",
    "        ...     num_experts=8,\n",
    "        ...     top_k=2,\n",
    "        ...     intermediate_size=2048\n",
    "        ... )\n",
    "        >>> x = torch.randn(2, 10, 512)  # (batch=2, seq=10, hidden=512)\n",
    "        >>> output, loss = moe(x, training=True)\n",
    "        >>> output.shape  # torch.Size([2, 10, 512])\n",
    "        >>> loss.item()   # –°–∫–∞–ª—è—Ä loss\n",
    "\n",
    "    Note:\n",
    "    ---------------\n",
    "        –≠—Ç–æ –ü–†–û–°–¢–ê–Ø –≤–µ—Ä—Å–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ü–∏–∫–ª—ã –≤–º–µ—Å—Ç–æ\n",
    "        –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö batch –æ–ø–µ—Ä–∞—Ü–∏–π. –î–ª—è production –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ\n",
    "        –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é MoELayer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_experts: int = 8,\n",
    "        top_k: int = 2,\n",
    "        intermediate_size: int = 2048,\n",
    "        expert_dropout: float = 0.0,\n",
    "        capacity_factor: float = 1.25,\n",
    "        balance_loss_coef: float = 0.01\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "        #       - hidden_size > 0\n",
    "        #       - num_experts > 0\n",
    "        #       - top_k > 0 –∏ top_k <= num_experts\n",
    "        #       - intermediate_size > 0\n",
    "        # TODO: –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç—ã –∫–ª–∞—Å—Å–∞\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ self.router - —ç–∫–∑–µ–º–ø–ª—è—Ä MoERouter\n",
    "        #       –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: hidden_size, num_experts, top_k, capacity_factor, balance_loss_coef\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ self.experts - nn.ModuleList –∏–∑ num_experts —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "        #       –ö–∞–∂–¥—ã–π —ç–∫—Å–ø–µ—Ä—Ç: Expert(hidden_size, intermediate_size, expert_dropout)\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º nn.ModuleList, –∞ –Ω–µ –æ–±—ã—á–Ω—ã–π Python list?\n",
    "        # - –ó–∞—á–µ–º –Ω—É–∂–µ–Ω residual connection –≤ MoE Layer?\n",
    "        # - –ö–∞–∫ top_k –≤–ª–∏—è–µ—Ç –Ω–∞ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å?\n",
    "        # - –ß—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç, –µ—Å–ª–∏ —ç–∫—Å–ø–µ—Ä—Ç –ø–æ–ª—É—á–∏—Ç 0 —Ç–æ–∫–µ–Ω–æ–≤?\n",
    "        # pass\n",
    "\n",
    "        assert hidden_size > 0, \"hidden_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0\"\n",
    "        assert num_experts > 0, \"num_experts –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0\"\n",
    "        assert top_k > 0 and top_k <= num_experts, \"top_k –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0 –∏ <= num_experts\"\n",
    "        assert intermediate_size > 0, \"intermediate_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0\"\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.expert_dropout = expert_dropout\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.balance_loss_coef = balance_loss_coef\n",
    "\n",
    "        self.router = MoERouter(\n",
    "            hidden_size=hidden_size,\n",
    "            num_experts=num_experts,\n",
    "            top_k=top_k,\n",
    "            capacity_factor=capacity_factor,\n",
    "            balance_loss_coef=balance_loss_coef\n",
    "        )\n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(hidden_size, intermediate_size, expert_dropout)\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        training: bool = True\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä–∏–º–µ–Ω—è–µ—Ç MoE —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –∫ –≤—Ö–æ–¥–Ω—ã–º —Å–∫—Ä—ã—Ç—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏—è–º.\n",
    "\n",
    "            –ù–∞–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Ü–∏–∫–ª—ã (–ø—Ä–æ—Å—Ç–∞—è, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–∞—è):\n",
    "            1. Router –≤—ã–±–∏—Ä–∞–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "            2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞:\n",
    "               - –ë–µ—Ä—ë–º top_k —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "               - –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω –∫–∞–∂–¥—ã–º —ç–∫—Å–ø–µ—Ä—Ç–æ–º\n",
    "               - –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "            3. Residual connection\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            hidden_states: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
    "            training: –§–ª–∞–≥ —Ä–µ–∂–∏–º–∞ –æ–±—É—á–µ–Ω–∏—è (–¥–ª—è balance loss)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            output: –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
    "                   –í—ã—Ö–æ–¥–Ω—ã–µ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "            balance_loss: –°–∫–∞–ª—è—Ä - load balancing loss\n",
    "        \"\"\"\n",
    "        # TODO: –®–∞–≥ 1 - –í—ã–∑–æ–≤–∏—Ç–µ self.router\n",
    "        # TODO: –®–∞–≥ 2 - –ü–æ–ª—É—á–∏—Ç–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ hidden_states.shape=\n",
    "        # TODO: –®–∞–≥ 3 - –°–æ–∑–¥–∞–π—Ç–µ output —Ç–µ–Ω–∑–æ—Ä\n",
    "        # TODO: –®–∞–≥ 4 - Dispatch + Process + Combine (–Ω–∞–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥)\n",
    "        # TODO: –®–∞–≥ 5 - output = output + hidden_states\n",
    "        # TODO: –®–∞–≥ 6 - –í–µ—Ä–Ω–∏—Ç–µ (output, balance_loss)\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º token = hidden_states[b, s:s+1, :] —Å s:s+1, –∞ –Ω–µ s?\n",
    "        # - –ó–∞—á–µ–º –Ω—É–∂–µ–Ω .item() –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ expert_idx –∏ weight?\n",
    "        # - –ß—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç, –µ—Å–ª–∏ —É–±—Ä–∞—Ç—å residual connection?\n",
    "        # - –ö–∞–∫ –º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–∏ —Ü–∏–∫–ª—ã?\n",
    "        # pass\n",
    "\n",
    "        # –®–∞–≥ 1 - Router\n",
    "        # nn.Module.__call__ –æ–±—ë—Ä—Ç–∫–∞: router(...) –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–∑—ã–≤–∞–µ—Ç router.forward(...)\n",
    "        routing_weights, selected_experts, balance_loss = self.router(hidden_states, training)\n",
    "\n",
    "        # –®–∞–≥ 2 - –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "\n",
    "        # –®–∞–≥ 3 - Output —Ç–µ–Ω–∑–æ—Ä\n",
    "        output = torch.zeros(batch_size, seq_len, hidden_size, device=hidden_states.device)\n",
    "\n",
    "        # –®–∞–≥ 4 - Dispatch + Process + Combine (–Ω–∞–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥)\n",
    "        for b in range(batch_size):\n",
    "            for s in range(seq_len):\n",
    "                token = hidden_states[b, s:s+1, :]  # (1, 1, H)\n",
    "                token_output = torch.zeros(1, 1, hidden_size, device=hidden_states.device)\n",
    "\n",
    "                for k in range(self.top_k):\n",
    "                    expert_idx = selected_experts[b, s, k].item()\n",
    "                    weight = routing_weights[b, s, k].item()\n",
    "\n",
    "                    expert_output = self.experts[expert_idx](token)  # (1, 1, H)\n",
    "\n",
    "                    token_output += weight * expert_output  # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "\n",
    "                output[b, s, :] = token_output.squeeze()    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "\n",
    "        # –®–∞–≥ 5 - Residual connection\n",
    "        output = output + hidden_states\n",
    "\n",
    "        # –®–∞–≥ 6 - Return\n",
    "        return output, balance_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è OptimizedMoELayer (–≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OptimizedMoELayer - –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è MoE –¥–ª—è production –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.\n",
    "\n",
    "–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç SimpleMoELayer (—É—á–µ–±–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å —Ü–∏–∫–ª–∞–º–∏), —ç—Ç–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç\n",
    "batch operations –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ GPU. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è:\n",
    "–≤–º–µ—Å—Ç–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É, –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –∫–∞–∂–¥–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞\n",
    "–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–µ–º.\n",
    "\n",
    "Speedup: 2-3x –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å SimpleMoELayer –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —á–∏—Å–ª–µ–Ω–Ω–æ–π —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç–∏.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# –õ–æ–∫–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã\n",
    "from experiments.domain.moe.router import MoERouter\n",
    "from experiments.domain.moe.expert import Expert\n",
    "\n",
    "\n",
    "class OptimizedMoELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è (–≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è) —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è MoE Layer –¥–ª—è production.\n",
    "\n",
    "        –≠—Ç–∞ –≤–µ—Ä—Å–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç batch operations –≤–º–µ—Å—Ç–æ —Ü–∏–∫–ª–æ–≤ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π\n",
    "        –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ GPU. API –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏–º —Å SimpleMoELayer.\n",
    "\n",
    "        –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (3 —Ñ–∞–∑—ã):\n",
    "        Input ‚Üí Router ‚Üí Phase 1 (Flatten) ‚Üí Phase 2 (Parallel Process) ‚Üí\n",
    "        Phase 3 (Combine) ‚Üí Residual ‚Üí Output\n",
    "\n",
    "        Pipeline:\n",
    "        1. Router: –≤—ã–±–∏—Ä–∞–µ—Ç top_k —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "        2. Phase 1 (Flatten):\n",
    "           - Expand —Ç–æ–∫–µ–Ω—ã –¥–ª—è K –≤—ã–±–æ—Ä–æ–≤: (B,S,H) ‚Üí (B,S,K,H)\n",
    "           - Flatten –≤—Å—ë –≤ 1D: (B,S,K,H) ‚Üí (B*S*K, H)\n",
    "        3. Phase 2 (Parallel Process):\n",
    "           - –î–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞: batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "           - Boolean masking: experts_flat == expert_idx\n",
    "           - –í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ: output * routing_weights\n",
    "        4. Phase 3 (Combine):\n",
    "           - Reshape: (B*S*K, H) ‚Üí (B,S,K,H)\n",
    "           - Sum –ø–æ –æ—Å–∏ K: (B,S,K,H) ‚Üí (B,S,H)\n",
    "        5. Residual: –¥–æ–±–∞–≤–ª—è–µ—Ç –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É\n",
    "\n",
    "        –î–ª—è –º–æ–¥–µ–ª–∏ 0.6B:\n",
    "        - num_experts = 8\n",
    "        - top_k = 2 (–∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω ‚Üí 2 —ç–∫—Å–ø–µ—Ä—Ç–∞)\n",
    "        - hidden_size = 512\n",
    "        - intermediate_size = 2048\n",
    "\n",
    "    Mathematical Flow:\n",
    "    ---------------\n",
    "        x ‚àà ‚Ñù^(B√óS√óH)\n",
    "            ‚Üì\n",
    "        Router: (weights, experts_idx, loss) = Router(x)\n",
    "            weights ‚àà ‚Ñù^(B√óS√óK)      # –í–µ—Å–∞ –¥–ª—è K —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "            experts_idx ‚àà ‚Ñ§^(B√óS√óK)  # –ò–Ω–¥–µ–∫—Å—ã —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ [0, N)\n",
    "            ‚Üì\n",
    "        Flatten: x_flat ‚àà ‚Ñù^(B*S*K √ó H)\n",
    "            ‚Üì\n",
    "        For each expert i in parallel:\n",
    "            mask_i = (experts_idx == i)\n",
    "            tokens_i = x_flat[mask_i]\n",
    "            outputs_i = Expert_i(tokens_i) * weights[mask_i]\n",
    "            ‚Üì\n",
    "        Combine: reshape ‚Üí sum(K) ‚Üí (B√óS√óH)\n",
    "            ‚Üì\n",
    "        output = output + x  # Residual connection\n",
    "            ‚Üì\n",
    "        return output, loss\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        hidden_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞/–≤—ã—Ö–æ–¥–∞ (–¥–æ–ª–∂–Ω–∞ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –º–æ–¥–µ–ª—å—é)\n",
    "        num_experts: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (8 –¥–ª—è –º–æ–¥–µ–ª–∏ 0.6B)\n",
    "        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ per token (2 –¥–ª—è –º–æ–¥–µ–ª–∏ 0.6B)\n",
    "        intermediate_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (–æ–±—ã—á–Ω–æ 4*hidden_size)\n",
    "        expert_dropout: Dropout –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (default: 0.0)\n",
    "        capacity_factor: –§–∞–∫—Ç–æ—Ä –µ–º–∫–æ—Å—Ç–∏ –¥–ª—è Router (default: 1.25)\n",
    "        balance_loss_coef: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è load balancing loss (default: 0.01)\n",
    "\n",
    "    Returns (from forward):\n",
    "    ---------------\n",
    "        output: –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
    "                –í—ã—Ö–æ–¥–Ω—ã–µ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ—Å–ª–µ MoE –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "        balance_loss: –°–∫–∞–ª—è—Ä - load balancing loss –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "    Example:\n",
    "    ---------------\n",
    "        >>> # –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π MoE Layer –¥–ª—è –º–æ–¥–µ–ª–∏ 0.6B\n",
    "        >>> moe = OptimizedMoELayer(\n",
    "        ...     hidden_size=512,\n",
    "        ...     num_experts=8,\n",
    "        ...     top_k=2,\n",
    "        ...     intermediate_size=2048\n",
    "        ... )\n",
    "        >>> x = torch.randn(2, 10, 512)  # (batch=2, seq=10, hidden=512)\n",
    "        >>> output, loss = moe(x, training=True)\n",
    "        >>> output.shape  # torch.Size([2, 10, 512])\n",
    "        >>> loss.item()   # –°–∫–∞–ª—è—Ä loss\n",
    "\n",
    "    Note:\n",
    "    ---------------\n",
    "        –≠—Ç–∞ –≤–µ—Ä—Å–∏—è –¥–ª—è PRODUCTION –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ\n",
    "        batch –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ß–∏—Å–ª–µ–Ω–Ω–æ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–∞\n",
    "        SimpleMoELayer (–¥–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ float32).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_experts: int = 8,\n",
    "        top_k: int = 2,\n",
    "        intermediate_size: int = 2048,\n",
    "        expert_dropout: float = 0.0,\n",
    "        capacity_factor: float = 1.25,\n",
    "        balance_loss_coef: float = 0.01\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO(human): –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "        # TODO(human): –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç—ã –∫–ª–∞—Å—Å–∞\n",
    "        # TODO(human): –°–æ–∑–¥–∞–π—Ç–µ self.router - —ç–∫–∑–µ–º–ø–ª—è—Ä MoERouter\n",
    "        # TODO(human): –°–æ–∑–¥–∞–π—Ç–µ self.experts - nn.ModuleList –∏–∑ num_experts —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º nn.ModuleList, –∞ –Ω–µ –æ–±—ã—á–Ω—ã–π Python list?\n",
    "        # - –ë—É–¥–µ—Ç –ª–∏ —ç—Ç–∞ –≤–µ—Ä—Å–∏—è API-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å SimpleMoELayer?\n",
    "        # - –ö–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–ª–∏—è—é—Ç –Ω–∞ memory usage?\n",
    "        # pass\n",
    "\n",
    "        assert hidden_size > 0, \"hidden_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0\"\n",
    "        assert num_experts > 0, \"num_experts –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0\"\n",
    "        assert top_k > 0 and top_k <= num_experts, \"top_k –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0 –∏ <= num_experts\"\n",
    "        assert intermediate_size > 0, \"intermediate_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0\"\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.expert_dropout = expert_dropout\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.balance_loss_coef = balance_loss_coef\n",
    "\n",
    "        self.router = MoERouter(\n",
    "            hidden_size = hidden_size,\n",
    "            num_experts = num_experts,\n",
    "            top_k = top_k,\n",
    "            capacity_factor = capacity_factor,\n",
    "            balance_loss_coef = balance_loss_coef\n",
    "        )\n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(\n",
    "                hidden_size = hidden_size,\n",
    "                intermediate_size = intermediate_size,\n",
    "                dropout = expert_dropout\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        training: bool = True\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—É—é MoE —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –∫ –≤—Ö–æ–¥–Ω—ã–º —Å–∫—Ä—ã—Ç—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏—è–º.\n",
    "\n",
    "            –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ batch operations:\n",
    "            1. Router –≤—ã–±–∏—Ä–∞–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "            2. Phase 1 - Flatten: (B,S,H) ‚Üí (B,S,K,H) ‚Üí (B*S*K, H)\n",
    "            3. Phase 2 - Parallel Process: batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥—ã–º —ç–∫—Å–ø–µ—Ä—Ç–æ–º\n",
    "            4. Phase 3 - Combine: (B*S*K, H) ‚Üí (B,S,K,H) ‚Üí sum(K) ‚Üí (B,S,H)\n",
    "            5. Residual connection\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            hidden_states: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
    "            training: –§–ª–∞–≥ —Ä–µ–∂–∏–º–∞ –æ–±—É—á–µ–Ω–∏—è (–¥–ª—è balance loss)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            output: –¢–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
    "                   –í—ã—Ö–æ–¥–Ω—ã–µ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "            balance_loss: –°–∫–∞–ª—è—Ä - load balancing loss\n",
    "\n",
    "        Shape Transformations:\n",
    "        ---------------\n",
    "            hidden_states:     (B, S, H)\n",
    "                ‚Üì unsqueeze(2)\n",
    "            tokens:            (B, S, 1, H)\n",
    "                ‚Üì expand(-1, -1, K, -1)\n",
    "            tokens_expanded:   (B, S, K, H)\n",
    "                ‚Üì reshape(-1, H)\n",
    "            tokens_flat:       (B*S*K, H)\n",
    "                ‚Üì expert processing + weighting\n",
    "            expert_outputs:    (B*S*K, H)\n",
    "                ‚Üì reshape(B, S, K, H)\n",
    "            expert_outputs:    (B, S, K, H)\n",
    "                ‚Üì sum(dim=2)\n",
    "            combined:          (B, S, H)\n",
    "                ‚Üì residual\n",
    "            output:            (B, S, H)\n",
    "        \"\"\"\n",
    "        # TODO(human): –®–∞–≥ 0 - Router\n",
    "        #       –ü–æ–ª—É—á–∏—Ç–µ routing_weights, selected_experts, balance_loss –æ—Ç self.router\n",
    "\n",
    "        # TODO(human): –®–∞–≥ 1 - Flatten –¥–ª—è batch processing\n",
    "        #       1.1. –ò–∑–≤–ª–µ–∫–∏—Ç–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: hidden_states.shape\n",
    "        #       1.2. –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ self.top_k\n",
    "        #       1.3. Expand —Ç–æ–∫–µ–Ω—ã –¥–ª—è K –≤—ã–±–æ—Ä–æ–≤:\n",
    "        #            –ü—Ä–µ–æ–±—Ä–∞–∑—É–π—Ç–µ hidden_states: (B, S, H) ‚Üí (B, S, 1, H) ‚Üí (B, S, K, H)\n",
    "        #       1.4. Flatten –≤—Å—ë –≤ 1D:\n",
    "        #            tokens_flat  = (B*S*K, H)\n",
    "        #            weights_flat = (B*S*K,)\n",
    "        #            experts_flat = (B*S*K,)\n",
    "\n",
    "        # TODO(human): –®–∞–≥ 2 - Parallel Expert Processing\n",
    "        #       2.1. –°–æ–∑–¥–∞–π—Ç–µ output —Ç–µ–Ω–∑–æ—Ä: expert_outputs\n",
    "        #       2.2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞:\n",
    "        #            a) –°–æ–∑–¥–∞–π—Ç–µ boolean –º–∞—Å–∫—É\n",
    "        #            b) –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: (skip –ø—É—Å—Ç—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤)\n",
    "        #            c) –ò–∑–≤–ª–µ–∫–∏—Ç–µ —Ç–æ–∫–µ–Ω—ã —ç–∫—Å–ø–µ—Ä—Ç–∞\n",
    "        #            d) –û–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –±–∞—Ç—á–µ–º\n",
    "        #            e) –í–∑–≤–µ—Å—å—Ç–µ –ø–æ routing_weights\n",
    "        #            f) –ó–∞–ø–∏—à–∏—Ç–µ –æ–±—Ä–∞—Ç–Ω–æ –≤ weighted_output\n",
    "        \n",
    "        # TODO(human): –®–∞–≥ 3 - Combine - —Å—É–º–º–∏—Ä—É–µ–º K –≤–∫–ª–∞–¥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "        # TODO(human): –®–∞–≥ 4 - Residual connection\n",
    "        # TODO(human): –®–∞–≥ 5 - Return\n",
    "        #       return output, balance_loss\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º unsqueeze(2).expand(), –∞ –Ω–µ repeat()?\n",
    "        # - –ó–∞—á–µ–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å mask.sum() > 0 –ø–µ—Ä–µ–¥ –≤—ã–∑–æ–≤–æ–º —ç–∫—Å–ø–µ—Ä—Ç–∞?\n",
    "        # - –ö–∞–∫ weights_flat[mask].unsqueeze(-1) –≤–ª–∏—è–µ—Ç –Ω–∞ broadcasting?\n",
    "        # - –ü–æ—á–µ–º—É sum(dim=2) –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç K –≤–∫–ª–∞–¥–æ–≤?\n",
    "        # - –í —á—ë–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —ç—Ç–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏ SimpleMoELayer –≤ –ø–ª–∞–Ω–µ –ø–∞–º—è—Ç–∏?\n",
    "        # pass\n",
    "\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        # –®–∞–≥ 0: Router - –≤—ã–±–∏—Ä–∞–µ–º top_k —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        routing_weights, selected_experts, load_balance_loss = self.router(hidden_states, training)\n",
    "\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        # –®–∞–≥ 1: Flatten - –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è batch processing\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º top_k –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞\n",
    "        top_k = self.top_k\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # –®–∞–≥ 1.1: Expand —Ç–æ–∫–µ–Ω—ã –¥–ª—è K –≤—ã–±–æ—Ä–æ–≤ (memory-efficient –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ)\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è: (B, S, H) ‚Üí (B, S, 1, H) ‚Üí (B, S, K, H)\n",
    "        # unsqueeze(2): –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –æ—Å—å —Ä–∞–∑–º–µ—Ä–∞ 1 –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ 2\n",
    "        # expand(): \"—Ä–∞—Å—Ç—è–≥–∏–≤–∞–µ–º\" –æ—Å—å 1 ‚Üí K (–ë–ï–ó –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –ø–∞–º—è—Ç–∏!)\n",
    "        # –†–µ–∑—É–ª—å—Ç–∞—Ç: –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω K —Ä–∞–∑ (–¥–ª—è K —ç–∫—Å–ø–µ—Ä—Ç–æ–≤)\n",
    "        # –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–æ–∫–µ–Ω (–≤–µ–∫—Ç–æ—Ä):\n",
    "        # hidden_states[b=0, s=0] = [1, 2, 3, 4]  # shape: (H=4,)\n",
    "\n",
    "        # # –ü–æ—Å–ª–µ unsqueeze(2).expand(..., K=2, ...):\n",
    "        # tokens[b=0, s=0] = [\n",
    "        #     [1, 2, 3, 4],  # k=0 (–∫–æ–ø–∏—è –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞)\n",
    "        #     [1, 2, 3, 4]   # k=1 (–∫–æ–ø–∏—è –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞)\n",
    "        # ]  # shape: (K=2, H=4) - —ç—Ç–æ –º–∞—Ç—Ä–∏—Ü–∞!\n",
    "        tokens = hidden_states.unsqueeze(2).expand(batch_size, seq_len, top_k, hidden_size)\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # –®–∞–≥ 1.2: Flatten –≤—Å—ë –≤ 1D –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # Flatten: (B, S, K, H) ‚Üí (B*S*K, H)\n",
    "        # \"–†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º\" 4D —Ç–µ–Ω–∑–æ—Ä –≤ 2D –º–∞—Ç—Ä–∏—Ü—É (—Å–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤)\n",
    "\n",
    "        # –î–æ reshape (4D): tokens.shape = (B=2, S=3, K=2, H=4)\n",
    "        # tokens = [\n",
    "        #   # Batch 0:\n",
    "        #   [ [[1,2,3,4], [1,2,3,4]],    # s=0: –º–∞—Ç—Ä–∏—Ü–∞ 2√ó4\n",
    "        #     [[5,6,7,8], [5,6,7,8]],    # s=1: –º–∞—Ç—Ä–∏—Ü–∞ 2√ó4\n",
    "        #     [[9,10,11,12], [9,10,11,12]] ], # s=2: –º–∞—Ç—Ä–∏—Ü–∞ 2√ó4\n",
    "        #   # Batch 1: –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ...\n",
    "        # ]\n",
    "\n",
    "        # –ü–æ—Å–ª–µ reshape (2D): tokens_flat.shape = (B*S*K=12, H=4)\n",
    "        # tokens_flat = [\n",
    "        #   [1, 2, 3, 4],      # –∏–Ω–¥–µ–∫—Å 0: (b=0, s=0, k=0)\n",
    "        #   [1, 2, 3, 4],      # –∏–Ω–¥–µ–∫—Å 1: (b=0, s=0, k=1)\n",
    "        #   [5, 6, 7, 8],      # –∏–Ω–¥–µ–∫—Å 2: (b=0, s=1, k=0)\n",
    "        #   [5, 6, 7, 8],      # –∏–Ω–¥–µ–∫—Å 3: (b=0, s=1, k=1)\n",
    "        #   [9, 10, 11, 12],   # –∏–Ω–¥–µ–∫—Å 4: (b=0, s=2, k=0)\n",
    "        #   [9, 10, 11, 12],   # –∏–Ω–¥–µ–∫—Å 5: (b=0, s=2, k=1)\n",
    "        #   # ... batch 1: –∏–Ω–¥–µ–∫—Å—ã 6-11\n",
    "        # ]\n",
    "        # ‚ö†Ô∏è –ü–æ—Ä—è–¥–æ–∫ flatten: —Å–Ω–∞—á–∞–ª–∞ batch, –ø–æ—Ç–æ–º sequence, –ø–æ—Ç–æ–º K\n",
    "        tokens_flat = tokens.reshape(-1, hidden_size)  # (B*S*K, H)\n",
    "\n",
    "        # Flatten –≤–µ—Å–æ–≤: (B, S, K) ‚Üí (B*S*K,)\n",
    "        # weights_flat[i] = –≤–µ—Å –¥–ª—è tokens_flat[i]\n",
    "        weights_flat = routing_weights.reshape(-1)     # (B*S*K,)\n",
    "\n",
    "        # Flatten –∏–Ω–¥–µ–∫—Å–æ–≤ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: (B, S, K) ‚Üí (B*S*K,)\n",
    "        # experts_flat[i] = –∫–∞–∫–æ–π —ç–∫—Å–ø–µ—Ä—Ç –¥–æ–ª–∂–µ–Ω –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å tokens_flat[i]\n",
    "        experts_flat = selected_experts.reshape(-1)    # (B*S*K,)\n",
    "\n",
    "        # ‚ö†Ô∏è –í–ê–ñ–ù–û: –ü–æ—Å–ª–µ flatten –≤—Å–µ 3 —Ç–µ–Ω–∑–æ—Ä–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –∏–Ω–¥–µ–∫—Å—É:\n",
    "        #   tokens_flat[i]   - —Ç–æ–∫–µ–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "        #   weights_flat[i]  - –≤–µ—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–∏ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏\n",
    "        #   experts_flat[i]  - –∏–Ω–¥–µ–∫—Å —ç–∫—Å–ø–µ—Ä—Ç–∞ [0, num_experts)\n",
    "\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        # –®–∞–≥ 2: Parallel Expert Processing - batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥—ã–º —ç–∫—Å–ø–µ—Ä—Ç–æ–º\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –Ω—É–ª—è–º–∏ (–±—É–¥–µ–º –∑–∞–ø–æ–ª–Ω—è—Ç—å –ø–æ –º–∞—Å–∫–∞–º)\n",
    "        # –ö–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –≤ tokens_flat –±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω —Ä–æ–≤–Ω–æ 1 —Ä–∞–∑ (–ø–æ —Å–≤–æ–µ–º—É —ç–∫—Å–ø–µ—Ä—Ç—É)\n",
    "        expert_outputs = torch.zeros_like(tokens_flat)  # (B*S*K, H)\n",
    "\n",
    "        # –¶–∏–∫–ª –ø–æ —ç–∫—Å–ø–µ—Ä—Ç–∞–º: –∫–∞–∂–¥—ã–π –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–≤–æ—é –≥—Ä—É–ø–ø—É —Ç–æ–∫–µ–Ω–æ–≤ –±–∞—Ç—á–µ–º\n",
    "        # ‚ö†Ô∏è –í–ê–ñ–ù–û: –≠—Ç–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —Ü–∏–∫–ª –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏!\n",
    "        #   SimpleMoELayer: 3 –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ü–∏–∫–ª–∞ (batch √ó sequence √ó top_k)\n",
    "        #   OptimizedMoELayer: 1 —Ü–∏–∫–ª (num_experts), –≤–Ω—É—Ç—Ä–∏ - batch operations\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            # –®–∞–≥ 2.1: Boolean masking - –Ω–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è —ç—Ç–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞\n",
    "            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            # mask = True –¥–ª—è –ø–æ–∑–∏—Ü–∏–π, –≥–¥–µ experts_flat == expert_idx\n",
    "            # –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ expert_idx=3, —Ç–æ mask –≤—ã–¥–µ–ª–∏—Ç –≤—Å–µ —Ç–æ–∫–µ–Ω—ã,\n",
    "            # –∫–æ—Ç–æ—Ä—ã–µ Router –Ω–∞–∑–Ω–∞—á–∏–ª —Ç—Ä–µ—Ç—å–µ–º—É —ç–∫—Å–ø–µ—Ä—Ç—É\n",
    "            mask = (experts_flat == expert_idx)  # (B*S*K,) - boolean —Ç–µ–Ω–∑–æ—Ä\n",
    "\n",
    "            # –ü—Ä–∏–º–µ—Ä mask –¥–ª—è expert_idx=0:\n",
    "            # experts_flat = [0, 2, 0, 5, 0, 1, ...]\n",
    "            # mask         = [T, F, T, F, T, F, ...]\n",
    "            # –ì–¥–µ T –æ–∑–Ω–∞—á–∞–µ—Ç \"—ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–∞ 0\"\n",
    "\n",
    "            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            # –®–∞–≥ 2.2: Skip –ø—É—Å—Ç—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)\n",
    "            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            # –ï—Å–ª–∏ Router –Ω–µ –Ω–∞–∑–Ω–∞—á–∏–ª –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ —ç—Ç–æ–º—É —ç–∫—Å–ø–µ—Ä—Ç—É, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º\n",
    "            # –≠—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç –≤—Ä–µ–º—è –Ω–∞ forward pass –ø—É—Å—Ç–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞\n",
    "            if mask.sum() > 0:\n",
    "                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                # –®–∞–≥ 2.3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ —ç–∫—Å–ø–µ—Ä—Ç–∞ —á–µ—Ä–µ–∑ boolean indexing\n",
    "                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ —Ç–æ–∫–µ–Ω—ã, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö mask == True\n",
    "                # –≠—Ç–æ —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä (–∫–æ–º–ø–∞–∫—Ç–Ω—ã–π, –±–µ–∑ –ø—É—Å—Ç—ã—Ö –º–µ—Å—Ç)\n",
    "                expert_tokens = tokens_flat[mask]  # (num_selected_tokens, H)\n",
    "\n",
    "                # –ü—Ä–∏–º–µ—Ä:\n",
    "                # tokens_flat = [\n",
    "                #   [1, 2, 3, 4],   # –∏–Ω–¥–µ–∫—Å 0 (mask=True –¥–ª—è expert_idx=0)\n",
    "                #   [5, 6, 7, 8],   # –∏–Ω–¥–µ–∫—Å 1 (mask=False)\n",
    "                #   [9, 10, 11, 12] # –∏–Ω–¥–µ–∫—Å 2 (mask=True –¥–ª—è expert_idx=0)\n",
    "                # ]\n",
    "                # expert_tokens = [\n",
    "                #   [1, 2, 3, 4],   # –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ 0\n",
    "                #   [9, 10, 11, 12] # –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ 2\n",
    "                # ]  # shape: (2, H) - —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã!\n",
    "\n",
    "                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                # –®–∞–≥ 2.4: Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–º\n",
    "                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                # –í—ã–∑—ã–≤–∞–µ–º —ç–∫—Å–ø–µ—Ä—Ç–∞ –û–î–ò–ù –†–ê–ó –¥–ª—è –í–°–ï–• –µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "                # –≠—Ç–æ –∫–ª—é—á –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é: –≤–º–µ—Å—Ç–æ N –≤—ã–∑–æ–≤–æ–≤ - 1 –≤—ã–∑–æ–≤ —Å –±–∞—Ç—á–µ–º\n",
    "                output = self.experts[expert_idx](expert_tokens)  # (num_selected_tokens, H)\n",
    "\n",
    "                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                # –®–∞–≥ 2.5: –í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –ø–æ routing weights\n",
    "                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å–∞ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É—è —Ç—É –∂–µ –º–∞—Å–∫—É)\n",
    "                # unsqueeze(-1): (num_tokens,) ‚Üí (num_tokens, 1) –¥–ª—è broadcasting\n",
    "                expert_weights = weights_flat[mask].unsqueeze(-1)  # (num_selected_tokens, 1)\n",
    "\n",
    "                # Broadcasting: (num_tokens, H) * (num_tokens, 1) ‚Üí (num_tokens, H)\n",
    "                # –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ output —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ —Å–≤–æ–π —Å–∫–∞–ª—è—Ä–Ω—ã–π –≤–µ—Å\n",
    "                # –ü—Ä–∏–º–µ—Ä:\n",
    "                # output = [[1, 2], [3, 4]]       # (2, 2)\n",
    "                # weights = [[0.7], [0.3]]        # (2, 1)\n",
    "                # result = [[0.7, 1.4], [0.9, 1.2]] # (2, 2)\n",
    "                weighted_output = output * expert_weights  # (num_selected_tokens, H)\n",
    "\n",
    "                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                # –®–∞–≥ 2.6: –ó–∞–ø–∏—Å—å –æ–±—Ä–∞—Ç–Ω–æ –≤ –∏—Å—Ö–æ–¥–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏\n",
    "                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –º–∞—Å–∫—É –¥–ª—è –∑–∞–ø–∏—Å–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ\n",
    "                # Boolean indexing —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –¥–ª—è –ø—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏—è!\n",
    "                expert_outputs[mask] = weighted_output\n",
    "\n",
    "                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è:\n",
    "                # –î–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏: expert_outputs = [[0,0,0,0], [0,0,0,0], [0,0,0,0]]\n",
    "                # –ü–æ—Å–ª–µ expert_idx=0: expert_outputs = [[1,2,3,4], [0,0,0,0], [9,10,11,12]]\n",
    "                # –ü–æ—Å–ª–µ expert_idx=1: expert_outputs = [[1,2,3,4], [5,6,7,8], [9,10,11,12]]\n",
    "\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        # –®–∞–≥ 3: Combine - —Å—É–º–º–∏—Ä—É–µ–º K –≤–∫–ª–∞–¥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # –®–∞–≥ 3.1: Reshape –æ–±—Ä–∞—Ç–Ω–æ –≤ 4D —Å—Ç—Ä—É–∫—Ç—É—Ä—É\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É: (B*S*K, H) ‚Üí (B, S, K, H)\n",
    "        # –≠—Ç–æ –æ–±—Ä–∞—Ç–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è –∫ flatten –∏–∑ –®–∞–≥–∞ 1.2\n",
    "        expert_outputs = expert_outputs.reshape(batch_size, seq_len, top_k, hidden_size)\n",
    "\n",
    "        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è reshape:\n",
    "        # –î–æ reshape (2D): expert_outputs.shape = (B*S*K=12, H=4)\n",
    "        # expert_outputs_flat = [\n",
    "        #   [1, 2, 3, 4],      # (b=0, s=0, k=0) - –≤–∫–ª–∞–¥ —ç–∫—Å–ø–µ—Ä—Ç–∞ 0\n",
    "        #   [0.5, 1, 1.5, 2],  # (b=0, s=0, k=1) - –≤–∫–ª–∞–¥ —ç–∫—Å–ø–µ—Ä—Ç–∞ 2\n",
    "        #   [5, 6, 7, 8],      # (b=0, s=1, k=0)\n",
    "        #   [2.5, 3, 3.5, 4],  # (b=0, s=1, k=1)\n",
    "        #   ...\n",
    "        # ]\n",
    "        #\n",
    "        # –ü–æ—Å–ª–µ reshape (4D): expert_outputs.shape = (B=2, S=3, K=2, H=4)\n",
    "        # expert_outputs = [\n",
    "        #   [ # Batch 0\n",
    "        #     [[1,2,3,4], [0.5,1,1.5,2]],           # s=0: K=2 –≤–∫–ª–∞–¥–∞\n",
    "        #     [[5,6,7,8], [2.5,3,3.5,4]],           # s=1: K=2 –≤–∫–ª–∞–¥–∞\n",
    "        #     [[9,10,11,12], [4.5,5,5.5,6]]         # s=2: K=2 –≤–∫–ª–∞–¥–∞\n",
    "        #   ],\n",
    "        #   [ # Batch 1: –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ... ]\n",
    "        # ]\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # –®–∞–≥ 3.2: –°—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –æ—Å–∏ K (–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤–∫–ª–∞–¥–æ–≤ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤)\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # –°—É–º–º–∏—Ä—É–µ–º –ø–æ –æ—Å–∏ K (dim=2): –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –ø–æ–ª—É—á–∞–µ—Ç —Å—É–º–º—É –æ—Ç K —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "        # (B, S, K, H) ‚Üí sum(dim=2) ‚Üí (B, S, H)\n",
    "        combined = expert_outputs.sum(dim=2)  # (B, S, H)\n",
    "\n",
    "        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è:\n",
    "        # –î–æ sum: expert_outputs[b=0, s=0] = [[1,2,3,4], [0.5,1,1.5,2]]  # (K=2, H=4)\n",
    "        # –ü–æ—Å–ª–µ sum: combined[b=0, s=0] = [1.5, 3, 4.5, 6]  # (H=4) - —Å—É–º–º–∞ –≤–∫–ª–∞–¥–æ–≤!\n",
    "        #\n",
    "        # –≠—Ç–æ –∏ –µ—Å—Ç—å \"–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ\" –≤—ã—Ö–æ–¥–æ–≤ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤.\n",
    "        # –ö–∞–∂–¥—ã–π —ç–∫—Å–ø–µ—Ä—Ç –≤–Ω—ë—Å —Å–≤–æ–π –≤–∫–ª–∞–¥ (—É–º–Ω–æ–∂–µ–Ω–Ω—ã–π –Ω–∞ routing_weight),\n",
    "        # –∞ –º—ã –∏—Ö —Å—É–º–º–∏—Ä—É–µ–º –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞.\n",
    "\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        # –®–∞–≥ 4: Residual Connection\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –≤—Ö–æ–¥ –∫ –≤—ã—Ö–æ–¥—É MoE —Å–ª–æ—è\n",
    "        # –≠—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –≤ Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö –¥–ª—è:\n",
    "        # 1. –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è (–≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ç–µ–∫—É—Ç –Ω–∞–ø—Ä—è–º—É—é)\n",
    "        # 2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (—Å–ª–æ–π –º–æ–∂–µ—Ç –Ω–∞—É—á–∏—Ç—å—Å—è \"–Ω–µ –¥–µ–ª–∞—Ç—å –Ω–∏—á–µ–≥–æ\")\n",
    "        # 3. –£–ª—É—á—à–µ–Ω–∏—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ (–∫–∞–∂–¥—ã–π —Å–ª–æ–π —É—á–∏—Ç —Ç–æ–ª—å–∫–æ –¥–µ–ª—å—Ç—É)\n",
    "        output = combined + hidden_states  # (B, S, H) + (B, S, H) ‚Üí (B, S, H)\n",
    "\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        # –®–∞–≥ 5: Return - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ loss\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        # output: —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ—Å–ª–µ MoE —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
    "        # load_balance_loss: –º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (—Å—Ç–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤)\n",
    "        return output, load_balance_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß± MoE Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66881cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# –õ–æ–∫–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "sys.path.insert(0, parent_dir)\n",
    "from normalization.rmsnorm import RMSNorm\n",
    "from attention.gqa import GroupedQueryAttention\n",
    "from moe.moe_layer import SimpleMoELayer\n",
    "\n",
    "\n",
    "class MoETransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        MoE Transformer Block –¥–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Qwen3. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Pre-Norm –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É\n",
    "        —Å Grouped-Query Attention –∏ SimpleMoELayer –≤–º–µ—Å—Ç–æ –æ–±—ã—á–Ω–æ–≥–æ FFN.\n",
    "\n",
    "        –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –±–ª–æ–∫–∞:\n",
    "        Input ‚Üí RMSNorm ‚Üí GQA ‚Üí Residual ‚Üí RMSNorm ‚Üí SimpleMoELayer ‚Üí Residual ‚Üí Output\n",
    "                                                           ‚Üì\n",
    "                                                     balance_loss\n",
    "\n",
    "        –û—Ç–ª–∏—á–∏–µ –æ—Ç –æ–±—ã—á–Ω–æ–≥–æ TransformerBlock:\n",
    "        - SwiGLU FFN –∑–∞–º–µ–Ω—ë–Ω –Ω–∞ SimpleMoELayer\n",
    "        - Forward –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (output, balance_loss) –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ output\n",
    "        - balance_loss –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –∫–æ–ª–ª–∞–ø—Å–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤)\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        hidden_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è (d_model)\n",
    "        num_query_groups: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä—É–ø–ø –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è GQA\n",
    "        num_attention_heads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è key/value\n",
    "        num_experts: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ MoE Layer (default: 8)\n",
    "        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ per token (default: 2)\n",
    "        intermediate_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è –≤ —ç–∫—Å–ø–µ—Ä—Ç–∞—Ö (default: 4 * hidden_size)\n",
    "        expert_dropout: Dropout –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (default: 0.0)\n",
    "        balance_loss_coef: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è load balancing loss (default: 0.01)\n",
    "\n",
    "    Returns (from forward):\n",
    "    ---------------\n",
    "        –ï—Å–ª–∏ use_cache –∏–ª–∏ output_attentions:\n",
    "            (hidden_states, balance_loss, present_key_value, attn_weights)\n",
    "        –ò–Ω–∞—á–µ:\n",
    "            (hidden_states, balance_loss)\n",
    "\n",
    "    Example:\n",
    "    ---------------\n",
    "        >>> # –°–æ–∑–¥–∞–Ω–∏–µ MoE Transformer Block\n",
    "        >>> block = MoETransformerBlock(\n",
    "        ...     hidden_size=512,\n",
    "        ...     num_query_groups=8,\n",
    "        ...     num_attention_heads=16,\n",
    "        ...     num_experts=8,\n",
    "        ...     top_k=2\n",
    "        ... )\n",
    "        >>> x = torch.randn(2, 10, 512)  # (batch, seq, hidden)\n",
    "        >>> output, balance_loss = block(x)\n",
    "        >>> output.shape  # torch.Size([2, 10, 512])\n",
    "        >>> balance_loss.item()  # –°–∫–∞–ª—è—Ä loss\n",
    "\n",
    "    Note:\n",
    "    ---------------\n",
    "        balance_loss –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω –∫ –æ–±—â–µ–º—É loss –º–æ–¥–µ–ª–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:\n",
    "        total_loss = language_model_loss + balance_loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_query_groups: int,\n",
    "        num_attention_heads: int,\n",
    "        num_experts: int = 8,\n",
    "        top_k: int = 2,\n",
    "        intermediate_size: Optional[int] = None,\n",
    "        expert_dropout: float = 0.0,\n",
    "        balance_loss_coef: float = 0.01\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "        #       –ü–æ–¥—Å–∫–∞–∑–∫–∞: –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ TransformerBlock (—Å—Ç—Ä–æ–∫–∏ 53-80)\n",
    "        #       –î–æ–±–∞–≤—å—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫—É –¥–ª—è MoE: top_k <= num_experts\n",
    "\n",
    "        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ intermediate_size –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω\n",
    "        #       –ü–æ–¥—Å–∫–∞–∑–∫–∞: –∫–∞–∫–æ–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ hidden_size?\n",
    "\n",
    "        # TODO: –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç—ã –∫–ª–∞—Å—Å–∞\n",
    "        #       –ü–æ–¥—Å–∫–∞–∑–∫–∞: self.hidden_size = ..., self.num_experts = ..., –∏ —Ç.–¥.\n",
    "\n",
    "        # TODO: –°–æ–∑–¥–∞–π—Ç–µ 4 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ (—Å–º. TransformerBlock —Å—Ç—Ä–æ–∫–∏ 91-102):\n",
    "        #       - self.attention_norm (–∫–∞–∫–æ–π —Ç–∏–ø –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏?)\n",
    "        #       - self.attention (–∫–∞–∫–æ–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è?)\n",
    "        #       - self.ffn_norm (—Å–Ω–æ–≤–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)\n",
    "        #       - self.moe_layer (–≤–º–µ—Å—Ç–æ self.feed_forward!)\n",
    "        #\n",
    "        #       –í–æ–ø—Ä–æ—Å: –∫–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω—É–∂–Ω—ã SimpleMoELayer?\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –º—ã –∑–∞–º–µ–Ω—è–µ–º FFN –Ω–∞ MoE Layer?\n",
    "        # - –ö–∞–∫ balance_loss –≤–ª–∏—è–µ—Ç –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏?\n",
    "        # - –ß—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥—ë—Ç, –µ—Å–ª–∏ –Ω–µ –¥–æ–±–∞–≤–ª—è—Ç—å balance_loss –∫ –æ–±—â–µ–º—É loss?\n",
    "        # pass\n",
    "\n",
    "        # --- –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ -------------------------------------------------\n",
    "        assert (\n",
    "            isinstance(hidden_size, int) and hidden_size > 0\n",
    "        ), \"hidden_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\"\n",
    "        assert (\n",
    "            isinstance(num_query_groups, int) and num_query_groups > 0\n",
    "        ), \"num_query_groups –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\"\n",
    "        assert (\n",
    "            isinstance(num_attention_heads, int) and num_attention_heads > 0\n",
    "        ), \"num_attention_heads –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\"\n",
    "\n",
    "        # MoE —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "        assert (\n",
    "            isinstance(num_experts, int) and num_experts > 0\n",
    "        ), \"num_experts –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\"\n",
    "        assert (\n",
    "            top_k > 0 and top_k <= num_experts\n",
    "        ), \"top_k –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 0 –∏ <= num_experts\"\n",
    "\n",
    "        # –ö–ª—é—á–µ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è GQA –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:\n",
    "        assert (\n",
    "            num_attention_heads % num_query_groups == 0\n",
    "        ), (\n",
    "            \"num_attention_heads –¥–æ–ª–∂–µ–Ω –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ num_query_groups –¥–ª—è \"\n",
    "            \"–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã GQA\"\n",
    "        )\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–µ–ª–∏–º–æ—Å—Ç–∏ hidden_size –Ω–∞ —á–∏—Å–ª–æ –≥–æ–ª–æ–≤\n",
    "        # –¢–µ–Ω–∑–æ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–æ–ª–∂–µ–Ω —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ —á–∏—Å–ª–æ –≥–æ–ª–æ–≤\n",
    "        assert (\n",
    "            hidden_size % num_attention_heads == 0\n",
    "        ), \"hidden_size –¥–æ–ª–∂–µ–Ω –¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ num_attention_heads\"\n",
    "\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ intermediate_size –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω\n",
    "        if intermediate_size is not None:\n",
    "            assert (\n",
    "                isinstance(intermediate_size, int) and intermediate_size > 0\n",
    "            ), \"intermediate_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º\"\n",
    "\n",
    "        # --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤ ----------------------------------------------\n",
    "        self.hidden_size         = hidden_size\n",
    "        self.num_query_groups    = num_query_groups\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_experts         = num_experts\n",
    "        self.top_k               = top_k\n",
    "        self.intermediate_size = (\n",
    "            intermediate_size if intermediate_size is not None else 4 * hidden_size\n",
    "        )\n",
    "        self.expert_dropout    = expert_dropout\n",
    "        self.balance_loss_coef = balance_loss_coef\n",
    "\n",
    "        # --- –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –ø–æ–¥–±–ª–æ–∫–æ–≤ ----------------------------------\n",
    "        self.attention_norm = RMSNorm(hidden_size)\n",
    "        self.attention = GroupedQueryAttention(\n",
    "            hidden_size=hidden_size,\n",
    "            num_query_groups=num_query_groups,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "        )\n",
    "        self.ffn_norm  = RMSNorm(hidden_size)\n",
    "        self.moe_layer = SimpleMoELayer(\n",
    "            hidden_size = hidden_size,\n",
    "            num_experts = num_experts,\n",
    "            top_k = top_k,\n",
    "            intermediate_size = self.intermediate_size,\n",
    "            expert_dropout = expert_dropout,\n",
    "            balance_loss_coef = balance_loss_coef\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        training: bool = True\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor, Optional[Tuple], Optional[torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä–∏–º–µ–Ω—è–µ—Ç MoE Transformer –±–ª–æ–∫ –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ç–µ–Ω–∑–æ—Ä—É.\n",
    "\n",
    "            Pipeline:\n",
    "            1. RMSNorm ‚Üí GQA ‚Üí Residual\n",
    "            2. RMSNorm ‚Üí SimpleMoELayer ‚Üí Residual\n",
    "            3. Return (output, balance_loss)\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            hidden_states: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: –ú–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è (optional)\n",
    "            position_ids: –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è RoPE (optional)\n",
    "            past_key_value: –ö—ç—à key/value –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (optional)\n",
    "            output_attentions: –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ attention weights (default: False)\n",
    "            use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ KV cache (default: False)\n",
    "            training: –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è –¥–ª—è balance_loss (default: True)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            –ï—Å–ª–∏ use_cache –∏–ª–∏ output_attentions:\n",
    "                (hidden_states, balance_loss, present_key_value, attn_weights)\n",
    "            –ò–Ω–∞—á–µ:\n",
    "                (hidden_states, balance_loss)\n",
    "        \"\"\"\n",
    "        # TODO: –ü–ï–†–í–´–ô RESIDUAL BLOCK - Self-Attention (GQA)\n",
    "        #       –ü–æ–¥—Å–∫–∞–∑–∫–∞: —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ –∏–∑ TransformerBlock (—Å—Ç—Ä–æ–∫–∏ 143-171)\n",
    "        #       –°—Ç—Ä—É–∫—Ç—É—Ä–∞: residual ‚Üí norm ‚Üí attention ‚Üí residual_add\n",
    "        #       –í–Ω–∏–º–∞–Ω–∏–µ: attention –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å tuple!\n",
    "\n",
    "        # TODO: –í–¢–û–†–û–ô RESIDUAL BLOCK - MoE Feed-Forward\n",
    "        #       –ü–æ–¥—Å–∫–∞–∑–∫–∞: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞–∫ –≤ TransformerBlock (—Å—Ç—Ä–æ–∫–∏ 177-187)\n",
    "        #       –ù–û: self.feed_forward ‚Üí self.moe_layer\n",
    "        #       –í–ê–ñ–ù–û: moe_layer –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (output, balance_loss) - tuple!\n",
    "        #       –ù–µ –∑–∞–±—É–¥—å—Ç–µ –ø–µ—Ä–µ–¥–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä training\n",
    "\n",
    "        # TODO: RETURN\n",
    "        #       –í–æ–ø—Ä–æ—Å: —Å–∫–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏–π –Ω—É–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å?\n",
    "        #       - –í—Å–µ–≥–¥–∞: (hidden_states, balance_loss)\n",
    "        #       - –ï—Å–ª–∏ use_cache/output_attentions: –¥–æ–±–∞–≤—å—Ç–µ present_kv –∏ attn_weights\n",
    "        #       –ü–æ–¥—Å–∫–∞–∑–∫–∞: –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ TransformerBlock —Å—Ç—Ä–æ–∫–∏ 189-192\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É balance_loss –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤–º–µ—Å—Ç–µ —Å hidden_states?\n",
    "        # - –ö–∞–∫ –±—É–¥–µ—Ç —Å–æ–±–∏—Ä–∞—Ç—å—Å—è balance_loss –æ—Ç –≤—Å–µ—Ö —Å–ª–æ—ë–≤ –º–æ–¥–µ–ª–∏?\n",
    "        # - –ß–µ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è forward –æ—Ç –æ–±—ã—á–Ω–æ–≥–æ TransformerBlock?\n",
    "        # pass\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # –ü–ï–†–í–´–ô RESIDUAL BLOCK: Self-Attention (GQA)\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ö–æ–¥ –¥–ª—è –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —Å–≤—è–∑–∏, —á—Ç–æ –±—ã –ø–æ—Ç–æ–º –ø—Ä–∏–±–∞–≤–∏—Ç—å –∫ –≤—ã—Ö–æ–¥—É –±–ª–æ–∫–∞\n",
    "        # p.s. –ü–æ–º–æ–≥–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Ö–æ–¥–Ω–æ–º —Ç–µ–Ω–∑–æ—Ä–µ (–≤–µ–∫—Ç–æ—Ä–∞—Ö), —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å –µ—ë.\n",
    "        residual_1 = hidden_states\n",
    "\n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä\n",
    "        normed = self.attention_norm(hidden_states)\n",
    "\n",
    "        # –í—ã–∑—ã–≤–∞–µ–º –º–æ–¥—É–ª—å –≥—Ä—É–ø–ø–æ–≤–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è. –û–Ω –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å:\n",
    "        # - —Ç–æ–ª—å–∫–æ –≤—ã—Ö–æ–¥ (Tensor), –ª–∏–±–æ\n",
    "        # - –∫–æ—Ä—Ç–µ–∂ (att_output, present_key_value, attn_weights).\n",
    "        att_output = self.attention(\n",
    "            hidden_states=normed,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "\n",
    "        if isinstance(att_output, tuple):\n",
    "            att_output, present_key_value, attn_weights = att_output\n",
    "        else:\n",
    "            present_key_value = None\n",
    "            attn_weights = None\n",
    "\n",
    "        # –ü–µ—Ä–≤–∞—è residual-—Å–≤—è–∑—å: —Å–∫–ª–∞–¥—ã–≤–∞–µ–º –≤—Ö–æ–¥ –∏ –≤—ã—Ö–æ–¥ –ø–æ–¥–±–ª–æ–∫–∞.\n",
    "        hidden_states = att_output + residual_1\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # –í–¢–û–†–û–ô RESIDUAL BLOCK: MoE Feed-Forward\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "        residual_2 = hidden_states\n",
    "\n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–¥ MoE –ø–æ —Ç–µ–º –∂–µ –ø—Ä–∏—á–∏–Ω–∞–º, —á—Ç–æ –∏ –ø–µ—Ä–µ–¥ –≤–Ω–∏–º–∞–Ω–∏–µ–º.\n",
    "        normed = self.ffn_norm(hidden_states)\n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º MoE —Å–ª–æ–π –≤–º–µ—Å—Ç–æ –æ–±—ã—á–Ω–æ–≥–æ FFN.\n",
    "        ffn_output, balance_loss = self.moe_layer(hidden_states=normed, training=training)\n",
    "        # –í—Ç–æ—Ä–∞—è residual-—Å–≤—è–∑—å.\n",
    "        hidden_states = ffn_output + residual_2\n",
    "\n",
    "        if use_cache or output_attentions:\n",
    "            return hidden_states, balance_loss, present_key_value, attn_weights\n",
    "\n",
    "        return hidden_states, balance_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å Qwen3MoEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fa2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qwen3 MoE Language Model\n",
    "\n",
    "–ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å MoE –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Tuple\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from .config import Qwen3Config\n",
    "from ..normalization.rmsnorm import RMSNorm\n",
    "from ..transformer.moe_transformer_block import MoETransformerBlock\n",
    "\n",
    "\n",
    "class Qwen3MoEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –ü–æ–ª–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å Qwen3 —Å MoE –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π.\n",
    "\n",
    "    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:\n",
    "    ------------\n",
    "    Input (batch, seq_len) ‚Äî token IDs\n",
    "        ‚Üì\n",
    "    Token Embedding (batch, seq_len, hidden_size)\n",
    "        ‚Üì\n",
    "    N √ó MoE Transformer Blocks\n",
    "        ‚îú‚îÄ RMSNorm\n",
    "        ‚îú‚îÄ Grouped-Query Attention + RoPE\n",
    "        ‚îú‚îÄ RMSNorm\n",
    "        ‚îî‚îÄ SimpleMoELayer (8 —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, 2 –∞–∫—Ç–∏–≤–Ω—ã—Ö)\n",
    "        ‚Üì\n",
    "    Final RMSNorm (batch, seq_len, hidden_size)\n",
    "        ‚Üì\n",
    "    LM Head: Linear(hidden_size ‚Üí vocab_size)\n",
    "        ‚Üì\n",
    "    Output Logits (batch, seq_len, vocab_size)\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        config: Qwen3Config —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –º–æ–¥–µ–ª–∏\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "        embed_tokens: Token embedding layer (vocab_size ‚Üí hidden_size)\n",
    "        layers: nn.ModuleList –∏–∑ N MoE Transformer –±–ª–æ–∫–æ–≤\n",
    "        norm: Final RMSNorm –ø–µ—Ä–µ–¥ LM head\n",
    "        lm_head: Language modeling head (hidden_size ‚Üí vocab_size)\n",
    "        tokenizer: GPT2Tokenizer –¥–ª—è encode/decode —Ç–µ–∫—Å—Ç–∞\n",
    "\n",
    "    Examples:\n",
    "    ---------\n",
    "        >>> config = Qwen3Config()\n",
    "        >>> model = Qwen3MoEModel(config)\n",
    "        >>>\n",
    "        >>> # Forward pass\n",
    "        >>> input_ids = torch.randint(0, config.vocab_size, (2, 10))\n",
    "        >>> logits, balance_loss = model(input_ids)\n",
    "        >>> print(logits.shape)  # (2, 10, 50257)\n",
    "        >>>\n",
    "        >>> # Generation\n",
    "        >>> generated = model.generate(input_ids, max_length=50)\n",
    "        >>> print(generated.shape)  # (2, 50)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Qwen3Config, tokenizer: Optional[GPT2Tokenizer] = None):\n",
    "        super().__init__()\n",
    "        # TODO: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏\n",
    "        # –í–æ–ø—Ä–æ—Å: –ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –±–ª–æ–∫–∏ –Ω—É–∂–Ω—ã –¥–ª—è –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏?\n",
    "        # –ü–æ–¥—Å–∫–∞–∑–∫–∞: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ config –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "        \n",
    "        # TODO: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è tokenizer (–µ—Å–ª–∏ None, –∑–∞–≥—Ä—É–∑–∏—Ç–µ GPT-2)\n",
    "\n",
    "        # TODO: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ token IDs ‚Üí continuous vectors\n",
    "        # –í–æ–ø—Ä–æ—Å: –ö–∞–∫–æ–π PyTorch —Å–ª–æ–π —Å–æ–∑–¥–∞—ë—Ç lookup table —Ä–∞–∑–º–µ—Ä–∞ (vocab_size, hidden_size)?\n",
    "        # –ü–æ–¥—Å–∫–∞–∑–∫–∞: –í SimpleMoELayer –≤—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ nn.ModuleList. –ê –¥–ª—è embeddings?\n",
    "\n",
    "        # TODO: –°—Ç–µ–∫ –∏–∑ N transformer –±–ª–æ–∫–æ–≤\n",
    "        # –í–æ–ø—Ä–æ—Å: –ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∏–∑ config.num_layers –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ MoETransformerBlock(config)?\n",
    "        # –ü–æ–¥—Å–∫–∞–∑–∫–∞: –í—Å–ø–æ–º–Ω–∏—Ç–µ, –∫–∞–∫ –≤ SimpleMoELayer —Å–æ–∑–¥–∞–≤–∞–ª–∏—Å—å —ç–∫—Å–ø–µ—Ä—Ç—ã\n",
    "\n",
    "        # TODO: –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π\n",
    "        # –í–æ–ø—Ä–æ—Å: –ö–∞–∫–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª–∏ –Ω–∞ –ø–µ—Ä–≤—ã—Ö —ç—Ç–∞–ø–∞—Ö?\n",
    "        # –ü–æ–¥—Å–∫–∞–∑–∫–∞: –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –æ–¥–∏–Ω –∞—Ä–≥—É–º–µ–Ω—Ç ‚Äî —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "\n",
    "        # TODO: –ü—Ä–æ–µ–∫—Ü–∏—è hidden_size ‚Üí vocab_size –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        # –í–æ–ø—Ä–æ—Å: –ö–∞–∫–æ–π —Å–ª–æ–π –ø—Ä–æ–µ—Ü–∏—Ä—É–µ—Ç –≤–µ–∫—Ç–æ—Ä—ã –∏–∑ –æ–¥–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤ –¥—Ä—É–≥–æ–µ?\n",
    "        # –ü–æ–¥—Å–∫–∞–∑–∫–∞: –í LM –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç bias=False –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –ø—Ä–æ–µ–∫—Ü–∏–∏\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –≤—Å–µ —á–µ—Ç—ã—Ä–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏ –∫–ª–∞—Å—Å–∞ (self.*)?\n",
    "        # - –ß—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥—ë—Ç, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Python list –≤–º–µ—Å—Ç–æ nn.ModuleList?\n",
    "        # - –ü–æ—á–µ–º—É —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embedding –¥–æ–ª–∂–Ω–∞ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å hidden_size –±–ª–æ–∫–æ–≤?\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è tokenizer –¥–ª—è text ‚Üî token_ids\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º GPT-2 tokenizer (vocab_size=50257), —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å config\n",
    "        if tokenizer is None:\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "            # –í–∞–∂–Ω–æ: –¥–æ–±–∞–≤–ª—è–µ–º pad_token, —Ç.–∫. GPT-2 –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –µ–≥–æ –Ω–µ –∏–º–µ–µ—Ç\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        else:\n",
    "            self.tokenizer = tokenizer\n",
    "\n",
    "        # Token Embedding Layer: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ token IDs ‚Üí continuous vectors\n",
    "        # –°–æ–∑–¥–∞—ë—Ç —Ç–∞–±–ª–∏—Ü—É —Ä–∞–∑–º–µ—Ä–∞ (y = vocab_size, x = hidden_size)\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            num_embeddings = config.vocab_size, \n",
    "            embedding_dim  = config.hidden_size\n",
    "            )\n",
    "\n",
    "        # –°—Ç–µ–∫ –∏–∑ N transformer –±–ª–æ–∫–æ–≤\n",
    "        # –ö–∞–∂–¥—ã–π –±–ª–æ–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç: RMSNorm ‚Üí GQA ‚Üí RMSNorm ‚Üí SimpleMoELayer\n",
    "        self.layers = nn.ModuleList([\n",
    "            MoETransformerBlock(\n",
    "                hidden_size=config.hidden_size,\n",
    "                num_query_groups=config.num_key_value_heads,\n",
    "                num_attention_heads=config.num_attention_heads,\n",
    "                num_experts=config.num_experts,\n",
    "                top_k=config.top_k,\n",
    "                intermediate_size=config.intermediate_size,\n",
    "                expert_dropout = config.dropout,\n",
    "                balance_loss_coef=config.balance_loss_coef\n",
    "            ) for _ in range(config.num_layers)\n",
    "        ])\n",
    "\n",
    "        # –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π\n",
    "        self.norm = RMSNorm(normalized_shape = self.config.hidden_size)\n",
    "\n",
    "        # –ü—Ä–æ–µ–∫—Ü–∏—è hidden_size ‚Üí vocab_size –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        # y = x A·µÄ (–±–µ–∑ bias –¥–ª—è LM head)\n",
    "        self.lm_head = nn.Linear(\n",
    "            in_features  = self.config.hidden_size,\n",
    "            out_features = self.config.vocab_size,\n",
    "            bias         = False\n",
    "        )\n",
    "\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "        –°—Ç—Ä–∞—Ç–µ–≥–∏—è:\n",
    "        ----------\n",
    "        - Embeddings: normal distribution N(0, initializer_range)\n",
    "        - Linear layers: —É–∂–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –≤ sub-–º–æ–¥—É–ª—è—Ö\n",
    "        - LM Head: normal distribution N(0, initializer_range)\n",
    "        \"\"\"\n",
    "        # TODO: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –≤–µ—Å–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ LM head\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è?\n",
    "        # - –ö–∞–∫ –≤–ª–∏—è–µ—Ç stddev –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ?\n",
    "        # - –ü–æ—á–µ–º—É –ª–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏ –Ω–µ —Ç—Ä–µ–±—É—é—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏?\n",
    "\n",
    "        # Embedding initialization\n",
    "        nn.init.normal_(self.embed_tokens.weight, mean=0.0, std=self.config.initializer_range)\n",
    "\n",
    "        # LM Head initialization\n",
    "        nn.init.normal_(self.lm_head.weight, mean=0.0, std=self.config.initializer_range)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Forward pass –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "        Pipeline:\n",
    "        ---------\n",
    "        1. Token IDs ‚Üí Embeddings (lookup)\n",
    "        2. Embeddings ‚Üí Transformer Blocks (N —Ä–∞–∑)\n",
    "        3. Hidden States ‚Üí Final Norm\n",
    "        4. Normalized States ‚Üí LM Head ‚Üí Logits\n",
    "        5. Accumulate balance loss –∏–∑ –≤—Å–µ—Ö MoE –±–ª–æ–∫–æ–≤\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "            input_ids: –¢–µ–Ω–∑–æ—Ä token IDs —Ñ–æ—Ä–º—ã (batch_size, seq_len)\n",
    "            attention_mask: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è (batch_size, seq_len)\n",
    "                           1 = attend, 0 = ignore. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã –≤–∏–¥–∏–º—ã)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            logits: –¢–µ–Ω–∑–æ—Ä –ª–æ–≥–∏—Ç–æ–≤ —Ñ–æ—Ä–º—ã (batch_size, seq_len, vocab_size)\n",
    "                   –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏\n",
    "            balance_loss: –°–∫–∞–ª—è—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä, —Å—É–º–º–∞ balance losses –∏–∑ –≤—Å–µ—Ö MoE —Å–ª–æ—ë–≤\n",
    "                         –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è load balancing —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "\n",
    "        Shape Flow:\n",
    "        -----------\n",
    "            input_ids: (B, S) ‚Üí embeddings: (B, S, H)\n",
    "            ‚Üí transformer blocks ‚Üí hidden_states: (B, S, H)\n",
    "            ‚Üí norm ‚Üí normalized: (B, S, H)\n",
    "            ‚Üí lm_head ‚Üí logits: (B, S, V)\n",
    "\n",
    "        Examples:\n",
    "        ---------\n",
    "            >>> model = Qwen3MoEModel(config)\n",
    "            >>> input_ids = torch.randint(0, 50257, (4, 32))  # batch=4, seq=32\n",
    "            >>> logits, loss = model(input_ids)\n",
    "            >>> print(f\"Logits: {logits.shape}, Loss: {loss.item():.4f}\")\n",
    "            Logits: torch.Size([4, 32, 50257]), Loss: 0.0234\n",
    "        \"\"\"\n",
    "        # TODO: –ü—Ä–µ–æ–±—Ä–∞–∑—É–π—Ç–µ input_ids ‚Üí embeddings —á–µ—Ä–µ–∑ —ç–º–±–µ–Ω–¥–∏–Ω–≥ —Å–ª–æ–π\n",
    "        # TODO: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ total_balance_loss –Ω—É–ª–µ–≤—ã–º —Ç–µ–Ω–∑–æ—Ä–æ–º –Ω–∞ device embeddings\n",
    "        # TODO: –ü—Ä–æ–π–¥–∏—Ç–µ —Ü–∏–∫–ª–æ–º –ø–æ self.layers, –Ω–∞–∫–∞–ø–ª–∏–≤–∞—è balance_loss\n",
    "        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ —Ñ–∏–Ω–∞–ª—å–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é self.norm\n",
    "        # TODO: –°–ø—Ä–æ–µ—Ü–∏—Ä—É–π—Ç–µ —á–µ—Ä–µ–∑ self.lm_head –≤ vocab space\n",
    "        # TODO: –í–µ—Ä–Ω–∏—Ç–µ (logits, total_balance_loss)\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –≤–∞–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å device –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ total_balance_loss?\n",
    "        # - –ß—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–∞–∂–¥—ã–π MoE –±–ª–æ–∫?\n",
    "        # - –ß–µ–º logits –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π?\n",
    "        # pass\n",
    "\n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ token IDs –≤ embeddings —á–µ—Ä–µ–∑ lookup table\n",
    "        embeddings = self.embed_tokens(input_ids)\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–Ω–∑–æ—Ä–∞ –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è balance loss –∏–∑ –≤—Å–µ—Ö MoE –±–ª–æ–∫–æ–≤\n",
    "        # –í–∞–∂–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–µ–º device –æ—Ç embeddings –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å GPU/CPU\n",
    "        total_balance_loss = torch.tensor(\n",
    "            data=0.0,\n",
    "            device=embeddings.device\n",
    "        )\n",
    "        \n",
    "        # –ü—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –≤—Å–µ transformer –±–ª–æ–∫–∏ —Å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ–º balance loss\n",
    "        # –ö–∞–∂–¥—ã–π layer - —ç—Ç–æ —ç–∫–∑–µ–º–ø–ª—è—Ä MoETransformerBlock, –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        # 1. –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ embeddings (RMSNorm ‚Üí GQA ‚Üí RMSNorm ‚Üí SimpleMoELayer)\n",
    "        # 2. Balance loss –¥–ª—è load balancing —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n",
    "        for layer in self.layers:\n",
    "            embeddings, balance_loss = layer(embeddings, attention_mask)\n",
    "            total_balance_loss += balance_loss\n",
    "\n",
    "        # –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –ø–µ—Ä–µ–¥ LM head\n",
    "        final_norm = self.norm(embeddings)\n",
    "        \n",
    "        # –ü—Ä–æ–µ–∫—Ü–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "        logits = self.lm_head(final_norm)\n",
    "\n",
    "        return logits, total_balance_loss\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        max_length: int = 100,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        do_sample: bool = True,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ê–≤—Ç–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "        –°—Ç—Ä–∞—Ç–µ–≥–∏—è:\n",
    "        ----------\n",
    "        1. –ù–∞—á–∏–Ω–∞–µ–º —Å input_ids (prompt)\n",
    "        2. –í —Ü–∏–∫–ª–µ (–¥–æ max_length):\n",
    "           a. Forward pass: –ø–æ–ª—É—á–∞–µ–º logits –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "           b. –ü—Ä–∏–º–µ–Ω—è–µ–º temperature/top-k/top-p\n",
    "           c. –°—ç–º–ø–ª–∏—Ä—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω\n",
    "           d. –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        3. –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "            input_ids: –ù–∞—á–∞–ª—å–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (prompt) —Ñ–æ—Ä–º—ã (batch, seq_len)\n",
    "            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è (>1 = –±–æ–ª–µ–µ —Å–ª—É—á–∞–π–Ω–æ, <1 = –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ)\n",
    "            top_k: –û—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ k —Å–∞–º—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (nucleus sampling)\n",
    "            top_p: –û—Å—Ç–∞–≤–∏—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ —Å —Å—É–º–º–∞—Ä–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é ‚â• p\n",
    "            do_sample: True = —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ, False = greedy (argmax)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            generated_ids: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º—ã (batch, max_length)\n",
    "\n",
    "        Examples:\n",
    "        ---------\n",
    "            >>> # Greedy decoding\n",
    "            >>> output = model.generate(input_ids, max_length=50, do_sample=False)\n",
    "            >>>\n",
    "            >>> # Nucleus sampling (top-p)\n",
    "            >>> output = model.generate(input_ids, temperature=0.8, top_p=0.9)\n",
    "            >>>\n",
    "            >>> # Top-k sampling\n",
    "            >>> output = model.generate(input_ids, temperature=1.0, top_k=50)\n",
    "        \"\"\"\n",
    "        # TODO: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "        # - –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å input_ids –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è\n",
    "        # - –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å key-value cache (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)\n",
    "        # - –í—ã—á–∏—Å–ª–∏—Ç—å –Ω–∞—á–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        # - –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å attention mask –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        \n",
    "        # TODO: –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –∞–≤—Ç–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "        # - while current_length < max_length:\n",
    "        #   a. Forward pass: –ø–æ–ª—É—á–∏—Ç—å logits –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "        #   b. –ò–∑–≤–ª–µ—á—å logits —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–∑–∏—Ü–∏–∏ (shape: [batch, vocab_size])\n",
    "        #   c. –ü—Ä–∏–º–µ–Ω–∏—Ç—å temperature scaling: logits = logits / temperature\n",
    "        #   d. –ü—Ä–∏–º–µ–Ω–∏—Ç—å top-k —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω top_k)\n",
    "        #   e. –ü—Ä–∏–º–µ–Ω–∏—Ç—å top-p (nucleus) —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω top_p)\n",
    "        #   f. –í—ã—á–∏—Å–ª–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ softmax\n",
    "        #   g. –°—ç–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω (greedy –∏–ª–∏ sampling)\n",
    "        #   h. –î–æ–±–∞–≤–∏—Ç—å —Ç–æ–∫–µ–Ω –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        #   i. –û–±–Ω–æ–≤–∏—Ç—å attention mask –¥–ª—è –Ω–æ–≤–æ–π –¥–ª–∏–Ω—ã\n",
    "        #   j. –û–±–Ω–æ–≤–∏—Ç—å key-value cache (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)\n",
    "        \n",
    "        # TODO: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏\n",
    "        # - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –æ–∫–æ–Ω—á–∞–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
    "        # - –û–±—Ä–µ–∑–∞—Ç—å –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã\n",
    "        # - –í–µ—Ä–Ω—É—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
    "        \n",
    "        # TODO: –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ö–∞–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å attention mask –ø—Ä–∏ —Ä–æ—Å—Ç–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏?\n",
    "        # - –ö–∞–∫–æ–π —Ñ–æ—Ä–º–∞—Ç –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å key-value cache –¥–ª—è MoE –±–ª–æ–∫–æ–≤?\n",
    "        # - –ö–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å batch —Å —Ä–∞–∑–Ω—ã–º–∏ –¥–ª–∏–Ω–∞–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π?\n",
    "        # - –ö–∞–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å memory usage –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π?\n",
    "        \n",
    "        # pass\n",
    "\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "        generated_ids  = input_ids.clone()\n",
    "        current_length = input_ids.shape[1]\n",
    "        # –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä —Ç–∞–∫–æ–π –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∫–∞–∫ input_ids, –Ω–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–π –µ–¥–∏–Ω–∏—Ü–∞–º–∏\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        while current_length < max_length:\n",
    "            # a. Forward pass: –ø–æ–ª—É—á–∏—Ç—å logits –¥–ª—è –≤—Å–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "            logits, _ = self.forward(generated_ids, attention_mask)\n",
    "\n",
    "            # b. –ò–∑–≤–ª–µ—á—å logits —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–∑–∏—Ü–∏–∏ (shape: [batch, vocab_size])\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # c. –ü—Ä–∏–º–µ–Ω–∏—Ç—å temperature scaling\n",
    "            # -------------------------------------------------------------\n",
    "            # –≠—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ –≤ LLM!\n",
    "            # –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞: temperature \"—Å–∂–∏–º–∞–µ—Ç\" –∏–ª–∏ \"—Ä–∞—Å—Ç—è–≥–∏–≤–∞–µ—Ç\" logits\n",
    "\n",
    "            # Temperature > 1: \"—Ä–∞–∑–æ–≥—Ä–µ–≤–∞–µ—Ç\" —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\n",
    "            # - –ë–æ–ª–µ–µ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "            # - –ë–æ–ª—å—à–µ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "\n",
    "            # Temperature < 1: \"–æ—Ö–ª–∞–∂–¥–∞–µ—Ç\" —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\n",
    "            # - –ë–æ–ª–µ–µ –æ—Å—Ç—Ä—ã–µ –ø–∏–∫–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π\n",
    "            # - –ë–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "\n",
    "            # Temperature = 1: –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π softmax)\n",
    "            probabilities = torch.softmax(logits / temperature, dim=-1)\n",
    "            # -------------------------------------------------------------\n",
    "\n",
    "            # d. –ü—Ä–∏–º–µ–Ω–∏—Ç—å top-k —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω)\n",
    "            # e. –ü—Ä–∏–º–µ–Ω–∏—Ç—å top-p —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω)\n",
    "            # -------------------------------------------------------------\n",
    "            # TOP-K –∞–ª–≥–æ—Ä–∏—Ç–º:\n",
    "            # 1. –ù–∞–π—Ç–∏ k —Å–∞–º—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "            # 2. –û–±–Ω—É–ª–∏—Ç—å –í–°–ï –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "            # 3. –û—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ top-k —Ç–æ–∫–µ–Ω–æ–≤\n",
    "\n",
    "            # TOP-P (nucleus) –∞–ª–≥–æ—Ä–∏—Ç–º:\n",
    "            # 1. –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã –ø–æ —É–±—ã–≤–∞–Ω–∏—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "            # 2. –ù–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø–æ—Ä–æ–≥–∞ p\n",
    "            # 3. –û–±–Ω—É–ª–∏—Ç—å –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –ø–æ—Ä–æ–≥–∞\n",
    "\n",
    "            # –ü—Ä–∏–º–µ—Ä:\n",
    "            # probabilities = [0.4, 0.3, 0.2, 0.1]\n",
    "            # top_k=2:   [0.4, 0.3, 0.0, 0.0]  # —Ç–æ–ª—å–∫–æ 2 –ª—É—á—à–∏—Ö\n",
    "            # top_p=0.6: [0.4, 0.3, 0.0, 0.0]  # –Ω–∞–∫–æ–ø–∏–ª–∏ –¥–æ 0.7 > 0.6\n",
    "            if top_k is not None:\n",
    "                top_k_values, top_k_indices = torch.topk(probabilities, top_k)\n",
    "                probabilities_filtered = torch.zeros_like(probabilities)\n",
    "                probabilities_filtered.scatter_(dim=-1, index=top_k_indices, src=top_k_values)\n",
    "                probabilities = probabilities_filtered\n",
    "\n",
    "            if top_p is not None:\n",
    "                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ —É–±—ã–≤–∞–Ω–∏—é\n",
    "                sorted_probs, sorted_indices = torch.sort(probabilities, descending=True, dim=-1)\n",
    "                # –í—ã—á–∏—Å–ª—è–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—É—é —Å—É–º–º—É\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "                # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å (cumsum > top_p)\n",
    "                # –°–¥–≤–∏–≥–∞–µ–º –Ω–∞ 1 –≤–ø—Ä–∞–≤–æ, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ö–æ—Ç—è –±—ã –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = False\n",
    "                # –û–±–Ω—É–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —É–¥–∞–ª—è–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "                sorted_probs[sorted_indices_to_remove] = 0.0\n",
    "                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫\n",
    "                probabilities = torch.zeros_like(probabilities)\n",
    "                probabilities.scatter_(dim=-1, index=sorted_indices, src=sorted_probs)\n",
    "            # -------------------------------------------------------------\n",
    "\n",
    "            # –†–µ-–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏\n",
    "            probabilities = probabilities / probabilities.sum(dim=-1, keepdim=True)\n",
    "\n",
    "            # g. –°—ç–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω (greedy –∏–ª–∏ sampling)\n",
    "            if do_sample:\n",
    "                # –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n",
    "                next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            else:\n",
    "                # Greedy decoding: –≤—ã–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é\n",
    "                next_token = torch.argmax(probabilities, dim=-1, keepdim=True)\n",
    "\n",
    "            # h. –î–æ–±–∞–≤–∏—Ç—å —Ç–æ–∫–µ–Ω –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "            generated_ids = torch.cat([generated_ids, next_token], dim=-1)\n",
    "\n",
    "            # i. –û–±–Ω–æ–≤–∏—Ç—å attention mask –¥–ª—è –Ω–æ–≤–æ–π –¥–ª–∏–Ω—ã\n",
    "            attention_mask = torch.cat(\n",
    "                [attention_mask, torch.ones_like(next_token)],\n",
    "                dim=-1\n",
    "            )\n",
    "\n",
    "            # j. –û–±–Ω–æ–≤–∏—Ç—å –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "            current_length += 1\n",
    "\n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –≤–æ–∑–≤—Ä–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "        return generated_ids\n",
    "\n",
    "\n",
    "    def chat(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_length: int = 100,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        do_sample: bool = True,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å text-to-text.\n",
    "\n",
    "        Pipeline:\n",
    "        ---------\n",
    "        1. Encode: prompt (str) ‚Üí token_ids (tensor)\n",
    "        2. Generate: token_ids ‚Üí generated_ids (–∞–≤—Ç–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è)\n",
    "        3. Decode: generated_ids ‚Üí response (str)\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "            prompt: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–≤ —Ç–æ–∫–µ–Ω–∞—Ö)\n",
    "            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.0)\n",
    "            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è top-k —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "            top_p: –ü–æ—Ä–æ–≥ –¥–ª—è nucleus sampling (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "            do_sample: True = —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ, False = greedy decoding\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            response: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–≤–∫–ª—é—á–∞—è –∏—Å—Ö–æ–¥–Ω—ã–π prompt)\n",
    "\n",
    "        Examples:\n",
    "        ---------\n",
    "            >>> # Greedy decoding (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)\n",
    "            >>> response = model.chat(\"Once upon a time\", do_sample=False)\n",
    "            >>> print(response)\n",
    "            \"Once upon a time there was a kingdom...\"\n",
    "\n",
    "            >>> # Nucleus sampling (–±–æ–ª–µ–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π)\n",
    "            >>> response = model.chat(\"Hello world\", temperature=0.8, top_p=0.9)\n",
    "            >>> print(response)\n",
    "            \"Hello world! How are you doing today?\"\n",
    "\n",
    "            >>> # Top-k sampling\n",
    "            >>> response = model.chat(\"The quick brown\", temperature=1.0, top_k=40)\n",
    "            >>> print(response)\n",
    "            \"The quick brown fox jumps over the lazy dog\"\n",
    "        \"\"\"\n",
    "        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ç—Ä–∏ —à–∞–≥–∞: Encode ‚Üí Generate ‚Üí Decode\n",
    "        # TODO: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ self.tokenizer –¥–ª—è encode/decode\n",
    "        # TODO: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ self.generate() –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "        # TODO: –í–µ—Ä–Ω–∏—Ç–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "\n",
    "        # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è:\n",
    "        # - –ü–æ—á–µ–º—É –≤–∞–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ—Ç –∂–µ tokenizer, —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏?\n",
    "        # - –ö–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (BOS/EOS/PAD) –ø—Ä–∏ encode/decode?\n",
    "        # - –ö–∞–∫ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å, —á—Ç–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç max_length?\n",
    "\n",
    "\n",
    "        # –®–∞–≥ 1: Encode - –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ token IDs\n",
    "        # return_tensors=\"pt\" –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç PyTorch —Ç–µ–Ω–∑–æ—Ä—ã\n",
    "        # add_special_tokens=True –¥–æ–±–∞–≤–ª—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (BOS/EOS –µ—Å–ª–∏ –µ—Å—Ç—å)\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –º–æ–¥–µ–ª—å\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º device —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, embed_tokens.weight)\n",
    "        device = next(self.parameters()).device\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        # –®–∞–≥ 2: Generate - –∞–≤—Ç–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ self.generate()\n",
    "        generated_ids = self.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample\n",
    "        )\n",
    "\n",
    "        # –®–∞–≥ 3: Decode - –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ token IDs –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç\n",
    "        # skip_special_tokens=True —É–¥–∞–ª—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (BOS/EOS/PAD)\n",
    "        # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—É—é (–∏ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—É—é) –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ batch: generated_ids[0]\n",
    "        response = self.tokenizer.decode(\n",
    "            generated_ids[0],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        return response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
