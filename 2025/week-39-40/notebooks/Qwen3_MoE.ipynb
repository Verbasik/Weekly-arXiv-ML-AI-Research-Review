{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤖 Qwen3 MoE — Учебный Walkthrough\n",
    "\n",
    "Добро пожаловать! Этот ноутбук проведёт вас через сборку Qwen3 MoE: от фундаментальных блоков до полной модели и мини‑демо генерации.\n",
    "\n",
    "**Что вы сделаете:**\n",
    "- 🧩 Соберёте модули: RMSNorm → RoPE → SwiGLU → GQA → Transformer → MoE\n",
    "- 🧪 После каждой секции — быстрый чек форм и численной стабильности\n",
    "- 🚀 Финал: `Qwen3MoEModel` + короткая генерация (greedy/sampling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Окружение и Reproducibility\n",
    "Подготовьте импорты, выберите `device`, зафиксируйте сиды и проверьте доступность CUDA.\n",
    "\n",
    "**🧪 Quick Check (кодовая ячейка):**\n",
    "\n",
    "```python\n",
    "import torch, random, numpy as np\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "set_seed(42); print('CUDA:', torch.cuda.is_available())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧭 Конфигурация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b39b623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_position_embeddings = 2048\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Qwen3 MoE Model Configuration\n",
    "\n",
    "Определяет все гиперпараметры модели в одном месте.\n",
    "\"\"\"\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Qwen3Config:\n",
    "    \"\"\"\n",
    "    Конфигурация для Qwen3 MoE модели.\n",
    "\n",
    "    Архитектура:\n",
    "    ------------\n",
    "    - Vocabulary: GPT-2 tokenizer (50257 токенов)\n",
    "    - Embedding: 1024-dim continuous vectors\n",
    "    - Transformer: 12 MoE blocks с GQA + RoPE + SwiGLU\n",
    "    - MoE: 8 экспертов, 2 активных per token (25% активация)\n",
    "    - Output: Language modeling head (1024 → 50257)\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    Model Architecture:\n",
    "        vocab_size: Размер словаря (GPT-2 = 50257)\n",
    "        hidden_size: Размерность скрытого слоя (embedding dimension)\n",
    "        num_layers: Количество MoE Transformer блоков\n",
    "        intermediate_size: Размерность FFN внутри каждого эксперта\n",
    "\n",
    "    Attention:\n",
    "        num_attention_heads: Количество Query голов (для GQA)\n",
    "        num_key_value_heads: Количество Key/Value голов (GQA группировка)\n",
    "        max_position_embeddings: Максимальная длина последовательности\n",
    "        rope_theta: Базовая частота для RoPE (10000.0 стандарт)\n",
    "\n",
    "    MoE Specific:\n",
    "        num_experts: Общее количество экспертов в каждом MoE слое\n",
    "        top_k: Количество активных экспертов per token\n",
    "        balance_loss_coef: Коэффициент для load balancing loss (обычно 0.01)\n",
    "\n",
    "    Regularization:\n",
    "        dropout: Dropout rate для регуляризации (0.0 = отключен)\n",
    "\n",
    "    Training:\n",
    "        initializer_range: Стандартное отклонение для инициализации весов\n",
    "\n",
    "    Примеры:\n",
    "    --------\n",
    "    >>> # Конфигурация по умолчанию (0.6B параметров)\n",
    "    >>> config = Qwen3Config()\n",
    "    >>> print(f\"Model size: ~{config.vocab_size * config.hidden_size / 1e9:.2f}B parameters\")\n",
    "\n",
    "    >>> # Кастомная конфигурация\n",
    "    >>> config = Qwen3Config(\n",
    "    ...     hidden_size=2048,\n",
    "    ...     num_layers=24,\n",
    "    ...     num_experts=16\n",
    "    ... )\n",
    "    \"\"\"\n",
    "\n",
    "    # Model Architecture\n",
    "    vocab_size: int = 50257  # GPT-2 tokenizer\n",
    "    hidden_size: int = 1024\n",
    "    num_layers: int = 12\n",
    "    intermediate_size: int = 2048  # 2 * hidden_size для каждого эксперта\n",
    "\n",
    "    # Attention Configuration\n",
    "    num_attention_heads: int = 16  # Query heads\n",
    "    num_key_value_heads: int = 4   # KV heads (GQA: 4x группировка)\n",
    "    max_position_embeddings: int = 2048\n",
    "    rope_theta: float = 10000.0\n",
    "\n",
    "    # MoE Configuration\n",
    "    num_experts: int = 8\n",
    "    top_k: int = 2\n",
    "    balance_loss_coef: float = 0.01\n",
    "\n",
    "    # Regularization\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # Training\n",
    "    initializer_range: float = 0.02\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Валидация конфигурации после инициализации.\"\"\"\n",
    "        # Базовые проверки\n",
    "        assert self.vocab_size > 0, \"vocab_size должен быть положительным\"\n",
    "        assert self.hidden_size > 0, \"hidden_size должен быть положительным\"\n",
    "        assert self.num_layers > 0, \"num_layers должен быть положительным\"\n",
    "        assert self.intermediate_size > 0, \"intermediate_size должен быть положительным\"\n",
    "\n",
    "        # Attention проверки\n",
    "        assert self.num_attention_heads > 0, \"num_attention_heads должен быть положительным\"\n",
    "        assert self.num_key_value_heads > 0, \"num_key_value_heads должен быть положительным\"\n",
    "        assert (\n",
    "            self.num_attention_heads % self.num_key_value_heads == 0\n",
    "        ), \"num_attention_heads должен делиться на num_key_value_heads\"\n",
    "        assert (\n",
    "            self.hidden_size % self.num_attention_heads == 0\n",
    "        ), \"hidden_size должен делиться на num_attention_heads\"\n",
    "\n",
    "        # MoE проверки\n",
    "        assert self.num_experts > 0, \"num_experts должен быть положительным\"\n",
    "        assert self.top_k > 0, \"top_k должен быть положительным\"\n",
    "        assert self.top_k <= self.num_experts, \"top_k не может быть больше num_experts\"\n",
    "\n",
    "        # Regularization проверки\n",
    "        assert 0.0 <= self.dropout <= 1.0, \"dropout должен быть в диапазоне [0, 1]\"\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Конвертация конфигурации в словарь.\"\"\"\n",
    "        return {\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"hidden_size\": self.hidden_size,\n",
    "            \"num_layers\": self.num_layers,\n",
    "            \"intermediate_size\": self.intermediate_size,\n",
    "            \"num_attention_heads\": self.num_attention_heads,\n",
    "            \"num_key_value_heads\": self.num_key_value_heads,\n",
    "            \"max_position_embeddings\": self.max_position_embeddings,\n",
    "            \"rope_theta\": self.rope_theta,\n",
    "            \"num_experts\": self.num_experts,\n",
    "            \"top_k\": self.top_k,\n",
    "            \"balance_loss_coef\": self.balance_loss_coef,\n",
    "            \"dropout\": self.dropout,\n",
    "            \"initializer_range\": self.initializer_range,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧮 Нормализация: RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84214c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартная библиотека\n",
    "from typing import Optional\n",
    "\n",
    "# Сторонние библиотеки\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Root Mean Square Layer Normalization — современная альтернатива LayerNorm.\n",
    "        Формула: RMSNorm(x) = x / sqrt(mean(x²) + eps) * weight\n",
    "\n",
    "        В отличие от LayerNorm, RMSNorm не центрирует данные (не вычитает среднее),\n",
    "        что обеспечивает лучшую численную стабильность и производительность\n",
    "        при обучении больших языковых моделей.\n",
    "\n",
    "        Ключевые преимущества:\n",
    "        - Меньше вычислений (нет центрирования)\n",
    "        - Лучшая стабильность при больших моделях\n",
    "        - Используется в современных архитектурах (LLaMA, Qwen, и др.)\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        normalized_shape: int или tuple размерностей для нормализации.\n",
    "                          Обычно равен hidden_size модели.\n",
    "        eps: Малая константа для численной устойчивости, предотвращает деление на ноль.\n",
    "                          Добавляется под корень: sqrt(mean(x²) + eps).\n",
    "        elementwise_affine: Если True, добавляет обучаемые параметры weight.\n",
    "                            Если False, применяет только нормализацию без масштабирования.\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> import torch\n",
    "        >>> rms_norm = RMSNorm(512)\n",
    "        >>> x = torch.randn(10, 20, 512)\n",
    "        >>> output = rms_norm(x)\n",
    "        >>> output.shape\n",
    "        torch.Size([10, 20, 512])\n",
    "\n",
    "        >>> # Проверка нормализации: RMS должен быть близок к 1\n",
    "        >>> rms_value = torch.sqrt(torch.mean(output**2, dim=-1))\n",
    "        >>> print(f\"RMS after normalization: {rms_value.mean():.4f}\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        normalized_shape: int,\n",
    "        eps: float = 1e-6,\n",
    "        elementwise_affine: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Сохраните normalized_shape для использования в forward\n",
    "        # TODO: Сохраните eps для численной стабильности\n",
    "        # TODO: Если elementwise_affine=True, создайте Parameter weight с формой (normalized_shape)\n",
    "        # TODO: Инициализируйте weight единицами: torch.ones()\n",
    "        # TODO: Если elementwise_affine=False, зарегистрируйте weight как None\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему weight инициализируется единицами, а не нулями?\n",
    "        # - Что произойдет, если eps слишком большой или слишком маленький?\n",
    "        # - Зачем нужен параметр elementwise_affine?\n",
    "        # pass\n",
    "\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "\n",
    "        # Создаем обучаемый параметр масштабирования (g в формуле RMSNorm)\n",
    "        if elementwise_affine == True:\n",
    "            # Инициализируем weight единицами для стабильности начального обучения\n",
    "            # weight соответствует вектору g в формуле: g ⊙ (x/RMS(x))\n",
    "            self.weight = nn.Parameter(\n",
    "                torch.ones(normalized_shape)\n",
    "            )\n",
    "        else:\n",
    "            # Если масштабирование не требуется, регистрируем пустой параметр\n",
    "            # для совместимости с механизмами PyTorch (state_dict и др.)\n",
    "            self.register_parameter('weight', None)\n",
    "                    \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Реализует формулу RMSNorm: x / sqrt(mean(x²) + eps) * weight.\n",
    "            Нормализует входной тензор по его среднеквадратичному значению,\n",
    "            без центрирования (в отличие от LayerNorm).\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            x: Входной тензор формы (..., normalized_shape)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            Нормализованный тензор той же формы\n",
    "\n",
    "        Raises:\n",
    "        ---------------\n",
    "            RuntimeError: Если последняя размерность x не совпадает с normalized_shape\n",
    "        \"\"\"\n",
    "        # TODO: Проверьте, что последняя размерность x равна self.normalized_shape\n",
    "        # TODO: Вычислите квадраты элементов: x_squared = x * x или x.pow(2)\n",
    "        # TODO: Вычислите среднее квадратов по последней оси: torch.mean\n",
    "        # TODO: Вычислите RMS: torch.sqrt\n",
    "        # TODO: Нормализуйте по формуле: RMSNorm(x) = x / sqrt(mean(x²) + eps)\n",
    "        # TODO: Если есть weight, примените масштабирование: output = normalized * self.weight\n",
    "        # TODO: Верните результат\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему важно использовать keepdim=True при вычислении среднего?\n",
    "        # - Как поведет себя функция на тензорах разной размерности?\n",
    "        # - Что произойдет с градиентами при обратном проходе?\n",
    "        # - Как RMSNorm влияет на распределение активаций?\n",
    "        # pass\n",
    "\n",
    "        # Проверяем соответствие размерности входного тензора\n",
    "        if x.shape[-1] != self.normalized_shape:\n",
    "            raise RuntimeError(\n",
    "                f\"Последняя размерность x должна быть равна {self.normalized_shape}\"\n",
    "            )\n",
    "\n",
    "        # Вычисляем квадраты элементов\n",
    "        x_sqr = x * x\n",
    "\n",
    "        # Вычисляем среднее квадратов по последней оси\n",
    "        # keepdim=True сохраняет размерность для корректного вещания при делении\n",
    "        mean_sqr = torch.mean(x_sqr, dim=-1, keepdim=True)\n",
    "\n",
    "        # Вычисляем RMS (корень из среднего квадратов) с добавлением eps для стабильности\n",
    "        rms = torch.sqrt(mean_sqr + self.eps)\n",
    "\n",
    "        # Нормализуем входной тензор, деля на RMS\n",
    "        x_norm = x / rms\n",
    "\n",
    "        # Применяем масштабирование, если есть weight\n",
    "        if self.weight is not None:\n",
    "            # Поэлементное умножение на обучаемый параметр weight\n",
    "            return x_norm * self.weight\n",
    "        \n",
    "        return x_norm\n",
    "        \n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        \"\"\"Строковое представление модуля для отладки.\"\"\"\n",
    "        return f'normalized_shape={self.normalized_shape}, eps={self.eps}, elementwise_affine={self.weight is not None}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧭 Позиционное кодирование: RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартная библиотека\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "# Сторонние библиотеки\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Rotary Position Embedding (RoPE) — метод позиционного кодирования,\n",
    "        основанный на вращении векторов в комплексной плоскости.\n",
    "        \n",
    "        В отличие от абсолютных позиционных эмбеддингов, RoPE кодирует\n",
    "        относительные позиции, что делает его особенно эффективным для\n",
    "        моделей с длинным контекстом.\n",
    "        \n",
    "        Ключевые преимущества:\n",
    "        - Инвариантность к сдвигу (shift invariance)\n",
    "        - Эффективность вычислений\n",
    "        - Хорошая экстраполяция на длины, превышающие обучающие\n",
    "        - Совместимость с линейными attention механизмами\n",
    "        \n",
    "    Args:\n",
    "    ---------------\n",
    "        dim: Размерность эмбеддинга (должна быть четной для комплексного представления)\n",
    "        base: База для вычисления частот (обычно 10000.0)\n",
    "        max_position: Максимальная позиция для предварительного вычисления (кэширования)\n",
    "        scale: Масштабирующий коэффициент для частот (используется для RoPE scaling)\n",
    "        \n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> import torch\n",
    "        >>> rope = RoPE(dim=128)\n",
    "        >>> q = torch.randn(2, 4, 128)  # [batch_size, seq_len, dim]\n",
    "        >>> k = torch.randn(2, 4, 128)  # [batch_size, seq_len, dim]\n",
    "        >>> q_pos, k_pos = rope(q, k)\n",
    "        >>> q_pos.shape, k_pos.shape\n",
    "        (torch.Size([2, 4, 128]), torch.Size([2, 4, 128]))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        base: float = 10000.0,\n",
    "        max_position: int = 2048,\n",
    "        scale: float = 1.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Проверьте, что dim четное (необходимо для комплексного представления)\n",
    "        # TODO: Сохраните параметры (dim, base, max_position, scale)\n",
    "        # TODO: Предварительно вычислите sin/cos таблицу частот для позиций [0, max_position)\n",
    "        #       - Создайте тензор позиций;\n",
    "        #       - Создайте тензор частот;\n",
    "        #       - Вычислите углы для каждой позиции и частоты;\n",
    "        #       - Вычислите sin и cos;\n",
    "        #       - Сохраните кэш как буферы (не параметры).\n",
    "        \n",
    "        # Вопросы для размышления:\n",
    "        # - Почему RoPE использует комплексное представление для кодирования позиций?\n",
    "        # - Как выбор base влияет на частотные характеристики позиционного кодирования?\n",
    "        # - Почему для длинных контекстов часто используют scale < 1.0?\n",
    "        # pass\n",
    "\n",
    "        if dim % 2 != 0:\n",
    "            raise ValueError('Для корректной работы параметр dim должен быть четным')\n",
    "        \n",
    "        # Сохраняем основные параметры для использования в других методах\n",
    "        self.dim = dim                    # Размерность эмбеддинга (должна быть четной)\n",
    "        self.base = base                  # База для вычисления частот (обычно 10000.0)\n",
    "        self.max_position = max_position  # Максимальная позиция для кэширования\n",
    "        self.scale = scale                # Масштабирующий коэффициент для длинных контекстов\n",
    "\n",
    "        # Создаем тензор позиций [0, 1, 2, ..., max_position-1]\n",
    "        position = torch.arange(start = 0, end = max_position, step =1).float()\n",
    "\n",
    "        # Создаем тензор с четными индексами [0, 2, 4, ...] для адресации пар измерений\n",
    "        # Каждая пара (2i, 2i+1) будет обрабатываться как комплексное число\n",
    "        idx = torch.arange(start=0, end=dim, step=2).float()\n",
    "\n",
    "        # Вычисляем частоты для каждой пары измерений по формуле: ω_d = base^(-2d/D)\n",
    "        # - Низкие частоты (начало тензора) меняются медленно с изменением позиции\n",
    "        # - Высокие частоты (конец тензора) меняются быстрее\n",
    "        # - scale < 1.0 замедляет вращение для лучшей экстраполяции на длинные контексты\n",
    "        freqs = base ** (-idx / dim)\n",
    "\n",
    "        # Вычисляем углы для каждой комбинации позиции и частоты\n",
    "        # Применяем scale для поддержки длинных контекстов\n",
    "        # Форма: (max_position, dim/2)\n",
    "        angles = position.unsqueeze(1) * freqs.unsqueeze(0) / scale\n",
    "\n",
    "        # Вычисляем sin и cos для каждого угла\n",
    "        cos = torch.cos(angles)  # (max_position, dim/2)\n",
    "        sin = torch.sin(angles)  # (max_position, dim/2)\n",
    "\n",
    "        # Сохраняем sin и cos как буферы (не параметры)\n",
    "        # Используем register_buffer для правильной работы с CUDA и сохранения/загрузки модели\n",
    "        self.register_buffer('sin_cached', sin)\n",
    "        self.register_buffer('cos_cached', cos)\n",
    "\n",
    "        \n",
    "    def _compute_rope_embeddings(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        positions: torch.Tensor,\n",
    "        is_query: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Применяет ротационное позиционное кодирование к входному тензору.\n",
    "        \n",
    "        Args:\n",
    "        ---------------\n",
    "            x: Входной тензор формы (..., seq_len, dim)\n",
    "            positions: Тензор позиций формы (..., seq_len)\n",
    "            is_query: Флаг, указывающий, является ли вход query (True) или key (False)\n",
    "            \n",
    "        Returns:\n",
    "        ---------------\n",
    "            Тензор с примененным позиционным кодированием той же формы, что и x\n",
    "        \"\"\"\n",
    "        # TODO: Получите форму входного тензора x\n",
    "        # TODO: Извлеките sin и cos для заданных позиций из кэша или вычислите их на лету\n",
    "        # TODO: Если positions выходят за пределы max_position, вычислите sin и cos динамически\n",
    "        # TODO: Примените вращение к каждой паре соседних измерений (dim[i], dim[i+1])\n",
    "        #       - Для четных индексов i: x[..., i] = x[..., i] * cos - x[..., i+1] * sin\n",
    "        #       - Для нечетных индексов i: x[..., i] = x[..., i] * sin + x[..., i-1] * cos\n",
    "        # TODO: Если is_query=False (для ключей), инвертируйте направление вращения\n",
    "        # TODO: Верните тензор с примененным позиционным кодированием\n",
    "        \n",
    "        # Вопросы для размышления:\n",
    "        # - Почему для query и key используются разные направления вращения?\n",
    "        # - Как RoPE обеспечивает относительное позиционное кодирование?\n",
    "        # - Как работает экстраполяция на позиции за пределами max_position?\n",
    "        # pass\n",
    "\n",
    "        # Получаем форму входного тензора\n",
    "        x_shape = x.shape\n",
    "\n",
    "        if positions.max() < self.max_position:\n",
    "            sin = self.sin_cached[positions]\n",
    "            cos = self.cos_cached[positions]\n",
    "        else:\n",
    "            # Так же как в __init__\n",
    "            idx = torch.arange(start=0, end=self.dim, step=2).float()\n",
    "            freqs = self.base ** (-idx / self.dim)\n",
    "            angles = positions.unsqueeze(1) * freqs.unsqueeze(0) / self.scale\n",
    "\n",
    "            cos = torch.cos(angles)\n",
    "            sin = torch.sin(angles)\n",
    "\n",
    "        # Создаем выходной тензор той же формы, что и входной\n",
    "        x_out = torch.zeros_like(x)\n",
    "\n",
    "        # Применяем вращение в зависимости от типа входа (query или key)\n",
    "        if is_query:\n",
    "            # Для query - положительное направление вращения\n",
    "            x_out[..., 0::2] = x[..., 0::2] * cos - x[..., 1::2] * sin\n",
    "            x_out[..., 1::2] = x[..., 1::2] * cos + x[..., 0::2] * sin\n",
    "        else:\n",
    "            # Для key - отрицательное направление вращения\n",
    "            x_out[..., 0::2] = x[..., 0::2] * cos + x[..., 1::2] * sin\n",
    "            x_out[..., 1::2] = x[..., 1::2] * cos - x[..., 0::2] * sin\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        positions: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Применяет RoPE к query и key тензорам.\n",
    "        \n",
    "        Args:\n",
    "        ---------------\n",
    "            query: Query тензор формы (..., seq_len, dim)\n",
    "            key: Key тензор формы (..., seq_len, dim)\n",
    "            positions: Опциональный тензор позиций. Если None, используется torch.arange(seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            Кортеж (query_pos, key_pos) с примененным позиционным кодированием\n",
    "        \"\"\"\n",
    "        # TODO: Проверьте, что последнее измерение query и key равно self.dim\n",
    "        # TODO: Получите seq_len из формы query\n",
    "        # TODO: Если positions не указаны, создайте их через torch\n",
    "        # TODO: Примените _compute_rope_embeddings к query\n",
    "        # TODO: Примените _compute_rope_embeddings к key\n",
    "        # TODO: Верните кортеж (query_pos, key_pos)\n",
    "        \n",
    "        # Вопросы для размышления:\n",
    "        # - Как RoPE влияет на взаимодействие между query и key в attention?\n",
    "        # - Почему важно применять RoPE к обоим тензорам - query и key?\n",
    "        # - Как можно оптимизировать вычисления RoPE для больших моделей?\n",
    "        # pass\n",
    "\n",
    "        if query.shape[-1] != self.dim or key.shape[-1] != self.dim:\n",
    "            raise ValueError(f'Размерность query и key должна быть равна {self.dim}')\n",
    "        \n",
    "        # Проверяем, что формы query и key совпадают по seq_len\n",
    "        if query.shape[-2] != key.shape[-2]:\n",
    "            raise ValueError(f'Длины последовательностей query и key должны совпадать')\n",
    "        \n",
    "        seq_len = query.shape[-2]\n",
    "\n",
    "        if positions is not None:\n",
    "            query_rope = self._compute_rope_embeddings(query, positions, is_query=True)\n",
    "            key_rope   = self._compute_rope_embeddings(key,   positions, is_query=False)\n",
    "\n",
    "            return query_rope, key_rope\n",
    "\n",
    "        else:\n",
    "            # Создаем позиции от 0 до seq_len-1 на том же устройстве, что и query\n",
    "            positions = torch.arange(seq_len, device=query.device)\n",
    "            # Применяем RoPE к query и key\n",
    "            query_rope = self._compute_rope_embeddings(query, positions, is_query=True)\n",
    "            key_rope = self._compute_rope_embeddings(key, positions, is_query=False)\n",
    "            \n",
    "            return query_rope, key_rope\n",
    "\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        \"\"\"Строковое представление модуля для отладки.\"\"\"\n",
    "        return f'dim={self.dim}, base={self.base}, max_position={self.max_position}, scale={self.scale}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡ Активации: SwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14881fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартная библиотека\n",
    "from typing import Optional\n",
    "\n",
    "# Сторонние библиотеки\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Swish активация: x * sigmoid(x)\n",
    "        \n",
    "        Предложена в статье \"Searching for Activation Functions\" (Ramachandran et al., 2017).\n",
    "        Также известна как SiLU (Sigmoid Linear Unit) в PyTorch.\n",
    "        \n",
    "        Формула: Swish(x) = x * sigmoid(x)\n",
    "        \n",
    "        Преимущества:\n",
    "        - Гладкая функция (все производные существуют)\n",
    "        - Не ограничена сверху (в отличие от sigmoid)\n",
    "        - Имеет нелинейность, близкую к ReLU для положительных значений\n",
    "        - Имеет небольшое подавление для отрицательных значений\n",
    "        \n",
    "    Args:\n",
    "    ---------------\n",
    "        beta: Опциональный параметр для масштабирования: x * sigmoid(beta * x)\n",
    "              По умолчанию beta=1.0 (стандартный Swish)\n",
    "        \n",
    "    Returns:\n",
    "    ---------------\n",
    "        Тензор той же формы, что и вход, с примененной Swish активацией\n",
    "        \n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> import torch\n",
    "        >>> swish = Swish()\n",
    "        >>> x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "        >>> swish(x)\n",
    "        tensor([-0.2384, -0.2689, 0.0000, 0.7311, 1.7616])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, beta: float = 1.0):\n",
    "        super().__init__()\n",
    "        # TODO: Сохраните beta параметр\n",
    "        # pass\n",
    "\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Применяет Swish активацию к входному тензору.\n",
    "        \n",
    "        Args:\n",
    "        ---------------\n",
    "            x: Входной тензор любой формы\n",
    "            \n",
    "        Returns:\n",
    "        ---------------\n",
    "            Тензор той же формы с примененной Swish активацией\n",
    "        \"\"\"\n",
    "        # TODO: Реализуйте Swish активацию: x * sigmoid(beta * x)\n",
    "        # pass\n",
    "\n",
    "        # Сигмоида масштабирует значения тензора от 0 до 1, \n",
    "        # затем умножаем на исходный тензор, тем самым сглаживая значения\n",
    "        return x * torch.sigmoid(self.beta * x)\n",
    "\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        SwiGLU (Swish-Gated Linear Unit) - активационная функция,\n",
    "        используемая в современных языковых моделях, включая Qwen3.\n",
    "        \n",
    "        Сочетает Swish активацию и механизм гейтинга (GLU).\n",
    "        \n",
    "        Формула:\n",
    "        SwiGLU(x, W1, W2, b1, b2) = Swish(W1*x + b1) ⊙ (W2*x + b2)\n",
    "        \n",
    "        где:\n",
    "        - W1, W2 - весовые матрицы\n",
    "        - b1, b2 - векторы смещения (опциональные)\n",
    "        - ⊙ - поэлементное умножение\n",
    "        \n",
    "        Преимущества:\n",
    "        - Лучшая производительность по сравнению с ReLU/GELU в глубоких моделях\n",
    "        - Эффективный механизм гейтинга для контроля потока информации\n",
    "        - Используется в современных LLM (Qwen, PaLM, LLaMA)\n",
    "        \n",
    "    Args:\n",
    "    ---------------\n",
    "        input_dim: Размерность входного вектора\n",
    "        output_dim: Размерность выходного вектора\n",
    "        intermediate_dim: Размерность промежуточных матриц W1 и W2 (как правило 4*input_dim)\n",
    "        bias: Использовать ли смещение в линейных преобразованиях (по умолчанию True)\n",
    "        \n",
    "    Returns:\n",
    "    ---------------\n",
    "        Тензор формы (..., output_dim) - результат применения SwiGLU\n",
    "        \n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> import torch\n",
    "        >>> swiglu = SwiGLU(512, 512)\n",
    "        >>> x = torch.randn(2, 10, 512)\n",
    "        >>> output = swiglu(x)\n",
    "        >>> output.shape\n",
    "        torch.Size([2, 10, 512])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        intermediate_dim: Optional[int] = None,\n",
    "        bias: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Если intermediate_dim не указан, установите его как 4*output_dim\n",
    "        # TODO: Создайте линейный слой gate_proj для проекции входа в промежуточное представление\n",
    "        # TODO: Создайте линейный слой value_proj для проекции входа в промежуточное представление\n",
    "        # TODO: Создайте экземпляр Swish активации\n",
    "        \n",
    "        # Вопросы для размышления:\n",
    "        # - Почему используется коэффициент 4 для промежуточной размерности?\n",
    "        # - Какое преимущество дает механизм гейтинга по сравнению с простой активацией?\n",
    "        # - Почему SwiGLU лучше работает в глубоких моделях по сравнению с ReLU/GELU?\n",
    "        # pass\n",
    "\n",
    "        self.input_dim  = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.bias = bias\n",
    "\n",
    "        self.swish = Swish()\n",
    "        \n",
    "        # Если intermediate_dim не указан, устанавливаем его как 4*input_dim\n",
    "        if intermediate_dim is None:\n",
    "            self.intermediate_dim = 4 * input_dim\n",
    "        else:\n",
    "            self.intermediate_dim = intermediate_dim\n",
    "\n",
    "        # nn.Linear создает матрицу весов размера [intermediate_dim, input_dim] и вектор смещения [intermediate_dim]\n",
    "        # Это соответствует проекции W1*x + b1 в формуле SwiGLU (расширяем)\n",
    "        self.gate_proj = nn.Linear(self.input_dim, self.intermediate_dim, bias=self.bias)\n",
    "        # Это соответствует проекции W2*x + b2 в формуле SwiGLU (расширяем)\n",
    "        self.value_proj = nn.Linear(self.input_dim, self.intermediate_dim, bias=self.bias)\n",
    "        # Проекция результата в выходную размерность (сжимаем)\n",
    "        self.output_proj = nn.Linear(self.intermediate_dim, self.output_dim, bias=self.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Применяет SwiGLU активацию к входному тензору.\n",
    "        \n",
    "        Args:\n",
    "        ---------------\n",
    "            x: Входной тензор формы (..., input_dim)\n",
    "            \n",
    "        Returns:\n",
    "        ---------------\n",
    "            Тензор формы (..., output_dim) - результат применения SwiGLU\n",
    "        \"\"\"\n",
    "        # TODO: Примените gate_proj к входу\n",
    "        # TODO: Примените Swish активацию к результату gate_proj\n",
    "        # TODO: Примените value_proj к входу\n",
    "        # TODO: Перемножьте поэлементно результаты Swish(gate_proj(x)) и value_proj(x)\n",
    "        # TODO: Верните результат\n",
    "        \n",
    "        # Вопросы для размышления:\n",
    "        # - Как механизм гейтинга влияет на градиенты при обратном распространении?\n",
    "        # - Почему важно использовать разные проекции для gate и value?\n",
    "        # - Как SwiGLU способствует обучению более глубоких моделей?\n",
    "        # pass\n",
    "\n",
    "        # Создаем линейный слой, W1*x + b1\n",
    "        gate_proj = self.gate_proj(x)\n",
    "        # Создаем линейный слой, W2*x + b2\n",
    "        value_proj = self.value_proj(x)\n",
    "        # Применяем Swish активацию к результату gate_proj\n",
    "        swish = self.swish.forward(gate_proj)\n",
    "\n",
    "        # Поэлементное умножение результатов (механизм гейтинга)\n",
    "        swiglu_intermediate = swish * value_proj\n",
    "        \n",
    "        # Проекция в выходную размерность (сжимаем: input == output)\n",
    "        output = self.output_proj(swiglu_intermediate)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👀 Внимание: Grouped‑Query Attention (GQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартная библиотека\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "# Сторонние библиотеки\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Локальные импорты\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "sys.path.insert(0, parent_dir)\n",
    "from positional_encoding.rope import RoPE\n",
    "\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Grouped-Query Attention (GQA) - оптимизированная версия Multi-Head Attention,\n",
    "        используемая в современных языковых моделях, включая Qwen3.\n",
    "        \n",
    "        В отличие от Multi-Head Attention, где каждая голова имеет свои проекции\n",
    "        для query, key и value, в GQA запросы (queries) группируются, а ключи (keys)\n",
    "        и значения (values) используются совместно несколькими головами внимания.\n",
    "        \n",
    "        Это позволяет сократить вычислительные затраты и память, сохраняя при этом\n",
    "        выразительную мощность механизма внимания.\n",
    "        \n",
    "        Формула:\n",
    "        GQA(Q, K, V) = Softmax(QK^T/√d_k)V\n",
    "        \n",
    "        где:\n",
    "        - Q разделен на G групп (меньше, чем количество голов для ключей и значений)\n",
    "        - K и V имеют H голов (H ≥ G)\n",
    "        - Каждая группа запросов использует несколько голов ключей и значений\n",
    "        \n",
    "        Преимущества:\n",
    "        - Снижение вычислительных затрат и использования памяти\n",
    "        - Сохранение выразительной мощности механизма внимания\n",
    "        - Улучшение масштабируемости для больших моделей\n",
    "        \n",
    "    Args:\n",
    "    ---------------\n",
    "        hidden_size: Размерность скрытого состояния == d_model (example: LLaMA 70B hidden_size = d_model = 8192)\n",
    "        num_query_groups: Количество групп запросов\n",
    "        num_attention_heads: Количество голов внимания (для ключей и значений)\n",
    "        head_dim: Размерность каждой головы внимания\n",
    "        dropout: Вероятность дропаута (по умолчанию 0.0)\n",
    "        bias: Использовать ли смещение в линейных преобразованиях (по умолчанию True)\n",
    "        use_rope: Использовать ли RoPE для позиционного кодирования (по умолчанию True)\n",
    "        rope_theta: База для RoPE (по умолчанию 10000.0)\n",
    "        rope_scaling: Масштабирование для RoPE (по умолчанию 1.0)\n",
    "        max_position: Максимальная длина последовательности для RoPE (по умолчанию 2048)\n",
    "        \n",
    "    Returns:\n",
    "    ---------------\n",
    "        Тензор формы (batch_size, seq_len, hidden_size) - результат применения GQA\n",
    "        \n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> import torch\n",
    "        >>> gqa = GroupedQueryAttention(\n",
    "        ...     hidden_size=512,\n",
    "        ...     num_query_groups=8,\n",
    "        ...     num_attention_heads=16,\n",
    "        ...     head_dim=64\n",
    "        ... )\n",
    "        >>> x = torch.randn(2, 10, 512)\n",
    "        >>> output = gqa(x)\n",
    "        >>> output.shape\n",
    "        torch.Size([2, 10, 512])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_query_groups: int,\n",
    "        num_attention_heads: int,\n",
    "        head_dim: Optional[int] = None,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = True,\n",
    "        use_rope: bool = True,\n",
    "        rope_theta: float = 10000.0,\n",
    "        rope_scaling: float = 1.0,\n",
    "        max_position: int = 2048\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Проверьте, что num_attention_heads делится на num_query_groups\n",
    "        # TODO: Проверьте, что hidden_size делится на num_attention_heads\n",
    "        # TODO: Вычислите head_dim, если он не указан\n",
    "        # TODO: Сохраните все параметры как атрибуты класса\n",
    "        # TODO: Создайте проекции для query, key и value\n",
    "        # TODO: Создайте проекцию для выхода\n",
    "        # TODO: Создайте dropout слой\n",
    "        # TODO: Если use_rope=True, создайте RoPE модуль\n",
    "        \n",
    "        # Вопросы для размышления:\n",
    "        # - Почему GQA эффективнее, чем обычный Multi-Head Attention?\n",
    "        # - Как количество групп запросов влияет на производительность и качество модели?\n",
    "        # - Какие преимущества дает совместное использование ключей и значений?\n",
    "        # - Как RoPE интегрируется с GQA?\n",
    "        # pass\n",
    "\n",
    "        assert num_attention_heads % num_query_groups == 0, \"Количество голов внимания должно делиться на количество групп запросов\"\n",
    "        assert hidden_size % num_attention_heads == 0, \"Размерность скрытого состояния должна делиться на количество голов внимания\"\n",
    "        \n",
    "        # Делим скрытую размерность d_model (hidden_size) на число голов внимания h (num_attention_heads)\n",
    "        # Каждая голова обрабатывает кусок размерности d_head = d_model / h, а конкатенация h голов возвращает исходную размерность\n",
    "        head_dim = hidden_size // num_attention_heads if head_dim is None else head_dim\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_query_groups = num_query_groups\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.use_rope = use_rope\n",
    "        self.rope_theta = rope_theta\n",
    "        self.rope_scaling = rope_scaling\n",
    "        self.max_position = max_position\n",
    "\n",
    "        # Проекции для query, key и value\n",
    "        # В GQA: query использует группы, а key/value используют все головы\n",
    "        self.query_proj = nn.Linear(in_features = hidden_size,\n",
    "                                    out_features = num_query_groups * head_dim,\n",
    "                                    bias=bias)\n",
    "\n",
    "        self.key_proj   = nn.Linear(in_features = hidden_size,\n",
    "                                    out_features = num_attention_heads * head_dim,\n",
    "                                    bias=bias)\n",
    "\n",
    "        self.value_proj = nn.Linear(in_features = hidden_size,\n",
    "                                    out_features = num_attention_heads * head_dim,\n",
    "                                    bias=bias)\n",
    "        \n",
    "        self.output_proj = nn.Linear(in_features = num_query_groups * head_dim,\n",
    "                                    out_features = hidden_size,\n",
    "                                    bias=bias)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "        self.rope = RoPE(dim=head_dim) if use_rope else None\n",
    "\n",
    "        \n",
    "    def _split_heads(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        num_heads: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Разделяет последнюю размерность тензора на несколько голов внимания.\n",
    "        \n",
    "        Args:\n",
    "        ---------------\n",
    "            x: Входной тензор формы (batch_size, seq_len, hidden_size)\n",
    "            num_heads: Количество голов внимания\n",
    "            \n",
    "        Returns:\n",
    "        ---------------\n",
    "            Тензор формы (batch_size, num_heads, seq_len, head_dim)\n",
    "        \"\"\"\n",
    "        # TODO: Получите новую форму тензора\n",
    "        # TODO: Измените форму тензора и транспонируйте размерности\n",
    "        # TODO: Верните результат\n",
    "        # pass\n",
    "\n",
    "        # batch_size - количество последовательностей, обрабатываемых одновременно\n",
    "        # seq_len - количество токенов в последовательности\n",
    "        # total_dim - общая размерность (num_heads * head_dim)\n",
    "        batch_size, seq_len, total_dim = x.shape\n",
    "\n",
    "        # Вычисляем head_dim на основе total_dim и количества голов\n",
    "        head_dim = total_dim // num_heads\n",
    "\n",
    "        # .view() перестраивает тензор в новую форму, не изменяя данные.\n",
    "        # Меняем форму с (batch, seq, total_dim) на (batch, seq, num_heads, head_dim)\n",
    "        x = x.view(batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "        # Транспонируем для формата (batch, num_heads, seq, head_dim) согласно документации\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _repeat_kv_heads(self, kv: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Повторяет key/value головы для соответствия количеству групп query.\n",
    "            Это ключевая особенность GQA - каждая группа query использует\n",
    "            несколько key/value голов.\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            kv: Тензор key или value формы (batch_size, num_attention_heads, seq_len, head_dim)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            Тензор формы (batch_size, num_query_groups, seq_len, head_dim)\n",
    "            где key/value головы сгруппированы для каждой группы query\n",
    "        \"\"\"\n",
    "        # TODO: Получите размерности входного тензора\n",
    "        # TODO: Вычислите количество key/value голов на одну группу query\n",
    "        # TODO: Измените форму тензора для группировки голов\n",
    "        # TODO: Усредните головы внутри каждой группы\n",
    "        # TODO: Верните результат\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему мы группируем key/value головы, а не дублируем их?\n",
    "        # - Как группировка влияет на выразительную способность модели?\n",
    "        # - Какие альтернативы усреднению можно использовать (concatenation, max pooling)?\n",
    "        # - Как соотношение num_attention_heads к num_query_groups влияет на эффективность?\n",
    "\n",
    "        batch_size, num_kv_heads, seq_len, head_dim = kv.shape\n",
    "\n",
    "        # Вычисляем, сколько key/value голов приходится на одну группу query\n",
    "        heads_per_group = num_kv_heads // self.num_query_groups\n",
    "\n",
    "        # Изменяем форму для группировки голов: (batch, heads, seq, dim) -> (batch, groups, heads_per_group, seq, dim)\n",
    "        kv = kv.view(batch_size, self.num_query_groups, heads_per_group, seq_len, head_dim)\n",
    "\n",
    "        # Усредняем головы внутри каждой группы по оси heads_per_group (индекс 2)\n",
    "        kv = kv.mean(dim=2)  # Размерность: (batch_size, num_query_groups, seq_len, head_dim)\n",
    "\n",
    "        return kv\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False\n",
    "    ) -> Union[\n",
    "        Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]], Optional[torch.Tensor]],\n",
    "        torch.Tensor\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Применяет Grouped-Query Attention к входному тензору.\n",
    "        \n",
    "        Args:\n",
    "        ---------------\n",
    "            hidden_states: Входной тензор формы (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: Маска внимания (опционально)\n",
    "            position_ids: Позиционные индексы для RoPE (опционально)\n",
    "            past_key_value: Кэшированные ключи и значения (опционально)\n",
    "            output_attentions: Возвращать ли веса внимания (опционально)\n",
    "            use_cache: Использовать ли кэширование ключей и значений (опционально)\n",
    "            \n",
    "        Returns:\n",
    "        ---------------\n",
    "            Тензор формы (batch_size, seq_len, hidden_size) - результат применения GQA\n",
    "            Опционально: кэшированные ключи и значения, веса внимания\n",
    "        \"\"\"\n",
    "        # TODO: Получите размерности входного тензора\n",
    "        # TODO: Примените проекции для query, key и value\n",
    "        # TODO: Разделите query на группы, а key и value на головы\n",
    "        # TODO: Если use_rope=True, примените RoPE к query и key\n",
    "        # TODO: Если past_key_value не None, объедините с текущими key и value\n",
    "        # TODO: Если use_cache=True, подготовьте новый past_key_value\n",
    "        # TODO: Вычислите скалярное произведение query и key\n",
    "        # TODO: Масштабируйте скалярное произведение\n",
    "        # TODO: Если attention_mask не None, примените маску\n",
    "        # TODO: Примените softmax к весам внимания\n",
    "        # TODO: Примените dropout к весам внимания\n",
    "        # TODO: Вычислите взвешенную сумму значений\n",
    "        # TODO: Объедините головы внимания\n",
    "        # TODO: Примените выходную проекцию\n",
    "        # TODO: Верните результат и опциональные выходы\n",
    "        \n",
    "        # Вопросы для размышления:\n",
    "        # - Как attention_mask влияет на веса внимания?\n",
    "        # - Как кэширование ключей и значений ускоряет генерацию?\n",
    "        # - Какие преимущества дает использование RoPE в GQA?\n",
    "        # pass\n",
    "\n",
    "        # Тензор скрытого состояния\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "\n",
    "        # Проекции для query, key и value\n",
    "        # Используем фабрику линейных слоев для создания проекций\n",
    "        query = self.query_proj(hidden_states)\n",
    "        key   = self.key_proj(hidden_states)\n",
    "        value = self.value_proj(hidden_states)\n",
    "\n",
    "        # Разделяем query на группы, а key и value на головы\n",
    "        query = self._split_heads(query, self.num_query_groups)\n",
    "        key   = self._split_heads(key,   self.num_attention_heads)\n",
    "        value = self._split_heads(value, self.num_attention_heads)\n",
    "\n",
    "        # Применяем RoPE до группировки, когда тензоры в правильном формате\n",
    "        if self.use_rope:\n",
    "            if position_ids is None:\n",
    "                # Создаем позиционные индексы для RoPE: [0, 1, 2, ..., seq_len-1]\n",
    "                position_ids = torch.arange(seq_len, dtype=torch.long, device=hidden_states.device)\n",
    "\n",
    "                # Трансформация 1D → 2D → Batch-compatible:\n",
    "                # 1. unsqueeze(0): [0,1,2,3] → [[0,1,2,3]] (добавляем batch размерность)\n",
    "                # 2. expand(batch_size, -1): [[0,1,2,3]] → [[0,1,2,3], [0,1,2,3], ...]\n",
    "                # Результат: каждый элемент в batch получает одинаковые позиции\n",
    "                # expand() эффективен - создает view без копирования данных\n",
    "                position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "            # RoPE применяется к каждой голове отдельно\n",
    "            # Сначала изменяем формат для применения RoPE\n",
    "            # Используем reshape() вместо view() так как после transpose() тензор может быть не-contiguous\n",
    "            query_for_rope = query.reshape(batch_size * self.num_query_groups, seq_len, self.head_dim)\n",
    "            key_for_rope   = key.reshape(batch_size * self.num_attention_heads, seq_len, self.head_dim)\n",
    "\n",
    "            # Расширяем position_ids для всех голов\n",
    "            pos_query = position_ids.unsqueeze(1).expand(-1, self.num_query_groups, -1).contiguous().view(-1, seq_len)\n",
    "            pos_key = position_ids.unsqueeze(1).expand(-1, self.num_attention_heads, -1).contiguous().view(-1, seq_len)\n",
    "\n",
    "            # Применяем RoPE\n",
    "            query_rope, _ = self.rope(query_for_rope, query_for_rope, pos_query)\n",
    "            _, key_rope = self.rope(key_for_rope, key_for_rope, pos_key)\n",
    "\n",
    "            # Возвращаем к исходному формату\n",
    "            query = query_rope.view(batch_size, self.num_query_groups, seq_len, self.head_dim)\n",
    "            key = key_rope.view(batch_size, self.num_attention_heads, seq_len, self.head_dim)\n",
    "\n",
    "        # Применяем группировку key/value для соответствия query группам\n",
    "        key   = self._repeat_kv_heads(key)\n",
    "        value = self._repeat_kv_heads(value)\n",
    "\n",
    "        # Все тензоры уже в формате (batch, heads, seq, dim) после _split_heads и _repeat_kv_heads\n",
    "        # query: (batch_size, num_query_groups, seq_len, head_dim)\n",
    "        # key:   (batch_size, num_query_groups, seq_len, head_dim)\n",
    "        # value: (batch_size, num_query_groups, seq_len, head_dim)\n",
    "\n",
    "        # Объединяем с past_key_value, если предоставлено\n",
    "        # Формат: (batch, heads, seq, dim), конкатенируем по seq_len (dim=2)\n",
    "        if past_key_value is not None:\n",
    "            key = torch.cat([past_key_value[0], key], dim=2)\n",
    "            value = torch.cat([past_key_value[1], value], dim=2)\n",
    "\n",
    "        # Подготавливаем новый past_key_value\n",
    "        if use_cache:\n",
    "            past_key_value = (key, value)\n",
    "\n",
    "        # Вычисляем скалярное произведение query и key и масштабируем один раз\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        # Применяем маску внимания\n",
    "        if attention_mask is not None:\n",
    "            scores = scores + attention_mask\n",
    "        # Применяем softmax к весам внимания\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        # Применяем dropout к весам внимания\n",
    "        weights = self.dropout(weights)\n",
    "        # Вычисляем взвешенную сумму значений\n",
    "        context = torch.matmul(weights, value)\n",
    "        # Объединяем головы внимания: (batch, heads, seq, dim) -> (batch, seq, heads, dim)\n",
    "        context = context.transpose(1, 2).contiguous()\n",
    "        # Объединяем головы: (batch, seq, heads, dim) -> (batch, seq, heads*dim)\n",
    "        context = context.view(batch_size, seq_len, self.num_query_groups * self.head_dim)\n",
    "        # Применяем выходную проекцию\n",
    "        output = self.output_proj(context)\n",
    "        # Возвращаем результат в зависимости от флагов\n",
    "        if use_cache and output_attentions:\n",
    "            return output, past_key_value, weights\n",
    "        elif use_cache:\n",
    "            return output, past_key_value, None\n",
    "        elif output_attentions:\n",
    "            return output, None, weights\n",
    "        else:\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧱 Базовый Transformer‑блок (без MoE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f187873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартная библиотека\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "# Сторонние библиотеки\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Локальные импорты\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "sys.path.insert(0, parent_dir)\n",
    "from normalization.rmsnorm import RMSNorm\n",
    "from attention.gqa import GroupedQueryAttention\n",
    "from activations.swiglu import SwiGLU\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Блок Transformer для архитектуры Qwen3 MoE. Использует Pre-Norm архитектуру\n",
    "        с Grouped-Query Attention и SwiGLU активацией.\n",
    "\n",
    "        Архитектура блока:\n",
    "        Input → RMSNorm → GQA → Residual → RMSNorm → SwiGLU → Residual → Output\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        hidden_size: Размерность скрытого состояния (d_model)\n",
    "        num_query_groups: Количество групп запросов для GQA\n",
    "        num_attention_heads: Количество голов внимания для key/value\n",
    "        intermediate_size: Размерность промежуточного слоя в SwiGLU (по умолчанию 4 * hidden_size)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_query_groups: int,\n",
    "        num_attention_heads: int,\n",
    "        intermediate_size: Optional[int] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # TODO: Проверьте валидность параметров\n",
    "        # TODO: Вычислите intermediate_size если не указан (4 * hidden_size)\n",
    "        # TODO: Сохраните параметры как атрибуты\n",
    "        # TODO: Создайте self.attention_norm = RMSNorm(hidden_size)\n",
    "        # TODO: Создайте self.attention = GroupedQueryAttention(...)\n",
    "        # TODO: Создайте self.ffn_norm = RMSNorm(hidden_size)\n",
    "        # TODO: Создайте self.feed_forward = SwiGLU(...)\n",
    "\n",
    "        # --- Валидация параметров -------------------------------------------------\n",
    "        assert (\n",
    "            isinstance(hidden_size, int) and hidden_size > 0\n",
    "        ), \"hidden_size должен быть положительным целым числом\"\n",
    "        assert (\n",
    "            isinstance(num_query_groups, int) and num_query_groups > 0\n",
    "        ), \"num_query_groups должен быть положительным целым числом\"\n",
    "        assert (\n",
    "            isinstance(num_attention_heads, int) and num_attention_heads > 0\n",
    "        ), \"num_attention_heads должен быть положительным целым числом\"\n",
    "\n",
    "        # Ключевая проверка для GQA архитектуры:\n",
    "        assert (\n",
    "            num_attention_heads % num_query_groups == 0\n",
    "        ), (\n",
    "            \"num_attention_heads должен делиться на num_query_groups для \"\n",
    "            \"корректной работы GQA\"\n",
    "        )\n",
    "        # Проверка делимости hidden_size на число голов\n",
    "        # Тензор скрытого состояния должен равномерно делиться на число голов\n",
    "        assert (\n",
    "            hidden_size % num_attention_heads == 0\n",
    "        ), \"hidden_size должен делиться на num_attention_heads\"\n",
    "\n",
    "        # Проверка intermediate_size если указан\n",
    "        if intermediate_size is not None:\n",
    "            assert (\n",
    "                isinstance(intermediate_size, int) and intermediate_size > 0\n",
    "            ), \"intermediate_size должен быть положительным целым числом\"\n",
    "\n",
    "        # --- Инициализация атрибутов ----------------------------------------------\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_query_groups = num_query_groups\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = (\n",
    "            intermediate_size if intermediate_size is not None else 4 * hidden_size\n",
    "        )\n",
    "\n",
    "        # --- Компоненты нормализации и подблоков ----------------------------------\n",
    "        self.attention_norm = RMSNorm(hidden_size)\n",
    "        self.attention = GroupedQueryAttention(\n",
    "            hidden_size=hidden_size,\n",
    "            num_query_groups=num_query_groups,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "        )\n",
    "        self.ffn_norm = RMSNorm(hidden_size)\n",
    "        self.feed_forward = SwiGLU(\n",
    "            input_dim=self.hidden_size,\n",
    "            output_dim=self.hidden_size,\n",
    "            intermediate_dim=self.intermediate_size,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False\n",
    "    ) -> Union[torch.Tensor, Tuple]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Применяет Transformer блок к входному тензору.\n",
    "            Input → RMSNorm → GQA → Residual → RMSNorm → SwiGLU → Residual → Output\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            hidden_states: Входной тензор формы (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            Тензор формы (batch_size, seq_len, hidden_size) - выход Transformer блока\n",
    "        \"\"\"\n",
    "        # TODO: Сохраните входной тензор для первого residual connection\n",
    "        # TODO: Примените attention_norm\n",
    "        # TODO: Примените self.attention\n",
    "        # TODO: Добавьте первый residual connection\n",
    "        # TODO: Сохраните результат для второго residual connection\n",
    "        # TODO: Примените ffn_norm\n",
    "        # TODO: Примените self.feed_forward\n",
    "        # TODO: Добавьте второй residual connection\n",
    "        # TODO: Верните результат\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему нормализация применяется ДО attention/ffn, а не после?\n",
    "        # - Как residual connections помогают при обучении глубоких сетей?\n",
    "\n",
    "        # ──────────────────────────────────────────────────────────────────────────\n",
    "        # ПЕРВЫЙ RESIDUAL BLOCK: Self-Attention (GQA)\n",
    "        # ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "        # Сохраняем вход для остаточной связи (residual).\n",
    "        residual_1 = hidden_states\n",
    "\n",
    "        # Преднормализация улучшает устойчивость и качество внимания.\n",
    "        normed = self.attention_norm(hidden_states)\n",
    "\n",
    "        # Вызываем модуль группового внимания. Он может вернуть:\n",
    "        # - только выход (Tensor), либо\n",
    "        # - кортеж (att_output, present_key_value, attn_weights).\n",
    "        att_output = self.attention(\n",
    "            hidden_states=normed,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "\n",
    "        if isinstance(att_output, tuple):\n",
    "            att_output, present_key_value, attn_weights = att_output\n",
    "        else:\n",
    "            present_key_value = None\n",
    "            attn_weights = None\n",
    "\n",
    "        # Первая residual-связь: складываем вход и выход подблока.\n",
    "        hidden_states = att_output + residual_1\n",
    "\n",
    "        # ──────────────────────────────────────────────────────────────────────────\n",
    "        # ВТОРОЙ RESIDUAL BLOCK: Feed-Forward (SwiGLU)\n",
    "        # ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "        residual_2 = hidden_states\n",
    "\n",
    "        # Преднормализация перед FFN по тем же причинам, что и перед вниманием.\n",
    "        normed = self.ffn_norm(hidden_states)\n",
    "\n",
    "        # Применяем нелинейную проекцию SwiGLU с расширением размерности\n",
    "        # до intermediate_size и обратной проекцией к hidden_size.\n",
    "        ffn_output = self.feed_forward(normed)\n",
    "\n",
    "        # Вторая residual-связь.\n",
    "        hidden_states = ffn_output + residual_2\n",
    "\n",
    "        if use_cache or output_attentions:\n",
    "            return hidden_states, present_key_value, attn_weights\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛣️ MoE: Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80694326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартная библиотека\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Сторонние библиотеки\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MoERouter(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        MoE Router (Mixture-of-Experts Router) для архитектуры Qwen3.\n",
    "\n",
    "        Роутер решает для каждого токена:\n",
    "        1. Какие K экспертов из N активировать (Top-K selection)\n",
    "        2. С какими весами комбинировать их выходы (gating weights)\n",
    "        3. Как балансировать нагрузку между экспертами (load balancing)\n",
    "\n",
    "        Архитектура:\n",
    "        Input Token → Linear Projection → Softmax → Top-K Selection → Gating Weights\n",
    "\n",
    "        Для этой модели (0.6B): N=8 экспертов, K=2 активных per token\n",
    "        Для справки, Qwen3-30B использует: N=128 экспертов, K=8 активных per token\n",
    "\n",
    "    Mathematical Formulation:\n",
    "    ---------------\n",
    "        1. Gating scores: g = Softmax(W_g * x)\n",
    "           где W_g - обучаемая матрица размера (hidden_size, num_experts)\n",
    "\n",
    "        2. Top-K selection: indices, weights = TopK(g, k=top_k)\n",
    "           Выбираем K экспертов с наибольшими весами\n",
    "\n",
    "        3. Renormalization: weights = Softmax(weights)\n",
    "           Нормализуем веса выбранных экспертов (сумма = 1)\n",
    "\n",
    "        4. Load balancing loss: L_balance = α * mean(f * P)\n",
    "           где f - частота выбора эксперта, P - средний вес эксперта\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        hidden_size: Размерность входного скрытого состояния\n",
    "        num_experts: Общее количество экспертов (N)\n",
    "        top_k: Количество активных экспертов per token (K)\n",
    "        capacity_factor: Фактор емкости для ограничения токенов per expert (default: 1.25)\n",
    "        balance_loss_coef: Коэффициент для load balancing loss (default: 0.01)\n",
    "\n",
    "    Returns (from forward):\n",
    "    ---------------\n",
    "        routing_weights: Тензор формы (batch_size, seq_len, top_k)\n",
    "                        Веса для каждого из K выбранных экспертов\n",
    "        selected_experts: Тензор формы (batch_size, seq_len, top_k) dtype=long\n",
    "                         Индексы выбранных экспертов [0, num_experts)\n",
    "        balance_loss: Скаляр - loss для балансировки нагрузки между экспертами\n",
    "\n",
    "    Example:\n",
    "    ---------------\n",
    "        >>> # Для модели 0.6B\n",
    "        >>> router = MoERouter(hidden_size=512, num_experts=8, top_k=2)\n",
    "        >>> x = torch.randn(2, 10, 512)  # (batch=2, seq=10, hidden=512)\n",
    "        >>> weights, experts, loss = router(x)\n",
    "        >>> weights.shape  # torch.Size([2, 10, 2])\n",
    "        >>> experts.shape  # torch.Size([2, 10, 2])\n",
    "        >>> loss.item()    # Скаляр loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_experts: int,\n",
    "        top_k: int = 2,\n",
    "        capacity_factor: float = 1.25,\n",
    "        balance_loss_coef: float = 0.01\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Проверьте валидность параметров (hidden_size, num_experts, top_k)\n",
    "        # TODO: Убедитесь что top_k <= num_experts\n",
    "        # TODO: Сохраните все параметры как атрибуты класса\n",
    "        # TODO: Создайте self.gate - линейный слой для проекции в пространство экспертов\n",
    "        #       Размеры: (hidden_size) -> (num_experts)\n",
    "        # TODO: Инициализируйте веса gate небольшими значениями для стабильности\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему важно, чтобы top_k был меньше num_experts?\n",
    "        # - Как capacity_factor влияет на балансировку нагрузки?\n",
    "        # - Зачем нужна небольшая инициализация весов gate?\n",
    "        # - Какие альтернативы Softmax можно использовать для gating?\n",
    "        # pass\n",
    "\n",
    "        # --- Валидация параметров -------------------------------------------------\n",
    "        # Используем assert для раннего обнаружения ошибок конфигурации.\n",
    "        assert isinstance(hidden_size, int) and hidden_size > 0, (\n",
    "            \"hidden_size должен быть положительным целым числом\"\n",
    "        )\n",
    "        assert isinstance(num_experts, int) and num_experts > 0, (\n",
    "            \"num_experts должен быть положительным целым числом\"\n",
    "        )\n",
    "        assert isinstance(top_k, int) and top_k > 0, (\n",
    "            \"top_k должен быть положительным целым числом\"\n",
    "        )\n",
    "        assert top_k <= num_experts, (\n",
    "            \"num_experts должено быть больше или равно top_k\"\n",
    "        )\n",
    "        assert isinstance(capacity_factor, float) and capacity_factor > 0, (\n",
    "            \"capacity_factor должен быть положительным числом\"\n",
    "        )\n",
    "        assert isinstance(balance_loss_coef, float) and balance_loss_coef >= 0, (\n",
    "            \"balance_loss_coef должен быть неотрицательным числом\"\n",
    "        )\n",
    "\n",
    "        # --- Инициализация атрибутов ----------------------------------------------\n",
    "        # Храним параметры как атрибуты экземпляра, чтобы использовать их\n",
    "        # при дальнейшей маршрутизации и расчёте регуляризаций.\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.balance_loss_coef = balance_loss_coef\n",
    "\n",
    "        # Линейный слой-gate предсказывает логиты по экспертам на основе входного скрытого состояния\n",
    "        # последующий softmax (как правило, в forward) превращает их в вероятности.\n",
    "        self.gate = nn.Linear(hidden_size, num_experts)\n",
    "\n",
    "        # Инициализация: небольшой нормальный шум ускоряет сходимость.\n",
    "        self.gate.weight.data.normal_(0, 0.01)\n",
    "\n",
    "        # Нулевой сдвиг предотвращает смещение распределения по экспертам на старте обучения\n",
    "        # проверка на наличие bias — на случай future-refactor.\n",
    "        if self.gate.bias is not None:\n",
    "            self.gate.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        training: bool = True\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Применяет MoE routing к входным скрытым состояниям.\n",
    "\n",
    "            Процесс:\n",
    "            1. Проекция входа в пространство экспертов\n",
    "            2. Вычисление gating scores через Softmax\n",
    "            3. Top-K selection - выбор K лучших экспертов\n",
    "            4. Renormalization весов выбранных экспертов\n",
    "            5. Вычисление load balancing loss (только при training=True)\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            hidden_states: Входной тензор формы (batch_size, seq_len, hidden_size)\n",
    "            training: Флаг режима обучения (для load balancing loss)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            routing_weights: Тензор формы (batch_size, seq_len, top_k)\n",
    "                           Нормализованные веса для выбранных экспертов\n",
    "            selected_experts: Тензор формы (batch_size, seq_len, top_k)\n",
    "                            Индексы выбранных экспертов\n",
    "            balance_loss: Скаляр - load balancing loss (0.0 если training=False)\n",
    "        \"\"\"\n",
    "        # TODO: Получите размерности входного тензора (batch_size, seq_len, hidden_size)\n",
    "        # TODO: Примените self.gate к hidden_states для получения логитов\n",
    "        # TODO: Примените Softmax по оси num_experts для получения gating_scores\n",
    "        #       Это дает распределение вероятностей по всем экспертам\n",
    "        # TODO: Используйте torch.topk для выбора top_k экспертов\n",
    "        #       Получите: routing_weights (веса), selected_experts (индексы)\n",
    "        # TODO: Ре-нормализуйте routing_weights через Softmax\n",
    "        #       Важно: веса K выбранных экспертов должны суммироваться в 1\n",
    "        # TODO: Если training=True, вычислите load balancing loss\n",
    "        #       Используйте вспомогательный метод _compute_balance_loss\n",
    "        # TODO: Верните (routing_weights, selected_experts, balance_loss)\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему нужна ре-нормализация после Top-K selection?\n",
    "        # - Что произойдет, если все токены выберут одних и тех же экспертов?\n",
    "        # - Как Top-K selection влияет на вычислительную эффективность?\n",
    "        # - Почему balance_loss вычисляется только при training=True?\n",
    "        # pass\n",
    "\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "\n",
    "        # Linear projection (W·x + b)\n",
    "        # Проекция токенов в пространство экспертов: (B, S, H) → (B, S, N)\n",
    "        # Логиты для всех N экспертов (например, 8 для модели 0.6B)\n",
    "        logits = self.gate(hidden_states)\n",
    "\n",
    "        # Softmax по оси num_experts для получения gating_scores\n",
    "        # Распределение вероятностей по всем экспертам\n",
    "        gating_scores = F.softmax(\n",
    "            input = logits,\n",
    "            dim = -1\n",
    "        )\n",
    "        \n",
    "        # Top-K selection - выбор K лучших экспертов\n",
    "        # Получение routing_weights (веса) и selected_experts (индексы)\n",
    "        routing_weights, selected_experts = torch.topk(\n",
    "            input = gating_scores,\n",
    "            k = self.top_k,\n",
    "            dim = -1\n",
    "        )\n",
    "\n",
    "        # Renormalization весов выбранных экспертов\n",
    "        # Нормализация весов K выбранных экспертов (сумма = 1)\n",
    "        routing_weights = F.softmax(\n",
    "            input = routing_weights,\n",
    "            dim = -1\n",
    "        )\n",
    "\n",
    "        if training:\n",
    "            load_balance_loss = self._compute_balance_loss(\n",
    "                gating_scores = gating_scores,\n",
    "                selected_experts = selected_experts\n",
    "            )\n",
    "        else:\n",
    "            load_balance_loss = torch.tensor(0.0, device=gating_scores.device)\n",
    "\n",
    "        return routing_weights, selected_experts, load_balance_loss\n",
    "\n",
    "\n",
    "    def _compute_balance_loss(\n",
    "        self,\n",
    "        gating_scores: torch.Tensor,\n",
    "        selected_experts: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Вычисляет load balancing loss для равномерного распределения нагрузки.\n",
    "\n",
    "            Цель: Предотвратить ситуацию, когда модель использует только малую часть экспертов.\n",
    "\n",
    "            Формула: L_balance = α * num_experts * Σ(f_i * P_i)\n",
    "            где:\n",
    "            - f_i - fraction of tokens routed to expert i\n",
    "            - P_i - mean gating score for expert i\n",
    "            - α - balance_loss_coef\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            gating_scores: Тензор формы (batch_size, seq_len, num_experts)\n",
    "                          Softmax scores для всех экспертов\n",
    "            selected_experts: Тензор формы (batch_size, seq_len, top_k)\n",
    "                            Индексы выбранных экспертов\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            balance_loss: Скаляр тензор - loss для балансировки\n",
    "        \"\"\"\n",
    "        # TODO: Вычислите frequency (f_i) - сколько токенов выбрали каждого эксперта\n",
    "        #       Подсказка: используйте torch.bincount или создайте one-hot и усредните\n",
    "        # TODO: Вычислите mean gating probability (P_i) для каждого эксперта\n",
    "        #       Подсказка: усредните gating_scores по batch и sequence dimensions\n",
    "        # TODO: Вычислите loss = balance_loss_coef * num_experts * sum(f_i * P_i)\n",
    "        # TODO: Верните balance_loss\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему мы умножаем на num_experts в формуле?\n",
    "        # - Как этот loss влияет на распределение нагрузки?\n",
    "        # - Что произойдет, если balance_loss_coef слишком большой?\n",
    "        # - Какие альтернативные метрики балансировки существуют?\n",
    "        # pass\n",
    "\n",
    "        if gating_scores.dim() != 3:\n",
    "            raise ValueError(\"gating_scores должен иметь форму (B, S, N).\")\n",
    "        if selected_experts.dim() != 3:\n",
    "            raise ValueError(\"selected_experts должен иметь форму (B, S, K).\")\n",
    "        if gating_scores.size(-1) != self.num_experts:\n",
    "            raise ValueError(\"Последняя размерность gating_scores должна быть N.\")\n",
    "\n",
    "        # # .view() перестраивает тензор уже выбранных экспертов в новую форму, не изменяя данные.\n",
    "        # Было: (batch_size, seq_len, top_k) = (2, 10, 8)\n",
    "        # Стало: (160,) — все индексы в одном массиве, мы получаем один длинный одномерный вектор\n",
    "        flattened_experts = selected_experts.view(-1)\n",
    "\n",
    "        # expert_counts[i] = сколько раз эксперт i был выбран\n",
    "        expert_counts = torch.bincount(\n",
    "            flattened_experts,\n",
    "            minlength=self.num_experts  # Гарантируем вектор длины num_experts (например, 8)\n",
    "        )\n",
    "\n",
    "        # Общее количество выборов = batch_size * seq_len * top_k\n",
    "        batch_size, seq_len, top_k = selected_experts.shape\n",
    "        total_selections = batch_size * seq_len * top_k\n",
    "\n",
    "        # f_i = (количество раз, когда эксперт i был выбран) / (общее количество выборов)\n",
    "        f_i = expert_counts.float() / total_selections\n",
    "\n",
    "        # Зачем вычисляем среднее для gating_scores, когда это уже тензор вероятностей экспертов после softmax?\n",
    "        # Потому что нам нужно знать среднюю уверенность модели в каждом эксперте по всем токенам. Это отличается от частоты выбора:\n",
    "        #   - f_i = как часто эксперт попадает в Top-K (0 или 1 для каждого токена)\n",
    "        #   - P_i = какую среднюю вероятность модель назначает эксперту (до Top-K)\n",
    "        # Произведение f_i * P_i максимально, когда эксперт и часто выбирается, и модель в нём уверена → это дисбаланс → высокий loss → градиент штрафует.\n",
    "        P_i = gating_scores.mean(dim=(0, 1))\n",
    "\n",
    "        balance_loss = self.balance_loss_coef * self.num_experts * (f_i * P_i).sum()\n",
    "\n",
    "        return balance_loss\n",
    "\n",
    "\n",
    "    def expert_capacity(self, num_tokens: int) -> int:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Вычисляет максимальную емкость каждого эксперта.\n",
    "\n",
    "            Capacity = (num_tokens / num_experts) * capacity_factor * top_k\n",
    "\n",
    "            Это ограничивает количество токенов, которые может обработать один эксперт,\n",
    "            предотвращая перегрузку отдельных экспертов.\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            num_tokens: Общее количество токенов (batch_size * seq_len)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            capacity: Максимальное количество токенов per expert\n",
    "        \"\"\"\n",
    "        # TODO: Вычислите базовую capacity = num_tokens / num_experts\n",
    "        # TODO: Умножьте на capacity_factor для запаса\n",
    "        # TODO: Умножьте на top_k (каждый токен идет к K экспертам)\n",
    "        # TODO: Округлите до целого числа (ceil)\n",
    "        # TODO: Верните capacity\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Зачем нужен capacity_factor > 1.0?\n",
    "        # - Что делать с токенами, превышающими capacity?\n",
    "        # - Как capacity влияет на memory footprint?\n",
    "        # pass\n",
    "\n",
    "        capacity = math.ceil((num_tokens / self.num_experts) * self.capacity_factor * self.top_k)\n",
    "\n",
    "        return capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧑‍🏫 MoE: Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593fea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартная библиотека\n",
    "from typing import Optional\n",
    "\n",
    "# Сторонние библиотеки\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Локальные импорты\n",
    "from experiments.domain.activations.swiglu import SwiGLU\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Expert Network для Mixture-of-Experts архитектуры.\n",
    "\n",
    "        Каждый эксперт - это независимая feed-forward сеть, которая обрабатывает\n",
    "        токены, направленные к ней Router'ом.\n",
    "\n",
    "        Архитектура:\n",
    "        Input (hidden_size) → SwiGLU FFN → Output (hidden_size)\n",
    "\n",
    "        Внутри SwiGLU:\n",
    "        hidden_size → intermediate_size (с gating) → hidden_size\n",
    "\n",
    "        Для модели 0.6B:\n",
    "        - hidden_size = 512\n",
    "        - intermediate_size = 2048 (обычно 4 * hidden_size)\n",
    "        - num_experts = 8 (каждый с независимыми весами)\n",
    "\n",
    "    Mathematical Flow:\n",
    "    ---------------\n",
    "        x ∈ ℝ^(batch×seq×hidden)\n",
    "            ↓\n",
    "        SwiGLU(x) = Swish(W1·x) ⊙ (W2·x)  [intermediate_dim]\n",
    "            ↓\n",
    "        W3·SwiGLU(x) + b3\n",
    "            ↓\n",
    "        output ∈ ℝ^(batch×seq×hidden)\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        hidden_size: Размерность входа и выхода (должна совпадать с hidden_size модели)\n",
    "        intermediate_size: Размерность промежуточного слоя (обычно 4 * hidden_size)\n",
    "        dropout: Dropout вероятность для регуляризации (default: 0.0)\n",
    "\n",
    "    Returns (from forward):\n",
    "    ---------------\n",
    "        output: Тензор формы (batch_size, seq_len, hidden_size)\n",
    "                Преобразованные скрытые состояния\n",
    "\n",
    "    Example:\n",
    "    ---------------\n",
    "        >>> # Создание одного эксперта для модели 0.6B\n",
    "        >>> expert = Expert(hidden_size=512, intermediate_size=2048)\n",
    "        >>> x = torch.randn(2, 10, 512)  # (batch=2, seq=10, hidden=512)\n",
    "        >>> output = expert(x)\n",
    "        >>> output.shape  # torch.Size([2, 10, 512])\n",
    "\n",
    "        >>> # Создание нескольких экспертов\n",
    "        >>> num_experts = 8\n",
    "        >>> experts = nn.ModuleList([\n",
    "        ...     Expert(hidden_size=512, intermediate_size=2048)\n",
    "        ...     for _ in range(num_experts)\n",
    "        ... ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Проверьте валидность параметров\n",
    "        #       - hidden_size должен быть положительным целым числом\n",
    "        #       - intermediate_size должен быть положительным целым числом\n",
    "        #       - dropout должен быть в диапазоне [0.0, 1.0)\n",
    "        # TODO: Сохраните параметры как атрибуты класса\n",
    "        # TODO: Создайте self.ffn - экземпляр SwiGLU\n",
    "        #       Параметры: input_dim=hidden_size, output_dim=hidden_size,\n",
    "        #                  intermediate_dim=intermediate_size\n",
    "        # TODO: Создайте self.dropout - слой Dropout с заданной вероятностью\n",
    "        #       (даже если dropout=0.0, создайте слой для единообразия)\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему intermediate_size обычно в 4 раза больше hidden_size?\n",
    "        # - Зачем нужен dropout в экспертах?\n",
    "        # - Как SwiGLU отличается от обычного ReLU FFN?\n",
    "        # - Почему каждый эксперт должен иметь одинаковую архитектуру?\n",
    "        # pass\n",
    "\n",
    "        # --- Валидация параметров -------------------------------------------------\n",
    "        # Используем assert для раннего обнаружения ошибок конфигурации.\n",
    "        assert isinstance(hidden_size, int) and hidden_size > 0, (\n",
    "            \"hidden_size должен быть положительным целым числом\"\n",
    "        )\n",
    "        assert isinstance(intermediate_size, int) and intermediate_size > 0, (\n",
    "            \"intermediate_size должен быть положительным целым числом\"\n",
    "        )\n",
    "        assert isinstance(dropout, float) and 0.0 <= dropout < 1.0, (\n",
    "            \"dropout должен быть в диапазоне [0.0, 1.0)\"\n",
    "        )\n",
    "\n",
    "        # --- Сохранение параметров ------------------------------------------------\n",
    "        # Храним параметры как атрибуты экземпляра\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.dropout_prob = dropout\n",
    "\n",
    "        # --- Создание слоев ------------------------------------------------------\n",
    "        # SwiGLU feed-forward сеть\n",
    "        self.ffn = SwiGLU(\n",
    "            input_dim=self.hidden_size,\n",
    "            output_dim=self.hidden_size,\n",
    "            intermediate_dim=self.intermediate_size\n",
    "        )\n",
    "\n",
    "        # Dropout слой для регуляризации\n",
    "        self.dropout = nn.Dropout(p=self.dropout_prob)\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Применяет преобразование эксперта к входным скрытым состояниям.\n",
    "\n",
    "            Процесс:\n",
    "            1. Применение SwiGLU feed-forward сети\n",
    "            2. Применение dropout для регуляризации\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            hidden_states: Входной тензор формы (batch_size, seq_len, hidden_size)\n",
    "                          Скрытые состояния токенов, направленных к этому эксперту\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            output: Тензор формы (batch_size, seq_len, hidden_size)\n",
    "                   Преобразованные скрытые состояния\n",
    "        \"\"\"\n",
    "        # TODO: Примените self.ffn к hidden_states\n",
    "        # TODO: Примените self.dropout к результату\n",
    "        # TODO: Верните результат\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Нужен ли residual connection внутри эксперта?\n",
    "        # - Когда применяется dropout - только при training или всегда?\n",
    "        # - Как размерности входа и выхода связаны?\n",
    "        # pass\n",
    "\n",
    "        # Прямое распространение через SwiGLU FFN\n",
    "        x = self.ffn(hidden_states)\n",
    "        # Применение dropout для регуляризации\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧰 SimpleMoELayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc0138",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SimpleMoELayer - это учебная версия MoE, которая фокусируется на правильности логики, а не на производительности. Используя простой цикл по токенам, мы избегаем сложных\n",
    "тензорных операций индексации, делая код понятным и легко отлаживаемым. Это идеальный first step перед оптимизированной версией.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Стандартная библиотека\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Сторонние библиотеки\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Локальные импорты\n",
    "from experiments.domain.moe.router import MoERouter\n",
    "from experiments.domain.moe.expert import Expert\n",
    "\n",
    "\n",
    "class SimpleMoELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Простая (наивная) реализация MoE Layer для обучения и тестирования.\n",
    "\n",
    "        Эта версия использует простые циклы вместо оптимизированных тензорных операций.\n",
    "        Идеально подходит для понимания логики MoE перед переходом к оптимизированной версии.\n",
    "\n",
    "        Архитектура:\n",
    "        Input → Router (выбор экспертов) → Dispatch → Experts → Combine → Residual → Output\n",
    "\n",
    "        Pipeline:\n",
    "        1. Router: выбирает top_k экспертов для каждого токена\n",
    "        2. Dispatch: распределяет токены по выбранным экспертам\n",
    "        3. Process: каждый эксперт обрабатывает свои токены\n",
    "        4. Combine: собирает результаты с весами от Router\n",
    "        5. Residual: добавляет входной тензор к выходному\n",
    "\n",
    "        Для модели 0.6B:\n",
    "        - num_experts = 8\n",
    "        - top_k = 2 (каждый токен → 2 эксперта)\n",
    "        - hidden_size = 512\n",
    "        - intermediate_size = 2048\n",
    "\n",
    "    Mathematical Flow:\n",
    "    ---------------\n",
    "        x ∈ ℝ^(B×S×H)\n",
    "            ↓\n",
    "        Router: (weights, experts_idx, loss) = Router(x)\n",
    "            weights ∈ ℝ^(B×S×K)      # Веса для K экспертов\n",
    "            experts_idx ∈ ℤ^(B×S×K)  # Индексы экспертов [0, N)\n",
    "            ↓\n",
    "        For each token t in (B×S):\n",
    "            output[t] = Σ(k=1 to K) weights[t,k] * Expert[experts_idx[t,k]](x[t])\n",
    "            ↓\n",
    "        output = output + x  # Residual connection\n",
    "            ↓\n",
    "        return output, loss\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        hidden_size: Размерность входа/выхода (должна совпадать с моделью)\n",
    "        num_experts: Количество экспертов (8 для модели 0.6B)\n",
    "        top_k: Количество активных экспертов per token (2 для модели 0.6B)\n",
    "        intermediate_size: Размерность промежуточного слоя экспертов (обычно 4*hidden_size)\n",
    "        expert_dropout: Dropout для экспертов (default: 0.0)\n",
    "        capacity_factor: Фактор емкости для Router (default: 1.25)\n",
    "        balance_loss_coef: Коэффициент для load balancing loss (default: 0.01)\n",
    "\n",
    "    Returns (from forward):\n",
    "    ---------------\n",
    "        output: Тензор формы (batch_size, seq_len, hidden_size)\n",
    "                Выходные скрытые состояния после MoE обработки\n",
    "        balance_loss: Скаляр - load balancing loss для обучения\n",
    "\n",
    "    Example:\n",
    "    ---------------\n",
    "        >>> # Создание MoE Layer для модели 0.6B\n",
    "        >>> moe = SimpleMoELayer(\n",
    "        ...     hidden_size=512,\n",
    "        ...     num_experts=8,\n",
    "        ...     top_k=2,\n",
    "        ...     intermediate_size=2048\n",
    "        ... )\n",
    "        >>> x = torch.randn(2, 10, 512)  # (batch=2, seq=10, hidden=512)\n",
    "        >>> output, loss = moe(x, training=True)\n",
    "        >>> output.shape  # torch.Size([2, 10, 512])\n",
    "        >>> loss.item()   # Скаляр loss\n",
    "\n",
    "    Note:\n",
    "    ---------------\n",
    "        Это ПРОСТАЯ версия для обучения. Использует циклы вместо\n",
    "        оптимизированных batch операций. Для production используйте\n",
    "        оптимизированную версию MoELayer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_experts: int = 8,\n",
    "        top_k: int = 2,\n",
    "        intermediate_size: int = 2048,\n",
    "        expert_dropout: float = 0.0,\n",
    "        capacity_factor: float = 1.25,\n",
    "        balance_loss_coef: float = 0.01\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Проверьте валидность параметров\n",
    "        #       - hidden_size > 0\n",
    "        #       - num_experts > 0\n",
    "        #       - top_k > 0 и top_k <= num_experts\n",
    "        #       - intermediate_size > 0\n",
    "        # TODO: Сохраните параметры как атрибуты класса\n",
    "        # TODO: Создайте self.router - экземпляр MoERouter\n",
    "        #       Параметры: hidden_size, num_experts, top_k, capacity_factor, balance_loss_coef\n",
    "        # TODO: Создайте self.experts - nn.ModuleList из num_experts экспертов\n",
    "        #       Каждый эксперт: Expert(hidden_size, intermediate_size, expert_dropout)\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему используем nn.ModuleList, а не обычный Python list?\n",
    "        # - Зачем нужен residual connection в MoE Layer?\n",
    "        # - Как top_k влияет на вычислительную сложность?\n",
    "        # - Что произойдет, если эксперт получит 0 токенов?\n",
    "        # pass\n",
    "\n",
    "        assert hidden_size > 0, \"hidden_size должен быть > 0\"\n",
    "        assert num_experts > 0, \"num_experts должен быть > 0\"\n",
    "        assert top_k > 0 and top_k <= num_experts, \"top_k должен быть > 0 и <= num_experts\"\n",
    "        assert intermediate_size > 0, \"intermediate_size должен быть > 0\"\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.expert_dropout = expert_dropout\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.balance_loss_coef = balance_loss_coef\n",
    "\n",
    "        self.router = MoERouter(\n",
    "            hidden_size=hidden_size,\n",
    "            num_experts=num_experts,\n",
    "            top_k=top_k,\n",
    "            capacity_factor=capacity_factor,\n",
    "            balance_loss_coef=balance_loss_coef\n",
    "        )\n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(hidden_size, intermediate_size, expert_dropout)\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        training: bool = True\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Применяет MoE трансформацию к входным скрытым состояниям.\n",
    "\n",
    "            Наивная реализация через циклы (простая, но медленная):\n",
    "            1. Router выбирает экспертов\n",
    "            2. Для каждого токена:\n",
    "               - Берём top_k экспертов\n",
    "               - Обрабатываем токен каждым экспертом\n",
    "               - Взвешенное суммирование результатов\n",
    "            3. Residual connection\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            hidden_states: Входной тензор формы (batch_size, seq_len, hidden_size)\n",
    "            training: Флаг режима обучения (для balance loss)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            output: Тензор формы (batch_size, seq_len, hidden_size)\n",
    "                   Выходные скрытые состояния\n",
    "            balance_loss: Скаляр - load balancing loss\n",
    "        \"\"\"\n",
    "        # TODO: Шаг 1 - Вызовите self.router\n",
    "        # TODO: Шаг 2 - Получите размерности hidden_states.shape=\n",
    "        # TODO: Шаг 3 - Создайте output тензор\n",
    "        # TODO: Шаг 4 - Dispatch + Process + Combine (наивный подход)\n",
    "        # TODO: Шаг 5 - output = output + hidden_states\n",
    "        # TODO: Шаг 6 - Верните (output, balance_loss)\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему используем token = hidden_states[b, s:s+1, :] с s:s+1, а не s?\n",
    "        # - Зачем нужен .item() при извлечении expert_idx и weight?\n",
    "        # - Что произойдет, если убрать residual connection?\n",
    "        # - Как можно оптимизировать эти циклы?\n",
    "        # pass\n",
    "\n",
    "        # Шаг 1 - Router\n",
    "        # nn.Module.__call__ обёртка: router(...) автоматически вызывает router.forward(...)\n",
    "        routing_weights, selected_experts, balance_loss = self.router(hidden_states, training)\n",
    "\n",
    "        # Шаг 2 - Размерности\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "\n",
    "        # Шаг 3 - Output тензор\n",
    "        output = torch.zeros(batch_size, seq_len, hidden_size, device=hidden_states.device)\n",
    "\n",
    "        # Шаг 4 - Dispatch + Process + Combine (наивный подход)\n",
    "        for b in range(batch_size):\n",
    "            for s in range(seq_len):\n",
    "                token = hidden_states[b, s:s+1, :]  # (1, 1, H)\n",
    "                token_output = torch.zeros(1, 1, hidden_size, device=hidden_states.device)\n",
    "\n",
    "                for k in range(self.top_k):\n",
    "                    expert_idx = selected_experts[b, s, k].item()\n",
    "                    weight = routing_weights[b, s, k].item()\n",
    "\n",
    "                    expert_output = self.experts[expert_idx](token)  # (1, 1, H)\n",
    "\n",
    "                    token_output += weight * expert_output  # Взвешенное суммирование\n",
    "\n",
    "                output[b, s, :] = token_output.squeeze()    # Сохранение результата\n",
    "\n",
    "        # Шаг 5 - Residual connection\n",
    "        output = output + hidden_states\n",
    "\n",
    "        # Шаг 6 - Return\n",
    "        return output, balance_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ OptimizedMoELayer (векторизованный)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OptimizedMoELayer - векторизованная версия MoE для production использования.\n",
    "\n",
    "В отличие от SimpleMoELayer (учебная версия с циклами), эта реализация использует\n",
    "batch operations для максимальной производительности на GPU. Ключевая идея:\n",
    "вместо обработки токенов по одному, группируем все токены каждого эксперта\n",
    "и обрабатываем батчем.\n",
    "\n",
    "Speedup: 2-3x по сравнению с SimpleMoELayer при сохранении численной эквивалентности.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Стандартная библиотека\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Сторонние библиотеки\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Локальные импорты\n",
    "from experiments.domain.moe.router import MoERouter\n",
    "from experiments.domain.moe.expert import Expert\n",
    "\n",
    "\n",
    "class OptimizedMoELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Оптимизированная (векторизованная) реализация MoE Layer для production.\n",
    "\n",
    "        Эта версия использует batch operations вместо циклов для максимальной\n",
    "        производительности на GPU. API полностью совместим с SimpleMoELayer.\n",
    "\n",
    "        Архитектура (3 фазы):\n",
    "        Input → Router → Phase 1 (Flatten) → Phase 2 (Parallel Process) →\n",
    "        Phase 3 (Combine) → Residual → Output\n",
    "\n",
    "        Pipeline:\n",
    "        1. Router: выбирает top_k экспертов для каждого токена\n",
    "        2. Phase 1 (Flatten):\n",
    "           - Expand токены для K выборов: (B,S,H) → (B,S,K,H)\n",
    "           - Flatten всё в 1D: (B,S,K,H) → (B*S*K, H)\n",
    "        3. Phase 2 (Parallel Process):\n",
    "           - Для каждого эксперта: batch обработка всех его токенов\n",
    "           - Boolean masking: experts_flat == expert_idx\n",
    "           - Взвешивание: output * routing_weights\n",
    "        4. Phase 3 (Combine):\n",
    "           - Reshape: (B*S*K, H) → (B,S,K,H)\n",
    "           - Sum по оси K: (B,S,K,H) → (B,S,H)\n",
    "        5. Residual: добавляет входной тензор к выходному\n",
    "\n",
    "        Для модели 0.6B:\n",
    "        - num_experts = 8\n",
    "        - top_k = 2 (каждый токен → 2 эксперта)\n",
    "        - hidden_size = 512\n",
    "        - intermediate_size = 2048\n",
    "\n",
    "    Mathematical Flow:\n",
    "    ---------------\n",
    "        x ∈ ℝ^(B×S×H)\n",
    "            ↓\n",
    "        Router: (weights, experts_idx, loss) = Router(x)\n",
    "            weights ∈ ℝ^(B×S×K)      # Веса для K экспертов\n",
    "            experts_idx ∈ ℤ^(B×S×K)  # Индексы экспертов [0, N)\n",
    "            ↓\n",
    "        Flatten: x_flat ∈ ℝ^(B*S*K × H)\n",
    "            ↓\n",
    "        For each expert i in parallel:\n",
    "            mask_i = (experts_idx == i)\n",
    "            tokens_i = x_flat[mask_i]\n",
    "            outputs_i = Expert_i(tokens_i) * weights[mask_i]\n",
    "            ↓\n",
    "        Combine: reshape → sum(K) → (B×S×H)\n",
    "            ↓\n",
    "        output = output + x  # Residual connection\n",
    "            ↓\n",
    "        return output, loss\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        hidden_size: Размерность входа/выхода (должна совпадать с моделью)\n",
    "        num_experts: Количество экспертов (8 для модели 0.6B)\n",
    "        top_k: Количество активных экспертов per token (2 для модели 0.6B)\n",
    "        intermediate_size: Размерность промежуточного слоя экспертов (обычно 4*hidden_size)\n",
    "        expert_dropout: Dropout для экспертов (default: 0.0)\n",
    "        capacity_factor: Фактор емкости для Router (default: 1.25)\n",
    "        balance_loss_coef: Коэффициент для load balancing loss (default: 0.01)\n",
    "\n",
    "    Returns (from forward):\n",
    "    ---------------\n",
    "        output: Тензор формы (batch_size, seq_len, hidden_size)\n",
    "                Выходные скрытые состояния после MoE обработки\n",
    "        balance_loss: Скаляр - load balancing loss для обучения\n",
    "\n",
    "    Example:\n",
    "    ---------------\n",
    "        >>> # Создание оптимизированной MoE Layer для модели 0.6B\n",
    "        >>> moe = OptimizedMoELayer(\n",
    "        ...     hidden_size=512,\n",
    "        ...     num_experts=8,\n",
    "        ...     top_k=2,\n",
    "        ...     intermediate_size=2048\n",
    "        ... )\n",
    "        >>> x = torch.randn(2, 10, 512)  # (batch=2, seq=10, hidden=512)\n",
    "        >>> output, loss = moe(x, training=True)\n",
    "        >>> output.shape  # torch.Size([2, 10, 512])\n",
    "        >>> loss.item()   # Скаляр loss\n",
    "\n",
    "    Note:\n",
    "    ---------------\n",
    "        Эта версия для PRODUCTION использования. Использует векторизованные\n",
    "        batch операции для максимальной производительности. Численно эквивалентна\n",
    "        SimpleMoELayer (до точности float32).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_experts: int = 8,\n",
    "        top_k: int = 2,\n",
    "        intermediate_size: int = 2048,\n",
    "        expert_dropout: float = 0.0,\n",
    "        capacity_factor: float = 1.25,\n",
    "        balance_loss_coef: float = 0.01\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO(human): Валидация параметров\n",
    "        # TODO(human): Сохраните параметры как атрибуты класса\n",
    "        # TODO(human): Создайте self.router - экземпляр MoERouter\n",
    "        # TODO(human): Создайте self.experts - nn.ModuleList из num_experts экспертов\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему используем nn.ModuleList, а не обычный Python list?\n",
    "        # - Будет ли эта версия API-совместима с SimpleMoELayer?\n",
    "        # - Какие параметры влияют на memory usage?\n",
    "        # pass\n",
    "\n",
    "        assert hidden_size > 0, \"hidden_size должен быть > 0\"\n",
    "        assert num_experts > 0, \"num_experts должен быть > 0\"\n",
    "        assert top_k > 0 and top_k <= num_experts, \"top_k должен быть > 0 и <= num_experts\"\n",
    "        assert intermediate_size > 0, \"intermediate_size должен быть > 0\"\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.expert_dropout = expert_dropout\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.balance_loss_coef = balance_loss_coef\n",
    "\n",
    "        self.router = MoERouter(\n",
    "            hidden_size = hidden_size,\n",
    "            num_experts = num_experts,\n",
    "            top_k = top_k,\n",
    "            capacity_factor = capacity_factor,\n",
    "            balance_loss_coef = balance_loss_coef\n",
    "        )\n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(\n",
    "                hidden_size = hidden_size,\n",
    "                intermediate_size = intermediate_size,\n",
    "                dropout = expert_dropout\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        training: bool = True\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Применяет векторизованную MoE трансформацию к входным скрытым состояниям.\n",
    "\n",
    "            Оптимизированная реализация через batch operations:\n",
    "            1. Router выбирает экспертов для всех токенов\n",
    "            2. Phase 1 - Flatten: (B,S,H) → (B,S,K,H) → (B*S*K, H)\n",
    "            3. Phase 2 - Parallel Process: batch обработка каждым экспертом\n",
    "            4. Phase 3 - Combine: (B*S*K, H) → (B,S,K,H) → sum(K) → (B,S,H)\n",
    "            5. Residual connection\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            hidden_states: Входной тензор формы (batch_size, seq_len, hidden_size)\n",
    "            training: Флаг режима обучения (для balance loss)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            output: Тензор формы (batch_size, seq_len, hidden_size)\n",
    "                   Выходные скрытые состояния\n",
    "            balance_loss: Скаляр - load balancing loss\n",
    "\n",
    "        Shape Transformations:\n",
    "        ---------------\n",
    "            hidden_states:     (B, S, H)\n",
    "                ↓ unsqueeze(2)\n",
    "            tokens:            (B, S, 1, H)\n",
    "                ↓ expand(-1, -1, K, -1)\n",
    "            tokens_expanded:   (B, S, K, H)\n",
    "                ↓ reshape(-1, H)\n",
    "            tokens_flat:       (B*S*K, H)\n",
    "                ↓ expert processing + weighting\n",
    "            expert_outputs:    (B*S*K, H)\n",
    "                ↓ reshape(B, S, K, H)\n",
    "            expert_outputs:    (B, S, K, H)\n",
    "                ↓ sum(dim=2)\n",
    "            combined:          (B, S, H)\n",
    "                ↓ residual\n",
    "            output:            (B, S, H)\n",
    "        \"\"\"\n",
    "        # TODO(human): Шаг 0 - Router\n",
    "        #       Получите routing_weights, selected_experts, balance_loss от self.router\n",
    "\n",
    "        # TODO(human): Шаг 1 - Flatten для batch processing\n",
    "        #       1.1. Извлеките размерности: hidden_states.shape\n",
    "        #       1.2. Сохраните self.top_k\n",
    "        #       1.3. Expand токены для K выборов:\n",
    "        #            Преобразуйте hidden_states: (B, S, H) → (B, S, 1, H) → (B, S, K, H)\n",
    "        #       1.4. Flatten всё в 1D:\n",
    "        #            tokens_flat  = (B*S*K, H)\n",
    "        #            weights_flat = (B*S*K,)\n",
    "        #            experts_flat = (B*S*K,)\n",
    "\n",
    "        # TODO(human): Шаг 2 - Parallel Expert Processing\n",
    "        #       2.1. Создайте output тензор: expert_outputs\n",
    "        #       2.2. Для каждого эксперта:\n",
    "        #            a) Создайте boolean маску\n",
    "        #            b) Проверьте: (skip пустых экспертов)\n",
    "        #            c) Извлеките токены эксперта\n",
    "        #            d) Обработайте батчем\n",
    "        #            e) Взвесьте по routing_weights\n",
    "        #            f) Запишите обратно в weighted_output\n",
    "        \n",
    "        # TODO(human): Шаг 3 - Combine - суммируем K вкладов для каждого токена\n",
    "        # TODO(human): Шаг 4 - Residual connection\n",
    "        # TODO(human): Шаг 5 - Return\n",
    "        #       return output, balance_loss\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему мы используем unsqueeze(2).expand(), а не repeat()?\n",
    "        # - Зачем проверять mask.sum() > 0 перед вызовом эксперта?\n",
    "        # - Как weights_flat[mask].unsqueeze(-1) влияет на broadcasting?\n",
    "        # - Почему sum(dim=2) корректно объединяет K вкладов?\n",
    "        # - В чём разница между этой реализацией и SimpleMoELayer в плане памяти?\n",
    "        # pass\n",
    "\n",
    "        # ════════════════════════════════════════════════════════════════════\n",
    "        # Шаг 0: Router - выбираем top_k экспертов для каждого токена\n",
    "        # ════════════════════════════════════════════════════════════════════\n",
    "        routing_weights, selected_experts, load_balance_loss = self.router(hidden_states, training)\n",
    "\n",
    "        # ════════════════════════════════════════════════════════════════════\n",
    "        # Шаг 1: Flatten - подготовка для batch processing\n",
    "        # ════════════════════════════════════════════════════════════════════\n",
    "        # Извлекаем размерности входного тензора\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "\n",
    "        # Сохраняем top_k для удобства\n",
    "        top_k = self.top_k\n",
    "\n",
    "        # ────────────────────────────────────────────────────────────────────\n",
    "        # Шаг 1.1: Expand токены для K выборов (memory-efficient дублирование)\n",
    "        # ────────────────────────────────────────────────────────────────────\n",
    "        # Трансформация: (B, S, H) → (B, S, 1, H) → (B, S, K, H)\n",
    "        # unsqueeze(2): добавляем новую ось размера 1 на позиции 2\n",
    "        # expand(): \"растягиваем\" ось 1 → K (БЕЗ копирования в памяти!)\n",
    "        # Результат: каждый токен виртуально дублирован K раз (для K экспертов)\n",
    "        # Исходный токен (вектор):\n",
    "        # hidden_states[b=0, s=0] = [1, 2, 3, 4]  # shape: (H=4,)\n",
    "\n",
    "        # # После unsqueeze(2).expand(..., K=2, ...):\n",
    "        # tokens[b=0, s=0] = [\n",
    "        #     [1, 2, 3, 4],  # k=0 (копия для первого эксперта)\n",
    "        #     [1, 2, 3, 4]   # k=1 (копия для второго эксперта)\n",
    "        # ]  # shape: (K=2, H=4) - это матрица!\n",
    "        tokens = hidden_states.unsqueeze(2).expand(batch_size, seq_len, top_k, hidden_size)\n",
    "\n",
    "        # ────────────────────────────────────────────────────────────────────\n",
    "        # Шаг 1.2: Flatten всё в 1D для векторизованной обработки\n",
    "        # ────────────────────────────────────────────────────────────────────\n",
    "        # Flatten: (B, S, K, H) → (B*S*K, H)\n",
    "        # \"Разворачиваем\" 4D тензор в 2D матрицу (список векторов)\n",
    "\n",
    "        # До reshape (4D): tokens.shape = (B=2, S=3, K=2, H=4)\n",
    "        # tokens = [\n",
    "        #   # Batch 0:\n",
    "        #   [ [[1,2,3,4], [1,2,3,4]],    # s=0: матрица 2×4\n",
    "        #     [[5,6,7,8], [5,6,7,8]],    # s=1: матрица 2×4\n",
    "        #     [[9,10,11,12], [9,10,11,12]] ], # s=2: матрица 2×4\n",
    "        #   # Batch 1: аналогично...\n",
    "        # ]\n",
    "\n",
    "        # После reshape (2D): tokens_flat.shape = (B*S*K=12, H=4)\n",
    "        # tokens_flat = [\n",
    "        #   [1, 2, 3, 4],      # индекс 0: (b=0, s=0, k=0)\n",
    "        #   [1, 2, 3, 4],      # индекс 1: (b=0, s=0, k=1)\n",
    "        #   [5, 6, 7, 8],      # индекс 2: (b=0, s=1, k=0)\n",
    "        #   [5, 6, 7, 8],      # индекс 3: (b=0, s=1, k=1)\n",
    "        #   [9, 10, 11, 12],   # индекс 4: (b=0, s=2, k=0)\n",
    "        #   [9, 10, 11, 12],   # индекс 5: (b=0, s=2, k=1)\n",
    "        #   # ... batch 1: индексы 6-11\n",
    "        # ]\n",
    "        # ⚠️ Порядок flatten: сначала batch, потом sequence, потом K\n",
    "        tokens_flat = tokens.reshape(-1, hidden_size)  # (B*S*K, H)\n",
    "\n",
    "        # Flatten весов: (B, S, K) → (B*S*K,)\n",
    "        # weights_flat[i] = вес для tokens_flat[i]\n",
    "        weights_flat = routing_weights.reshape(-1)     # (B*S*K,)\n",
    "\n",
    "        # Flatten индексов экспертов: (B, S, K) → (B*S*K,)\n",
    "        # experts_flat[i] = какой эксперт должен обработать tokens_flat[i]\n",
    "        experts_flat = selected_experts.reshape(-1)    # (B*S*K,)\n",
    "\n",
    "        # ⚠️ ВАЖНО: После flatten все 3 тензора синхронизированы по индексу:\n",
    "        #   tokens_flat[i]   - токен для обработки\n",
    "        #   weights_flat[i]  - вес результата при комбинировании\n",
    "        #   experts_flat[i]  - индекс эксперта [0, num_experts)\n",
    "\n",
    "        # ════════════════════════════════════════════════════════════════════\n",
    "        # Шаг 2: Parallel Expert Processing - batch обработка каждым экспертом\n",
    "        # ════════════════════════════════════════════════════════════════════\n",
    "        # Инициализируем выходной тензор нулями (будем заполнять по маскам)\n",
    "        # Каждый токен в tokens_flat будет обработан ровно 1 раз (по своему эксперту)\n",
    "        expert_outputs = torch.zeros_like(tokens_flat)  # (B*S*K, H)\n",
    "\n",
    "        # Цикл по экспертам: каждый обрабатывает свою группу токенов батчем\n",
    "        # ⚠️ ВАЖНО: Это единственный цикл в оптимизированной версии!\n",
    "        #   SimpleMoELayer: 3 вложенных цикла (batch × sequence × top_k)\n",
    "        #   OptimizedMoELayer: 1 цикл (num_experts), внутри - batch operations\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            # ────────────────────────────────────────────────────────────────\n",
    "            # Шаг 2.1: Boolean masking - находим все токены для этого эксперта\n",
    "            # ────────────────────────────────────────────────────────────────\n",
    "            # mask = True для позиций, где experts_flat == expert_idx\n",
    "            # Например, если expert_idx=3, то mask выделит все токены,\n",
    "            # которые Router назначил третьему эксперту\n",
    "            mask = (experts_flat == expert_idx)  # (B*S*K,) - boolean тензор\n",
    "\n",
    "            # Пример mask для expert_idx=0:\n",
    "            # experts_flat = [0, 2, 0, 5, 0, 1, ...]\n",
    "            # mask         = [T, F, T, F, T, F, ...]\n",
    "            # Где T означает \"этот токен для эксперта 0\"\n",
    "\n",
    "            # ────────────────────────────────────────────────────────────────\n",
    "            # Шаг 2.2: Skip пустых экспертов (оптимизация)\n",
    "            # ────────────────────────────────────────────────────────────────\n",
    "            # Если Router не назначил ни одного токена этому эксперту, пропускаем\n",
    "            # Это экономит время на forward pass пустого эксперта\n",
    "            if mask.sum() > 0:\n",
    "                # ────────────────────────────────────────────────────────────\n",
    "                # Шаг 2.3: Извлечение токенов эксперта через boolean indexing\n",
    "                # ────────────────────────────────────────────────────────────\n",
    "                # Извлекаем только те токены, для которых mask == True\n",
    "                # Это создаёт новый тензор (компактный, без пустых мест)\n",
    "                expert_tokens = tokens_flat[mask]  # (num_selected_tokens, H)\n",
    "\n",
    "                # Пример:\n",
    "                # tokens_flat = [\n",
    "                #   [1, 2, 3, 4],   # индекс 0 (mask=True для expert_idx=0)\n",
    "                #   [5, 6, 7, 8],   # индекс 1 (mask=False)\n",
    "                #   [9, 10, 11, 12] # индекс 2 (mask=True для expert_idx=0)\n",
    "                # ]\n",
    "                # expert_tokens = [\n",
    "                #   [1, 2, 3, 4],   # из индекса 0\n",
    "                #   [9, 10, 11, 12] # из индекса 2\n",
    "                # ]  # shape: (2, H) - только выбранные токены!\n",
    "\n",
    "                # ────────────────────────────────────────────────────────────\n",
    "                # Шаг 2.4: Batch обработка экспертом\n",
    "                # ────────────────────────────────────────────────────────────\n",
    "                # Вызываем эксперта ОДИН РАЗ для ВСЕХ его токенов\n",
    "                # Это ключ к ускорению: вместо N вызовов - 1 вызов с батчем\n",
    "                output = self.experts[expert_idx](expert_tokens)  # (num_selected_tokens, H)\n",
    "\n",
    "                # ────────────────────────────────────────────────────────────\n",
    "                # Шаг 2.5: Взвешивание по routing weights\n",
    "                # ────────────────────────────────────────────────────────────\n",
    "                # Извлекаем веса для выбранных токенов (используя ту же маску)\n",
    "                # unsqueeze(-1): (num_tokens,) → (num_tokens, 1) для broadcasting\n",
    "                expert_weights = weights_flat[mask].unsqueeze(-1)  # (num_selected_tokens, 1)\n",
    "\n",
    "                # Broadcasting: (num_tokens, H) * (num_tokens, 1) → (num_tokens, H)\n",
    "                # Каждая строка output умножается на свой скалярный вес\n",
    "                # Пример:\n",
    "                # output = [[1, 2], [3, 4]]       # (2, 2)\n",
    "                # weights = [[0.7], [0.3]]        # (2, 1)\n",
    "                # result = [[0.7, 1.4], [0.9, 1.2]] # (2, 2)\n",
    "                weighted_output = output * expert_weights  # (num_selected_tokens, H)\n",
    "\n",
    "                # ────────────────────────────────────────────────────────────\n",
    "                # Шаг 2.6: Запись обратно в исходные позиции\n",
    "                # ────────────────────────────────────────────────────────────\n",
    "                # Используем ту же маску для записи результатов обратно\n",
    "                # Boolean indexing работает и для присваивания!\n",
    "                expert_outputs[mask] = weighted_output\n",
    "\n",
    "                # Визуализация процесса заполнения:\n",
    "                # До обработки: expert_outputs = [[0,0,0,0], [0,0,0,0], [0,0,0,0]]\n",
    "                # После expert_idx=0: expert_outputs = [[1,2,3,4], [0,0,0,0], [9,10,11,12]]\n",
    "                # После expert_idx=1: expert_outputs = [[1,2,3,4], [5,6,7,8], [9,10,11,12]]\n",
    "\n",
    "        # ════════════════════════════════════════════════════════════════════\n",
    "        # Шаг 3: Combine - суммируем K вкладов для каждого токена\n",
    "        # ════════════════════════════════════════════════════════════════════\n",
    "        # ────────────────────────────────────────────────────────────────────\n",
    "        # Шаг 3.1: Reshape обратно в 4D структуру\n",
    "        # ────────────────────────────────────────────────────────────────────\n",
    "        # Восстанавливаем исходную структуру: (B*S*K, H) → (B, S, K, H)\n",
    "        # Это обратная операция к flatten из Шага 1.2\n",
    "        expert_outputs = expert_outputs.reshape(batch_size, seq_len, top_k, hidden_size)\n",
    "\n",
    "        # Визуализация reshape:\n",
    "        # До reshape (2D): expert_outputs.shape = (B*S*K=12, H=4)\n",
    "        # expert_outputs_flat = [\n",
    "        #   [1, 2, 3, 4],      # (b=0, s=0, k=0) - вклад эксперта 0\n",
    "        #   [0.5, 1, 1.5, 2],  # (b=0, s=0, k=1) - вклад эксперта 2\n",
    "        #   [5, 6, 7, 8],      # (b=0, s=1, k=0)\n",
    "        #   [2.5, 3, 3.5, 4],  # (b=0, s=1, k=1)\n",
    "        #   ...\n",
    "        # ]\n",
    "        #\n",
    "        # После reshape (4D): expert_outputs.shape = (B=2, S=3, K=2, H=4)\n",
    "        # expert_outputs = [\n",
    "        #   [ # Batch 0\n",
    "        #     [[1,2,3,4], [0.5,1,1.5,2]],           # s=0: K=2 вклада\n",
    "        #     [[5,6,7,8], [2.5,3,3.5,4]],           # s=1: K=2 вклада\n",
    "        #     [[9,10,11,12], [4.5,5,5.5,6]]         # s=2: K=2 вклада\n",
    "        #   ],\n",
    "        #   [ # Batch 1: аналогично... ]\n",
    "        # ]\n",
    "\n",
    "        # ────────────────────────────────────────────────────────────────────\n",
    "        # Шаг 3.2: Суммирование по оси K (объединение вкладов экспертов)\n",
    "        # ────────────────────────────────────────────────────────────────────\n",
    "        # Суммируем по оси K (dim=2): каждый токен получает сумму от K экспертов\n",
    "        # (B, S, K, H) → sum(dim=2) → (B, S, H)\n",
    "        combined = expert_outputs.sum(dim=2)  # (B, S, H)\n",
    "\n",
    "        # Визуализация суммирования:\n",
    "        # До sum: expert_outputs[b=0, s=0] = [[1,2,3,4], [0.5,1,1.5,2]]  # (K=2, H=4)\n",
    "        # После sum: combined[b=0, s=0] = [1.5, 3, 4.5, 6]  # (H=4) - сумма вкладов!\n",
    "        #\n",
    "        # Это и есть \"взвешенное комбинирование\" выходов экспертов.\n",
    "        # Каждый эксперт внёс свой вклад (умноженный на routing_weight),\n",
    "        # а мы их суммируем для финального представления токена.\n",
    "\n",
    "        # ════════════════════════════════════════════════════════════════════\n",
    "        # Шаг 4: Residual Connection\n",
    "        # ════════════════════════════════════════════════════════════════════\n",
    "        # Добавляем исходный вход к выходу MoE слоя\n",
    "        # Это стандартная практика в Transformer архитектурах для:\n",
    "        # 1. Стабилизации обучения (градиенты текут напрямую)\n",
    "        # 2. Сохранения исходной информации (слой может научиться \"не делать ничего\")\n",
    "        # 3. Улучшения сходимости (каждый слой учит только дельту)\n",
    "        output = combined + hidden_states  # (B, S, H) + (B, S, H) → (B, S, H)\n",
    "\n",
    "        # ════════════════════════════════════════════════════════════════════\n",
    "        # Шаг 5: Return - возвращаем результат и loss\n",
    "        # ════════════════════════════════════════════════════════════════════\n",
    "        # output: финальные скрытые состояния после MoE трансформации\n",
    "        # load_balance_loss: метрика для обучения (стимулирует равномерное использование экспертов)\n",
    "        return output, load_balance_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧱 MoE Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66881cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартная библиотека\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "# Сторонние библиотеки\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Локальные импорты\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "sys.path.insert(0, parent_dir)\n",
    "from normalization.rmsnorm import RMSNorm\n",
    "from attention.gqa import GroupedQueryAttention\n",
    "from moe.moe_layer import SimpleMoELayer\n",
    "\n",
    "\n",
    "class MoETransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        MoE Transformer Block для архитектуры Qwen3. Использует Pre-Norm архитектуру\n",
    "        с Grouped-Query Attention и SimpleMoELayer вместо обычного FFN.\n",
    "\n",
    "        Архитектура блока:\n",
    "        Input → RMSNorm → GQA → Residual → RMSNorm → SimpleMoELayer → Residual → Output\n",
    "                                                           ↓\n",
    "                                                     balance_loss\n",
    "\n",
    "        Отличие от обычного TransformerBlock:\n",
    "        - SwiGLU FFN заменён на SimpleMoELayer\n",
    "        - Forward возвращает (output, balance_loss) вместо просто output\n",
    "        - balance_loss используется для обучения (предотвращение коллапса экспертов)\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        hidden_size: Размерность скрытого состояния (d_model)\n",
    "        num_query_groups: Количество групп запросов для GQA\n",
    "        num_attention_heads: Количество голов внимания для key/value\n",
    "        num_experts: Количество экспертов в MoE Layer (default: 8)\n",
    "        top_k: Количество активных экспертов per token (default: 2)\n",
    "        intermediate_size: Размерность промежуточного слоя в экспертах (default: 4 * hidden_size)\n",
    "        expert_dropout: Dropout для экспертов (default: 0.0)\n",
    "        balance_loss_coef: Коэффициент для load balancing loss (default: 0.01)\n",
    "\n",
    "    Returns (from forward):\n",
    "    ---------------\n",
    "        Если use_cache или output_attentions:\n",
    "            (hidden_states, balance_loss, present_key_value, attn_weights)\n",
    "        Иначе:\n",
    "            (hidden_states, balance_loss)\n",
    "\n",
    "    Example:\n",
    "    ---------------\n",
    "        >>> # Создание MoE Transformer Block\n",
    "        >>> block = MoETransformerBlock(\n",
    "        ...     hidden_size=512,\n",
    "        ...     num_query_groups=8,\n",
    "        ...     num_attention_heads=16,\n",
    "        ...     num_experts=8,\n",
    "        ...     top_k=2\n",
    "        ... )\n",
    "        >>> x = torch.randn(2, 10, 512)  # (batch, seq, hidden)\n",
    "        >>> output, balance_loss = block(x)\n",
    "        >>> output.shape  # torch.Size([2, 10, 512])\n",
    "        >>> balance_loss.item()  # Скаляр loss\n",
    "\n",
    "    Note:\n",
    "    ---------------\n",
    "        balance_loss должен быть добавлен к общему loss модели во время обучения:\n",
    "        total_loss = language_model_loss + balance_loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_query_groups: int,\n",
    "        num_attention_heads: int,\n",
    "        num_experts: int = 8,\n",
    "        top_k: int = 2,\n",
    "        intermediate_size: Optional[int] = None,\n",
    "        expert_dropout: float = 0.0,\n",
    "        balance_loss_coef: float = 0.01\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Проверьте валидность параметров\n",
    "        #       Подсказка: посмотрите TransformerBlock (строки 53-80)\n",
    "        #       Добавьте проверку для MoE: top_k <= num_experts\n",
    "\n",
    "        # TODO: Вычислите intermediate_size если не указан\n",
    "        #       Подсказка: какое стандартное соотношение к hidden_size?\n",
    "\n",
    "        # TODO: Сохраните параметры как атрибуты класса\n",
    "        #       Подсказка: self.hidden_size = ..., self.num_experts = ..., и т.д.\n",
    "\n",
    "        # TODO: Создайте 4 компонента (см. TransformerBlock строки 91-102):\n",
    "        #       - self.attention_norm (какой тип нормализации?)\n",
    "        #       - self.attention (какой механизм внимания?)\n",
    "        #       - self.ffn_norm (снова нормализация)\n",
    "        #       - self.moe_layer (вместо self.feed_forward!)\n",
    "        #\n",
    "        #       Вопрос: какие параметры нужны SimpleMoELayer?\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему мы заменяем FFN на MoE Layer?\n",
    "        # - Как balance_loss влияет на обучение модели?\n",
    "        # - Что произойдёт, если не добавлять balance_loss к общему loss?\n",
    "        # pass\n",
    "\n",
    "        # --- Валидация параметров -------------------------------------------------\n",
    "        assert (\n",
    "            isinstance(hidden_size, int) and hidden_size > 0\n",
    "        ), \"hidden_size должен быть положительным целым числом\"\n",
    "        assert (\n",
    "            isinstance(num_query_groups, int) and num_query_groups > 0\n",
    "        ), \"num_query_groups должен быть положительным целым числом\"\n",
    "        assert (\n",
    "            isinstance(num_attention_heads, int) and num_attention_heads > 0\n",
    "        ), \"num_attention_heads должен быть положительным целым числом\"\n",
    "\n",
    "        # MoE специфичные проверки\n",
    "        assert (\n",
    "            isinstance(num_experts, int) and num_experts > 0\n",
    "        ), \"num_experts должен быть положительным целым числом\"\n",
    "        assert (\n",
    "            top_k > 0 and top_k <= num_experts\n",
    "        ), \"top_k должен быть > 0 и <= num_experts\"\n",
    "\n",
    "        # Ключевая проверка для GQA архитектуры:\n",
    "        assert (\n",
    "            num_attention_heads % num_query_groups == 0\n",
    "        ), (\n",
    "            \"num_attention_heads должен делиться на num_query_groups для \"\n",
    "            \"корректной работы GQA\"\n",
    "        )\n",
    "        # Проверка делимости hidden_size на число голов\n",
    "        # Тензор скрытого состояния должен равномерно делиться на число голов\n",
    "        assert (\n",
    "            hidden_size % num_attention_heads == 0\n",
    "        ), \"hidden_size должен делиться на num_attention_heads\"\n",
    "\n",
    "        # Проверка intermediate_size если указан\n",
    "        if intermediate_size is not None:\n",
    "            assert (\n",
    "                isinstance(intermediate_size, int) and intermediate_size > 0\n",
    "            ), \"intermediate_size должен быть положительным целым числом\"\n",
    "\n",
    "        # --- Инициализация атрибутов ----------------------------------------------\n",
    "        self.hidden_size         = hidden_size\n",
    "        self.num_query_groups    = num_query_groups\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_experts         = num_experts\n",
    "        self.top_k               = top_k\n",
    "        self.intermediate_size = (\n",
    "            intermediate_size if intermediate_size is not None else 4 * hidden_size\n",
    "        )\n",
    "        self.expert_dropout    = expert_dropout\n",
    "        self.balance_loss_coef = balance_loss_coef\n",
    "\n",
    "        # --- Компоненты нормализации и подблоков ----------------------------------\n",
    "        self.attention_norm = RMSNorm(hidden_size)\n",
    "        self.attention = GroupedQueryAttention(\n",
    "            hidden_size=hidden_size,\n",
    "            num_query_groups=num_query_groups,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "        )\n",
    "        self.ffn_norm  = RMSNorm(hidden_size)\n",
    "        self.moe_layer = SimpleMoELayer(\n",
    "            hidden_size = hidden_size,\n",
    "            num_experts = num_experts,\n",
    "            top_k = top_k,\n",
    "            intermediate_size = self.intermediate_size,\n",
    "            expert_dropout = expert_dropout,\n",
    "            balance_loss_coef = balance_loss_coef\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        training: bool = True\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor, Optional[Tuple], Optional[torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Применяет MoE Transformer блок к входному тензору.\n",
    "\n",
    "            Pipeline:\n",
    "            1. RMSNorm → GQA → Residual\n",
    "            2. RMSNorm → SimpleMoELayer → Residual\n",
    "            3. Return (output, balance_loss)\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            hidden_states: Входной тензор формы (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: Маска внимания (optional)\n",
    "            position_ids: Позиционные индексы для RoPE (optional)\n",
    "            past_key_value: Кэш key/value для генерации (optional)\n",
    "            output_attentions: Возвращать ли attention weights (default: False)\n",
    "            use_cache: Использовать ли KV cache (default: False)\n",
    "            training: Режим обучения для balance_loss (default: True)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            Если use_cache или output_attentions:\n",
    "                (hidden_states, balance_loss, present_key_value, attn_weights)\n",
    "            Иначе:\n",
    "                (hidden_states, balance_loss)\n",
    "        \"\"\"\n",
    "        # TODO: ПЕРВЫЙ RESIDUAL BLOCK - Self-Attention (GQA)\n",
    "        #       Подсказка: скопируйте из TransformerBlock (строки 143-171)\n",
    "        #       Структура: residual → norm → attention → residual_add\n",
    "        #       Внимание: attention может вернуть tuple!\n",
    "\n",
    "        # TODO: ВТОРОЙ RESIDUAL BLOCK - MoE Feed-Forward\n",
    "        #       Подсказка: структура как в TransformerBlock (строки 177-187)\n",
    "        #       НО: self.feed_forward → self.moe_layer\n",
    "        #       ВАЖНО: moe_layer возвращает (output, balance_loss) - tuple!\n",
    "        #       Не забудьте передать параметр training\n",
    "\n",
    "        # TODO: RETURN\n",
    "        #       Вопрос: сколько значений нужно вернуть?\n",
    "        #       - Всегда: (hidden_states, balance_loss)\n",
    "        #       - Если use_cache/output_attentions: добавьте present_kv и attn_weights\n",
    "        #       Подсказка: посмотрите TransformerBlock строки 189-192\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему balance_loss возвращается вместе с hidden_states?\n",
    "        # - Как будет собираться balance_loss от всех слоёв модели?\n",
    "        # - Чем отличается forward от обычного TransformerBlock?\n",
    "        # pass\n",
    "\n",
    "        # ──────────────────────────────────────────────────────────────────────────\n",
    "        # ПЕРВЫЙ RESIDUAL BLOCK: Self-Attention (GQA)\n",
    "        # ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "        # Сохраняем вход для остаточной связи, что бы потом прибавить к выходу блока\n",
    "        # p.s. Помогает сохранить информацию о входном тензоре (векторах), чтобы не терять её.\n",
    "        residual_1 = hidden_states\n",
    "\n",
    "        # Нормализуем входной тензор\n",
    "        normed = self.attention_norm(hidden_states)\n",
    "\n",
    "        # Вызываем модуль группового внимания. Он может вернуть:\n",
    "        # - только выход (Tensor), либо\n",
    "        # - кортеж (att_output, present_key_value, attn_weights).\n",
    "        att_output = self.attention(\n",
    "            hidden_states=normed,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "\n",
    "        if isinstance(att_output, tuple):\n",
    "            att_output, present_key_value, attn_weights = att_output\n",
    "        else:\n",
    "            present_key_value = None\n",
    "            attn_weights = None\n",
    "\n",
    "        # Первая residual-связь: складываем вход и выход подблока.\n",
    "        hidden_states = att_output + residual_1\n",
    "\n",
    "        # ──────────────────────────────────────────────────────────────────────────\n",
    "        # ВТОРОЙ RESIDUAL BLOCK: MoE Feed-Forward\n",
    "        # ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "        residual_2 = hidden_states\n",
    "\n",
    "        # Нормализуем перед MoE по тем же причинам, что и перед вниманием.\n",
    "        normed = self.ffn_norm(hidden_states)\n",
    "        # Применяем MoE слой вместо обычного FFN.\n",
    "        ffn_output, balance_loss = self.moe_layer(hidden_states=normed, training=training)\n",
    "        # Вторая residual-связь.\n",
    "        hidden_states = ffn_output + residual_2\n",
    "\n",
    "        if use_cache or output_attentions:\n",
    "            return hidden_states, balance_loss, present_key_value, attn_weights\n",
    "\n",
    "        return hidden_states, balance_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧩 Полная модель Qwen3MoEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fa2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qwen3 MoE Language Model\n",
    "\n",
    "Полная реализация генеративной языковой модели с MoE архитектурой.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Tuple\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from .config import Qwen3Config\n",
    "from ..normalization.rmsnorm import RMSNorm\n",
    "from ..transformer.moe_transformer_block import MoETransformerBlock\n",
    "\n",
    "\n",
    "class Qwen3MoEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Полная генеративная языковая модель Qwen3 с MoE архитектурой.\n",
    "\n",
    "    Архитектура:\n",
    "    ------------\n",
    "    Input (batch, seq_len) — token IDs\n",
    "        ↓\n",
    "    Token Embedding (batch, seq_len, hidden_size)\n",
    "        ↓\n",
    "    N × MoE Transformer Blocks\n",
    "        ├─ RMSNorm\n",
    "        ├─ Grouped-Query Attention + RoPE\n",
    "        ├─ RMSNorm\n",
    "        └─ SimpleMoELayer (8 экспертов, 2 активных)\n",
    "        ↓\n",
    "    Final RMSNorm (batch, seq_len, hidden_size)\n",
    "        ↓\n",
    "    LM Head: Linear(hidden_size → vocab_size)\n",
    "        ↓\n",
    "    Output Logits (batch, seq_len, vocab_size)\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        config: Qwen3Config с параметрами модели\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "        embed_tokens: Token embedding layer (vocab_size → hidden_size)\n",
    "        layers: nn.ModuleList из N MoE Transformer блоков\n",
    "        norm: Final RMSNorm перед LM head\n",
    "        lm_head: Language modeling head (hidden_size → vocab_size)\n",
    "        tokenizer: GPT2Tokenizer для encode/decode текста\n",
    "\n",
    "    Examples:\n",
    "    ---------\n",
    "        >>> config = Qwen3Config()\n",
    "        >>> model = Qwen3MoEModel(config)\n",
    "        >>>\n",
    "        >>> # Forward pass\n",
    "        >>> input_ids = torch.randint(0, config.vocab_size, (2, 10))\n",
    "        >>> logits, balance_loss = model(input_ids)\n",
    "        >>> print(logits.shape)  # (2, 10, 50257)\n",
    "        >>>\n",
    "        >>> # Generation\n",
    "        >>> generated = model.generate(input_ids, max_length=50)\n",
    "        >>> print(generated.shape)  # (2, 50)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Qwen3Config, tokenizer: Optional[GPT2Tokenizer] = None):\n",
    "        super().__init__()\n",
    "        # TODO: Инициализируйте все компоненты модели\n",
    "        # Вопрос: Какие основные блоки нужны для полной модели?\n",
    "        # Подсказка: Используйте config для параметров\n",
    "        \n",
    "        # TODO: Инициализация tokenizer (если None, загрузите GPT-2)\n",
    "\n",
    "        # TODO: Преобразование token IDs → continuous vectors\n",
    "        # Вопрос: Какой PyTorch слой создаёт lookup table размера (vocab_size, hidden_size)?\n",
    "        # Подсказка: В SimpleMoELayer вы использовали nn.ModuleList. А для embeddings?\n",
    "\n",
    "        # TODO: Стек из N transformer блоков\n",
    "        # Вопрос: Как создать список из config.num_layers одинаковых блоков MoETransformerBlock(config)?\n",
    "        # Подсказка: Вспомните, как в SimpleMoELayer создавались эксперты\n",
    "\n",
    "        # TODO: Финальная нормализация скрытых состояний\n",
    "        # Вопрос: Какой компонент нормализации вы реализовали на первых этапах?\n",
    "        # Подсказка: Принимает один аргумент — размерность для нормализации\n",
    "\n",
    "        # TODO: Проекция hidden_size → vocab_size для предсказания токенов\n",
    "        # Вопрос: Какой слой проецирует векторы из одного пространства в другое?\n",
    "        # Подсказка: В LM обычно используют bias=False в финальной проекции\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему все четыре компонента должны быть атрибутами класса (self.*)?\n",
    "        # - Что произойдёт, если использовать Python list вместо nn.ModuleList?\n",
    "        # - Почему размерность embedding должна совпадать с hidden_size блоков?\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # Инициализация tokenizer для text ↔ token_ids\n",
    "        # Используем GPT-2 tokenizer (vocab_size=50257), совместимый с config\n",
    "        if tokenizer is None:\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "            # Важно: добавляем pad_token, т.к. GPT-2 изначально его не имеет\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        else:\n",
    "            self.tokenizer = tokenizer\n",
    "\n",
    "        # Token Embedding Layer: преобразование token IDs → continuous vectors\n",
    "        # Создаёт таблицу размера (y = vocab_size, x = hidden_size)\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            num_embeddings = config.vocab_size, \n",
    "            embedding_dim  = config.hidden_size\n",
    "            )\n",
    "\n",
    "        # Стек из N transformer блоков\n",
    "        # Каждый блок содержит: RMSNorm → GQA → RMSNorm → SimpleMoELayer\n",
    "        self.layers = nn.ModuleList([\n",
    "            MoETransformerBlock(\n",
    "                hidden_size=config.hidden_size,\n",
    "                num_query_groups=config.num_key_value_heads,\n",
    "                num_attention_heads=config.num_attention_heads,\n",
    "                num_experts=config.num_experts,\n",
    "                top_k=config.top_k,\n",
    "                intermediate_size=config.intermediate_size,\n",
    "                expert_dropout = config.dropout,\n",
    "                balance_loss_coef=config.balance_loss_coef\n",
    "            ) for _ in range(config.num_layers)\n",
    "        ])\n",
    "\n",
    "        # Финальная нормализация скрытых состояний\n",
    "        self.norm = RMSNorm(normalized_shape = self.config.hidden_size)\n",
    "\n",
    "        # Проекция hidden_size → vocab_size для предсказания токенов\n",
    "        # y = x Aᵀ (без bias для LM head)\n",
    "        self.lm_head = nn.Linear(\n",
    "            in_features  = self.config.hidden_size,\n",
    "            out_features = self.config.vocab_size,\n",
    "            bias         = False\n",
    "        )\n",
    "\n",
    "        # Инициализация весов\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Инициализация весов модели.\n",
    "\n",
    "        Стратегия:\n",
    "        ----------\n",
    "        - Embeddings: normal distribution N(0, initializer_range)\n",
    "        - Linear layers: уже инициализированы в sub-модулях\n",
    "        - LM Head: normal distribution N(0, initializer_range)\n",
    "        \"\"\"\n",
    "        # TODO: Инициализируйте веса эмбеддингов и LM head\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему инициализация важна для стабильного обучения?\n",
    "        # - Как влияет stddev на обучение?\n",
    "        # - Почему линейные слои не требуют дополнительной инициализации?\n",
    "\n",
    "        # Embedding initialization\n",
    "        nn.init.normal_(self.embed_tokens.weight, mean=0.0, std=self.config.initializer_range)\n",
    "\n",
    "        # LM Head initialization\n",
    "        nn.init.normal_(self.lm_head.weight, mean=0.0, std=self.config.initializer_range)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Forward pass модели.\n",
    "\n",
    "        Pipeline:\n",
    "        ---------\n",
    "        1. Token IDs → Embeddings (lookup)\n",
    "        2. Embeddings → Transformer Blocks (N раз)\n",
    "        3. Hidden States → Final Norm\n",
    "        4. Normalized States → LM Head → Logits\n",
    "        5. Accumulate balance loss из всех MoE блоков\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "            input_ids: Тензор token IDs формы (batch_size, seq_len)\n",
    "            attention_mask: Опциональная маска внимания (batch_size, seq_len)\n",
    "                           1 = attend, 0 = ignore. По умолчанию None (все токены видимы)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            logits: Тензор логитов формы (batch_size, seq_len, vocab_size)\n",
    "                   Вероятности следующего токена для каждой позиции\n",
    "            balance_loss: Скалярный тензор, сумма balance losses из всех MoE слоёв\n",
    "                         Используется для load balancing экспертов\n",
    "\n",
    "        Shape Flow:\n",
    "        -----------\n",
    "            input_ids: (B, S) → embeddings: (B, S, H)\n",
    "            → transformer blocks → hidden_states: (B, S, H)\n",
    "            → norm → normalized: (B, S, H)\n",
    "            → lm_head → logits: (B, S, V)\n",
    "\n",
    "        Examples:\n",
    "        ---------\n",
    "            >>> model = Qwen3MoEModel(config)\n",
    "            >>> input_ids = torch.randint(0, 50257, (4, 32))  # batch=4, seq=32\n",
    "            >>> logits, loss = model(input_ids)\n",
    "            >>> print(f\"Logits: {logits.shape}, Loss: {loss.item():.4f}\")\n",
    "            Logits: torch.Size([4, 32, 50257]), Loss: 0.0234\n",
    "        \"\"\"\n",
    "        # TODO: Преобразуйте input_ids → embeddings через эмбендинг слой\n",
    "        # TODO: Инициализируйте total_balance_loss нулевым тензором на device embeddings\n",
    "        # TODO: Пройдите циклом по self.layers, накапливая balance_loss\n",
    "        # TODO: Примените финальную нормализацию self.norm\n",
    "        # TODO: Спроецируйте через self.lm_head в vocab space\n",
    "        # TODO: Верните (logits, total_balance_loss)\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему важно указать device при создании total_balance_loss?\n",
    "        # - Что возвращает каждый MoE блок?\n",
    "        # - Чем logits отличаются от вероятностей?\n",
    "        # pass\n",
    "\n",
    "        # Преобразование token IDs в embeddings через lookup table\n",
    "        embeddings = self.embed_tokens(input_ids)\n",
    "        \n",
    "        # Инициализация тензора для накопления balance loss из всех MoE блоков\n",
    "        # Важно: используем device от embeddings для совместимости с GPU/CPU\n",
    "        total_balance_loss = torch.tensor(\n",
    "            data=0.0,\n",
    "            device=embeddings.device\n",
    "        )\n",
    "        \n",
    "        # Проход через все transformer блоки с накоплением balance loss\n",
    "        # Каждый layer - это экземпляр MoETransformerBlock, который возвращает:\n",
    "        # 1. Обработанные embeddings (RMSNorm → GQA → RMSNorm → SimpleMoELayer)\n",
    "        # 2. Balance loss для load balancing экспертов\n",
    "        for layer in self.layers:\n",
    "            embeddings, balance_loss = layer(embeddings, attention_mask)\n",
    "            total_balance_loss += balance_loss\n",
    "\n",
    "        # Финальная нормализация скрытых состояний перед LM head\n",
    "        final_norm = self.norm(embeddings)\n",
    "        \n",
    "        # Проекция в пространство словаря для предсказания следующего токена\n",
    "        logits = self.lm_head(final_norm)\n",
    "\n",
    "        return logits, total_balance_loss\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        max_length: int = 100,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        do_sample: bool = True,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Автогрессивная генерация текста.\n",
    "\n",
    "        Стратегия:\n",
    "        ----------\n",
    "        1. Начинаем с input_ids (prompt)\n",
    "        2. В цикле (до max_length):\n",
    "           a. Forward pass: получаем logits для следующего токена\n",
    "           b. Применяем temperature/top-k/top-p\n",
    "           c. Сэмплируем следующий токен\n",
    "           d. Добавляем токен к последовательности\n",
    "        3. Возвращаем сгенерированную последовательность\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "            input_ids: Начальная последовательность (prompt) формы (batch, seq_len)\n",
    "            max_length: Максимальная длина генерируемой последовательности\n",
    "            temperature: Температура для сэмплирования (>1 = более случайно, <1 = более детерминированно)\n",
    "            top_k: Оставить только k самых вероятных токенов (nucleus sampling)\n",
    "            top_p: Оставить минимальное множество токенов с суммарной вероятностью ≥ p\n",
    "            do_sample: True = сэмплирование, False = greedy (argmax)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            generated_ids: Сгенерированная последовательность формы (batch, max_length)\n",
    "\n",
    "        Examples:\n",
    "        ---------\n",
    "            >>> # Greedy decoding\n",
    "            >>> output = model.generate(input_ids, max_length=50, do_sample=False)\n",
    "            >>>\n",
    "            >>> # Nucleus sampling (top-p)\n",
    "            >>> output = model.generate(input_ids, temperature=0.8, top_p=0.9)\n",
    "            >>>\n",
    "            >>> # Top-k sampling\n",
    "            >>> output = model.generate(input_ids, temperature=1.0, top_k=50)\n",
    "        \"\"\"\n",
    "        # TODO: Инициализация переменных для генерации\n",
    "        # - Скопировать input_ids для безопасного изменения\n",
    "        # - Инициализировать key-value cache (если используется)\n",
    "        # - Вычислить начальную длину последовательности\n",
    "        # - Подготовить attention mask для начальной последовательности\n",
    "        \n",
    "        # TODO: Основной цикл автогрессивной генерации\n",
    "        # - while current_length < max_length:\n",
    "        #   a. Forward pass: получить logits для последнего токена\n",
    "        #   b. Извлечь logits только для последней позиции (shape: [batch, vocab_size])\n",
    "        #   c. Применить temperature scaling: logits = logits / temperature\n",
    "        #   d. Применить top-k фильтрацию (если задан top_k)\n",
    "        #   e. Применить top-p (nucleus) фильтрацию (если задан top_p)\n",
    "        #   f. Вычислить вероятности через softmax\n",
    "        #   g. Сэмплировать следующий токен (greedy или sampling)\n",
    "        #   h. Добавить токен к последовательности\n",
    "        #   i. Обновить attention mask для новой длины\n",
    "        #   j. Обновить key-value cache (если используется)\n",
    "        \n",
    "        # TODO: Обработка критериев остановки\n",
    "        # - Проверить специальные токены окончания (если есть)\n",
    "        # - Обрезать до максимальной длины\n",
    "        # - Вернуть финальную последовательность\n",
    "        \n",
    "        # TODO: Вопросы для размышления:\n",
    "        # - Как эффективно обновлять attention mask при росте последовательности?\n",
    "        # - Какой формат должен иметь key-value cache для MoE блоков?\n",
    "        # - Как обрабатывать batch с разными длинами последовательностей?\n",
    "        # - Как оптимизировать memory usage для длинных последовательностей?\n",
    "        \n",
    "        # pass\n",
    "\n",
    "        # Инициализация переменных для генерации\n",
    "        generated_ids  = input_ids.clone()\n",
    "        current_length = input_ids.shape[1]\n",
    "        # Создаем тензор такой же размерности как input_ids, но заполненный единицами\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        while current_length < max_length:\n",
    "            # a. Forward pass: получить logits для всей последовательности\n",
    "            logits, _ = self.forward(generated_ids, attention_mask)\n",
    "\n",
    "            # b. Извлечь logits только для последней позиции (shape: [batch, vocab_size])\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # c. Применить temperature scaling\n",
    "            # -------------------------------------------------------------\n",
    "            # Это стандартная формула в LLM!\n",
    "            # Математика: temperature \"сжимает\" или \"растягивает\" logits\n",
    "\n",
    "            # Temperature > 1: \"разогревает\" распределение\n",
    "            # - Более равномерные вероятности\n",
    "            # - Больше случайности в генерации\n",
    "\n",
    "            # Temperature < 1: \"охлаждает\" распределение\n",
    "            # - Более острые пики вероятностей\n",
    "            # - Более детерминированная генерация\n",
    "\n",
    "            # Temperature = 1: без изменений (стандартный softmax)\n",
    "            probabilities = torch.softmax(logits / temperature, dim=-1)\n",
    "            # -------------------------------------------------------------\n",
    "\n",
    "            # d. Применить top-k фильтрацию (если задан)\n",
    "            # e. Применить top-p фильтрацию (если задан)\n",
    "            # -------------------------------------------------------------\n",
    "            # TOP-K алгоритм:\n",
    "            # 1. Найти k самых вероятных токенов\n",
    "            # 2. Обнулить ВСЕ остальные вероятности\n",
    "            # 3. Оставить только top-k токенов\n",
    "\n",
    "            # TOP-P (nucleus) алгоритм:\n",
    "            # 1. Отсортировать токены по убыванию вероятности\n",
    "            # 2. Накапливать вероятности до достижения порога p\n",
    "            # 3. Обнулить все токены после этого порога\n",
    "\n",
    "            # Пример:\n",
    "            # probabilities = [0.4, 0.3, 0.2, 0.1]\n",
    "            # top_k=2:   [0.4, 0.3, 0.0, 0.0]  # только 2 лучших\n",
    "            # top_p=0.6: [0.4, 0.3, 0.0, 0.0]  # накопили до 0.7 > 0.6\n",
    "            if top_k is not None:\n",
    "                top_k_values, top_k_indices = torch.topk(probabilities, top_k)\n",
    "                probabilities_filtered = torch.zeros_like(probabilities)\n",
    "                probabilities_filtered.scatter_(dim=-1, index=top_k_indices, src=top_k_values)\n",
    "                probabilities = probabilities_filtered\n",
    "\n",
    "            if top_p is not None:\n",
    "                # Сортируем вероятности по убыванию\n",
    "                sorted_probs, sorted_indices = torch.sort(probabilities, descending=True, dim=-1)\n",
    "                # Вычисляем накопленную сумму\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "                # Находим токены, которые нужно удалить (cumsum > top_p)\n",
    "                # Сдвигаем на 1 вправо, чтобы сохранить хотя бы первый токен\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = False\n",
    "                # Обнуляем вероятности для удаляемых токенов\n",
    "                sorted_probs[sorted_indices_to_remove] = 0.0\n",
    "                # Возвращаем вероятности в исходный порядок\n",
    "                probabilities = torch.zeros_like(probabilities)\n",
    "                probabilities.scatter_(dim=-1, index=sorted_indices, src=sorted_probs)\n",
    "            # -------------------------------------------------------------\n",
    "\n",
    "            # Ре-нормализуем вероятности после фильтрации\n",
    "            probabilities = probabilities / probabilities.sum(dim=-1, keepdim=True)\n",
    "\n",
    "            # g. Сэмплировать следующий токен (greedy или sampling)\n",
    "            if do_sample:\n",
    "                # Стохастическое сэмплирование из распределения\n",
    "                next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            else:\n",
    "                # Greedy decoding: выбираем токен с максимальной вероятностью\n",
    "                next_token = torch.argmax(probabilities, dim=-1, keepdim=True)\n",
    "\n",
    "            # h. Добавить токен к последовательности\n",
    "            generated_ids = torch.cat([generated_ids, next_token], dim=-1)\n",
    "\n",
    "            # i. Обновить attention mask для новой длины\n",
    "            attention_mask = torch.cat(\n",
    "                [attention_mask, torch.ones_like(next_token)],\n",
    "                dim=-1\n",
    "            )\n",
    "\n",
    "            # j. Обновить длину последовательности\n",
    "            current_length += 1\n",
    "\n",
    "        # Обработка критериев остановки и возврат результата\n",
    "        return generated_ids\n",
    "\n",
    "\n",
    "    def chat(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_length: int = 100,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        do_sample: bool = True,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Высокоуровневый интерфейс text-to-text.\n",
    "\n",
    "        Pipeline:\n",
    "        ---------\n",
    "        1. Encode: prompt (str) → token_ids (tensor)\n",
    "        2. Generate: token_ids → generated_ids (автогрессивная генерация)\n",
    "        3. Decode: generated_ids → response (str)\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "            prompt: Входной текст от пользователя\n",
    "            max_length: Максимальная длина сгенерированного текста (в токенах)\n",
    "            temperature: Температура для сэмплирования (по умолчанию 1.0)\n",
    "            top_k: Количество токенов для top-k фильтрации (опционально)\n",
    "            top_p: Порог для nucleus sampling (опционально)\n",
    "            do_sample: True = стохастическое сэмплирование, False = greedy decoding\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            response: Сгенерированный текст (включая исходный prompt)\n",
    "\n",
    "        Examples:\n",
    "        ---------\n",
    "            >>> # Greedy decoding (детерминированный)\n",
    "            >>> response = model.chat(\"Once upon a time\", do_sample=False)\n",
    "            >>> print(response)\n",
    "            \"Once upon a time there was a kingdom...\"\n",
    "\n",
    "            >>> # Nucleus sampling (более креативный)\n",
    "            >>> response = model.chat(\"Hello world\", temperature=0.8, top_p=0.9)\n",
    "            >>> print(response)\n",
    "            \"Hello world! How are you doing today?\"\n",
    "\n",
    "            >>> # Top-k sampling\n",
    "            >>> response = model.chat(\"The quick brown\", temperature=1.0, top_k=40)\n",
    "            >>> print(response)\n",
    "            \"The quick brown fox jumps over the lazy dog\"\n",
    "        \"\"\"\n",
    "        # TODO: Реализуйте три шага: Encode → Generate → Decode\n",
    "        # TODO: Используйте self.tokenizer для encode/decode\n",
    "        # TODO: Используйте self.generate() для генерации\n",
    "        # TODO: Верните сгенерированный текст\n",
    "\n",
    "        # Вопросы для размышления:\n",
    "        # - Почему важно использовать тот же tokenizer, что и при обучении?\n",
    "        # - Как обработать специальные токены (BOS/EOS/PAD) при encode/decode?\n",
    "        # - Как гарантировать, что сгенерированный текст не превышает max_length?\n",
    "\n",
    "\n",
    "        # Шаг 1: Encode - преобразование текста в token IDs\n",
    "        # return_tensors=\"pt\" возвращает PyTorch тензоры\n",
    "        # add_special_tokens=True добавляет специальные токены (BOS/EOS если есть)\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Перемещаем на то же устройство, где находится модель\n",
    "        # Проверяем device через параметры модели (например, embed_tokens.weight)\n",
    "        device = next(self.parameters()).device\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        # Шаг 2: Generate - автогрессивная генерация через self.generate()\n",
    "        generated_ids = self.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample\n",
    "        )\n",
    "\n",
    "        # Шаг 3: Decode - преобразование token IDs обратно в текст\n",
    "        # skip_special_tokens=True удаляет специальные токены (BOS/EOS/PAD)\n",
    "        # Берём первую (и единственную) последовательность из batch: generated_ids[0]\n",
    "        response = self.tokenizer.decode(\n",
    "            generated_ids[0],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        return response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
