# ü§ñ Qwen3 0.6B MoE - Mixture-of-Experts Transformer

> –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-of-Experts Transformer —Å –Ω—É–ª—è –Ω–∞ PyTorch

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Tests](https://img.shields.io/badge/Tests-110%20passing-success.svg)]()
[![Progress](https://img.shields.io/badge/Progress-92%25-brightgreen.svg)]()

---

## üìã –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞

–î–∞–Ω–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π **–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é** –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Mixture-of-Experts (MoE) Transformer, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –º–æ–¥–µ–ª–∏ Qwen3, –Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –¥–ª—è –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (0.6B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤).

### –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:

- ‚úÖ **–ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å –Ω—É–ª—è** - –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω—ã –≤—Ä—É—á–Ω—É—é
- ‚úÖ **–î–µ—Ç–∞–ª—å–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ** - 110 unit —Ç–µ—Å—Ç–æ–≤ (–≤—Å–µ –ø—Ä–æ—Ö–æ–¥—è—Ç)
- ‚úÖ **–ü–æ–¥—Ä–æ–±–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è** - –∫–∞–∂–¥—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
- ‚úÖ **–£—á–µ–±–Ω–∞—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å** - TODO-—à–∞–±–ª–æ–Ω—ã –∏ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è
- ‚úÖ **DDD –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** - —á–∏—Å—Ç–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –∫–æ–¥–∞
- ‚úÖ **Text-to-text –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å** - –º–µ—Ç–æ–¥ chat() —Å GPT-2 tokenizer
- ‚úÖ **–ì–æ—Ç–æ–≤–∞ –∫ –æ–±—É—á–µ–Ω–∏—é** - –ø–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å Qwen3MoEModel —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞

---

## üéØ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏

### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Qwen3 0.6B MoE

```
Model Size:        0.6B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
Vocab Size:        50257 (GPT-2 tokenizer)
Num Layers:        12 MoE Transformer Blocks
Num Experts:       8 (–≤–º–µ—Å—Ç–æ 128 –≤ Qwen3-30B)
Active Experts:    2 per token (–≤–º–µ—Å—Ç–æ 8 –≤ Qwen3-30B)
Activation:        25% —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (vs 6.25% –≤ 30B)
Hidden Size:       1024
Intermediate Size: 2048 (2 √ó hidden_size per expert)
Attention Heads:   16 query heads, 4 KV heads (GQA ratio 4:1)
Max Seq Length:    2048 tokens
Normalization:     RMSNorm
Position Encoding: RoPE (Rotary Position Embedding)
Activation:        SwiGLU
Dropout:           0.1
```

### –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

```
Qwen3 MoE Model
‚îÇ
‚îú‚îÄ‚îÄ Embedding Layer
‚îÇ
‚îú‚îÄ‚îÄ N √ó MoE Transformer Blocks
‚îÇ   ‚îú‚îÄ‚îÄ RMSNorm
‚îÇ   ‚îú‚îÄ‚îÄ Grouped-Query Attention (GQA)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Query projection (8 groups)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Key/Value projection (shared)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ RoPE position encoding
‚îÇ   ‚îú‚îÄ‚îÄ RMSNorm
‚îÇ   ‚îî‚îÄ‚îÄ MoE Feed-Forward Layer
‚îÇ       ‚îú‚îÄ‚îÄ MoE Router (Top-K gating)
‚îÇ       ‚îú‚îÄ‚îÄ 8 √ó Expert Networks (SwiGLU)
‚îÇ       ‚îî‚îÄ‚îÄ Load Balancing Loss
‚îÇ
‚îî‚îÄ‚îÄ LM Head (Output projection)
```

---

## üìä –ü—Ä–æ–≥—Ä–µ—Å—Å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### ‚úÖ –§–∞–∑–∞ 1: –ë–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (100%)

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –°—Ç–∞—Ç—É—Å | –¢–µ—Å—Ç—ã | –§–∞–π–ª |
|-----------|--------|-------|------|
| RMSNorm | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ –í—Å–µ | `experiments/domain/normalization/rmsnorm.py` |
| RoPE | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ –í—Å–µ | `experiments/domain/positional_encoding/rope.py` |
| SwiGLU | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ –í—Å–µ | `experiments/domain/activations/swiglu.py` |
| GQA | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ 12/12 | `experiments/domain/attention/gqa.py` |
| TransformerBlock | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ 14/14 | `experiments/domain/transformer/transformer_block.py` |

### ‚úÖ –§–∞–∑–∞ 2: MoE –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (100%)

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –°—Ç–∞—Ç—É—Å | –¢–µ—Å—Ç—ã | –§–∞–π–ª |
|-----------|--------|-------|------|
| MoE Router | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ 15/15 | `experiments/domain/moe/router.py` |
| Expert Network | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ 15/15 | `experiments/domain/moe/expert.py` |
| SimpleMoELayer | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ 14/14 | `experiments/domain/moe/moe_layer.py` |
| MoE TransformerBlock | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ 17/17 | `experiments/domain/transformer/moe_transformer_block.py` |

### ‚úÖ –§–∞–∑–∞ 3: –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å (100%)

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –°—Ç–∞—Ç—É—Å | –¢–µ—Å—Ç—ã | –§–∞–π–ª |
|-----------|--------|-------|------|
| Qwen3Config | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | N/A | `experiments/domain/model/config.py` |
| Qwen3MoEModel | ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ | ‚úÖ 19/19 | `experiments/domain/model/qwen3_model.py` |

**–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:**
- ‚úÖ Token embedding layer (vocab_size ‚Üí hidden_size)
- ‚úÖ 12 √ó MoE Transformer Blocks —á–µ—Ä–µ–∑ nn.ModuleList
- ‚úÖ Final RMSNorm –ø–µ—Ä–µ–¥ LM head
- ‚úÖ LM head (hidden_size ‚Üí vocab_size, –±–µ–∑ bias)
- ‚úÖ Weight initialization (_init_weights)
- ‚úÖ Forward pass —Å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ–º balance_loss
- ‚úÖ Autoregressive generation —Å temperature/top-k/top-p sampling

### ‚è≥ –§–∞–∑–∞ 4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (0%)

- ‚è≥ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (WikiText-2 –∏–ª–∏ tiny shakespeare)
- ‚è≥ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ GPT-2 tokenizer
- ‚è≥ Training loop —Å AdamW optimizer
- ‚è≥ Validation –∏ –º–µ—Ç—Ä–∏–∫–∏ (perplexity, accuracy)
- ‚è≥ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ checkpoints

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install torch pytest
```

### –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤

```bash
# –í—Å–µ —Ç–µ—Å—Ç—ã
pytest experiments/ -v

# –û—Ç–¥–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
pytest experiments/domain/moe/test/test_router.py -v
pytest experiments/domain/moe/test/test_expert.py -v
pytest experiments/domain/attention/test/test_gqa.py -v
```

### –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ MoE

```bash
python3 experiments/domain/moe/test_integration.py
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏

```python
import torch
from experiments.domain.model.config import Qwen3Config
from experiments.domain.model.qwen3_model import Qwen3MoEModel

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
config = Qwen3Config(
    vocab_size=50257,
    hidden_size=1024,
    num_layers=12,
    num_experts=8,
    top_k=2
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (GPT-2 tokenizer –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
model = Qwen3MoEModel(config)

# ============================================
# 1. Text-to-Text –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø)
# ============================================
response = model.chat(
    "Once upon a time",
    max_length=50,
    temperature=0.8,
    top_k=40,
    top_p=0.9,
    do_sample=True
)
print(response)
# Output: "Once upon a time there was a..."

# ============================================
# 2. Forward pass (–¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
# ============================================
input_ids = torch.randint(0, config.vocab_size, (2, 10))
logits, balance_loss = model(input_ids)
# logits: (batch=2, seq=10, vocab=50257)
# balance_loss: scalar (–¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∫ CE loss)

# ============================================
# 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å token IDs
# ============================================
generated_ids = model.generate(
    input_ids=torch.tensor([[1, 2, 3]]),  # prompt
    max_length=50,
    temperature=0.8,
    top_k=40,
    top_p=0.9,
    do_sample=True
)
# generated_ids: (1, 50) - –∞–≤—Ç–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

```python
import torch
from experiments.domain.moe.router import MoERouter
from experiments.domain.moe.expert import Expert

# –°–æ–∑–¥–∞–Ω–∏–µ Router –¥–ª—è –º–æ–¥–µ–ª–∏ 0.6B
router = MoERouter(
    hidden_size=1024,
    num_experts=8,
    top_k=2
)

# –°–æ–∑–¥–∞–Ω–∏–µ Expert Network
expert = Expert(
    hidden_size=1024,
    intermediate_size=2048
)

# Forward pass
x = torch.randn(2, 10, 1024)  # (batch, seq, hidden)
routing_weights, selected_experts, balance_loss = router(x)
```

---

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
experiments/
‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îú‚îÄ‚îÄ normalization/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rmsnorm.py              # RMS Normalization
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ positional_encoding/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rope.py                 # Rotary Position Embedding
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ activations/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ swiglu.py              # SwiGLU Activation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SwiGLU.md              # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ attention/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gqa.py                 # Grouped-Query Attention
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GQA.md                 # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GQA_Forward_Explained.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ transformer/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformer_block.py       # Transformer Block
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ moe_transformer_block.py   # MoE Transformer Block
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ moe/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ router.py              # MoE Router (Top-K gating)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ expert.py              # Expert Network
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ moe_layer.py           # MoE Layer (–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MoE_Router_Gate_Initialization.md     # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (885 —Å—Ç—Ä–æ–∫)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MoE_Router_Load_Balancing_Loss.md     # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (1067 —Å—Ç—Ä–æ–∫)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ModuleList_Explained.md                # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (400+ —Å—Ç—Ä–æ–∫)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_integration.py    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ   ‚îî‚îÄ‚îÄ model/
‚îÇ       ‚îú‚îÄ‚îÄ config.py              # Qwen3Config (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏)
‚îÇ       ‚îú‚îÄ‚îÄ qwen3_model.py         # Qwen3MoEModel (–ø–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å)
‚îÇ       ‚îî‚îÄ‚îÄ test/
‚îÇ           ‚îî‚îÄ‚îÄ test_qwen3_model.py  # 19 –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤
‚îÇ
‚îî‚îÄ‚îÄ memory/
    ‚îú‚îÄ‚îÄ memory-bank/               # –ë–∞–Ω–∫ –ø–∞–º—è—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞
    ‚îÇ   ‚îú‚îÄ‚îÄ projectbrief.md
    ‚îÇ   ‚îú‚îÄ‚îÄ activeContext.md
    ‚îÇ   ‚îú‚îÄ‚îÄ progress.md
    ‚îÇ   ‚îú‚îÄ‚îÄ techContext.md
    ‚îÇ   ‚îî‚îÄ‚îÄ systemPatterns.md
    ‚îî‚îÄ‚îÄ rules/
        ‚îî‚îÄ‚îÄ memory-bank.mdc        # –í—ã—É—á–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
```

---

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ—Å—Ç–æ–≤

```
–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: 110
–£—Å–ø–µ—à–Ω–æ:      110 (100%)
–ü–æ–∫—Ä—ã—Ç–∏–µ:     –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤–∫–ª—é—á–∞—è –ø–æ–ª–Ω—É—é –º–æ–¥–µ–ª—å –∏ chat()

Breakdown:
‚îú‚îÄ‚îÄ RMSNorm:             ‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã
‚îú‚îÄ‚îÄ RoPE:                ‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã
‚îú‚îÄ‚îÄ SwiGLU:              ‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã
‚îú‚îÄ‚îÄ GQA:                 ‚úÖ 12/12
‚îú‚îÄ‚îÄ TransformerBlock:    ‚úÖ 14/14
‚îú‚îÄ‚îÄ MoE Router:          ‚úÖ 15/15
‚îú‚îÄ‚îÄ Expert Network:      ‚úÖ 15/15
‚îú‚îÄ‚îÄ SimpleMoELayer:      ‚úÖ 14/14
‚îú‚îÄ‚îÄ MoE TransformerBlock: ‚úÖ 17/17
‚îú‚îÄ‚îÄ Qwen3MoEModel (core): ‚úÖ 19/19
‚îî‚îÄ‚îÄ Qwen3MoEModel (chat): ‚úÖ 7/7
```

### –¢–∏–ø—ã —Ç–µ—Å—Ç–æ–≤

- ‚úÖ **–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è**: –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∞—Ç—Ä–∏–±—É—Ç–æ–≤
- ‚úÖ **Forward pass**: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ç–µ–Ω–∑–æ—Ä–æ–≤, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
- ‚úÖ **Gradient flow**: —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
- ‚úÖ **–ß–∏—Å–ª–µ–Ω–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å**: –±–æ–ª—å—à–∏–µ/–º–∞–ª–µ–Ω—å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
- ‚úÖ **–î–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º**: –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- ‚úÖ **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è**: –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

---

## üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

### –ü–æ–¥—Ä–æ–±–Ω—ã–µ –≥–∞–π–¥—ã

- **[SwiGLU.md](experiments/domain/activations/SwiGLU.md)** - –ê–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
- **[GQA.md](experiments/domain/attention/GQA.md)** - Grouped-Query Attention –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- **[GQA_Forward_Explained.md](experiments/domain/attention/GQA_Forward_Explained.md)** - –ü–æ—Å—Ç—Ä–æ—á–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ forward pass
- **[MoE_Router_Gate_Initialization.md](experiments/domain/moe/MoE_Router_Gate_Initialization.md)** - –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è gate layer (885 —Å—Ç—Ä–æ–∫)
- **[MoE_Router_Load_Balancing_Loss.md](experiments/domain/moe/MoE_Router_Load_Balancing_Loss.md)** - Load balancing loss (1067 —Å—Ç—Ä–æ–∫)
- **[ModuleList_Explained.md](experiments/domain/moe/ModuleList_Explained.md)** - PyTorch ModuleList –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ (400+ —Å—Ç—Ä–æ–∫)

### –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏

#### Grouped-Query Attention (GQA)
–°–Ω–∏–∂–∞–µ—Ç —Ä–∞–∑–º–µ—Ä KV cache –∑–∞ —Å—á—ë—Ç –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤:
```
Query Heads:   16 –≥–æ–ª–æ–≤ (num_attention_heads)
KV Heads:      4 –≥–æ–ª–æ–≤—ã (num_key_value_heads)
Group Size:    4 query heads per KV head
Head Dim:      64 (hidden_size / num_attention_heads = 1024 / 16)
Memory Saving: 4x –º–µ–Ω—å—à–µ KV cache –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Multi-Head Attention
```

#### Load Balancing Loss
–ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç "–∫–æ–ª–ª–∞–ø—Å —ç–∫—Å–ø–µ—Ä—Ç–æ–≤":
```python
L = Œ± * N * Œ£(f_i * P_i)
–≥–¥–µ:
  f_i = frequency (–∫–∞–∫ —á–∞—Å—Ç–æ —ç–∫—Å–ø–µ—Ä—Ç –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è)
  P_i = mean probability (—Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏)
  Œ± = balance_loss_coef (0.01)
  N = num_experts (8)
```

---

## üéì –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å

### –î–ª—è –∫–æ–≥–æ —ç—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç

- üìö **–°—Ç—É–¥–µ–Ω—Ç—ã ML/DL** - –∏–∑—É—á–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Transformer –∏ MoE —Å –Ω—É–ª—è
- üî¨ **–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏** - —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å MoE –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
- üë®‚Äçüíª **ML –∏–Ω–∂–µ–Ω–µ—Ä—ã** - –ø–æ–Ω–∏–º–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ LLM
- üè´ **–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–∏** - —É—á–µ–±–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª —Å TODO-—à–∞–±–ª–æ–Ω–∞–º–∏

### –ü–µ–¥–∞–≥–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥

- ‚úÖ **TODO-driven development** - –ø–æ—à–∞–≥–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
- ‚úÖ **–í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è** - —Ä–∞–∑–≤–∏—Ç–∏–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è
- ‚úÖ **–ü–æ–¥—Ä–æ–±–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏** - –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏
- ‚úÖ **–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã** - —Ç–µ–æ—Ä–∏—è + –ø—Ä–∞–∫—Ç–∏–∫–∞
- ‚úÖ **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏** - ASCII-art –¥–∏–∞–≥—Ä–∞–º–º—ã –ø—Ä–æ—Ü–µ—Å—Å–æ–≤

---

## üîç –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### 1. RMSNorm vs LayerNorm
```python
# RMSNorm: x / sqrt(mean(x¬≤) + eps) * weight
# –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
# - –ù–µ—Ç —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è (–Ω–µ –≤—ã—á–∏—Ç–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ)
# - –õ—É—á—à–∞—è —á–∏—Å–ª–µ–Ω–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
# - –ú–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
# - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ LLaMA, Qwen
```

### 2. RoPE Position Encoding
```python
# Rotary Position Embedding
# - –í—Ä–∞—â–µ–Ω–∏–µ –≤ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –ø–ª–æ—Å–∫–æ—Å—Ç–∏
# - –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
# - –•–æ—Ä–æ—à–æ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
```

### 3. SwiGLU Activation
```python
# SwiGLU(x, W1, W2) = Swish(W1¬∑x) ‚äô (W2¬∑x)
# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ PaLM, LLaMA, Qwen3
# –õ—É—á—à–µ —á–µ–º ReLU/GELU –≤ –≥–ª—É–±–æ–∫–∏—Ö –º–æ–¥–µ–ª—è—Ö
```

### 4. MoE Router
```python
# Top-K gating —Å load balancing
# 1. Gate projection: logits = Linear(hidden_states)
# 2. Softmax: probabilities = softmax(logits)
# 3. Top-K: –≤—ã–±–æ—Ä K –ª—É—á—à–∏—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
# 4. Re-normalization: normalize(selected weights)
# 5. Balance loss: –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –∫–æ–ª–ª–∞–ø—Å–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
```

### 5. Autoregressive Generation
```python
# –¢—Ä–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞:

# 1. Temperature scaling - –∫–æ–Ω—Ç—Ä–æ–ª—å "–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏"
probabilities = softmax(logits / temperature)
# temperature < 1.0 ‚Üí –±–æ–ª–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä (—Ñ–æ–∫—É—Å –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö)
# temperature > 1.0 ‚Üí –±–æ–ª–µ–µ —Å–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä (—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ)

# 2. Top-k sampling - –≤—ã–±–æ—Ä –∏–∑ k –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
top_k_probs, top_k_indices = torch.topk(probabilities, k=40)
# –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤—ã–±–æ—Ä —Ç–æ–ª—å–∫–æ —Ç–æ–ø-40 —Ç–æ–∫–µ–Ω–∞–º–∏

# 3. Top-p (nucleus) sampling - –≤—ã–±–æ—Ä –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤ —Å –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é p
cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
mask = cumulative_probs > p  # p=0.9 ‚Üí –±–µ—Ä—ë–º 90% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–π –º–∞—Å—Å—ã
# –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
```

---

## ü§ù –í–∫–ª–∞–¥ –≤ –ø—Ä–æ–µ–∫—Ç

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Å–æ–∑–¥–∞–Ω –≤ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª—è—Ö. –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –≤–Ω–µ—Å—Ç–∏ –≤–∫–ª–∞–¥:

1. üêõ **–ù–∞–π–¥–µ–Ω–∞ –æ—à–∏–±–∫–∞?** –û—Ç–∫—Ä–æ–π—Ç–µ issue —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º
2. üí° **–ò–¥–µ—è —É–ª—É—á—à–µ–Ω–∏—è?** –ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ –≤ discussions
3. üìù **–£–ª—É—á—à–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏?** Pull request –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É–µ—Ç—Å—è
4. ‚ú® **–ù–æ–≤—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç?** –°–ª–µ–¥—É–π—Ç–µ —Å—Ç–∏–ª—é –ø—Ä–æ–µ–∫—Ç–∞

---

## üìñ –û–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –ø–æ—Ä—è–¥–æ–∫ –∏–∑—É—á–µ–Ω–∏—è

1. **–ë–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã**:
   - RMSNorm ‚Üí RoPE ‚Üí SwiGLU
   - Grouped-Query Attention
   - TransformerBlock

2. **MoE –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã**:
   - MoE Router (gating mechanism)
   - Expert Network (feed-forward)
   - MoE Layer (–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)

3. **–ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å**:
   - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (Qwen3Config)
   - Embedding + N√óBlocks + LM Head
   - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (temperature/top-k/top-p)

4. **–û–±—É—á–µ–Ω–∏–µ** (–ø—Ä–µ–¥—Å—Ç–æ—è—â–µ–µ):
   - –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
   - Training loop —Å optimizer
   - Evaluation –∏ –º–µ—Ç—Ä–∏–∫–∏

### –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
python3 experiments/domain/moe/test_integration.py

# –í—ã —É–≤–∏–¥–∏—Ç–µ:
# - –ö–∞–∫ Router –≤—ã–±–∏—Ä–∞–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
# - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —ç–∫—Å–ø–µ—Ä—Ç–∞–º
# - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é load balancing
# - –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è MoE Layer
```

---

## üìà –ú–µ—Ç—Ä–∏–∫–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

### –ü—Ä–æ–≥—Ä–µ—Å—Å –ø—Ä–æ–µ–∫—Ç–∞
```
–û–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å:    92%
–°—Ç—Ä–æ–∫ –∫–æ–¥–∞:        ~5200+ (—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
–°—Ç—Ä–æ–∫ —Ç–µ—Å—Ç–æ–≤:      ~4200+
–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:      ~6400+ —Å—Ç—Ä–æ–∫ –≤ .md —Ñ–∞–π–ª–∞—Ö
–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏:       –ü–æ–¥—Ä–æ–±–Ω—ã–µ –≤ –∫–∞–∂–¥–æ–º —Ñ–∞–π–ª–µ
–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:        10/10 —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ (100%)
–£–ª—É—á—à–µ–Ω–∏—è:         1/2 (Tokenizer ‚úÖ, –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è MoE ‚è≥)
```

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
```
–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏:     0.6B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
–ê–∫—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: ~0.15B (25% –∑–∞ —Å—á—ë—Ç MoE)
–ü–∞–º—è—Ç—å:            –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —á–µ—Ä–µ–∑ GQA (4x KV cache saving)
–¢–µ—Å—Ç—ã:             110 —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ—Ö–æ–¥—è—Ç –∑–∞ ~35 —Å–µ–∫—É–Ω–¥
–ì–µ–Ω–µ—Ä–∞—Ü–∏—è:         –ü–æ–¥–¥–µ—Ä–∂–∫–∞ temperature/top-k/top-p sampling
Text Interface:    chat() –º–µ—Ç–æ–¥ —Å GPT-2 tokenizer ‚úÖ
```

---

## üîó –°—Å—ã–ª–∫–∏ –∏ —Ä–µ—Å—É—Ä—Å—ã

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- [Qwen3 Technical Report](https://arxiv.org/abs/2409.12186)
- [Mixture-of-Experts with Expert Choice Routing](https://arxiv.org/abs/2202.09368)
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
- [GQA: Training Generalized Multi-Query Transformer Models](https://arxiv.org/abs/2305.13245)

### –†–µ–∞–ª–∏–∑–∞—Ü–∏–∏
- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [PyTorch](https://pytorch.org/)

---

## üìù –õ–∏—Ü–µ–Ω–∑–∏—è

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Å–æ–∑–¥–∞–Ω –≤ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–ª—è—Ö. –ö–æ–¥ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è "–∫–∞–∫ –µ—Å—Ç—å" –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã MoE Transformer.

---

## üôè –ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏

–û—Å–æ–±–∞—è –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å:
- –ö–æ–º–∞–Ω–¥–µ Qwen3 –∑–∞ –æ—Ç–∫—Ä—ã—Ç—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
- –°–æ–æ–±—â–µ—Å—Ç–≤—É PyTorch –∑–∞ –æ—Ç–ª–∏—á–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫
- –í—Å–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º –≤ –æ–±–ª–∞—Å—Ç–∏ MoE –∏ Transformers

---

<div align="center">

**Made with ‚ù§Ô∏è for ML Education**

‚≠ê Star this repo if you found it helpful!

</div>
