# Машины непрерывного мышления (Continuous Thought Machines): внедрение нейронной синхронизации в качестве основы для искусственного интеллекта

## Введение и мотивация

Современные нейросети добились больших успехов, но остаются упрощёнными по сравнению с биологическим мозгом. В частности, они обычно не учитывают **временную динамику нейронов** – **точное время спайков и синхронизацию активности**, которые играют ключевую роль в биологических нейросетях. Как правило, искусственные нейроны выдают лишь одно статическое значение активации, игнорируя *когда* нейрон активируется относительно других. Биологические принципы вроде зависимой от времени спайков пластичности (STDP) указывают, что тайминг важен для обучения и обработки информации мозгом. Разрыв между гибким человеческим мышлением и текущим ИИ позволяет предположить, что в моделях отсутствуют некоторые фундаментальные компоненты, связанные с **временной обработкой** сигналов.

**Continuous Thought Machine (CTM)** – это новая архитектура нейросети, предложенная Sakana AI, которая *возвращает время в основу вычислений нейросети*. Модель CTM специально разработана, чтобы использовать синхронизацию активности нейронов как механизм рассуждения. В отличие от традиционных сетей, CTM снабжает каждый нейрон информацией о его прошлых активациях, позволяя ему адаптировать текущее поведение на основе паттернов во времени. Благодаря этому CTM может «размышлять» над задачей пошагово, координируя нейроны во времени и делая ход решения интерпретируемым для человека. Исследования показывают, что такой подход улучшает способности к решению сложных задач и повышает эффективность модели на ряде разнообразных задач. CTM является существенным шагом к сближению искусственных и биологических нейросетей, раскрывая новые возможности для ИИ.

## Архитектура Continuous Thought Machine

**Continuous Thought Machine** вводит три ключевых нововведения в архитектуру нейросети: (1) *внутреннее рекуррентное измерение времени* (отделённое от времени входных данных), на котором могут разворачиваться динамики «мышления» модели; (2) *модели уровня нейрона* (Neuron-Level Models, NLMs) – отдельные параметры для каждого нейрона, обрабатывающие историю входных сигналов во времени; (3) *представление синхронизации нейронов* – использование **синхронности в активациях** непосредственно в качестве латентного признакового пространства для принятия решений. Ниже мы подробно рассмотрим компоненты CTM и их математическую формализацию.

![Рисунок 1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-20_&_21/assets/Image_01.png)

**Рис. 1:** Архитектура Continuous Thought Machine (CTM) с обозначением основных компонентов.  

1. **Synapse model** – модель синапсов (синие связи) вычисляет *пре-активации* $a^t$ для каждого нейрона, моделируя межнейронные связи.  
2. **History (pre-activations)** – буфер из $M$ последних пре-активаций каждого нейрона $A^t$ (отображён волнами).  
3. **Neuron-Level Models** – персональные модели нейронов (красным) $g_{\theta_d}$ обрабатывают историю $A^t_d$ и выдают *пост-активации* $z^{t+1}_d$.  
4. **Post-activations** – вектор выходных активаций $z^{t+1}$ всех $D$ нейронов на текущем шаге.  
5. **History (post-activations)** – накопленная с начала работы история пост-активаций $Z^t$.  
6. **Synchronization matrix $S^t$** – матрица синхронности, вычисленная как скалярные произведения между временными рядами активаций нейронов.  
7. **Selected neuron pairs** – выбор подмножества элементов из $S^t$ (пары нейронов), соответствующих латентным признакам синхронизации.  
8. **Latent representation** – вектор синхронизации (зелёный) из выбранных элементов $S^t$, разделённый на две части: для вывода и для внимания.  
9. **OUT/ATTN projections** – линейные слои: $W_{\text{out}}$ проецирует латентный вектор в выход (например, классы), $W_{\text{in}}$ – в вектор запроса внимания *q*.  
10. **Data modulation (Attention output)** – с помощью *q* модель извлекает из данных релевантную информацию (через механизм внимания *ATTN*, жёлтый блок $o^t$), и эта модифицированная информация объединяется с текущими пост-активациями, замыкая цикл на следующий внутренний тик.  

