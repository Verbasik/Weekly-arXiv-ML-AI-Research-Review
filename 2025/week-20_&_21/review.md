# Машины непрерывного мышления (Continuous Thought Machines): внедрение нейронной синхронизации в качестве основы для искусственного интеллекта

## Введение и мотивация

Современные нейросети добились больших успехов, но остаются упрощёнными по сравнению с биологическим мозгом. В частности, они обычно не учитывают **временную динамику нейронов** – **точное время спайков и синхронизацию активности**, которые играют ключевую роль в биологических нейросетях. Как правило, искусственные нейроны выдают лишь одно статическое значение активации, игнорируя *когда* нейрон активируется относительно других. Биологические принципы вроде зависимой от времени спайков пластичности (STDP) указывают, что тайминг важен для обучения и обработки информации мозгом. Разрыв между гибким человеческим мышлением и текущим ИИ позволяет предположить, что в моделях отсутствуют некоторые фундаментальные компоненты, связанные с **временной обработкой** сигналов.

**Continuous Thought Machine (CTM)** – это новая архитектура нейросети, предложенная Sakana AI, которая *возвращает время в основу вычислений нейросети*. Модель CTM специально разработана, чтобы использовать синхронизацию активности нейронов как механизм рассуждения. В отличие от традиционных сетей, CTM снабжает каждый нейрон информацией о его прошлых активациях, позволяя ему адаптировать текущее поведение на основе паттернов во времени. Благодаря этому CTM может «размышлять» над задачей пошагово, координируя нейроны во времени и делая ход решения интерпретируемым для человека. Исследования показывают, что такой подход улучшает способности к решению сложных задач и повышает эффективность модели на ряде разнообразных задач. CTM является существенным шагом к сближению искусственных и биологических нейросетей, раскрывая новые возможности для ИИ.

## Архитектура Continuous Thought Machine

**Continuous Thought Machine** вводит три ключевых нововведения в архитектуру нейросети: 

1. *внутреннее рекуррентное измерение времени* (отделённое от времени входных данных), на котором могут разворачиваться динамики «мышления» модели; 
2. *модели уровня нейрона* (Neuron-Level Models, NLMs) – отдельные параметры для каждого нейрона, обрабатывающие историю входных сигналов во времени; 
3. *представление синхронизации нейронов* – использование **синхронности в активациях** непосредственно в качестве латентного признакового пространства для принятия решений. Ниже мы подробно рассмотрим компоненты CTM и их математическую формализацию.

![Рисунок 1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-20_&_21/assets/Image_01.jpeg)

**Рис. 1:** Архитектура Continuous Thought Machine (CTM) с обозначением основных компонентов.  

1. **Synapse model** – модель синапсов (синие связи) вычисляет *пре-активации* $a^t$ для каждого нейрона, моделируя межнейронные связи.  
2. **History (pre-activations)** – буфер из $M$ последних пре-активаций каждого нейрона $A^t$ (отображён волнами).  
3. **Neuron-Level Models** – персональные модели нейронов (красным) $g_{\theta_d}$ обрабатывают историю $A^t_d$ и выдают *пост-активации* $z^{t+1}_d$.  
4. **Post-activations** – вектор выходных активаций $z^{t+1}$ всех $D$ нейронов на текущем шаге.  
5. **History (post-activations)** – накопленная с начала работы история пост-активаций $Z^t$.  
6. **Synchronization matrix $S^t$** – матрица синхронности, вычисленная как скалярные произведения между временными рядами активаций нейронов.  
7. **Selected neuron pairs** – выбор подмножества элементов из $S^t$ (пары нейронов), соответствующих латентным признакам синхронизации.  
8. **Latent representation** – вектор синхронизации (зелёный) из выбранных элементов $S^t$, разделённый на две части: для вывода и для внимания.  
9. **OUT/ATTN projections** – линейные слои: $W_{\text{out}}$ проецирует латентный вектор в выход (например, классы), $W_{\text{in}}$ – в вектор запроса внимания *q*.  
10. **Data modulation (Attention output)** – с помощью *q* модель извлекает из данных релевантную информацию (через механизм внимания *ATTN*, жёлтый блок $o^t$), и эта модифицированная информация объединяется с текущими пост-активациями, замыкая цикл на следующий внутренний тик.

## **Пример применения CTM к задаче NLP: предсказание следующего токена**

Рассмотрим типичную задачу из области обработки естественного языка: **предсказание следующего токена** в последовательности. Пусть входом модели является токенизированная последовательность текста:

$$
\text{Input: } \quad x = (x_1, x_2, \dots, x_{t})
$$

Задача модели — предсказать токен $x_{t+1}$. Для этого CTM будет **итеративно размышлять над входной последовательностью** на множестве внутренних шагов $\tau = 1, 2, \dots, T$, постепенно формируя предсказание и уточняя его по мере синхронизации нейронов.

## Применение компонентов архитектуры CTM:

### **1. Synapse model**

На каждом внутреннем шаге $\tau$, модель синтезирует вектор *пре-активаций* $a^\tau \in \mathbb{R}^D$ через синаптическую модель:

$$
a^\tau = f_{\theta_{\text{syn}}}([z^\tau, o^\tau])
$$

где:

* $z^\tau \in \mathbb{R}^D$ — пост-активации всех нейронов на предыдущем шаге;
* $o^\tau \in \mathbb{R}^{d_{\text{attn}}}$ — вектор внимания (модифицированная информация из входа);
* $f_{\theta_{\text{syn}}}$ — MLP с архитектурой типа U-Net.

В NLP-задаче:

* вход $o^\tau$ формируется из токенов $x = (x_1, ..., x_t)$, представленных как эмбеддинги через энкодер $F(x) \in \mathbb{R}^{t \times d_{\text{attn}}}$.
* выход $a^\tau$ можно интерпретировать как «оценку нейронов» по состоянию внимания и собственного контекста.

<details> 
    <summary><em><strong>MLP с архитектурой типа U-Net</strong></em></summary>

## **MLP с архитектурой типа U-Net для синаптической модели**

В контексте CTM под **U-Net-подобным MLP** понимается полносвязная сеть, организованная по принципу «контракции – расширения» с *пропусками* (skip connections), аналогично классическому U-Net в компьютерном зрении, но применённая к одномерному вектору входных признаков.

Ниже поэтапно разберём, как именно может быть устроен такой модуль $f_{\theta_{\text{syn}}}$.

## 1. Входной вектор и его размеры

На шаге $\tau$ мы имеем два вектора:

* $z^\tau \in \mathbb{R}^D$ — пост-активации всех $D$ нейронов с прошлого шага.
* $o^\tau \in \mathbb{R}^{d_{\text{attn}}}$ — выход внимания, извлечённый из входных данных.

Объединяем их в единый вектор

$$
v^\tau = 
\begin{bmatrix}
z^\tau \\[4pt]
o^\tau
\end{bmatrix}
\;\in\;
\mathbb{R}^{\,D + d_{\text{attn}}}\
$$

### **Пример:**

1. Задаём размерности: $D=3$, $d_{\text{attn}}=2$

2. Выбираем конкретные векторы:

   $$
   z^τ = (0.1,\;-0.4,\;0.7)
   $$

   $$
   o^τ = (0.5,\;-0.2)
   $$

3. Объединяем:

   $$
   v^τ = 
   \begin{bmatrix}
     0.1\\
     -0.4\\
     0.7\\
     0.5\\
     -0.2
   \end{bmatrix}
   \in \mathbb R^5
   $$

**Python-псевдокод**

```python
import numpy as np

# Размерности
D = 3
d_attn = 2

# Примерные данные
z = np.array([0.1, -0.4, 0.7])       # форма (3,)
o = np.array([0.5, -0.2])            # форма (2,)

# Конкатенация
v = np.concatenate([z, o])           # форма (5,)
print("v =", v)                      # [ 0.1 -0.4  0.7  0.5 -0.2]
print("Shape of v:", v.shape)        # (5,)
```

Таким образом, вектор $v^τ$ размерности $D + d\_{\text{attn}}$ содержит в себе как информацию о предыдущих пост-активациях нейронов, так и сведения из механизма внимания.

## 2. Контрактивный (сжатый) путь

Цель контрактивного пути — постепенно уменьшить размерность признакового пространства, извлекая высокоуровневые представления. Пусть у нас задано $L$ уровней сжатия. На каждом уровне $\ell = 1,2,\dots,L$ выполняются две операции:

1. **Полносвязный слой** (Linear) с понижением размерности:

   $$
     e^\ell = \sigma\bigl(W_e^\ell\,e^{\ell-1} + b_e^\ell\bigr),
     \quad
     e^0 \equiv v^\tau,
   $$

   где
   $\sigma(\cdot)$ — нелинейность (ReLU, GELU и т.п.);
   $W_e^\ell$ имеет размер $\;d_{\ell}\times d_{\ell-1}$,
   $d_0 = D + d_{\text{attn}}$,
   $d_\ell < d_{\ell-1}$.

2. **Дополнительный сжатий** (по желанию) — например, второй Linear или BatchNorm+ReLU, но ключевое — фиксируем результат как «контрактивный» выход уровня:

   $$
     \tilde e^\ell = \sigma\bigl(W_{\tilde e}^\ell\,e^\ell + b_{\tilde e}^\ell\bigr).
   $$

Таким образом, после $L$-го уровня имеем «бутылочное горлышко»:

$$
b = \tilde e^L \in \mathbb{R}^{d_L},
$$

где $d_L$ — минимальная размерность.

### **Пример:**

Возьмём:

- $d_0 = D + d_{\mathrm{attn}} = 5$;  
- число уровней $L = 2$;  
- на первом уровне $d_1 = 4$, на втором $d_2 = 2$.

Пусть вектор на входе

$$
e^0 = v^\tau = \begin{pmatrix}0.1\\ -0.4\\ 0.7\\ 0.5\\ -0.2\end{pmatrix}
$$

Зададим простые матрицы и смещения:

1. Уровень $\ell=1$:

   $$
   W_e^1 = 
   \begin{pmatrix}
     1 & 0 & 0 & 0 & 0 \\
     0 & 1 & 0 & 0 & 0 \\
     0 & 0 & 1 & 0 & 0 \\
     0 & 0 & 0 & 1 & 0 
   \end{pmatrix},\quad
   b_e^1 = \begin{pmatrix}0\\0\\0\\0\end{pmatrix}
   $$

   Тогда

   $$
   e^1 = \mathrm{ReLU}(W_e^1 e^0 + b_e^1)
       = \mathrm{ReLU}\!\bigl([0.1,\,-0.4,\,0.7,\,0.5]^\top\bigr)
       = [0.1,\,0,\,0.7,\,0.5]^\top
   $$

   Дополнительное сжатие:

   $$
   W_{\tilde e}^1 = I_{4},\quad b_{\tilde e}^1=0,\qquad
   \tilde e^1 = \mathrm{ReLU}(e^1) = [0.1,\,0,\,0.7,\,0.5]^\top
   $$

2. Уровень $\ell=2$:

   $$
   W_e^2 = 
   \begin{pmatrix}
     0 & 1 & 0 & 0 \\
     0 & 0 & 1 & 0 
   \end{pmatrix},\quad
   b_e^2 = \begin{pmatrix}0\\0\end{pmatrix}
   $$

   Тогда

   $$
   e^2 = \mathrm{ReLU}(W_e^2 \tilde e^1 + b_e^2)
       = \mathrm{ReLU}\!\bigl([0,\,0.7]^\top\bigr)
       = [0,\,0.7]^\top
   $$

   И дополнительное:

   $$
   W_{\tilde e}^2 = I_{2},\quad b_{\tilde e}^2=0,\qquad
   \tilde e^2 = \mathrm{ReLU}(e^2) = [0,\,0.7]^\top
   $$

В итоге «бутылочное горлышко»

$$
b = \tilde e^2 = \begin{pmatrix}0\\0.7\end{pmatrix}
\;\in\;\mathbb R^{d_2},\quad d_2=2.
$$


### **Вывод:**

1. Конкатенация

   $$
   v^\tau = 
   \begin{bmatrix}
     z^\tau \\[4pt]
     o^\tau
   \end{bmatrix}
   \;\in\;
   \mathbb{R}^{D + d_{\text{attn}}}
   $$

   где сверху идут компоненты $z^\tau\in\mathbb R^D$, а снизу $o^\tau\in\mathbb R^{d\_{\text{attn}}}$

2. «Бутылочное горлышко»

   После двух уровней сжатия $L=2$ мы получили

   $$
   b = \tilde e^2 = 
   \begin{pmatrix}
     0\\
     0.7
   \end{pmatrix}
   \;\in\;
   \mathbb{R}^{d_2},
   \quad d_2 = 2
   $$

Это именно то, к чему стремится контрактивный путь: из входного вектора размерности $d\_0=D+d\_{\mathrm{attn}}$ поэтапно сжать представление до $d\_L$, здесь до 2.


---

## 3. Расширительный путь с пропусками

Теперь начинаем **расширять** представление обратно к размерности $D$. При этом на каждом уровне используется **пропуск** (skip connection) из контрактивного пути того же уровня:

Для $\ell = L, L-1, \dots, 1$:

1. **Конкатенация** текущего декодерного актива и соответствующего контрактивного:

   $$
     c^\ell = 
     \begin{bmatrix}
       \tilde e^\ell \\[4pt]
       d^{\ell+1}
     \end{bmatrix},
     \quad
     d^{L+1} \equiv b.
   $$

   Здесь $c^\ell \in \mathbb{R}^{\,d_\ell + d_\ell}$.

2. **Полносвязный слой расширения**:

   $$
     d^\ell = \sigma\bigl(W_d^\ell\,c^\ell + b_d^\ell\bigr),
   $$

   где $W_d^\ell\colon \mathbb{R}^{d_\ell + d_\ell} \to \mathbb{R}^{d_{\ell-1}}$.

Итогом после уровня $\ell=1$ будет вектор

$$
d^1 \in \mathbb{R}^{d_0} = \mathbb{R}^{\,D + d_{\text{attn}}}.
$$

## 4. Выходной слой

Чтобы получить **пре-активации** $a^\tau \in \mathbb{R}^D$, извлекаем из расширенного вектора первые $D$ компонент (или применяем отдельный Linear-слой):

$$
a^\tau = W_{\text{out}}^{\text{syn}}\;d^1 + b_{\text{out}}^{\text{syn}},
\qquad
W_{\text{out}}^{\text{syn}}\colon \mathbb{R}^{D + d_{\text{attn}}}\to\mathbb{R}^D.
$$

## 5. Схематичный псевдокод

```python
def f_syn(v: Tensor) -> Tensor:
    # v.shape = (batch_size, D + d_attn)
    # Контрактивный путь
    e = v
    enc_skips = []
    for ℓ in range(1, L+1):
        e = ReLU(Linear_e[ℓ](e))        # d_{ℓ} <-- d_{ℓ-1}
        enc_skips.append(e)             # сохраняем для skip
        e = ReLU(Linear_e_tilde[ℓ](e))  

    b = e  # bottleneck vector, shape=(batch, d_L)

    # Расширительный путь
    d = b
    for ℓ in reversed(range(1, L+1)):
        skip = enc_skips[ℓ-1]          # соответствующий уровень
        d = torch.cat([skip, d], dim=-1)
        d = ReLU(Linear_d[ℓ](d))       # d_{ℓ-1} <-- 2*d_{ℓ}

    # Выход
    a = Linear_out(d)                  # (batch, D)
    return a
```

## 6. Зачем столько уровней и пропусков?

* **Контракция** позволяет $f_{\theta_{\text{syn}}}$ улавливать глобальные, многомерные зависимости между разными частями вектора $[z^\tau, o^\tau]$, сжимая информацию в узкое «бутылочное горлышко».
* **Пропуски (skip connections)** гарантируют, что точная локальная (низкоуровневая) информация не потеряется при сжатии: она напрямую передаётся на этап восстановления, обеспечивая устойчивость обучения и сохранение мелких деталей.
* **Расширение** восстанавливает окончательный размер признакового вектора, обогащённый результатами глобальной агрегации.

### Итоговая формула

Суммарно:

$$
\begin{aligned}
e^0 &= [z^\tau; o^\tau],\\
e^\ell &= \sigma\bigl(W_e^\ell\,e^{\ell-1} + b_e^\ell\bigr),\quad
\tilde e^\ell = \sigma\bigl(W_{\tilde e}^\ell\,e^\ell + b_{\tilde e}^\ell\bigr),\\
b &= \tilde e^L,\\
c^\ell &= [\,\tilde e^\ell; d^{\ell+1}],\quad
d^\ell = \sigma\bigl(W_d^\ell\,c^\ell + b_d^\ell\bigr),\\
a^\tau &= W_{\text{out}}^{\text{syn}}\,d^1 + b_{\text{out}}^{\text{syn}}.
\end{aligned}
$$

**Вывод**

MLP с архитектурой «контракция–расширение» и skip-связями в CTM предназначен не просто для выявления зависимости между векторами $z^\tau$ и $o^\tau$, а для **многомасштабной, гибкой и устойчивой** обработки их совместного представления. Он формирует из них богатый, иерархический маппинг, служащий источником пре-активаций $a^\tau$, с учётом как глобальных, так и локальных паттернов, и одновременно обеспечивая стабильное обучение внутри многократной рекурсии:

1. **Извлечение многомасштабных взаимодействий.**

   * **Глобальные зависимости:** контрактивный путь «сжимает» объединённый вектор $\bigl[z^\tau; o^\tau\bigr]$ в узкое «бутылочное горлышко» $b$, где сеть вычленяет обобщённые, высокоуровневые паттерны взаимодействия.
   * **Локальные детали:** благодаря skip-связям соответствующих уровней, на этап расширения попадает «сырой» сигнал сжатия, что сохраняет точные, низкоуровневые зависимости в каждой компоненте исходного вектора.

2. **Иерархическое объединение.**
   Расширительный путь «разворачивает» представление обратно к размерности $D + d_{\text{attn}}$, на каждом уровне смешивая в себе и глобальное обобщение из узла $b$, и локальные признаки из skip-связей. В итоге каждый элемент выходного вектора $a^\tau$ учитывает и «широкую картину», и тонкие нюансы синхронизации нейронов.

3. **Стабильность и эффективность обучения.**
   Skip-connections обеспечивают прямой путь для градиентов от глубоких слоёв обратно к входу, что устраняет проблему затухающего градиента при многократном применении модуля в рекуррентной петле CTM. Это критично для надёжного обучения «мыслительных» итераций модели.

4. **Гибкость масштабирования.**
   Количество уровней $L$ и размерности $d_\ell$ могут увеличиваться для сложных задач или уменьшаться для простых, сохраняя при этом богатую выражающую способность. Такая адаптивность позволяет применять синоптическую U-Net-конструкцию к самым разным объёмам и типам входных сигналов.

5. **Баланс обобщения и детализации.**
   «Бутылочное горлышко» даёт возможность учить обобщённые соотношения между нейронами, а skip-связи— сохранять критичные мелкие детали. Этот баланс предотвращает как чрезмерную примитивность, так и переобучение, что важно для формирования корректных пре-активаций $a^\tau$ и всего последующего процесса рассуждения CTM.

Таким образом, **MLP типа U-Net** в CTM — это компактный, но мощный механизм «синаптической» обработки, который многомасштабно интегрирует и глобальные, и локальные связи в объединённом пространстве $[z^\tau, o^\tau]$, обеспечивает устойчивый градиентный поток и гибко адаптируется к сложности задач, формируя выразительные пре-активации для дальнейших этапов внутреннего рассуждения.

</details>

### **2. History (pre-activations)**

Для каждого нейрона $d = 1, \dots, D$, поддерживается **история последних $M$** пре-активаций:

$$
A^\tau_d = [\, a^{\tau - M + 1}_d,\; \dots,\; a^{\tau}_d \,] \in \mathbb{R}^M
$$

а вся история для всех нейронов:

$$
A^\tau \in \mathbb{R}^{D \times M}
$$

Этот буфер позволяет каждому нейрону анализировать динамику своего активационного сигнала во времени (внутренние шаги).

---

### **3. Neuron-Level Models**

Каждый нейрон $d$ имеет собственную функцию $g_{\theta_d}$, которая преобразует его историю $A^\tau_d$ в пост-активацию:

$$
z^{\tau+1}_d = g_{\theta_d}(A^\tau_d)
$$

Здесь $g_{\theta_d}$ — это индивидуальный MLP нейрона, обрабатывающий $M$-длину временного окна. Например, он может выучить шаблон: «если нейрон был активен 3 раза подряд → активируйся снова». Это приближает поведение к **спайковой активности** биологических нейронов.

---

### **4. Post-activations**

Полученные выходы всех нейронов на шаге $\tau+1$ формируют:

$$
z^{\tau+1} = [z^{\tau+1}_1, z^{\tau+1}_2, \dots, z^{\tau+1}_D] \in \mathbb{R}^D
$$

Этот вектор — **внутреннее нейронное состояние модели**, эволюционирующее во времени.

---

### **5. History (post-activations)**

Накопление всех пост-активаций за все внутренние тики:

$$
Z^{\tau+1} = [z^1, z^2, \dots, z^{\tau+1}] \in \mathbb{R}^{D \times (\tau+1)}
$$

Это память активаций всех нейронов, отражающая их поведенческую траекторию.

---

### **6. Synchronization matrix $S^{\tau+1}$**

Рассчитываем **матрицу синхронности** по формуле:

$$
S^{\tau+1} = Z^{\tau+1} \cdot (Z^{\tau+1})^\top \in \mathbb{R}^{D \times D}
$$

Каждый элемент $S^{\tau+1}_{i,j}$ отражает степень **совпадения временных паттернов** нейронов $i$ и $j$:

* если они часто активируются одновременно — высокая синхронность;
* если несогласованы — низкая.

---

### **7. Selected neuron pairs**

Из $S^{\tau+1}$ выбирается фиксированное **подмножество пар** $\mathcal{I}_{\text{out}}, \mathcal{I}_{\text{action}} \subset \{(i,j)\ |\ i < j\}$, чтобы получить:

* $S^{\tau+1}_{\text{out}} = [S^{\tau+1}_{(i,j)}\ |\ (i,j) \in \mathcal{I}_{\text{out}}] \in \mathbb{R}^{D_{\text{out}}}$
* $S^{\tau+1}_{\text{action}} = [S^{\tau+1}_{(i,j)}\ |\ (i,j) \in \mathcal{I}_{\text{action}}] \in \mathbb{R}^{D_{\text{action}}}$

Индексы $\mathcal{I}_{\text{out}}$ и $\mathcal{I}_{\text{action}}$ выбираются случайно при инициализации и фиксируются.

---

### **8. Latent representation**

На основе выбранных пар:

* $S^{\tau+1}_{\text{out}}$ служит как **вектор признаков** для предсказания;
* $S^{\tau+1}_{\text{action}}$ — как **вектор запроса внимания**.

Оба вектора являются низкоразмерными сжатыми представлениями **внутренней динамики** сети.

---

### **9. OUT/ATTN projections**

Два линейных слоя:

* для предсказания следующего токена:

  $$
  y^{\tau+1} = W_{\text{out}} \cdot S^{\tau+1}_{\text{out}} \in \mathbb{R}^{V}
  $$

  где $V$ — размер словаря токенов;

* для формирования запроса к вниманию:

  $$
  q^{\tau+1} = W_{\text{in}} \cdot S^{\tau+1}_{\text{action}} \in \mathbb{R}^{d_{\text{attn}}}
  $$

---

### **10. Data modulation (Attention output)**

Вычисляется **внимание** к эмбеддингам входных токенов $F(x) \in \mathbb{R}^{t \times d_{\text{attn}}}$ с использованием $q^{\tau+1}$ как запроса:

$$
o^{\tau+1} = \text{Attention}(q^{\tau+1}, K = F(x), V = F(x)) \in \mathbb{R}^{d_{\text{attn}}}
$$

Здесь используется dot-product attention:

$$
\text{Attention}(q, K, V) = \text{softmax}\left( \frac{qK^\top}{\sqrt{d}} \right) V
$$

Вектор $o^{\tau+1}$ объединяется с $z^{\tau+1}$ и подаётся на следующий внутренний тик $\tau+2$, формируя вход в синаптическую модель:

$$
[z^{\tau+1}, o^{\tau+1}] \longrightarrow f_{\theta_{\text{syn}}}
$$

---

## 🔁 Общий процесс на шаге $\tau$:

1. $z^\tau, o^\tau \rightarrow a^\tau$ через синапс
2. Обновление истории $A^\tau$
3. $A^\tau_d \rightarrow z_d^{\tau+1}$ через NLM
4. Обновление $Z^{\tau+1}$
5. $Z^{\tau+1} \rightarrow S^{\tau+1}$
6. $S^{\tau+1} \rightarrow S_{\text{out}}, S_{\text{action}}$
7. $S_{\text{out}} \rightarrow y^{\tau+1}$, $S_{\text{action}} \rightarrow q^{\tau+1}$
8. $q^{\tau+1} \rightarrow o^{\tau+1}$
9. Подаём $[z^{\tau+1}, o^{\tau+1}]$ на следующий тик.

---

## 🧮 Пример:

Допустим, CTM работает над текстом:

```
"Albert Einstein was a ..."
```

### Этапы:

* $x = (\text{“Albert”}, \text{“Einstein”}, \text{“was”}, \text{“a”})$
* Задача: предсказать следующий токен $x_5$ (ожидается “physicist”)
* На каждом внутреннем шаге $\tau$, CTM обновляет свои активации, накапливает синхронизацию нейронов.
* Например, на $\tau=1$: $z^1$ – случайны, $S^1$ – почти нулевая.
* К $\tau=5$: активность стабилизируется, пары нейронов, чувствительные к шаблонам “персона → профессия”, синхронизируются.
* $y^5 = \text{softmax}(W_{\text{out}} \cdot S^5_{\text{out}})$ — уже с высокой вероятностью указывает на “physicist”.
* Если $C^5 = 0.95$, а порог уверенности $\tau = 0.9$, модель завершает цикл.

---
