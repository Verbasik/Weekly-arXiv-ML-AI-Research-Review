{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dbf1193",
   "metadata": {},
   "source": [
    "# Fine-Tuning Round 2: ModernBERT —Å —É—á–µ—Ç–æ–º –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ üéØ\n",
    "\n",
    "–í –¥–∞–Ω–Ω–æ–π —Ç–µ—Ç—Ä–∞–¥–∫–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∫–æ–¥ –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ —Ä–∞—É–Ω–¥–∞ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ ModernBERT –¥–ª—è –∑–∞–¥–∞—á–∏ –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤. –û—Å–Ω–æ–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª–µ–Ω–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–µ –∫–ª–∞—Å—Å–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ ‚öñÔ∏è.\n",
    "\n",
    "## –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:\n",
    "- **–í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:**  \n",
    "  –†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ `calculate_class_weights` –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –≤–ª–∏—è–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö –º–µ—Ç–æ–∫ –∏ —É—á–µ—Å—Ç—å –¥–∏—Å–±–∞–ª–∞–Ω—Å –≤ –¥–∞–Ω–Ω—ã—Ö üìä.\n",
    "\n",
    "- **–ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –º–µ—Ç–æ–∫:**  \n",
    "  –ú–æ–¥—É–ª—å `LabelAttention` –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—ã–≤–∞—Ç—å –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏, —á—Ç–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π üß†.\n",
    "\n",
    "- **Focal Loss:**  \n",
    "  –†–µ–∞–ª–∏–∑–∞—Ü–∏—è `FocalLoss` –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–∏–º–µ—Ä–∞—Ö, —Å–Ω–∏–∂–∞—è –≤–ª–∏—è–Ω–∏–µ –ª–µ–≥–∫–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º—ã—Ö –∫–ª–∞—Å—Å–æ–≤ üéØ.\n",
    "\n",
    "- **–°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ (Label Smoothing):**  \n",
    "  –í–≤–µ–¥–µ–Ω–∏–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫ –ø–æ–º–æ–≥–∞–µ—Ç –±–æ—Ä–æ—Ç—å—Å—è —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—è –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç–æ–∫ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å ü§ñ.\n",
    "\n",
    "- **–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**  \n",
    "  –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –≤—ã—Ö–æ–¥–æ–≤ –±–∞–∑–æ–≤–æ–π BERT –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è —á–µ—Ä–µ–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ (`intermediate`) –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π üöÄ.\n",
    "\n",
    "- **–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è:**  \n",
    "  –ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö, –Ω–∞—Å—Ç—Ä–æ–π–∫—É DataLoader-–æ–≤, –æ–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ AdamW, scheduler –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç—å—é –æ–±—É—á–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è ‚è±Ô∏è.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpy==1.26\n",
    "!pip install -q packaging\n",
    "!pip install -q torch==2.1.0 torchvision==0.16.0\n",
    "!pip install -q setuptools scikit-learn\n",
    "!pip install --upgrade -q  datasets==3.1.0 accelerate==1.2.1\n",
    "!pip install -q \"git+https://github.com/huggingface/transformers.git@6e0515e99c39444caae39472ee1b2fd76ece32f1\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0a025-f6d6-431c-b6a9-4383bb232a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c542b8-49d6-41d8-8543-6f967ca21996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb  4 07:24:40 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             121W / 700W |  71823MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          On  | 00000000:82:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             113W / 700W |  73423MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA H100 80GB HBM3          On  | 00000000:83:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             108W / 700W |  73103MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   39C    P0             124W / 700W |  73103MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA H100 80GB HBM3          On  | 00000000:91:00.0 Off |                    0 |\n",
      "| N/A   30C    P0              75W / 700W |      0MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA H100 80GB HBM3          On  | 00000000:92:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              71W / 700W |      0MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA H100 80GB HBM3          On  | 00000000:93:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             115W / 700W |   5304MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA H100 80GB HBM3          On  | 00000000:94:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             114W / 700W |  10064MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   4139947      C   /opt/conda/bin/python3.11                 71812MiB |\n",
      "|    1   N/A  N/A   4139948      C   /opt/conda/bin/python3.11                 73412MiB |\n",
      "|    2   N/A  N/A   4139949      C   /opt/conda/bin/python3.11                 73092MiB |\n",
      "|    3   N/A  N/A   4139950      C   /opt/conda/bin/python3.11                 73092MiB |\n",
      "|    6   N/A  N/A   3588455      C   python3                                    2936MiB |\n",
      "|    7   N/A  N/A    219201      C   /home/ubuntu/speaches/.venv/bin/python     3496MiB |\n",
      "|    7   N/A  N/A   3380383      C   /home/ubuntu/speaches/.venv/bin/python     6554MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c9db82-20be-497c-a314-85766d28fd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/username/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Python\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    hamming_loss,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    jaccard_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PyTorch –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "# Typing\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CUDA\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "torch._dynamo.config.disable = True\n",
    "\n",
    "\n",
    "class MultiLabelTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–ª–∞—Å—Å –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤.\n",
    "        \n",
    "    Args:\n",
    "        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "        labels: –ú–∞—Å—Å–∏–≤ –º–µ—Ç–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "        \n",
    "    Returns:\n",
    "        –û–±—ä–µ–∫—Ç Dataset –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å DataLoader\n",
    "        \n",
    "    Examples:\n",
    "        >>> dataset = MultiLabelTextDataset(\n",
    "        ...     texts=['—Ç–µ–∫—Å—Ç1', '—Ç–µ–∫—Å—Ç2'],\n",
    "        ...     labels=np.array([[1, 0], [0, 1]]),\n",
    "        ...     tokenizer=tokenizer,\n",
    "        ...     max_length=512\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        labels: np.ndarray,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_length: int = 512\n",
    "    ) -> None:\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç—ã –∫–ª–∞—Å—Å–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ __getitem__\n",
    "        self.texts      = texts\n",
    "        self.labels     = labels\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤)\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∏ –º–µ—Ç–∫–∏ –ø–æ –∏–Ω–¥–µ–∫—Å—É\n",
    "        text = str(self.texts[idx])\n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫–∏ –≤ float32 –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n",
    "        labels = self.labels[idx].astype(np.float32)\n",
    "\n",
    "        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,     # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã ([CLS], [SEP] –∏ —Ç.–¥.)\n",
    "            max_length=self.max_length,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "            padding='max_length',        # –î–æ–ø–æ–ª–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–æ max_length\n",
    "            truncation=True,             # –û–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç—ã, –ø—Ä–µ–≤—ã—à–∞—é—â–∏–µ max_length\n",
    "            return_tensors='pt'          # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã PyTorch –≤–º–µ—Å—Ç–æ —Å–ø–∏—Å–∫–æ–≤\n",
    "        )\n",
    "\n",
    "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ –º–µ—Ç–∫–∞–º–∏\n",
    "        return {\n",
    "            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (1, max_length) –≤ (max_length,)\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            \n",
    "            # attention_mask: 1 –¥–ª—è –Ω–∞—Å—Ç–æ—è—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤, 0 –¥–ª—è padding —Ç–æ–∫–µ–Ω–æ–≤\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            \n",
    "            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy –º–∞—Å—Å–∏–≤ –º–µ—Ç–æ–∫ –≤ —Ç–µ–Ω–∑–æ—Ä PyTorch\n",
    "            'labels': torch.FloatTensor(labels)\n",
    "        }\n",
    "\n",
    "\n",
    "def load_data(\n",
    "    data_path: str,\n",
    "    text_column: str = '—Ç–µ–∫—Å—Ç'\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ CSV —Ñ–∞–π–ª–∞.\n",
    "        \n",
    "    Args:\n",
    "        data_path: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏\n",
    "        text_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–∞–º–∏\n",
    "        \n",
    "    Returns:\n",
    "        –ö–æ—Ä—Ç–µ–∂ (—Ç–µ–∫—Å—Ç—ã, –º–µ—Ç–∫–∏, –Ω–∞–∑–≤–∞–Ω–∏—è_–º–µ—Ç–æ–∫)\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω\n",
    "    \"\"\"\n",
    "    print(f\"\\n–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {data_path}...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –º–µ—Ç–æ–∫\n",
    "    texts = df[text_column].values\n",
    "    label_columns = [col for col in df.columns if col != text_column]\n",
    "    labels = df[label_columns].values\n",
    "    \n",
    "    print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} –∑–∞–ø–∏—Å–µ–π —Å {len(label_columns)} –º–µ—Ç–∫–∞–º–∏\")\n",
    "    return texts, labels, label_columns\n",
    "\n",
    "\n",
    "def evaluate_multilabel(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    label_columns: Optional[List[str]] = None\n",
    ") -> Dict[str, Union[float, Dict[str, Dict[str, float]]]]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º—É–ª—å—Ç–∏–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
    "        \n",
    "    Args:\n",
    "        y_true: –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏\n",
    "        y_pred: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏\n",
    "        label_columns: –ù–∞–∑–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏\n",
    "        \n",
    "    Returns:\n",
    "        –°–ª–æ–≤–∞—Ä—å —Å –æ–±—â–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø–æ –∫–∞–∂–¥–æ–π –º–µ—Ç–∫–µ\n",
    "    \"\"\"\n",
    "    # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º—É–ª—å—Ç–∏–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "    metrics = {\n",
    "        'hamming_loss':    hamming_loss(y_true, y_pred),\n",
    "        'subset_accuracy': accuracy_score(y_true, y_pred),\n",
    "        'macro_precision': precision_score(y_true, y_pred, average='macro'),\n",
    "        'macro_recall':    recall_score(y_true, y_pred, average='macro'),\n",
    "        'macro_f1':        f1_score(y_true, y_pred, average='macro'),\n",
    "        'micro_f1':        f1_score(y_true, y_pred, average='micro'),\n",
    "        'samples_f1':      f1_score(y_true, y_pred, average='samples'),\n",
    "        'jaccard_score':   jaccard_score(y_true, y_pred, average='samples')\n",
    "    }\n",
    "    \n",
    "    # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –º–µ—Ç–∫–∞–º\n",
    "    if label_columns is not None:\n",
    "        per_label_metrics = {}\n",
    "        for i, label in enumerate(label_columns):\n",
    "            per_label_metrics[label] = {\n",
    "                'precision': precision_score(y_true[:, i], y_pred[:, i]),\n",
    "                'recall': recall_score(y_true[:, i], y_pred[:, i]),\n",
    "                'f1': f1_score(y_true[:, i], y_pred[:, i])\n",
    "            }\n",
    "        metrics['per_label'] = per_label_metrics\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "class LabelAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —É—á–µ—Ç–∞ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π –º–µ–∂–¥—É –º–µ—Ç–∫–∞–º–∏ –≤ –∑–∞–¥–∞—á–µ \n",
    "        –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
    "        \n",
    "    Args:\n",
    "        hidden_size (int): –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "        num_labels (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: –¢–µ–Ω–∑–æ—Ä —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–º –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        \n",
    "    Examples:\n",
    "        >>> attention = LabelAttention(hidden_size=768, num_labels=10)\n",
    "        >>> hidden_states = torch.randn(32, 512, 768)  # batch_size=32, seq_len=512\n",
    "        >>> output = attention(hidden_states)\n",
    "        >>> output.shape\n",
    "        torch.Size([32, 10, 768])\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size: int, num_labels: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,    # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "            num_heads=8,              # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "            dropout=0.1,              # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout\n",
    "            batch_first=True          # –§–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö [batch, seq, features]\n",
    "        )\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –º–µ—Ç–æ–∫ –∫–∞–∫ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "        self.label_embeddings = nn.Parameter(\n",
    "            torch.randn(num_labels, hidden_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            –ü—Ä–∏–º–µ–Ω—è–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –∫ –≤—Ö–æ–¥–Ω—ã–º —Å–∫—Ä—ã—Ç—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏—è–º\n",
    "        \n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏\n",
    "                                         (batch_size, sequence_length, hidden_size)\n",
    "                                         \n",
    "        Returns:\n",
    "            torch.Tensor: –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "                         (batch_size, num_labels, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size = hidden_states.size(0)  # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä batch\n",
    "        \n",
    "        # –†–∞—Å—à–∏—Ä—è–µ–º embedding –º–µ—Ç–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –≤ batch\n",
    "        label_embeddings = self.label_embeddings.unsqueeze(0).expand(\n",
    "            batch_size, -1, -1\n",
    "        )  # (batch_size, num_labels, hidden_size)\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        attn_output, _ = self.attention(\n",
    "            query=label_embeddings,  # –ú–µ—Ç–∫–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∑–∞–ø—Ä–æ—Å–∞ (Q)\n",
    "            key=hidden_states,       # –°–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫–ª—é—á–µ–π (K)\n",
    "            value=hidden_states      # –°–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∑–Ω–∞—á–µ–Ω–∏–π (V)\n",
    "        )\n",
    "        \n",
    "        return attn_output\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Focal Loss –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏, \n",
    "        —É–º–µ–Ω—å—à–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –ª–µ–≥–∫–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ \n",
    "        —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö.\n",
    "\n",
    "    Args:\n",
    "        alpha (float, optional): –í–µ—Å—è—â–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –≤–∫–ª–∞–¥–∞ —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤ (default: 1.0)\n",
    "        gamma (float, optional): –ü–∞—Ä–∞–º–µ—Ç—Ä —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏, —É–≤–µ–ª–∏—á–∏–≤–∞—é—â–∏–π –≤–µ—Å —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (default: 2.0)\n",
    "        reduction (str, optional): –°–ø–æ—Å–æ–± –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –º–æ–∂–µ—Ç –±—ã—Ç—å 'mean', 'sum' –∏–ª–∏ 'none' (default: 'mean')\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: –ó–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
    "\n",
    "    Examples:\n",
    "        >>> criterion = FocalLoss(alpha=1.0, gamma=2.0, reduction='mean')\n",
    "        >>> inputs = torch.randn(10, requires_grad=True)\n",
    "        >>> targets = torch.randint(0, 2, (10,), dtype=torch.float)\n",
    "        >>> loss = criterion(inputs, targets)\n",
    "        >>> loss.backward()\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = 1.0,\n",
    "        gamma: float = 2.0,\n",
    "        reduction: str = 'mean'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        inputs: torch.Tensor,\n",
    "        targets: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        –í—ã—á–∏—Å–ª—è–µ—Ç Focal Loss.\n",
    "        \n",
    "        Args:\n",
    "            inputs (torch.Tensor): –í—Ö–æ–¥–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–ª–æ–≥–∏—Ç—ã)\n",
    "            targets (torch.Tensor): –ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (0 –∏–ª–∏ 1)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: –ó–Ω–∞—á–µ–Ω–∏–µ Focal Loss\n",
    "        \"\"\"\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º BCE Loss\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(\n",
    "            inputs,\n",
    "            targets,\n",
    "            reduction='none'\n",
    "        )\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º Focal Loss\n",
    "        pt = torch.exp(-bce_loss)  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "class MultilabelClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "        num_labels (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "        hidden_dropout_prob (float): –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout\n",
    "        focal_loss_params (Dict): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Focal Loss\n",
    "        \n",
    "    Attributes:\n",
    "        bert (AutoModelForSequenceClassification): –ë–∞–∑–æ–≤–∞—è BERT –º–æ–¥–µ–ª—å\n",
    "        label_attention (LabelAttention): –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –º–µ—Ç–æ–∫\n",
    "        intermediate (nn.Sequential): –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏\n",
    "        classifier (nn.Linear): –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–ª–æ–π\n",
    "        criterion (FocalLoss): –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_labels: int,\n",
    "        hidden_dropout_prob: float = 0.2,\n",
    "        focal_loss_params: Optional[Dict] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            problem_type=\"multi_label_classification\",\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Label Attention\n",
    "        self.label_attention = LabelAttention(\n",
    "            hidden_size=hidden_size,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        \n",
    "        # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π\n",
    "        self.intermediate = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(hidden_dropout_prob),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(hidden_dropout_prob)\n",
    "        )\n",
    "        \n",
    "        # –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–ª–æ–π\n",
    "        self.classifier = nn.Linear(hidden_size // 2, num_labels)\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Focal Loss\n",
    "        if focal_loss_params is None:\n",
    "            focal_loss_params = {'alpha': 1.0, 'gamma': 2.0}\n",
    "        self.criterion = FocalLoss(**focal_loss_params)\n",
    "\n",
    "    def get_bert_outputs(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        –ü–æ–ª—É—á–∞–µ—Ç –≤—ã—Ö–æ–¥—ã –∏–∑ BERT –º–æ–¥–µ–ª–∏.\n",
    "        \"\"\"\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º attention –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Å–∫—Ä—ã—Ç–æ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é\n",
    "        attention_output = self.label_attention(last_hidden_state)\n",
    "        \n",
    "        return last_hidden_state, attention_output\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        labels: Optional[torch.Tensor] = None\n",
    "    ) -> Any:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: –í—Ö–æ–¥–Ω—ã–µ ID —Ç–æ–∫–µ–Ω–æ–≤\n",
    "            attention_mask: –ú–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "            labels: –ú–µ—Ç–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "            \n",
    "        Returns:\n",
    "            –í—ã—Ö–æ–¥—ã –º–æ–¥–µ–ª–∏ —Å –ª–æ–≥–∏—Ç–∞–º–∏ –∏ –ø–æ—Ç–µ—Ä—è–º–∏\n",
    "        \"\"\"\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –≤—ã—Ö–æ–¥—ã BERT –∏ attention\n",
    "        hidden_states, attention_output = self.get_bert_outputs(\n",
    "            input_ids,\n",
    "            attention_mask\n",
    "        )\n",
    "        \n",
    "        # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏ –≤—ã—Ö–æ–¥ attention\n",
    "        concatenated = torch.cat(\n",
    "            [hidden_states[:, 0, :], attention_output.mean(dim=1)],\n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏\n",
    "        intermediate_output = self.intermediate(concatenated)\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥–∏—Ç—ã\n",
    "        logits = self.classifier(intermediate_output)\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–≤–∞—Ä—å\n",
    "        outputs = {\n",
    "            'logits': logits\n",
    "        }\n",
    "        \n",
    "        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã –º–µ—Ç–∫–∏, –≤—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏\n",
    "        if labels is not None:\n",
    "            outputs['loss'] = self.criterion(logits, labels)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        threshold: float = 0.5\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –ø–æ—Ä–æ–≥–∞.\n",
    "        \"\"\"\n",
    "        outputs = self.forward(input_ids, attention_mask)\n",
    "        predictions = torch.sigmoid(outputs['logits']) > threshold\n",
    "        return predictions.float()\n",
    "\n",
    "def calculate_class_weights(\n",
    "    labels: np.ndarray,\n",
    "    beta: float = 0.999,\n",
    "    min_weight: float = 0.1,\n",
    "    max_weight: float = 10.0,\n",
    "    eps: float = 1e-6\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –í—ã—á–∏—Å–ª—è–µ—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤.\n",
    "    \n",
    "    Args:\n",
    "        labels: –ú–∞—Å—Å–∏–≤ –º–µ—Ç–æ–∫\n",
    "        beta: –§–∞–∫—Ç–æ—Ä —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è\n",
    "        min_weight: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å\n",
    "        max_weight: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: –¢–µ–Ω–∑–æ—Ä –≤–µ—Å–æ–≤\n",
    "    \"\"\"\n",
    "    positive_counts = labels.sum(axis=0)\n",
    "    total_samples = len(labels)\n",
    "    \n",
    "    # –ó–∞—â–∏—Ç–∞ –æ—Ç –Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "    positive_counts = np.where(positive_counts == 0, eps, positive_counts)\n",
    "    \n",
    "    # –†–∞—Å—á–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —á–∏—Å–ª–∞ –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "    effective_num = 1.0 - np.power(beta, positive_counts)\n",
    "    weights = (1.0 - beta) / np.where(effective_num < eps, eps, effective_num)\n",
    "    \n",
    "    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "    weights = weights * (total_samples / (weights.sum() + eps))\n",
    "    \n",
    "    # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ —Å–∂–∞—Ç–∏–µ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –¥–∏—Å–ø—Ä–æ–ø–æ—Ä—Ü–∏–π\n",
    "    weights = np.log(weights + 1.0)\n",
    "    \n",
    "    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∑–∞–¥–∞–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω\n",
    "    weights = (weights - weights.min()) / (weights.max() - weights.min() + eps)\n",
    "    weights = weights * (max_weight - min_weight) + min_weight\n",
    "    \n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: MultilabelClassifier,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "    device: torch.device,\n",
    "    num_epochs: int,\n",
    "    patience: int,\n",
    "    output_dir: Path,\n",
    "    label_columns: List[str],\n",
    "    pos_weights: torch.Tensor,\n",
    "    label_smoothing: float = 0.1\n",
    ") -> MultilabelClassifier:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–∫.\n",
    "        \n",
    "    Args:\n",
    "        model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "        train_loader: –ó–∞–≥—Ä—É–∑—á–∏–∫ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "        val_loader: –ó–∞–≥—Ä—É–∑—á–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "        optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
    "        scheduler: –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (CPU/GPU)\n",
    "        num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è\n",
    "        patience: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏\n",
    "        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        label_columns: –ù–∞–∑–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫\n",
    "        pos_weights: –í–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤\n",
    "        label_smoothing: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è –º–µ—Ç–æ–∫\n",
    "        \n",
    "    Returns:\n",
    "        –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å\n",
    "    \"\"\"\n",
    "    def smooth_labels(\n",
    "        labels: torch.Tensor,\n",
    "        smoothing: float,\n",
    "        eps: float = 1e-6\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –∫ –±–∏–Ω–∞—Ä–Ω—ã–º –º–µ—Ç–∫–∞–º –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
    "        \n",
    "        Args:\n",
    "            labels: –¢–µ–Ω–∑–æ—Ä –±–∏–Ω–∞—Ä–Ω—ã—Ö –º–µ—Ç–æ–∫ —Ä–∞–∑–º–µ—Ä–∞ (batch_size, num_classes)\n",
    "            smoothing: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è (0-1)\n",
    "            eps: –ú–∞–ª–æ–µ —á–∏—Å–ª–æ –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: –¢–µ–Ω–∑–æ—Ä —Å–≥–ª–∞–∂–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: –ï—Å–ª–∏ smoothing –Ω–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1)\n",
    "        \"\"\"\n",
    "        if not 0 <= smoothing < 1:\n",
    "            raise ValueError(\"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1)\")\n",
    "        \n",
    "        if smoothing == 0:\n",
    "            return labels\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            # –ó–∞—â–∏—Ç–∞ –æ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫\n",
    "            smoothed = labels.clone().float()\n",
    "            \n",
    "            # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ: –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–∫–∏ -> (1-smoothing), –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ -> smoothing\n",
    "            smoothed = smoothed * (1.0 - smoothing) + (1.0 - smoothed) * smoothing\n",
    "            \n",
    "            # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º —á–∏—Å–ª–µ–Ω–Ω—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å\n",
    "            smoothed = torch.clamp(smoothed, min=eps, max=1.0 - eps)\n",
    "            \n",
    "            return smoothed\n",
    "    \n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å —Å –≤–µ—Å–∞–º–∏\n",
    "    criterion = nn.BCEWithLogitsLoss(\n",
    "        pos_weight=pos_weights\n",
    "    )\n",
    "    \n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'metrics': []\n",
    "    }\n",
    "    \n",
    "    print(\"\\n–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –ø–æ —ç–ø–æ—Ö–∞–º\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n–≠–ø–æ—Ö–∞ {epoch + 1}/{num_epochs}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # ===================== –§–ê–ó–ê –û–ë–£–ß–ï–ù–ò–Ø =====================\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        print(\"\\n–û–±—É—á–µ–Ω–∏–µ:\")\n",
    "        for batch in tqdm(train_loader, desc='–ü—Ä–æ–≥—Ä–µ—Å—Å'):\n",
    "            # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –∫ –º–µ—Ç–∫–∞–º\n",
    "            smoothed_labels = smooth_labels(labels, label_smoothing)\n",
    "            \n",
    "            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
    "            loss = criterion(outputs['logits'], smoothed_labels)\n",
    "            \n",
    "            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # –°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –∑–∞ —ç–ø–æ—Ö—É\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # ===================== –§–ê–ó–ê –í–ê–õ–ò–î–ê–¶–ò–ò =====================\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        print(\"\\n–í–∞–ª–∏–¥–∞—Ü–∏—è:\")\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='–ü—Ä–æ–≥—Ä–µ—Å—Å'):\n",
    "                # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "                loss = criterion(outputs['logits'], labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # –°–±–æ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "                preds = torch.sigmoid(outputs['logits']).cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # –°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        # ===================== –í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö =====================\n",
    "        all_preds = np.array(all_preds) > 0.5\n",
    "        all_labels = np.array(all_labels)\n",
    "        \n",
    "        # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫\n",
    "        metrics = evaluate_multilabel(all_labels, all_preds, label_columns)\n",
    "        history['metrics'].append(metrics)\n",
    "        \n",
    "        # –í—ã–≤–æ–¥ –æ–±—â–∏—Ö –º–µ—Ç—Ä–∏–∫\n",
    "        print(\"\\n–û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –º—É–ª—å—Ç–∏–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\")\n",
    "        print(\"-\" * 50)\n",
    "        for metric_name, value in metrics.items():\n",
    "            if metric_name != 'per_label':\n",
    "                print(f\"{metric_name:20s}: {value:.4f}\")\n",
    "        \n",
    "        # –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫ –ø–æ –º–µ—Ç–∫–∞–º\n",
    "        print(\"\\n–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –º–µ—Ç–∫–∞–º:\")\n",
    "        print(\"-\" * 50)\n",
    "        for label, label_metrics in metrics['per_label'].items():\n",
    "            print(f\"{label:25s}:\", end=\" \")\n",
    "            print(\", \".join([\n",
    "                f\"{k}: {v:.4f}\"\n",
    "                for k, v in label_metrics.items()\n",
    "            ]))\n",
    "        \n",
    "        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–ø–æ—Ö–∏\n",
    "        print(\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–æ—Ö–∏:\")\n",
    "        print(f\"–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è:    {avg_train_loss:.4f}\")\n",
    "        print(f\"–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:   {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # ===================== –†–ê–ù–ù–Ø–Ø –û–°–¢–ê–ù–û–í–ö–ê =====================\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            early_stopping_counter = 0\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "            print(f\"\\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –≤ {output_dir}/best_model\")\n",
    "            model.bert.save_pretrained(Path(output_dir) / \"best_model\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(\"\\n–†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞!\")\n",
    "                break\n",
    "        \n",
    "        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ learning rate\n",
    "        scheduler.step()\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "    history_path = Path(output_dir) / \"training_history.json\"\n",
    "    print(f\"\\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è –≤ {history_path}\")\n",
    "    with open(history_path, \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
    "    \"\"\"\n",
    "    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è\n",
    "    CONFIG = {\n",
    "        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –ø–µ—Ä–≤–æ–º –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–∏\n",
    "        'model_id': \"modernBERT-large-multilingual\",          # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏\n",
    "        'data_path': \"data/dataset.csv\",                      # –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º\n",
    "        'output_dir': \"modernbert-russian-multilabel-final\",  # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        'max_length': 1024,                                   # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        'num_epochs': 5,                                      # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "        'patience': 3,                                        # –ü–∞—Ä–∞–º–µ—Ç—Ä —Ç–µ—Ä–ø–µ–Ω–∏—è –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏\n",
    "        'learning_rate': 2e-5,                                # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
    "        'weight_decay': 0.01,                                 # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (L2)\n",
    "        'train_batch_size': 16,                               # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "        'eval_batch_size': 32,                                # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏\n",
    "    \n",
    "        # –ù–æ–≤—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ)\n",
    "        'gradient_clip_max_norm': 1.0,                        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –±–æ—Ä—å–±—ã —Å –≤–∑—Ä—ã–≤–æ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "        'dropout_rate': 0.1,                                  # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –¥—Ä–æ–ø–∞—É—Ç\n",
    "        'scheduler_type': 'cosine',                           # –¢–∏–ø —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è: 'linear', 'cosine', 'warmup'\n",
    "        'warmup_steps': 100,                                  # –®–∞–≥–∏ —Ä–∞–∑–æ–≥—Ä–µ–≤–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "        'accumulation_steps': 2,                              # –ê–∫–∫—É–º—É–ª—è—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –±–∞—Ç—á–∞\n",
    "        'pos_weight': 'balanced',                             # –í–µ—Å–∞ –¥–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤ ('balanced' –∏–ª–∏ –º–∞—Å—Å–∏–≤)\n",
    "        'threshold': 0.4,                                     # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–º–æ–∂–Ω–æ –ø–æ–¥–æ–±—Ä–∞—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏)\n",
    "        'use_fp16': True,                                     # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n",
    "        'num_workers': 8,                                     # –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö\n",
    "        'label_smoothing': 0.1,                               # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –¥–ª—è –±–æ—Ä—å–±—ã —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º\n",
    "        'layerwise_lr_decay': 0.85,                           # –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LR –ø–æ —Å–ª–æ—è–º BERT\n",
    "        'freeze_layers': 6,                                   # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –Ω–∏–∂–Ω–∏—Ö —Å–ª–æ–µ–≤ BERT\n",
    "\n",
    "        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤–µ—Å–æ–≤ –º–µ—Ç–æ–∫\n",
    "        'class_weight_beta': 0.9999,                          # –§–∞–∫—Ç–æ—Ä —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤\n",
    "        'class_weight_min': 0.5,                              # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å –∫–ª–∞—Å—Å–∞\n",
    "        'class_weight_max': 5.0,                              # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å –∫–ª–∞—Å—Å–∞\n",
    "        'label_smoothing': 0.1,                               # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –¥–ª—è –±–æ—Ä—å–±—ã —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    # ================== –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ==================\n",
    "    print(\"\\n–ù–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã\")\n",
    "    print(f\"–ú–æ–¥–µ–ª—å: {CONFIG['model_id']}\")\n",
    "    print(f\"–î–∞–Ω–Ω—ã–µ: {CONFIG['data_path']}\")\n",
    "    print(f\"–í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {CONFIG['output_dir']}\")\n",
    "    \n",
    "    output_dir = Path(CONFIG['output_dir'])\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ================== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ==================\n",
    "    texts, labels, label_columns = load_data(CONFIG['data_path'])\n",
    "    num_labels = len(label_columns)\n",
    "    \n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        texts, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # ================== –†–ê–°–ß–ï–¢ –í–ï–°–û–í –ö–õ–ê–°–°–û–í ==================\n",
    "    print(\"\\n–†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤:\")\n",
    "    pos_weights = calculate_class_weights(\n",
    "        train_labels,\n",
    "        beta=CONFIG['class_weight_beta'],\n",
    "        min_weight=CONFIG['class_weight_min'],\n",
    "        max_weight=CONFIG['class_weight_max']\n",
    "    )\n",
    "    \n",
    "    # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤–µ—Å–æ–≤\n",
    "    assert pos_weights.shape[0] == len(label_columns), \"–û—à–∏–±–∫–∞: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ—Å–æ–≤ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —á–∏—Å–ª—É –º–µ—Ç–æ–∫\"\n",
    "    \n",
    "    print(\"\\n–ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ):\")\n",
    "    normalized_weights = pos_weights / pos_weights.sum()\n",
    "    for label, weight, norm_weight in zip(label_columns, pos_weights, normalized_weights):\n",
    "        print(f\"{label:25s}: raw={weight:.2f}, norm={norm_weight:.4f}\")\n",
    "\n",
    "    # ================== –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ò ==================\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_id'])\n",
    "    model = MultilabelClassifier(CONFIG['model_id'], num_labels)\n",
    "\n",
    "    # print(model)\n",
    "\n",
    "    # ================== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–õ–û–ê–î–ï–†–û–í ==================\n",
    "    train_dataset = MultiLabelTextDataset(\n",
    "        train_texts, train_labels, tokenizer, CONFIG['max_length']\n",
    "    )\n",
    "    val_dataset = MultiLabelTextDataset(\n",
    "        val_texts, val_labels, tokenizer, CONFIG['max_length']\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['train_batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=CONFIG['num_workers']\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['eval_batch_size'],\n",
    "        num_workers=CONFIG['num_workers']\n",
    "    )\n",
    "\n",
    "    # ================== –ù–ê–°–¢–†–û–ô–ö–ê –û–ë–£–ß–ï–ù–ò–Ø ==================\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\n–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "    \n",
    "    model.to(device)\n",
    "    pos_weights = pos_weights.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        start_factor=1.0,\n",
    "        end_factor=0.0,\n",
    "        total_iters=CONFIG['num_epochs']\n",
    "    )\n",
    "\n",
    "    # ================== –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò ==================\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        num_epochs=CONFIG['num_epochs'],\n",
    "        patience=CONFIG['patience'],\n",
    "        output_dir=output_dir,\n",
    "        label_columns=label_columns,\n",
    "        pos_weights=pos_weights,\n",
    "        label_smoothing=CONFIG['label_smoothing']\n",
    "    )\n",
    "\n",
    "    # ================== –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ==================\n",
    "    print(f\"\\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ {output_dir}/final_model\")\n",
    "    model.bert.save_pretrained(output_dir / \"final_model\")\n",
    "    tokenizer.save_pretrained(output_dir / \"final_model\")\n",
    "    \n",
    "    label_columns_path = output_dir / \"label_columns.json\"\n",
    "    with open(label_columns_path, \"w\") as f:\n",
    "        json.dump(label_columns, f)\n",
    "    \n",
    "    print(\"\\n–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a6003f7-743d-4713-8651-76532ac466e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–ù–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã\n",
      "–ú–æ–¥–µ–ª—å: modernBERT-large-multilingual\n",
      "–î–∞–Ω–Ω—ã–µ: data/dataset.csv\n",
      "–í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: modernbert-russian-multilabel-final\n",
      "\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ data/dataset.csv...\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 35303 –∑–∞–ø–∏—Å–µ–π —Å 19 –º–µ—Ç–∫–∞–º–∏\n",
      "\n",
      "–†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤:\n",
      "\n",
      "–ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ):\n",
      "–æ—Ñ—Ñ–ª–∞–π–Ω_–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è     : raw=3.07, norm=0.0506\n",
      "–æ–Ω–ª–∞–π–Ω_–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è      : raw=5.00, norm=0.0825\n",
      "–Ω–∞—Ä–∫–æ—Ç–∏–∫–∏                : raw=0.67, norm=0.0110\n",
      "–∞–∑–∞—Ä—Ç–Ω—ã–µ_–∏–≥—Ä—ã            : raw=2.67, norm=0.0440\n",
      "–ø–æ—Ä–Ω–æ–≥—Ä–∞—Ñ–∏—è              : raw=2.82, norm=0.0465\n",
      "–ø—Ä–æ—Å—Ç–∏—Ç—É—Ü–∏—è              : raw=4.22, norm=0.0697\n",
      "—Ä–∞–±—Å—Ç–≤–æ                  : raw=4.90, norm=0.0808\n",
      "—Å–∞–º–æ—É–±–∏–π—Å—Ç–≤–æ             : raw=3.95, norm=0.0651\n",
      "—Ç–µ—Ä—Ä–æ—Ä–∏–∑–º                : raw=4.26, norm=0.0702\n",
      "–æ—Ä—É–∂–∏–µ                   : raw=2.42, norm=0.0400\n",
      "–±–æ–¥–∏—à–µ–π–º–∏–Ω–≥              : raw=4.48, norm=0.0739\n",
      "—Ö–µ–π–ª—Ç—à–µ–π–º–∏–Ω–≥             : raw=3.32, norm=0.0547\n",
      "–ø–æ–ª–∏—Ç–∏–∫–∞                 : raw=2.56, norm=0.0422\n",
      "—Ä–∞—Å–∏–∑–º                   : raw=3.62, norm=0.0597\n",
      "—Ä–µ–ª–∏–≥–∏—è                  : raw=0.50, norm=0.0082\n",
      "—Å–µ–∫—Å—É–∞–ª—å–Ω—ã–µ_–º–µ–Ω—å—à–∏–Ω—Å—Ç–≤–∞  : raw=2.87, norm=0.0473\n",
      "—Å–µ–∫—Å–∏–∑–º                  : raw=3.52, norm=0.0581\n",
      "—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è_–Ω–µ—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å: raw=3.05, norm=0.0503\n",
      "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π              : raw=2.73, norm=0.0451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at modernBERT-large-multilingual and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 1024]) in the checkpoint and torch.Size([19, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n",
      "\n",
      "–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è:\n",
      "--------------------------------------------------\n",
      "\n",
      "–≠–ø–æ—Ö–∞ 1/5\n",
      "==================================================\n",
      "\n",
      "–û–±—É—á–µ–Ω–∏–µ:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ü—Ä–æ–≥—Ä–µ—Å—Å: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1766/1766 [32:18<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–í–∞–ª–∏–¥–∞—Ü–∏—è:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ü—Ä–æ–≥—Ä–µ—Å—Å: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 221/221 [02:50<00:00,  1.30it/s]\n",
      "/home/username/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/username/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –º—É–ª—å—Ç–∏–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\n",
      "--------------------------------------------------\n",
      "hamming_loss        : 0.0284\n",
      "subset_accuracy     : 0.6621\n",
      "macro_precision     : 0.7225\n",
      "macro_recall        : 0.7897\n",
      "macro_f1            : 0.7379\n",
      "micro_f1            : 0.7476\n",
      "samples_f1          : 0.7022\n",
      "jaccard_score       : 0.6789\n",
      "\n",
      "–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –º–µ—Ç–∫–∞–º:\n",
      "--------------------------------------------------\n",
      "–æ—Ñ—Ñ–ª–∞–π–Ω_–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è     : precision: 0.6619, recall: 0.5082, f1: 0.5750\n",
      "–æ–Ω–ª–∞–π–Ω_–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è      : precision: 0.3883, recall: 0.8861, f1: 0.5400\n",
      "–Ω–∞—Ä–∫–æ—Ç–∏–∫–∏                : precision: 0.9748, recall: 0.8223, f1: 0.8921\n",
      "–∞–∑–∞—Ä—Ç–Ω—ã–µ_–∏–≥—Ä—ã            : precision: 0.8626, recall: 0.8988, f1: 0.8803\n",
      "–ø–æ—Ä–Ω–æ–≥—Ä–∞—Ñ–∏—è              : precision: 0.6247, recall: 0.6143, f1: 0.6194\n",
      "–ø—Ä–æ—Å—Ç–∏—Ç—É—Ü–∏—è              : precision: 0.6326, recall: 0.9197, f1: 0.7496\n",
      "—Ä–∞–±—Å—Ç–≤–æ                  : precision: 0.7025, recall: 0.8586, f1: 0.7727\n",
      "—Å–∞–º–æ—É–±–∏–π—Å—Ç–≤–æ             : precision: 0.9476, recall: 0.9167, f1: 0.9319\n",
      "—Ç–µ—Ä—Ä–æ—Ä–∏–∑–º                : precision: 0.7164, recall: 0.9041, f1: 0.7993\n",
      "–æ—Ä—É–∂–∏–µ                   : precision: 0.7925, recall: 0.7671, f1: 0.7796\n",
      "–±–æ–¥–∏—à–µ–π–º–∏–Ω–≥              : precision: 0.4415, recall: 0.9280, f1: 0.5984\n",
      "—Ö–µ–π–ª—Ç—à–µ–π–º–∏–Ω–≥             : precision: 0.8920, recall: 0.6957, f1: 0.7817\n",
      "–ø–æ–ª–∏—Ç–∏–∫–∞                 : precision: 0.6209, recall: 0.4685, f1: 0.5340\n",
      "—Ä–∞—Å–∏–∑–º                   : precision: 0.6610, recall: 0.7156, f1: 0.6872\n",
      "—Ä–µ–ª–∏–≥–∏—è                  : precision: 0.9912, recall: 0.8411, f1: 0.9100\n",
      "—Å–µ–∫—Å—É–∞–ª—å–Ω—ã–µ_–º–µ–Ω—å—à–∏–Ω—Å—Ç–≤–∞  : precision: 0.8843, recall: 0.8431, f1: 0.8632\n",
      "—Å–µ–∫—Å–∏–∑–º                  : precision: 0.4764, recall: 0.8746, f1: 0.6168\n",
      "—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è_–Ω–µ—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å: precision: 0.4559, recall: 0.5950, f1: 0.5163\n",
      "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π              : precision: 1.0000, recall: 0.9469, f1: 0.9728\n",
      "\n",
      "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–æ—Ö–∏:\n",
      "–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è:    0.7488\n",
      "–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:   0.3718\n",
      "\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –≤ modernbert-russian-multilabel-final/best_model\n",
      "\n",
      "–≠–ø–æ—Ö–∞ 2/5\n",
      "==================================================\n",
      "\n",
      "–û–±—É—á–µ–Ω–∏–µ:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ü—Ä–æ–≥—Ä–µ—Å—Å: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1766/1766 [32:17<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–í–∞–ª–∏–¥–∞—Ü–∏—è:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ü—Ä–æ–≥—Ä–µ—Å—Å: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 221/221 [02:50<00:00,  1.30it/s]\n",
      "/home/username/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/username/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –º—É–ª—å—Ç–∏–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\n",
      "--------------------------------------------------\n",
      "hamming_loss        : 0.0260\n",
      "subset_accuracy     : 0.6911\n",
      "macro_precision     : 0.7244\n",
      "macro_recall        : 0.8365\n",
      "macro_f1            : 0.7687\n",
      "micro_f1            : 0.7759\n",
      "samples_f1          : 0.7451\n",
      "jaccard_score       : 0.7201\n",
      "\n",
      "–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –º–µ—Ç–∫–∞–º:\n",
      "--------------------------------------------------\n",
      "–æ—Ñ—Ñ–ª–∞–π–Ω_–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è     : precision: 0.5707, recall: 0.6503, f1: 0.6079\n",
      "–æ–Ω–ª–∞–π–Ω_–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è      : precision: 0.5101, recall: 0.8762, f1: 0.6448\n",
      "–Ω–∞—Ä–∫–æ—Ç–∏–∫–∏                : precision: 0.9654, recall: 0.8873, f1: 0.9247\n",
      "–∞–∑–∞—Ä—Ç–Ω—ã–µ_–∏–≥—Ä—ã            : precision: 0.8491, recall: 0.9309, f1: 0.8881\n",
      "–ø–æ—Ä–Ω–æ–≥—Ä–∞—Ñ–∏—è              : precision: 0.6343, recall: 0.6690, f1: 0.6512\n",
      "–ø—Ä–æ—Å—Ç–∏—Ç—É—Ü–∏—è              : precision: 0.6512, recall: 0.9598, f1: 0.7760\n",
      "—Ä–∞–±—Å—Ç–≤–æ                  : precision: 0.6146, recall: 0.8939, f1: 0.7284\n",
      "—Å–∞–º–æ—É–±–∏–π—Å—Ç–≤–æ             : precision: 0.9021, recall: 0.9348, f1: 0.9181\n",
      "—Ç–µ—Ä—Ä–æ—Ä–∏–∑–º                : precision: 0.7470, recall: 0.9262, f1: 0.8270\n",
      "–æ—Ä—É–∂–∏–µ                   : precision: 0.8655, recall: 0.8082, f1: 0.8359\n",
      "–±–æ–¥–∏—à–µ–π–º–∏–Ω–≥              : precision: 0.7380, recall: 0.8475, f1: 0.7890\n",
      "—Ö–µ–π–ª—Ç—à–µ–π–º–∏–Ω–≥             : precision: 0.8669, recall: 0.7962, f1: 0.8300\n",
      "–ø–æ–ª–∏—Ç–∏–∫–∞                 : precision: 0.5416, recall: 0.6306, f1: 0.5827\n",
      "—Ä–∞—Å–∏–∑–º                   : precision: 0.5296, recall: 0.8196, f1: 0.6435\n",
      "—Ä–µ–ª–∏–≥–∏—è                  : precision: 0.9876, recall: 0.8999, f1: 0.9417\n",
      "—Å–µ–∫—Å—É–∞–ª—å–Ω—ã–µ_–º–µ–Ω—å—à–∏–Ω—Å—Ç–≤–∞  : precision: 0.7753, recall: 0.8627, f1: 0.8167\n",
      "—Å–µ–∫—Å–∏–∑–º                  : precision: 0.5898, recall: 0.8553, f1: 0.6982\n",
      "—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è_–Ω–µ—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å: precision: 0.4253, recall: 0.6975, f1: 0.5284\n",
      "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π              : precision: 1.0000, recall: 0.9469, f1: 0.9728\n",
      "\n",
      "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–æ—Ö–∏:\n",
      "–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è:    0.7108\n",
      "–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:   0.3567\n",
      "\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –≤ modernbert-russian-multilabel-final/best_model\n",
      "\n",
      "–≠–ø–æ—Ö–∞ 3/5\n",
      "==================================================\n",
      "\n",
      "–û–±—É—á–µ–Ω–∏–µ:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ü—Ä–æ–≥—Ä–µ—Å—Å: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1766/1766 [32:17<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–í–∞–ª–∏–¥–∞—Ü–∏—è:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ü—Ä–æ–≥—Ä–µ—Å—Å: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 221/221 [02:50<00:00,  1.30it/s]\n",
      "/home/username/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/username/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –º—É–ª—å—Ç–∏–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\n",
      "--------------------------------------------------\n",
      "hamming_loss        : 0.0241\n",
      "subset_accuracy     : 0.7057\n",
      "macro_precision     : 0.7496\n",
      "macro_recall        : 0.8416\n",
      "macro_f1            : 0.7876\n",
      "micro_f1            : 0.7909\n",
      "samples_f1          : 0.7545\n",
      "jaccard_score       : 0.7291\n",
      "\n",
      "–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –º–µ—Ç–∫–∞–º:\n",
      "--------------------------------------------------\n",
      "–æ—Ñ—Ñ–ª–∞–π–Ω_–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è     : precision: 0.4926, recall: 0.7268, f1: 0.5872\n",
      "–æ–Ω–ª–∞–π–Ω_–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è      : precision: 0.6750, recall: 0.8020, f1: 0.7330\n",
      "–Ω–∞—Ä–∫–æ—Ç–∏–∫–∏                : precision: 0.9727, recall: 0.8992, f1: 0.9345\n",
      "–∞–∑–∞—Ä—Ç–Ω—ã–µ_–∏–≥—Ä—ã            : precision: 0.8744, recall: 0.9284, f1: 0.9006\n",
      "–ø–æ—Ä–Ω–æ–≥—Ä–∞—Ñ–∏—è              : precision: 0.6621, recall: 0.6952, f1: 0.6783\n",
      "–ø—Ä–æ—Å—Ç–∏—Ç—É—Ü–∏—è              : precision: 0.7024, recall: 0.9478, f1: 0.8068\n",
      "—Ä–∞–±—Å—Ç–≤–æ                  : precision: 0.7254, recall: 0.8939, f1: 0.8009\n",
      "—Å–∞–º–æ—É–±–∏–π—Å—Ç–≤–æ             : precision: 0.9446, recall: 0.9275, f1: 0.9360\n",
      "—Ç–µ—Ä—Ä–æ—Ä–∏–∑–º                : precision: 0.7774, recall: 0.9151, f1: 0.8407\n",
      "–æ—Ä—É–∂–∏–µ                   : precision: 0.8421, recall: 0.8402, f1: 0.8411\n",
      "–±–æ–¥–∏—à–µ–π–º–∏–Ω–≥              : precision: 0.6689, recall: 0.8475, f1: 0.7477\n",
      "—Ö–µ–π–ª—Ç—à–µ–π–º–∏–Ω–≥             : precision: 0.7677, recall: 0.8533, f1: 0.8082\n",
      "–ø–æ–ª–∏—Ç–∏–∫–∞                 : precision: 0.4326, recall: 0.7523, f1: 0.5493\n",
      "—Ä–∞—Å–∏–∑–º                   : precision: 0.7666, recall: 0.7431, f1: 0.7547\n",
      "—Ä–µ–ª–∏–≥–∏—è                  : precision: 0.9836, recall: 0.9011, f1: 0.9406\n",
      "—Å–µ–∫—Å—É–∞–ª—å–Ω—ã–µ_–º–µ–Ω—å—à–∏–Ω—Å—Ç–≤–∞  : precision: 0.8165, recall: 0.8725, f1: 0.8436\n",
      "—Å–µ–∫—Å–∏–∑–º                  : precision: 0.5896, recall: 0.8778, f1: 0.7054\n",
      "—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è_–Ω–µ—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å: precision: 0.5591, recall: 0.6150, f1: 0.5857\n",
      "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π              : precision: 0.9890, recall: 0.9523, f1: 0.9703\n",
      "\n",
      "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–æ—Ö–∏:\n",
      "–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è:    0.6967\n",
      "–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:   0.3471\n",
      "\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –≤ modernbert-russian-multilabel-final/best_model\n",
      "\n",
      "–≠–ø–æ—Ö–∞ 4/5\n",
      "==================================================\n",
      "\n",
      "–û–±—É—á–µ–Ω–∏–µ:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ü—Ä–æ–≥—Ä–µ—Å—Å: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1766/1766 [32:17<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–í–∞–ª–∏–¥–∞—Ü–∏—è:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ü—Ä–æ–≥—Ä–µ—Å—Å: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 221/221 [02:50<00:00,  1.30it/s]\n",
      "/home/username/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/username/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –º—É–ª—å—Ç–∏–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\n",
      "--------------------------------------------------\n",
      "hamming_loss        : 0.0240\n",
      "subset_accuracy     : 0.7141\n",
      "macro_precision     : 0.7440\n",
      "macro_recall        : 0.8459\n",
      "macro_f1            : 0.7882\n",
      "micro_f1            : 0.7923\n",
      "samples_f1          : 0.7578\n",
      "jaccard_score       : 0.7335\n",
      "\n",
      "–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –º–µ—Ç–∫–∞–º:\n",
      "--------------------------------------------------\n",
      "–æ—Ñ—Ñ–ª–∞–π–Ω_–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è     : precision: 0.5393, recall: 0.6749, f1: 0.5995\n",
      "–æ–Ω–ª–∞–π–Ω_–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è      : precision: 0.6545, recall: 0.7970, f1: 0.7188\n",
      "–Ω–∞—Ä–∫–æ—Ç–∏–∫–∏                : precision: 0.9729, recall: 0.9058, f1: 0.9382\n",
      "–∞–∑–∞—Ä—Ç–Ω—ã–µ_–∏–≥—Ä—ã            : precision: 0.8701, recall: 0.9259, f1: 0.8971\n",
      "–ø–æ—Ä–Ω–æ–≥—Ä–∞—Ñ–∏—è              : precision: 0.5850, recall: 0.7786, f1: 0.6680\n",
      "–ø—Ä–æ—Å—Ç–∏—Ç—É—Ü–∏—è              : precision: 0.7639, recall: 0.9357, f1: 0.8412\n",
      "—Ä–∞–±—Å—Ç–≤–æ                  : precision: 0.7763, recall: 0.8939, f1: 0.8310\n",
      "—Å–∞–º–æ—É–±–∏–π—Å—Ç–≤–æ             : precision: 0.8958, recall: 0.9348, f1: 0.9149\n",
      "—Ç–µ—Ä—Ä–æ—Ä–∏–∑–º                : precision: 0.7654, recall: 0.9151, f1: 0.8336\n",
      "–æ—Ä—É–∂–∏–µ                   : precision: 0.8214, recall: 0.8607, f1: 0.8406\n",
      "–±–æ–¥–∏—à–µ–π–º–∏–Ω–≥              : precision: 0.6394, recall: 0.8941, f1: 0.7456\n",
      "—Ö–µ–π–ª—Ç—à–µ–π–º–∏–Ω–≥             : precision: 0.7839, recall: 0.8179, f1: 0.8005\n",
      "–ø–æ–ª–∏—Ç–∏–∫–∞                 : precision: 0.4620, recall: 0.7387, f1: 0.5685\n",
      "—Ä–∞—Å–∏–∑–º                   : precision: 0.6471, recall: 0.8073, f1: 0.7184\n",
      "—Ä–µ–ª–∏–≥–∏—è                  : precision: 0.9864, recall: 0.9099, f1: 0.9466\n",
      "—Å–µ–∫—Å—É–∞–ª—å–Ω—ã–µ_–º–µ–Ω—å—à–∏–Ω—Å—Ç–≤–∞  : precision: 0.8141, recall: 0.8799, f1: 0.8457\n",
      "—Å–µ–∫—Å–∏–∑–º                  : precision: 0.6997, recall: 0.7942, f1: 0.7440\n",
      "—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è_–Ω–µ—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å: precision: 0.4990, recall: 0.6525, f1: 0.5655\n",
      "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π              : precision: 0.9600, recall: 0.9549, f1: 0.9574\n",
      "\n",
      "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–æ—Ö–∏:\n",
      "–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è:    0.6863\n",
      "–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:   0.3437\n",
      "\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –≤ modernbert-russian-multilabel-final/best_model\n",
      "\n",
      "–≠–ø–æ—Ö–∞ 5/5\n",
      "==================================================\n",
      "\n",
      "–û–±—É—á–µ–Ω–∏–µ:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ü—Ä–æ–≥—Ä–µ—Å—Å: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1766/1766 [32:17<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–í–∞–ª–∏–¥–∞—Ü–∏—è:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ü—Ä–æ–≥—Ä–µ—Å—Å: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 221/221 [02:50<00:00,  1.30it/s]\n",
      "/home/username/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/username/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –º—É–ª—å—Ç–∏–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\n",
      "--------------------------------------------------\n",
      "hamming_loss        : 0.0212\n",
      "subset_accuracy     : 0.7373\n",
      "macro_precision     : 0.7689\n",
      "macro_recall        : 0.8364\n",
      "macro_f1            : 0.7985\n",
      "micro_f1            : 0.8099\n",
      "samples_f1          : 0.7598\n",
      "jaccard_score       : 0.7383\n",
      "\n",
      "–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –º–µ—Ç–∫–∞–º:\n",
      "--------------------------------------------------\n",
      "–æ—Ñ—Ñ–ª–∞–π–Ω_–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è     : precision: 0.6081, recall: 0.6530, f1: 0.6298\n",
      "–æ–Ω–ª–∞–π–Ω_–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è      : precision: 0.6627, recall: 0.8168, f1: 0.7317\n",
      "–Ω–∞—Ä–∫–æ—Ç–∏–∫–∏                : precision: 0.9677, recall: 0.9125, f1: 0.9392\n",
      "–∞–∑–∞—Ä—Ç–Ω—ã–µ_–∏–≥—Ä—ã            : precision: 0.8744, recall: 0.9284, f1: 0.9006\n",
      "–ø–æ—Ä–Ω–æ–≥—Ä–∞—Ñ–∏—è              : precision: 0.6733, recall: 0.7262, f1: 0.6987\n",
      "–ø—Ä–æ—Å—Ç–∏—Ç—É—Ü–∏—è              : precision: 0.7872, recall: 0.9357, f1: 0.8550\n",
      "—Ä–∞–±—Å—Ç–≤–æ                  : precision: 0.6525, recall: 0.9293, f1: 0.7667\n",
      "—Å–∞–º–æ—É–±–∏–π—Å—Ç–≤–æ             : precision: 0.9117, recall: 0.9348, f1: 0.9231\n",
      "—Ç–µ—Ä—Ä–æ—Ä–∏–∑–º                : precision: 0.8140, recall: 0.9041, f1: 0.8566\n",
      "–æ—Ä—É–∂–∏–µ                   : precision: 0.8410, recall: 0.8333, f1: 0.8372\n",
      "–±–æ–¥–∏—à–µ–π–º–∏–Ω–≥              : precision: 0.7000, recall: 0.8898, f1: 0.7836\n",
      "—Ö–µ–π–ª—Ç—à–µ–π–º–∏–Ω–≥             : precision: 0.7482, recall: 0.8478, f1: 0.7949\n",
      "–ø–æ–ª–∏—Ç–∏–∫–∞                 : precision: 0.5779, recall: 0.6599, f1: 0.6162\n",
      "—Ä–∞—Å–∏–∑–º                   : precision: 0.7353, recall: 0.7645, f1: 0.7496\n",
      "—Ä–µ–ª–∏–≥–∏—è                  : precision: 0.9864, recall: 0.9099, f1: 0.9466\n",
      "—Å–µ–∫—Å—É–∞–ª—å–Ω—ã–µ_–º–µ–Ω—å—à–∏–Ω—Å—Ç–≤–∞  : precision: 0.8452, recall: 0.8701, f1: 0.8575\n",
      "—Å–µ–∫—Å–∏–∑–º                  : precision: 0.6446, recall: 0.8457, f1: 0.7316\n",
      "—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è_–Ω–µ—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å: precision: 0.6079, recall: 0.5775, f1: 0.5923\n",
      "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π              : precision: 0.9703, recall: 0.9523, f1: 0.9612\n",
      "\n",
      "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–æ—Ö–∏:\n",
      "–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è:    0.6807\n",
      "–°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:   0.3403\n",
      "\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –≤ modernbert-russian-multilabel-final/best_model\n",
      "\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è –≤ modernbert-russian-multilabel-final/training_history.json\n",
      "\n",
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ modernbert-russian-multilabel-final/final_model\n",
      "\n",
      "–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d87f3b9",
   "metadata": {},
   "source": [
    "# –ò—Ç–æ–≥ –≤—Ç–æ—Ä–æ–≥–æ —Ä–∞—É–Ω–¥–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ ‚öñÔ∏è\n",
    "\n",
    "–í —ç—Ç–æ–º —Ä–∞—É–Ω–¥–µ –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤ –±—ã–ª–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞. –ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥—ë–Ω –∫—Ä–∞—Ç–∫–∏–π —Ä–∞–∑–±–æ—Ä –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –≤–∞—Ä–∏–∞–Ω—Ç–æ–º –±–µ–∑ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:\n",
    "\n",
    "- **–ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ñ—É–Ω–∫—Ü–∏—è—Ö –ø–æ—Ç–µ—Ä—å –∏ –æ–±—É—á–µ–Ω–∏–∏:**\n",
    "  - –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤–µ—Å–æ–≤ —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ—à–∏–±–∫–∏ (train loss –∏ val loss) –æ–∫–∞–∑–∞–ª–∏—Å—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤—ã—à–µ (–æ–∫–æ–ª–æ 0.68‚Äì0.34) –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ä–∞–Ω–Ω–∏–º —Ä–∞—É–Ω–¥–æ–º –±–µ–∑ –≤–µ—Å–æ–≤ (–≥–¥–µ –æ—à–∏–±–∫–∏ —Å–Ω–∏–∂–∞–ª–∏—Å—å –¥–æ ~0.0017). –≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –≤–µ—Å–æ–≤, —á—Ç–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç –∏–Ω–æ–π –±–∞–ª–∞–Ω—Å –≤–∫–ª–∞–¥–æ–≤ —Ä–µ–¥–∫–∏—Ö –∏ —á–∞—Å—Ç—ã—Ö –∫–ª–∞—Å—Å–æ–≤.\n",
    "\n",
    "- **–û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏:**\n",
    "  - **Hamming Loss:**  \n",
    "    –ù–µ–º–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏–ª–∞—Å—å —Å ~0.0172 –¥–æ 0.0212 üëÄ ‚Äî –Ω–µ–±–æ–ª—å—à–æ–µ —É—Ö—É–¥—à–µ–Ω–∏–µ.\n",
    "  - **Subset Accuracy:**  \n",
    "    –ü–æ–Ω–∏–∑–∏–ª–∞—Å—å —Å 76.1% –¥–æ 73.7% üìâ.\n",
    "  - **Macro Precision:**  \n",
    "    –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∑–∏–ª–∞—Å—å (—Å ~89.95% –¥–æ 76.89%) üîª, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ä–æ—Å—Ç–µ —á–∏—Å–ª–∞ –ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∫–ª–∞—Å—Å–æ–≤.\n",
    "  - **Macro Recall:**  \n",
    "    –£–ª—É—á—à–∏–ª–∞—Å—å (—Å 73.47% –¥–æ 83.64%) üî∫ ‚Äî –º–æ–¥–µ–ª—å —Å—Ç–∞–ª–∞ –ª–æ–≤–∏—Ç—å –±–æ–ª—å—à–µ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –ø–æ–∑–∏—Ç–∏–≤–æ–≤.\n",
    "  - **Macro F1:**  \n",
    "    –û—Å—Ç–∞–ª–∞—Å—å –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ –ø—Ä–µ–∂–Ω–µ–º —É—Ä–æ–≤–Ω–µ (80.31% –ø—Ä–æ—Ç–∏–≤ 79.85%), —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –ø–æ–ª–Ω–æ—Ç–æ–π.\n",
    "  - **Micro F1:**  \n",
    "    –ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∑–∏–ª–∞—Å—å (—Å 82.49% –¥–æ 80.99%).\n",
    "  - **Samples F1 –∏ Jaccard Score:**  \n",
    "    –ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–∏—Å—å (Samples F1 –≤—ã—Ä–æ—Å–ª–∞ —Å 72.06% –¥–æ 75.98%, –∞ Jaccard ‚Äî —Å 70.82% –¥–æ 73.83%) üòä.\n",
    "\n",
    "- **–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –º–µ—Ç–∫–∞–º:**\n",
    "  - –î–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∫–ª–∞—Å—Å–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä, **¬´–æ–Ω–ª–∞–π–Ω_–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è¬ª** –∏ **¬´–ø–æ—Ä–Ω–æ–≥—Ä–∞—Ñ–∏—è¬ª**, –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Å–Ω–∏–∂–µ–Ω–∏–µ precision, –Ω–æ –∑–∞–º–µ—Ç–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ recall.  \n",
    "  - –¢–∞–∫–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –º–æ–¥–µ–ª—å —Å—Ç–∞–ª–∞ –±–æ–ª–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–π –∫ —Ä–µ–¥–∫–∏–º –ø—Ä–∏–º–µ—Ä–∞–º, –Ω–æ –∑–∞ —Å—á—ë—Ç —ç—Ç–æ–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –±–æ–ª—å—à–µ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä –í—ã–≤–æ–¥:\n",
    "\n",
    "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ –ø–æ–∑–≤–æ–ª–∏–ª–æ **–ø–æ–≤—ã—Å–∏—Ç—å recall –∏ —É–ª—É—á—à–∏—Ç—å –º–µ—Ç—Ä–∏–∫—É Jaccard**, —á—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –ø–æ —Ä–µ–¥–∫–∏–º –∫–ª–∞—Å—Å–∞–º. –û–¥–Ω–∞–∫–æ —ç—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ –∑–∞ —Å—á—ë—Ç —Å–Ω–∏–∂–µ–Ω–∏—è precision –∏ subset accuracy ‚Äî –º–æ–¥–µ–ª—å —Å—Ç–∞–ª–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª—å—à–µ –ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π. –ò—Ç–æ–≥–æ–≤—ã–π **Macro F1** –æ—Å—Ç–∞–ª—Å—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–∏–∑–º–µ–Ω–Ω—ã–º, —á—Ç–æ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç –æ —Ç–æ–º, —á—Ç–æ –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –ø–æ–ª–Ω–æ—Ç–æ–π —Å–º–µ—Å—Ç–∏–ª—Å—è, –Ω–æ —Å—É–º–º–∞—Ä–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –æ—Å—Ç–∞–ª–æ—Å—å –Ω–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–º —É—Ä–æ–≤–Ω–µ.\n",
    "\n",
    "**–ó–∞–∫–ª—é—á–µ–Ω–∏–µ:**  \n",
    "–ï—Å–ª–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è —Å–Ω–∏–∂–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ (—É–≤–µ–ª–∏—á–µ–Ω–∏–µ recall) –∏ —É–ª—É—á—à–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ Jaccard, —Ç–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ –¥–∞—ë—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç. –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ (–µ—Å–ª–∏ –∫—Ä–∏—Ç–∏—á–Ω–∞ –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å) –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–ª–∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Å—Ö–µ–º—ã –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è.\n",
    "\n",
    "–ü—Ä–µ–¥–ª–∞–≥–∞—é –í–∞–º –¥–æ–æ–±–∞–≥–∞—Ç–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç —á—Ç–æ –±—ã —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å—ã –∏ –ø—Ä–æ–≥–Ω–∞—Ç—å –Ω–∞ —Ç–µ—Ç—Ä–∞–¥–∫–µ –ø–µ—Ä–≤–æ–≥–æ —Ä–∞—É–Ω–¥–∞, –¥—É–º–∞—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–µ—Å—å–º–∞ —Ö–æ—Ä–æ—à–∏–π.\n",
    "\n",
    "üöÄ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞ –º–µ–∂–¥—É precision –∏ recall!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f436f05-2715-4c9c-af07-3284d95ba4c6",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
