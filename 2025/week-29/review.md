# Kimi-K2

## Введение

Современные большие языковые модели (LLM) стали ключевым инструментом в самых разных областях — от автоматической генерации текста и программного кода до поддержки интеллектуальных агентов, способных выполнять сложные задачи. С ростом объёма параметров и числа токенов предобучения мы наблюдаем качественный скачок в способностях моделей: улучшение понимания контекста, точности генерации и умений решать узкоспециализированные задачи.

Модель Kimi-K2, разработанная Moonshot AI, представляет собой один из самых амбициозных проектов в экосистеме открытых LLM. Она использует архитектуру Mixture-of-Experts (MoE) и насчитывает триллион параметров, при этом благодаря «разряженной» активации задействует для каждого токена лишь порядка 32 миллиардов параметров. Kimi-K2 сочетает в себе передовые методы оптимизации внимания для обработки сверхдлинных контекстов (до 128 тысяч токенов), инновационный оптимизатор MuonClip для стабильного и эффективного обучения на потрясающем объёме данных (15,5 триллионов токенов), а также комплексный пост-тюнинг для превращения базовой модели в интерактивного, агентно-ориентированного ассистента.

В этом обзоре мы подробно рассмотрим:

1. **Архитектуру** Kimi-K2 — принципы работы MoE, модификации механизма внимания, ключевые параметры и инженерные решения для ускорения инференса.
2. **Процесс обучения** модели — предобучение на огромном корпусе данных, используемые оптимизаторы и техники распределённого обучения, а также этапы fine-tuning и RL-подкрепления для формирования «agentic»-возможностей.
3. **Ключевые результаты и сравнение** с предыдущими версиями и лидерами отрасли — на академических бенчмарках, в задачах программирования и агентных сценариях.


## Архитектура модели Kimi-K2

### **Общие характеристики:** 

Kimi-K2 – это крупная языковая модель с архитектурой *Mixture-of-Experts* (MoE), про MoE мы как-то писали вот [тут](https://verbasik.github.io/Weekly-arXiv-ML-AI-Research-Review/#2025/week-06). Общий размер модели составляет **1 триллион параметров**, однако в каждый момент времени активна лишь примерно **32 млрд параметров** – то есть только небольшая часть весов участвует в обработке конкретного токена. Модель содержит **61 трансформер-слой** (из них один – «плотный», без разбиения на экспертов). Размер скрытых представлений (эмбеддингов) равен **7168**, используется **64 головы внимания**. Словарный запас – **160k** токенов, а максимальная длина контекста модели – **128k токенов**. Модель является авто-регрессионным декодером (аналогично GPT) – то есть генерирует текст, последовательно предсказывая следующий токен на основе предыдущих.

![Figure_01](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/Figure_01.jpg)

### **Mixture-of-Experts:** 

В каждом слое трансформера, реализованном как MoE, присутствует **384 отдельных эксперта** (специализированных подсетей-MLP). Для каждого токена *динамический маршрутизатор* (gating) выбирает **топ-8 экспертов** из этих 384, которые будут использованы для обработки данного токена, плюс один специальный **общий эксперт**, активный всегда. Таким образом, на каждый токен приходится вычисление по 9 экспертным подсетям (8 выбранных + 1 общий), что значительно повышает эффективность модели без линейного роста вычислений. Ненужные эксперты остаются «спящими», благодаря чему достигается огромный общий объём параметров (1T) при приемлемых затратах на вывод (активно 32B). Маршрутизация выполняется по принципу *top-k* (в Kimi-K2 выбрано k=8), вероятно с помощью механизмов top-2 или top-8 gating, что позволяет каждому токену получать комбинацию из нескольких «мнений» экспертов вместо одного. Наличие единого **shared (общего) эксперта**, который участвует всегда, служит для улучшения устойчивости и базового качества – этот элемент обеспечивает, что в каждом слое присутствует одна общая плотная подсеть, дополняющая узкоспециализированных экспертов. Активация только части подсетей (**sparse activation**) экономит вычисления: не все параметры участвуют в каждом проходе, а только наиболее релевантные для текущего токена.

### **Механизмы внимания:** 

Модель использует модифицированный механизм самовнимания, оптимизированный для длинного контекста. Во-первых, количество голов внимания **уменьшено** по сравнению со стандартными трансформерами – Kimi-K2 имеет 64 головы на слой при размере эмбеддинга 7168 (что даёт нетипичный размер проекции \~112 на голову). Более крупные головы в меньшем количестве помогают сделать расчёты внимания более стабильными на больших длинах последовательности. Во-вторых, заимствованный метод **Multi-Head *Latent* Attention (MLA)** – «многоголовое латентное внимание», который был впервые продемонстрирован у DeepSeek V3, об этом мы максимально подробно писали [тут](https://verbasik.github.io/Weekly-arXiv-ML-AI-Research-Review/#2025/week-07_&_08). Этот подход радикально снижает требования к памяти и вычислениям при работе с длинным контекстом. В традиционном многоголовом внимании требуется хранить для каждого токена большие матрицы ключей и значений (размерность пропорциональна числу голов и их размерности). В MLA же каждая позиция в контексте сохраняется в виде **компрессированного латентного вектора** фиксированной размерности (например, \~512-576 измерений, включая позиции) независимо от числа голов. По сути, ключи/значения не хранятся раздельно для каждой головы, а сжимаются в общее представление; затем входной *query*-вектор проецируется в это сжатое пространство для вычисления скалярных весов внимания, и результат проецируется обратно. Такой механизм позволяет уменьшить объём кеша ключей/значений **примерно в 60 раз по сравнению с обычным MHA** и в \~12 раз по сравнению с группированным вниманием (GQA), что делает практичным использование контекста в 128k токенов без переполнения памяти. Несмотря на дополнительные операции проекции, общая сложность вычислений внимания на длинных последовательностях существенно падает – расчёты для обновления KV-кеша и для шага внимания в MLA требуют на порядки меньше FLOPs и объёма памяти, чем в стандартном случае.

Для реализации внимания в таком огромном контексте разработчики также применили **FlashAttention** – высокоэффективный слитный GPU-алгоритм вычисления матриц внимания. В публикациях отмечается, что при авто-регрессии (генерации по одному токену) MLA интегрируется с FlashAttention-подобным ядром для этапов матричного умножения *QK* и *AV*, выполняя их в т.н. «тайлах» для повышения производительности. Кроме того, Kimi-K2 использует *ротари позиционные эмбеддинги* (RoPE) для кодирования положения токенов в последовательности. Благодаря технике **RoPE scaling** (линейному или динамическому масштабированию шага фазовой ротари-функции) модель поддерживает удлинённое окно контекста без потери способности различать близкие позиции. Наконец, в проекциях внимания **не используются смещения (bias)**, и вероятно отключён dropout в слоях внимания – эти упрощения часто применяются в крупных LLM для экономии параметров и стабильности. В активационных функциях модель задействует **SwiGLU** (Swish + Gated Linear Unit) в позиционно-независимых MLP слоёв – эта функция активации доказала эффективность в больших трансформерах (применялась, например, в PaLM).