# Kimi-K2

## Введение

Современные большие языковые модели (LLM) стали ключевым инструментом в самых разных областях — от автоматической генерации текста и программного кода до поддержки интеллектуальных агентов, способных выполнять сложные задачи. С ростом объёма параметров и числа токенов предобучения мы наблюдаем качественный скачок в способностях моделей: улучшение понимания контекста, точности генерации и умений решать узкоспециализированные задачи.

Модель Kimi-K2, разработанная Moonshot AI, представляет собой один из самых амбициозных проектов в экосистеме открытых LLM. Она использует архитектуру Mixture-of-Experts (MoE) и насчитывает триллион параметров, при этом благодаря «разряженной» активации задействует для каждого токена лишь порядка 32 миллиардов параметров. Kimi-K2 сочетает в себе передовые методы оптимизации внимания для обработки сверхдлинных контекстов (до 128 тысяч токенов), инновационный оптимизатор MuonClip для стабильного и эффективного обучения на потрясающем объёме данных (15,5 триллионов токенов), а также комплексный пост-тюнинг для превращения базовой модели в интерактивного, агентно-ориентированного ассистента.

В этом обзоре мы подробно рассмотрим:

1. **Архитектуру** Kimi-K2 — принципы работы MoE, модификации механизма внимания, ключевые параметры и инженерные решения для ускорения инференса.
2. **Процесс обучения** модели — предобучение на огромном корпусе данных, используемые оптимизаторы и техники распределённого обучения, а также этапы fine-tuning и RL-подкрепления для формирования «agentic»-возможностей.
3. **Ключевые результаты и сравнение** с предыдущими версиями и лидерами отрасли — на академических бенчмарках, в задачах программирования и агентных сценариях.


## Архитектура модели Kimi-K2

### Общие характеристики:

Kimi-K2 – это крупная языковая модель с архитектурой *Mixture-of-Experts* (MoE), про MoE мы как-то писали вот [тут](https://verbasik.github.io/Weekly-arXiv-ML-AI-Research-Review/#2025/week-06). Общий размер модели составляет **1 триллион параметров**, однако в каждый момент времени активна лишь примерно **32 млрд параметров** – то есть только небольшая часть весов участвует в обработке конкретного токена. Модель содержит **61 трансформер-слой** (из них один – «плотный», без разбиения на экспертов). Размер скрытых представлений (эмбеддингов) равен **7168**, используется **64 головы внимания**. Словарный запас – **160k** токенов, а максимальная длина контекста модели – **128k токенов**. Модель является авто-регрессионным декодером (аналогично GPT) – то есть генерирует текст, последовательно предсказывая следующий токен на основе предыдущих.

![Figure_01](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/Figure_01.jpg)

### Mixture-of-Experts:

В каждом слое трансформера, реализованном как MoE, присутствует **384 отдельных эксперта** (специализированных подсетей-MLP). Для каждого токена *динамический маршрутизатор* (gating) выбирает **топ-8 экспертов** из этих 384, которые будут использованы для обработки данного токена, плюс один специальный **общий эксперт**, активный всегда. Таким образом, на каждый токен приходится вычисление по 9 экспертным подсетям (8 выбранных + 1 общий), что значительно повышает эффективность модели без линейного роста вычислений. Ненужные эксперты остаются «спящими», благодаря чему достигается огромный общий объём параметров (1T) при приемлемых затратах на вывод (активно 32B). Маршрутизация выполняется по принципу *top-k* (в Kimi-K2 выбрано k=8), вероятно с помощью механизмов top-2 или top-8 gating, что позволяет каждому токену получать комбинацию из нескольких «мнений» экспертов вместо одного. Наличие единого **shared (общего) эксперта**, который участвует всегда, служит для улучшения устойчивости и базового качества – этот элемент обеспечивает, что в каждом слое присутствует одна общая плотная подсеть, дополняющая узкоспециализированных экспертов. Активация только части подсетей (**sparse activation**) экономит вычисления: не все параметры участвуют в каждом проходе, а только наиболее релевантные для текущего токена.

### Механизмы внимания:

Модель использует модифицированный механизм самовнимания, оптимизированный для длинного контекста. Во-первых, количество голов внимания **уменьшено** по сравнению со стандартными трансформерами – Kimi-K2 имеет 64 головы на слой при размере эмбеддинга 7168 (что даёт нетипичный размер проекции ~112 на голову). Более крупные головы в меньшем количестве помогают сделать расчёты внимания более стабильными на больших длинах последовательности. Во-вторых, заимствованный метод **Multi-Head *Latent* Attention (MLA)** – «многоголовое латентное внимание», который был впервые продемонстрирован у DeepSeek V3, об этом мы максимально подробно писали [тут](https://verbasik.github.io/Weekly-arXiv-ML-AI-Research-Review/#2025/week-07_&_08). Этот подход радикально снижает требования к памяти и вычислениям при работе с длинным контекстом. В традиционном многоголовом внимании требуется хранить для каждого токена большие матрицы ключей и значений (размерность пропорциональна числу голов и их размерности). В MLA же каждая позиция в контексте сохраняется в виде **компрессированного латентного вектора** фиксированной размерности (например, ~512-576 измерений, включая позиции) независимо от числа голов. По сути, ключи/значения не хранятся раздельно для каждой головы, а сжимаются в общее представление; затем входной *query*-вектор проецируется в это сжатое пространство для вычисления скалярных весов внимания, и результат проецируется обратно. Такой механизм позволяет уменьшить объём кеша ключей/значений **примерно в 60 раз по сравнению с обычным MHA** и в ~12 раз по сравнению с группированным вниманием (GQA), что делает практичным использование контекста в 128k токенов без переполнения памяти. Несмотря на дополнительные операции проекции, общая сложность вычислений внимания на длинных последовательностях существенно падает – расчёты для обновления KV-кеша и для шага внимания в MLA требуют на порядки меньше FLOPs и объёма памяти, чем в стандартном случае.

Для реализации внимания в таком огромном контексте разработчики также применили **FlashAttention** – высокоэффективный слитный GPU-алгоритм вычисления матриц внимания. В публикациях отмечается, что при авто-регрессии (генерации по одному токену) MLA интегрируется с FlashAttention-подобным ядром для этапов матричного умножения *QK* и *AV*, выполняя их в т.н. «тайлах» для повышения производительности. Кроме того, Kimi-K2 использует *ротари позиционные эмбеддинги* (RoPE) для кодирования положения токенов в последовательности. Благодаря технике **RoPE scaling** (линейному или динамическому масштабированию шага фазовой ротари-функции) модель поддерживает удлинённое окно контекста без потери способности различать близкие позиции. Наконец, в проекциях внимания **не используются смещения (bias)**, и вероятно отключён dropout в слоях внимания – эти упрощения часто применяются в крупных LLM для экономии параметров и стабильности. В активационных функциях модель задействует **SwiGLU** (Swish + Gated Linear Unit) в позиционно-независимых MLP слоёв – эта функция активации доказала эффективность в больших трансформерах (применялась, например, в PaLM).

## Процесс обучения

### Датасеты и объёмы данных:

Kimi-K2 обучена на *огромном корпусе текстовых данных* объёмом **15,5 триллионов токенов** – это один из самых больших датасетов, использованных для обучения LLM на сегодняшний день. По сути модель «прочитала» практически весь доступный интернет-контент (включая множество источников на английском, китайском и других языках, кодовые репозитории, научные тексты и пр.) многократно. Такая задача обучения заставляет модель формировать обобщённое представление о языке и знаниях, содержащихся в данных. Обучение производилось на последовательностях с переменной длиной и, вероятно, с увеличением максимальной длины контекста по мере обучения (чтобы эффективно задействовать 128k контекст к концу обучения). В состав предобучающего корпуса вошли тексты разнообразных доменов: от энциклопедий и новостей до кода и математических решений. Особый упор, судя по результатам, делался на данные для программирования и математические задачи – модель демонстрирует выдающиеся результаты в кодинге и математике, результаты бенчов будут чуть ниже.

### Оптимизатор Muon и его ограничения

Muon — это алгоритм оптимизации, основанный на принципах ортогонализации матриц, в частности, использующий итерацию Ньютона-Шульца для ортогонализации матриц градиентов. Основная идея заключается в поощрении разнообразных направлений обновления, предотвращая коллапс весовых матриц в низкоранговые структуры, что может ограничивать выразительность модели.

Исходный алгоритм Muon применяет следующее правило обновления:

$$
\begin{aligned}
M_t &= \mu M_{t-1} + \nabla \mathcal{L}(W_{t-1}) \\
O_t &= \text{NewtonSchulz}(M_t) \\
W_t &= W_{t-1} - \eta_t \left( \gamma \cdot O_t \cdot \sqrt{\max(A,B)} + \lambda W_{t-1} \right)
\end{aligned}
$$

### Формирование импульса (momentum)

![momentum_visualization](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/momentum_visualization.png)

$$
M_t = \mu M_{t-1} + \nabla \mathcal{L}(W_{t-1})
$$

#### Ключевые компоненты:
1. **$\mu$ (коэффициент импульса)**  
   - Аналогичен параметру $\beta_1$ в Adam (обычно выбирается в диапазоне **0.9–0.99**).  
   - Определяет, какая доля предыдущего импульса сохраняется. Например, при $\mu=0.9$ "истории" градиентов учитывается на каждом шаге.

2. **$\nabla \mathcal{L}(W_{t-1})$ (градиент функции потерь)**  
   - Это матрица той же размерности, что и обучаемые параметры $W_{t-1}$.  
   - Показывает, как нужно изменить веса, чтобы уменьшить ошибку модели на текущем шаге.

3. **$M_t$ (накопленный импульс)**  
   - Итоговая матрица, объединяющая текущий градиент с "историей" предыдущих обновлений.  
   - Размерность: идентична $W_{t-1}$ (например, $7168 \times 7168$ для матриц внимания в Kimi-K2).

---

#### Подробное объяснение:

*   **Что это?**  
    - Механизм импульса (momentum) — это аналог **инерции** в физике.  
    - Вместо резкого изменения направления на каждом шаге (как в обычном SGD), модель "накапливает" градиенты, усредняя их с коэффициентом $\mu$.  
    - Формула $M_t$ — это **взвешенная сумма** прошлого импульса ($M_{t-1}$) и нового градиента.

*   **Как это работает?**  
    - На шаге $t=0$: $M_0 = \nabla \mathcal{L}(W_0)$ (нет истории).  
    - На шаге $t=1$: $M_1 = \mu M_0 + \nabla \mathcal{L}(W_1)$.  
    - На шаге $t=k$: $M_k$ содержит вклад всех предыдущих градиентов, но чем старше градиент, тем меньше его влияние (из-за умножения на $\mu^k$).

*   **Зачем это нужно?**  
    1. **Подавление шума**: градиенты в нейронных сетях часто "зашумлены" (особенно при обучении на мини-батчах). Импульс сглаживает эти колебания.  
    2. **Ускорение сходимости**: в "оврагах" ландшафта функции потерь (узких областях с резкими перепадами) импульс помогает быстрее двигаться вдоль дна оврага.  
    3. **Проблема "застревания"**: без импульса модель может колебаться вокруг локального минимума, не достигая его (см. визуализацию выше).

---

#### Аналогия:
Представьте, что вы спускаетесь с холма:  
- **Без импульса (SGD)**: Вы делаете маленькие шаги строго вниз по склону. Если встретите камень (шум), резко меняете направление.  
- **С импульсом**: Вы набираете скорость, как шар, катящийся по склону. Мелкие препятствия (шум) не меняют вашу траекторию, а общее движение становится плавнее и быстрее.

---

#### Особенности в Muon:
- В отличие от Adam, где импульс комбинируется с адаптивным шагом, Muon использует "чистый" импульс перед ортогонализацией.  
- Матрица $M_t$ позже преобразуется в ортогональную матрицу $O_t$ (через итерацию Ньютона-Шульца), что предотвращает схлопывание весов в низкоранговое подпространство.

> **Важно**: В Muon импульс применяется ко всем матрицам параметров модели (например, $W^Q$, $W^K$ в механизме внимания), а не только к скалярным величинам.

### Ортогонализация градиента с помощью итерации Ньютона-Шульца

![orthogonalization_iterations_3d](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/orthogonalization_iterations_3d.png)

![orthogonalization_visualization](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/orthogonalization_visualization.png)

$$
O_t = \text{Newton-Schulz}(M_t)
$$

#### Ключевые компоненты:  
1. **$M_t$ (накопленный импульс)**  
   - Исходная матрица градиентов с учётом истории (из предыдущего шага Momentum).  
   - Размерность: например, $7168 \times 7168$ для матриц внимания в Kimi-K2.  

2. **Итерация Ньютона-Шульца**  
   - Быстрый численный метод приближённой ортогонализации матриц.  
   - Альтернатива дорогостоящему SVD-разложению.  

3. **$O_t$ (ортогонализованный градиент)**  
   - Итоговая матрица с перпендикулярными направлениями обновления.  
   - Гарантирует разнонаправленные шаги оптимизации.  

---  

#### Подробное объяснение:  

* **Что это?**  
  - Процесс преобразования градиента $M_t$ в матрицу $O_t$, где все направления обновления становятся **взаимно перпендикулярными**.  
  - Аналог "разведки местности" по разным осям вместо движения по одной линии.  

* **Как это работает?**  
  1. **Нормировка**:  
     - $M_t$ масштабируется по Фробениусу (аналог "деления на длину вектора" для матриц):  
     $$  
     X_0 = \frac{M_t}{\|M_t\|_F}  
     $$  
  2. **Итеративное уточнение**:  
     - За 5 шагов ($k=1..5$) вычисляется последовательность матриц $X_k$ по формуле:  
     $$  
     X_{k} = 3.4445 \cdot X_{k-1} - 4.7750 \cdot (X_{k-1}X_{k-1}^T)X_{k-1} + 2.0315 \cdot (X_{k-1}X_{k-1}^T)^2 X_{k-1}  
     $$  
     - Коэффициенты подобраны для устойчивой сходимости.  
  3. **Результат**:  
     - После 5 итераций $O_t = X_5$ — почти ортогональная матрица.  

* **Зачем это нужно?**  
  1. **Борьба с коллапсом ранга**:  
     - Предотвращает "сплющивание" матриц весов в низкоразмерное подпространство.  
  2. **Разнообразие направлений**:  
     - Каждое обновление исследует новое направление, а не повторяет предыдущие.  
  3. **Эффективность**:  
     - Дешевле SVD (в 5-10 раз быстрее для больших матриц).  

---  

#### Особенности в Muon:  
- **Локальность**: ортогонализация применяется отдельно к каждой матрице весов (например, $W_Q$, $W_K$ в слоях внимания).  
- **Фиксированные затраты**: всегда 5 итераций независимо от размера матрицы.  
- **Совместимость с BF16**: метод устойчив к погрешностям низкоточной арифметики.  

> **Важно**: Ортогонализация не меняет "силу" градиента (норму), только его **направления**. Это как перераспределить тот же бюджет шагов по разным осям координат.

### Обновление весов

![weight_update_visualization](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/weight_update_visualization.png)

![weight_decay_visualization](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/weight_decay_visualization.png)

![scale_normalization_visualization](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/scale_normalization_visualization.png)

С учётом weight decay и согласования масштаба обновлений:

$$
W_t = W_{t-1} - \eta_t \left( \gamma \cdot O_t \cdot \sqrt{\max(A,B)} + \lambda W_{t-1} \right)
$$

где:

* $\gamma = 0.2$ — масштаб нормализации, согласующий Muon с RMS обновлений AdamW (обычно в диапазоне 0.2–0.4);
* $\lambda$ — коэффициент weight decay;
* $\sqrt{\max(A,B)}$ компенсирует несогласованный масштаб обновлений, обусловленный размерностью матрицы.

#### Ключевые компоненты:  
1. **$O_t$ (ортогонализованный градиент)**  
   - Результат предыдущего шага (итерация Ньютона-Шульца).  
   - Содержит перпендикулярные направления для обновления.  

2. **$\sqrt{\max(A,B)}$ (нормализация масштаба)**  
   - $A$ и $B$ — размеры матрицы весов (например, для $W_Q$ размером $7168 \times 7168$: $A=B=7168$).  
   - Компенсирует разницу в масштабах обновлений для матриц разной формы.  

3. **$\lambda W_{t-1}$ (weight decay)**  
   - Регуляризация для предотвращения переобучения.  
   - Аналог "трения" — постепенно уменьшает величину весов.  

4. **$\gamma$ (коэффициент масштабирования)**  
   - Подбирается эмпирически (0.2 для Kimi-K2).  
   - Согласует шаг Muon с типичными обновлениями AdamW.  

---  

#### Подробное объяснение:  

* **Что происходит?**  
  Формула обновляет веса модели, комбинируя:  
  - **Интеллектуальное направление** ($O_t$ — ортогонализованный градиент)  
  - **Стабилизирующие поправки** (нормализация масштаба + weight decay)  

* **Пошаговая логика:**  
  1. **Ортогональный шаг**:  
     $\gamma \cdot O_t$ задаёт основное направление обновления, где:  
     - $\gamma=0.2$ уменьшает шаг для стабильности  
     - $O_t$ гарантирует разнонаправленность обновлений  

  2. **Коррекция масштаба**:  
     Умножение на $\sqrt{\max(A,B)}$ решает проблему:  
     - Для матрицы $1024 \times 4096$ ($\max=4096$):  
       $\sqrt{4096} = 64$ увеличит шаг  
     - Для матрицы $128 \times 128$ ($\max=128$):  
       $\sqrt{128} \approx 11.3$ уменьшит шаг  

  3. **Регуляризация**:  
     $\lambda W_{t-1}$ действует как:  
     - "Тормоз" для больших весов (L2-регуляризация)  
     - Предотвращает перерост параметров  

  4. **Итоговое обновление**:  
     Всё суммируется и умножается на скорость обучения $\eta_t$  

* **Зачем такие сложности?**  
  1. **Для больших моделей**:  
     - Без $\sqrt{\max(A,B)}$ широкие матрицы получали бы гигантские обновления  
     - Без $\lambda$ веса выходили бы за пределы bf16  

  2. **Для качества обучения**:  
     - Ортогональность $O_t$ улучшает исследование пространства параметров  
     - Weight decay сохраняет обобщающую способность  

---  

#### Практические нюансы:  
- **Для матриц внимания**:  
  $W_Q$, $W_K$ дополнительно проходят через **QK-clip** (отдельное ограничение норм)  
- **Значения параметров в Kimi-K2**:  
  - $\gamma = 0.2$  
  - $\lambda \approx 0.01$ (типично для LLM)  
  - $\eta_t$ уменьшается по расписанию косинусного затухания

### Оптимизатор **MuonClip**

Обучение столь большой MoE-модели сопряжено с серьёзными трудностями – прежде всего, **неустойчивость обучения**, проявляющаяся во взрывающихся логитах внимания. Стандартно для LLM применяется **AdamW**, но команда **Moonshot AI** разработала более токен-эффективный оптимизатор — [**Muon**](https://arxiv.org/abs/2502.16982), который показал превосходство над AdamW при обучении больших языковых моделей. Однако при масштабировании (например, в модели **Kimi K2**, построенной по архитектуре, схожей с DeepSeek-V3) возникла проблема нестабильности — логиты внимания становились чрезмерно высокими, особенно на поздних этапах обучения. Это приводило к "дивергенции" — резкому скачку значений loss и остановке обучения.

Чтобы устранить эту проблему, была предложена модификация под названием **MuonClip**, ключевым элементом которой является техника **QK-clip**. Её суть — в **прямом масштабировании весов проекций Query и Key после обновления оптимизатором**. Тем самым логиты внимания контролируются «на источнике» — ещё до применения softmax. Это оказалось более устойчивым решением по сравнению с логит-клиппингом, нормализациями query/key и другими эвристиками.

![loss_vs_tokens](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/loss_vs_tokens.png)

Формально, MuonClip вводит адаптивный масштабирующий множитель $\eta$ и балансирующий гиперпараметр $\alpha$, по следующим формулам:

$$
q_i = \eta^\alpha W_q x_i,\quad
k_i = \eta^{1 - \alpha} W_k x_i,
$$

где $W_q$, $W_k$ — веса слоёв внимания, $x_i$ — входной вектор, а итоговый логит внимания становится:

$$
(\eta^\alpha q_i)^T (\eta^{1 - \alpha} k_j) = \eta \, q_i^T k_j.
$$

Таким образом, масштаб логитов внимания $q_i^T k_j$ прямо регулируется $\eta$, который обновляется на каждом шаге:

$$
\eta = \min\left( \frac{t}{\max_{i,j} (q_i^T k_j)}, 1 \right),
$$

где $t$ — заранее заданный порог. Это позволяет гарантировать, что никакой логит не превысит допустимого значения, даже в случае накопленных градиентов. Такая адаптация предотвращает взрывы в softmax, сохраняя стабильность градиентов и управление энергией внимания.

**Симптомы**:  
- На поздних этапах обучения логиты $q_i^T k_j$ достигают аномально высоких значений (например, $10^3$ вместо типичных $[-10, 10]$).  
- Это приводит к:  
  - **Численной нестабильности**: softmax выдаёт NaN из-за переполнения экспоненты.  
  - **Дивергенции loss**: Резкий скачок ошибки и коллапс обучения.  

**Причины**:  
1. Накопление градиентов в $W_q$ и $W_k$ при большой глубине сети.  
2. Отсутствие естественного ограничения на рост норм весов в Muon (в отличие от AdamW, где адаптивный шаг частично решает эту проблему).  

---

### Решение: QK-clip — контроль логитов "у истоков"

![qk_clip_visualization](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/qk_clip_visualization.png)

Вместо постобработки логитов (например, через `torch.clamp`) MuonClip **встраивает ограничение прямо в веса проекций** Query и Key. Это достигается введением адаптивного параметра $\eta$ и балансирующего коэффициента $\alpha$:

$$
q_i = \eta^\alpha W_q x_i, \quad  
k_i = \eta^{1 - \alpha} W_k x_i
$$

**Как это работает**:  
1. **Масштабирование логитов**:  
   Результирующий логит $q_i^T k_j$ превращается в $\eta \cdot (W_q x_i)^T (W_k x_j)$.  
   - Если $\eta = 0.5$, все логиты уменьшаются вдвое.  
   - При $\eta = 1$ — система работает "как есть".  

2. **Автоматическая регулировка**:  
   На каждом шаге $\eta$ пересчитывается по правилу:  
   $$
   \eta = \min\left( \frac{t}{\max_{i,j} (q_i^T k_j)}, 1 \right),  
   $$  
   где $t$ — пороговое значение (например, $t=50$).  

3. **Баланс между Query и Key**:  
   Гиперпараметр $\alpha \in [0,1]$ распределяет "ответственность" за масштабирование:  
   - $\alpha=0.5$: равномерное масштабирование обеих проекций.  
   - $\alpha=0.8$: большая нагрузка на $W_q$ (полезно, если Key должны оставаться стабильными).  

---

### Практическая реализация в Kimi K2

1. **Место в пайплайне**:  
   - QK-clip применяется **после каждого обновления весов** Muon, но **перед forward-pass**.  
   - Вычисляется для **каждого головы внимания** независимо.  

2. **Гиперпараметры**:  
   - Порог $t$: выбирается из диапазона $[30, 100]$ (зависит от глубины сети).  
   - $\alpha$: обычно $0.5$ или $0.6$ (подбирается на валидации).  

3. **Совместимость с BF16/FP16**:  
   - Масштабирование через $\eta$ предотвращает переполнение при вычислении softmax.  
   - Градиенты для $\eta$ не требуются — это детерминированная операция.  

4. **Результаты**:  
   - Обучение на **15.5 трлн токенов** без инцидентов.  
   - Отсутствие NaN даже в слоях с 7168 размерностью.  

---

### Дополнительные техники в MuonClip

1. **Динамический порог $t$**:  
   - На ранних этапах $t=100$ (разрешает исследование).  
   - К концу обучения $t=30$ (жесткий контроль).  

2. **Экспоненциальное сглаживание $\eta$**:  
   Чтобы избежать резких скачков, используется:  
   $$
   \eta_{\text{новое}} = 0.9 \cdot \eta_{\text{старое}} + 0.1 \cdot \eta_{\text{расчетное}}  
   $$  

3. **Интеграция с Weight Decay**:  
   QK-clip дополняет, но не заменяет L2-регуляризацию. Общая формула обновления:  
   $$
   W_t = (1 - \lambda) W_{t-1} - \eta_t \cdot \text{MuonGrad} \quad \text{→} \quad \text{QK-clip}  
   $$  

На практике, **Kimi K2** была успешно предобучена на **15.5 триллионов токенов** с использованием MuonClip — **без единого сбоя, всплеска loss или остановки обучения**. Это стало возможным благодаря точному контролю за логитами внимания и адаптивному масштабированию весов. Отметим, что обучение, вероятнее всего, происходило в формате **BF16** или **FP16** с динамическим скейлингом потерь, что позволило эффективно использовать память GPU. На этапе инференса веса были переведены в формат **FP8 с блоковым квантованием**, но сама тренировка проводилась в высокой точности.

Таким образом, **MuonClip** представляет собой не просто очередной оптимизатор, а **инженерное решение проблемы масштабируемости LLM**. Он объединяет преимущества токен-эффективности Muon с точной стабилизацией механизма внимания — и становится одним из ключевых факторов, позволивших обучить модель такого масштаба без сбоев.

### Распределённое обучение

Триллион параметров – запредельно много для памяти одного устройства, поэтому обучение Kimi-K2 проводилось распределённо на большом количестве GPU. Moonshot не раскрывает точной конфигурации, но оценки экспертов предполагают сотни высокопроизводительных карт (например, NVIDIA A100/H100) и затраты порядка десятков миллионов долларов. Для эффективной параллелизации модели использовался стек решений на базе [**DeepSpeed**](https://github.com/deepspeedai/DeepSpeed) и техник [Zero Redundancy Optimizer (**ZeRO**)](https://arxiv.org/abs/1910.02054). В частности, применялась по крайней мере **ZeRO Stage-1** или Stage-2, при которых градиенты и стейты оптимизатора распределяются между узлами, уменьшая требование к памяти на каждую карту. Вероятно, модель также разрезалась по экспертам между различными узлами (естественное решение для MoE – разные эксперты хранятся на разных девайсах, а токены роутятся к ним). Такой подход масштабируется почти линейно – добавление новых GPU позволяет вместить больше экспертов. Кроме того, применялись стандартные приёмы вроде **градиентного чекпоутинга** (checkpointing activations) – промежуточные активации не сохраняются полностью, а пересчитываются при обратном проходе, что существенно экономит память при обучении на длинных последовательностях ценой небольшого дополнительного времени. Все эти инженерные решения вместе сделали возможным обучение модели невероятного размера.

### Fine-tuning и RLHF:

После окончания предобучения, разработчики провели дополнительную этапную донастройку модели для придания ей *agentic*-возможностей и пользовательского интерфейса. Было выпущено две версии: **Kimi-K2-Base** – базовая модель после предобучения (предназначена для исследователей, можно самостоятельно дообучать), и **Kimi-K2-Instruct** – модель, прошедшая специальный пост-тюнинг, готовая для интерактивного использования в качестве чат-бота или агентной системы.

В пост-тюнинге особое внимание уделялось обучению модели выполнять *действия*, а не только отвечать текстом. Этот этап можно условно разделить на **supervised fine-tuning на синтетических задачах** и **подкрепляющее обучение с обратной связью**.

* **Имитирование использования инструментов:** команда Moonshot сгенерировала обширный набор задач, требующих взаимодействия с внешними инструментами (API, базы данных, shell-команды, веб-поиск и т.д.), чтобы научить модель последовательности действий. Вместо ручной разметки был применён метод *Large-Scale Agentic Data Synthesis*: с помощью вспомогательных ИИ-агентов симулировались тысячи сценариев из сотен доменов, где агент (модель) должен был пользоваться различными инструментами для достижения цели. Все шаги (запросы к инструментам, полученные ответы, финальные решения) фиксировались в виде псевдодиалогов. Затем отдельная модель-судья (LLM-critic) оценивала эти сгенерированные эпизоды по заданным рубрикам качества, отбирая только лучшие, наиболее успешные попытки. Отфильтрованные таким образом высококачественные последовательности действий были использованы для *обучения с учителем* – Kimi-K2 дообучалась повторять такие многошаговые решения, фактически впитывая шаблоны, как планировать и вызывать инструменты. Этот процесс заложил фундамент «агентного мышления» уже в веса базовой модели.

* **RL с самооценкой (в духе RLHF):** помимо имитационного обучения, разработчики внедрили механизм *Reinforcement Learning* для дальнейшего повышения навыков модели решать задачи, особенно те, где нет однозначного правильного ответа. Главная проблема классического RLHF (обучения с подкреплением от отклика человека) – ограниченность и узость сигналов вознаграждения для творческих или аналитических задач. В Kimi-K2 подошли творчески: модель обучалась *самостоятельно оценивать свои ответы* по заданным критериям. Реализована система **самокритики (self-critique)**: модель генерирует ответ и параллельно (или последующим шагом) генерирует оценку этому ответу на основе заранее заданных “рубрик” качества. Поскольку такой критик сам может быть несовершенным, его регулярно улучшали на заданиях, где успех легко проверяется (например, решение матем. задач или кодовых тестов) – эти *верифицируемые задачи* использовались для *обучения критика* более точному прогнозированию качества. Затем этот улучшенный критик применялся к неверифицируемым заданиям (написание эссе, анализ) и давал сигнал награды/штрафа основной модели. Таким образом, шло итеративное обучение с подкреплением без непосредственного участия человека: модель училась улучшать свои действия, опираясь на внутреннюю «судейскую систему», калиброванную на решаемых задачах. Такой подход родственен RLHF, но заменяет человеческий фидбэк на масштабируемый AI-фидбэк. В результате **Kimi-K2-Instruct** получила «рефлекторные» навыки: она сразу выдаёт действие или ответ, близкий к оптимальному, без необходимости в длинных раздумьях (т.н. *reflex-grade model* без длительного chain-of-thought).

Итогом финального обучения стала модель, способная **следовать инструкциям**, поддерживать диалог и *автономно выполнять сложные последовательности действий*. Отметим, что на текущий момент Kimi-K2-Instruct не является мультимодальной – в отличие от предыдущей версии (Kimi k1.5) она не умеет обрабатывать визуальные данные напрямую и не имеет отдельного «режима раздумий». Команда сконцентрировалась на текстовых и агентных возможностях, планируя добавить поддержку изображений и более продвинутые механизмы рассуждения («длительное размышление») в будущих версиях.

## Дополнительные детали и сравнение с предшественниками

### Эволюция по сравнению с Kimi k1.5 

Новая модель Kimi-K2 знаменует существенный шаг вперёд относительно предыдущих моделей Moonshot AI. Предшественник (Kimi k1.5) был выпущен ранее в 2025 году и представлял собой мультимодальный LLM с поддержкой изображений и расширенным контекстом 128k. Kimi k1.5 также использовала RL-подходы в обучении и имела внушительный размер, однако значительно уступающий Kimi-K2: около **389 млрд параметров (52 млрд активных)** при архитектуре MoE, то есть была почти втрое меньше нынешней модели. Kimi-K2 расширила масштабы: 1 трлн параметров (+157% к K1.5) и внедрила новые технологические решения – в частности, механизм MLA для внимания, тогда как Kimi k1.5 в своих длинноконтекстных способностях опиралась на более традиционные подходы (позиционное интерполирование). Кроме того, K1.5 была ориентирована на мультимодальность и диалог, тогда как K2 сделала упор на **agentic-возможности** (автономное выполнение задач). Kimi-K2 в текущей версии не поддерживает обработку изображений или аудио (мультимодальные аспекты планируются позднее), но заметно превосходит K1.5 по *текстовым* и *кодовым* задачам, а также по способности пользоваться инструментами. Еще одно отличие – **открытость**: Kimi-K2 выпущена с открытым исходным кодом и весами (Modified MIT License), тогда как Kimi k1.5 была скорее свободно доступна через API/интерфейс, но без полноценной открытой модели. Таким образом, Kimi-K2 представляет собой более масштабную, узкоспециализированную на агентности эволюцию семейства Kimi.

### Производительность на бенчмарках

![Figure_02](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/Figure_02.png)

Kimi-K2 на момент выпуска демонстрирует *state-of-the-art* результаты среди открытых моделей и вплотную подбирается к закрытым лидерам. На академическом тесте знаний и мышления **MMLU** (57 предметов) модель набирает около **87,8%** точности, что превышает результаты всех предшествующих open-source LLM (для сравнения, OpenAI GPT-4 оценивался \~86.4% на MMLU). На конкурсе **C-Eval** (китайский аналог MMLU) Kimi-K2 показала \~**92,5%**, существенно опередив предыдущие модели на китайском языке – подтверждение её глубокого понимания китайских данных. В сложных математических задачах (**MATH** комплексные задачи школьной программы) достигнуто **70,2%** правильных решений – это заметный скачок относительно моделей прошлого поколения (для сравнения, GPT-4 – около 85%, Llama-2 70B – \~50%). На арифметических задачах начальной школы (**GSM8K**) модель верно решает **92,1%** задач, практически устранив ранее характерные ошибки в многошаговых вычислениях.

Особенно впечатляют показатели в программировании. В тестах генерации кода Kimi-K2 устанавливает новые рекорды среди открытых моделей. Например, **LiveCodeBench v6** (реалистичный бенчмарк по конкурентному программированию) – базовая модель Kimi-K2-Base достигает \~**26,3%** точности *pass\@1*, а финальная инструкционная версия Kimi-K2-Instruct – **53,7%** *pass\@1*, **опережая** даже GPT-4.1 (около 44.7%) на этих задачах. В мульти-язычном программировании (**MultiPL-E** бенчмарк) модель приближается к верхним строчкам с \~85-86% точности, а на внутреннем тесте **SWE-bench (Software Engineering)** показала **65,8%** успешных решений, что сравнимо с некоторыми проприетарными моделями Anthropic и значительно лучше большинства open-source моделей.

Также Kimi-K2 лидирует на специализированных агентных бенчмарках: так, на наборах **Tau** и **AceBench** (оценка умения модели использовать инструменты) она заняла первые места среди открытых моделей. Например, в сценариях Tau (поиск решения в доменах ритейла, авиабилетов, телеком и пр. с помощью инструментов) Kimi-K2-Instruct показывает 70-75% успеха, приближаясь к уровням Claude 2 и превосходя другие открытые аналоги.

В совокупности эти результаты свидетельствуют, что **Kimi-K2 установила новый уровень качества** для открытых моделей. По многим метрикам она **догоняет, а порой и превосходит** крупнейшие закрытые системы. Например, разработчики отмечают, что Kimi-K2-Instruct обходит версии Claude 4 (Anthropic) и даже обновлённый GPT-4.1 на ряде ключевых тестов. VentureBeat также подчёркивает, что Kimi-K2 превзошла GPT-4 в некоторых «болевых точках» вроде математических доказательств и сложного кода.

Конечно, модель не идеальна – разработчики указывают, что Kimi-K2 всё ещё может ошибаться в очень длинных цепочках рассуждений, может давать избыточно подробные ответы на простые вопросы, и пока *не* обладает мультимодальными способностями (не «видит» изображения). Однако эти недостатки признаны и активно прорабатываются (планируется улучшение «долгого мышления» и добавление зрения в следующих версиях).

### Вывод

Kimi-K2 представляет собой выдающуюся в техническом плане LLM: инновационная архитектура MoE с топ-8 экспертизой и QK-клип оптимизацией позволили создать *открытую* модель с 1 трлн параметров, обученную на беспрецедентном объёме данных без сбоев. Процесс обучения включал передовые методы устойчивой оптимизации (MuonClip, BF16), распределения нагрузки (ZeRO) и имитационного/подкрепляющего обучения для формирования агентных навыков. Получившаяся модель задаёт новый стандарт качества среди open-source AI, особенно блистая в программировании, математике и автономном выполнении задач. Kimi-K2-Base предоставляет исследователям мощную базу для собственных экспериментов и дообучения, а Kimi-K2-Instruct уже сейчас доступна для прямого использования – её можно запустить локально или через API без каких-либо платных подписок. Модель Kimi-K2 демонстрирует, что открытые инициативы могут конкурировать с лидерами индустрии, и открывает путь к созданию ещё более продвинутых и доступных ИИ-систем в ближайшем будущем.