# Kimi-K2

## Введение

Современные большие языковые модели (LLM) стали ключевым инструментом в самых разных областях — от автоматической генерации текста и программного кода до поддержки интеллектуальных агентов, способных выполнять сложные задачи. С ростом объёма параметров и числа токенов предобучения мы наблюдаем качественный скачок в способностях моделей: улучшение понимания контекста, точности генерации и умений решать узкоспециализированные задачи.

Модель Kimi-K2, разработанная Moonshot AI, представляет собой один из самых амбициозных проектов в экосистеме открытых LLM. Она использует архитектуру Mixture-of-Experts (MoE) и насчитывает триллион параметров, при этом благодаря «разряженной» активации задействует для каждого токена лишь порядка 32 миллиардов параметров. Kimi-K2 сочетает в себе передовые методы оптимизации внимания для обработки сверхдлинных контекстов (до 128 тысяч токенов), инновационный оптимизатор MuonClip для стабильного и эффективного обучения на потрясающем объёме данных (15,5 триллионов токенов), а также комплексный пост-тюнинг для превращения базовой модели в интерактивного, агентно-ориентированного ассистента.

В этом обзоре мы подробно рассмотрим:

1. **Архитектуру** Kimi-K2 — принципы работы MoE, модификации механизма внимания, ключевые параметры и инженерные решения для ускорения инференса.
2. **Процесс обучения** модели — предобучение на огромном корпусе данных, используемые оптимизаторы и техники распределённого обучения, а также этапы fine-tuning и RL-подкрепления для формирования «agentic»-возможностей.
3. **Ключевые результаты и сравнение** с предыдущими версиями и лидерами отрасли — на академических бенчмарках, в задачах программирования и агентных сценариях.


## Архитектура модели Kimi-K2

### Общие характеристики:

Kimi-K2 – это крупная языковая модель с архитектурой *Mixture-of-Experts* (MoE), про MoE мы как-то писали вот [тут](https://verbasik.github.io/Weekly-arXiv-ML-AI-Research-Review/#2025/week-06). Общий размер модели составляет **1 триллион параметров**, однако в каждый момент времени активна лишь примерно **32 млрд параметров** – то есть только небольшая часть весов участвует в обработке конкретного токена. Модель содержит **61 трансформер-слой** (из них один – «плотный», без разбиения на экспертов). Размер скрытых представлений (эмбеддингов) равен **7168**, используется **64 головы внимания**. Словарный запас – **160k** токенов, а максимальная длина контекста модели – **128k токенов**. Модель является авто-регрессионным декодером (аналогично GPT) – то есть генерирует текст, последовательно предсказывая следующий токен на основе предыдущих.

![Figure_01](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/Figure_01.jpg)

### Mixture-of-Experts:

В каждом слое трансформера, реализованном как MoE, присутствует **384 отдельных эксперта** (специализированных подсетей-MLP). Для каждого токена *динамический маршрутизатор* (gating) выбирает **топ-8 экспертов** из этих 384, которые будут использованы для обработки данного токена, плюс один специальный **общий эксперт**, активный всегда. Таким образом, на каждый токен приходится вычисление по 9 экспертным подсетям (8 выбранных + 1 общий), что значительно повышает эффективность модели без линейного роста вычислений. Ненужные эксперты остаются «спящими», благодаря чему достигается огромный общий объём параметров (1T) при приемлемых затратах на вывод (активно 32B). Маршрутизация выполняется по принципу *top-k* (в Kimi-K2 выбрано k=8), вероятно с помощью механизмов top-2 или top-8 gating, что позволяет каждому токену получать комбинацию из нескольких «мнений» экспертов вместо одного. Наличие единого **shared (общего) эксперта**, который участвует всегда, служит для улучшения устойчивости и базового качества – этот элемент обеспечивает, что в каждом слое присутствует одна общая плотная подсеть, дополняющая узкоспециализированных экспертов. Активация только части подсетей (**sparse activation**) экономит вычисления: не все параметры участвуют в каждом проходе, а только наиболее релевантные для текущего токена.

### Механизмы внимания:

Модель использует модифицированный механизм самовнимания, оптимизированный для длинного контекста. Во-первых, количество голов внимания **уменьшено** по сравнению со стандартными трансформерами – Kimi-K2 имеет 64 головы на слой при размере эмбеддинга 7168 (что даёт нетипичный размер проекции ~112 на голову). Более крупные головы в меньшем количестве помогают сделать расчёты внимания более стабильными на больших длинах последовательности. Во-вторых, заимствованный метод **Multi-Head *Latent* Attention (MLA)** – «многоголовое латентное внимание», который был впервые продемонстрирован у DeepSeek V3, об этом мы максимально подробно писали [тут](https://verbasik.github.io/Weekly-arXiv-ML-AI-Research-Review/#2025/week-07_&_08). Этот подход радикально снижает требования к памяти и вычислениям при работе с длинным контекстом. В традиционном многоголовом внимании требуется хранить для каждого токена большие матрицы ключей и значений (размерность пропорциональна числу голов и их размерности). В MLA же каждая позиция в контексте сохраняется в виде **компрессированного латентного вектора** фиксированной размерности (например, ~512-576 измерений, включая позиции) независимо от числа голов. По сути, ключи/значения не хранятся раздельно для каждой головы, а сжимаются в общее представление; затем входной *query*-вектор проецируется в это сжатое пространство для вычисления скалярных весов внимания, и результат проецируется обратно. Такой механизм позволяет уменьшить объём кеша ключей/значений **примерно в 60 раз по сравнению с обычным MHA** и в ~12 раз по сравнению с группированным вниманием (GQA), что делает практичным использование контекста в 128k токенов без переполнения памяти. Несмотря на дополнительные операции проекции, общая сложность вычислений внимания на длинных последовательностях существенно падает – расчёты для обновления KV-кеша и для шага внимания в MLA требуют на порядки меньше FLOPs и объёма памяти, чем в стандартном случае.

Для реализации внимания в таком огромном контексте разработчики также применили **FlashAttention** – высокоэффективный слитный GPU-алгоритм вычисления матриц внимания. В публикациях отмечается, что при авто-регрессии (генерации по одному токену) MLA интегрируется с FlashAttention-подобным ядром для этапов матричного умножения *QK* и *AV*, выполняя их в т.н. «тайлах» для повышения производительности. Кроме того, Kimi-K2 использует *ротари позиционные эмбеддинги* (RoPE) для кодирования положения токенов в последовательности. Благодаря технике **RoPE scaling** (линейному или динамическому масштабированию шага фазовой ротари-функции) модель поддерживает удлинённое окно контекста без потери способности различать близкие позиции. Наконец, в проекциях внимания **не используются смещения (bias)**, и вероятно отключён dropout в слоях внимания – эти упрощения часто применяются в крупных LLM для экономии параметров и стабильности. В активационных функциях модель задействует **SwiGLU** (Swish + Gated Linear Unit) в позиционно-независимых MLP слоёв – эта функция активации доказала эффективность в больших трансформерах (применялась, например, в PaLM).

## Процесс обучения

### Датасеты и объёмы данных:

Kimi-K2 обучена на *огромном корпусе текстовых данных* объёмом **15,5 триллионов токенов** – это один из самых больших датасетов, использованных для обучения LLM на сегодняшний день. По сути модель «прочитала» практически весь доступный интернет-контент (включая множество источников на английском, китайском и других языках, кодовые репозитории, научные тексты и пр.) многократно. Такая задача обучения заставляет модель формировать обобщённое представление о языке и знаниях, содержащихся в данных. Обучение производилось на последовательностях с переменной длиной и, вероятно, с увеличением максимальной длины контекста по мере обучения (чтобы эффективно задействовать 128k контекст к концу обучения). В состав предобучающего корпуса вошли тексты разнообразных доменов: от энциклопедий и новостей до кода и математических решений. Особый упор, судя по результатам, делался на данные для программирования и математические задачи – модель демонстрирует выдающиеся результаты в кодинге и математике, результаты бенчов будут чуть ниже.

### Оптимизатор Muon и его ограничения

Muon — это алгоритм оптимизации, основанный на принципах ортогонализации матриц, в частности, использующий итерацию Ньютона-Шульца для ортогонализации матриц градиентов. Основная идея заключается в поощрении разнообразных направлений обновления, предотвращая коллапс весовых матриц в низкоранговые структуры, что может ограничивать выразительность модели.

Исходный алгоритм Muon применяет следующее правило обновления:

$$
\begin{aligned}
M_t &= \mu M_{t-1} + \nabla \mathcal{L}(W_{t-1}) \\
O_t &= \text{NewtonSchulz}(M_t) \\
W_t &= W_{t-1} - \eta_t \left( \gamma \cdot O_t \cdot \sqrt{\max(A,B)} + \lambda W_{t-1} \right)
\end{aligned}
$$

Однако исходная реализация столкнулась с тремя критическими проблемами для крупномасштабного развёртывания:

1. **Отсутствие весового распада (weight decay)**: критически важно для стабильности обучения больших моделей;
2. **Несогласованные масштабы обновления для каждого параметра**: обновления непредсказуемо варьировались в разных измерениях матрицы;
3. **Требования к распределённым вычислениям**: необходимость полных матриц градиентов конфликтовала со стандартными подходами к параллелизму данных.

#### Формирование импульса (momentum)

$$
M_t = \mu M_{t-1} + \nabla \mathcal{L}(W_{t-1})
$$

* здесь $\mu$ — коэффициент импульса, аналогичный β₁ в Adam;
* $\nabla \mathcal{L}(W_{t-1})$ — градиент функции потерь по параметрам $W_{t-1}$.

Параметр $M_t$ — это накопленный градиент, отражающий предыдущие направления обновлений и сглаживающий шум.

#### Ортогонализация градиента с помощью итерации Ньютона-Шульца

$$
O_t = \text{Newton-Schulz}(M_t)
$$

Цель — аппроксимировать ортогональное преобразование от градиента:

#### Математическая цель:

$$
O_t \approx (M_t M_t^T)^{-\frac{1}{2}} M_t \approx U V^T \quad \text{где } M_t = U \Sigma V^T
$$

т.е. ортогонализация направлений обновления с сохранением спектральных свойств матрицы градиентов, чтобы избежать схлопывания в низкоранговое подпространство.

#### Итерационная формула Ньютона-Шульца:

Инициализация:

$$
X_0 = \frac{M_t}{\|M_t\|_F}
$$

Рекурсия:

$$
X_{k} = aX_{k-1} + b(X_{k-1}X_{k-1}^T)X_{k-1} + c(X_{k-1}X_{k-1}^T)^2 X_{k-1}
$$

Коэффициенты подобраны для устойчивой сходимости при малых сингулярных значениях:

$$
a = 3.4445,\quad b = -4.7750,\quad c = 2.0315
$$

После $N = 5$ итераций получается матрица $O_t$, приближающая ортогонализованное направление.

#### Обновление весов

С учётом weight decay и согласования масштаба обновлений:

$$
W_t = W_{t-1} - \eta_t \left( \gamma \cdot O_t \cdot \sqrt{\max(A,B)} + \lambda W_{t-1} \right)
$$

где:

* $\gamma = 0.2$ — масштаб нормализации, согласующий Muon с RMS обновлений AdamW (обычно в диапазоне 0.2–0.4);
* $\lambda$ — коэффициент weight decay;
* $\sqrt{\max(A,B)}$ компенсирует несогласованный масштаб обновлений, обусловленный размерностью матрицы (см. лемму в Appendix A: $\text{RMS} \propto \sqrt{1/\max(A,B)}$).

#### 🔺 Ограничения исходного Muon

В отчёте идентифицированы **три** ключевые проблемы оригинальной реализации:

1. **Нет weight decay** → рост L2-норм веса и выход за диапазон bf16;
2. **Несогласованные масштабы обновлений** → нестабильность и деградация качества;
3. **Неадаптивность к распределённым вычислениям** → необходимость сборки всей матрицы градиентов в Data Parallel, что противоречит ZeRO-1.

### Оптимизатор MuonClip 

Обучение столь большой MoE-модели сопряжено с серьёзными трудностями – прежде всего, *неустойчивость обучения* (взрывающиеся градиенты, особенно в механизме внимания). Стандартно для LLM применяется AdamW, но команда Moonshot AI разработала собственный оптимизатор под названием [**Muon**](https://arxiv.org/abs/2502.16982) (и его улучшенную версию MuonClip). 

Основная новация – приём, называемый **QK-clip**: после каждого шага обновления весов происходит *масштабирование (ограничение) весовых матриц* в проекциях *Query* и *Key* слоёв внимания. Проще говоря, модель отслеживает норму/разброс коэффициентов в матрицах $W^Q$ и $W^K$ и принудительно удерживает их в устойчивом диапазоне. Это предотвращает взрыв значений внимания (логитов softmax), которые могли бы нарушить обучение. Muon сам по себе был более «токен-эффективным» оптимизатором, чем AdamW (т.е. давал лучшее качество на токен), но страдал от нестабильности. Добавление же *Clip*-механизма решило проблему. Благодаря MuonClip удалось провести предобучение на всех 15,5 трлн токенов **без единого сбоя или всплеска лосса** – обучение прошло гладко, без столкновения с режимом дивергенции. Это выдающееся достижение: ранее попытки обучения моделей подобного масштаба часто натыкались на неожиданную нестабильность по мере роста размеров.

Стоит отметить, что для ускорения обучения применялась смешанная числовая точность. Скорее всего, модель обучалась в режиме **BF16** (brain floating point 16) либо FP16 с динамическим скейлингом потерь, что является стандартом для современных LLM – это позволяло ускорить вычисления на GPU и вместить модель в память. В финальной версии веса были конвертированы в компактный формат FP8 (с блоковым квантованием) для удобства развертывания, однако само предобучение проводилось в более высокой точности для сохранения качества.