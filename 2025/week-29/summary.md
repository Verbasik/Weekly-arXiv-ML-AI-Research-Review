# Kimi-K2

![Figure_0](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/Figure_0.png)

## Введение

Современные большие языковые модели (LLM) стали ключевым инструментом в самых разных областях — от автоматической генерации текста и программного кода до поддержки интеллектуальных агентов, способных выполнять сложные задачи. С ростом объёма параметров и числа токенов предобучения мы наблюдаем качественный скачок в способностях моделей: улучшение понимания контекста, точности генерации и умений решать узкоспециализированные задачи.

Модель Kimi-K2, разработанная Moonshot AI, представляет собой один из самых амбициозных проектов в экосистеме открытых LLM. Она использует архитектуру Mixture-of-Experts (MoE) и насчитывает триллион параметров, при этом благодаря «разряженной» активации задействует для каждого токена лишь порядка 32 миллиардов параметров. Kimi-K2 сочетает в себе передовые методы оптимизации внимания для обработки сверхдлинных контекстов (до 128 тысяч токенов), инновационный оптимизатор MuonClip для стабильного и эффективного обучения на потрясающем объёме данных (15,5 триллионов токенов), а также комплексный пост-тюнинг для превращения базовой модели в интерактивного, агентно-ориентированного ассистента.

В этом обзоре мы подробно рассмотрим:

1. **Архитектуру** Kimi-K2 — принципы работы MoE, модификации механизма внимания, ключевые параметры и инженерные решения для ускорения инференса.
2. **Процесс обучения** модели — предобучение на огромном корпусе данных, используемые оптимизаторы и техники распределённого обучения, а также этапы fine-tuning и RL-подкрепления для формирования «agentic»-возможностей.
3. **Ключевые результаты и сравнение** с предыдущими версиями и лидерами отрасли — на академических бенчмарках, в задачах программирования и агентных сценариях.


## Архитектура модели Kimi-K2

### Общие характеристики:

Kimi-K2 – это крупная языковая модель с архитектурой *Mixture-of-Experts* (MoE), про MoE мы как-то писали вот [тут](https://verbasik.github.io/Weekly-arXiv-ML-AI-Research-Review/#2025/week-06). Общий размер модели составляет **1 триллион параметров**, однако в каждый момент времени активна лишь примерно **32 млрд параметров** – то есть только небольшая часть весов участвует в обработке конкретного токена. Модель содержит **61 трансформер-слой** (из них один – «плотный», без разбиения на экспертов). Размер скрытых представлений (эмбеддингов) равен **7168**, используется **64 головы внимания**. Словарный запас – **160k** токенов, а максимальная длина контекста модели – **128k токенов**. Модель является авто-регрессионным декодером (аналогично GPT) – то есть генерирует текст, последовательно предсказывая следующий токен на основе предыдущих.

![Figure_01](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/Figure_01.jpg)

### Mixture-of-Experts:

В каждом слое трансформера, реализованном как MoE, присутствует **384 отдельных эксперта** (специализированных подсетей-MLP). Для каждого токена *динамический маршрутизатор* (gating) выбирает **топ-8 экспертов** из этих 384, которые будут использованы для обработки данного токена, плюс один специальный **общий эксперт**, активный всегда. Таким образом, на каждый токен приходится вычисление по 9 экспертным подсетям (8 выбранных + 1 общий), что значительно повышает эффективность модели без линейного роста вычислений. Ненужные эксперты остаются «спящими», благодаря чему достигается огромный общий объём параметров (1T) при приемлемых затратах на вывод (активно 32B). Маршрутизация выполняется по принципу *top-k* (в Kimi-K2 выбрано k=8), вероятно с помощью механизмов top-2 или top-8 gating, что позволяет каждому токену получать комбинацию из нескольких «мнений» экспертов вместо одного. Наличие единого **shared (общего) эксперта**, который участвует всегда, служит для улучшения устойчивости и базового качества – этот элемент обеспечивает, что в каждом слое присутствует одна общая плотная подсеть, дополняющая узкоспециализированных экспертов. Активация только части подсетей (**sparse activation**) экономит вычисления: не все параметры участвуют в каждом проходе, а только наиболее релевантные для текущего токена.

### Механизмы внимания:

Модель использует модифицированный механизм самовнимания, оптимизированный для длинного контекста. Во-первых, количество голов внимания **уменьшено** по сравнению со стандартными трансформерами – Kimi-K2 имеет 64 головы на слой при размере эмбеддинга 7168 (что даёт нетипичный размер проекции ~112 на голову). Более крупные головы в меньшем количестве помогают сделать расчёты внимания более стабильными на больших длинах последовательности. Во-вторых, заимствованный метод **Multi-Head *Latent* Attention (MLA)** – «многоголовое латентное внимание», который был впервые продемонстрирован у DeepSeek V3, об этом мы максимально подробно писали [тут](https://verbasik.github.io/Weekly-arXiv-ML-AI-Research-Review/#2025/week-07_&_08). Этот подход радикально снижает требования к памяти и вычислениям при работе с длинным контекстом. В традиционном многоголовом внимании требуется хранить для каждого токена большие матрицы ключей и значений (размерность пропорциональна числу голов и их размерности). В MLA же каждая позиция в контексте сохраняется в виде **компрессированного латентного вектора** фиксированной размерности (например, ~512-576 измерений, включая позиции) независимо от числа голов. По сути, ключи/значения не хранятся раздельно для каждой головы, а сжимаются в общее представление; затем входной *query*-вектор проецируется в это сжатое пространство для вычисления скалярных весов внимания, и результат проецируется обратно. Такой механизм позволяет уменьшить объём кеша ключей/значений **примерно в 60 раз по сравнению с обычным MHA** и в ~12 раз по сравнению с группированным вниманием (GQA), что делает практичным использование контекста в 128k токенов без переполнения памяти. Несмотря на дополнительные операции проекции, общая сложность вычислений внимания на длинных последовательностях существенно падает – расчёты для обновления KV-кеша и для шага внимания в MLA требуют на порядки меньше FLOPs и объёма памяти, чем в стандартном случае.

Для реализации внимания в таком огромном контексте разработчики также применили **FlashAttention** – высокоэффективный слитный GPU-алгоритм вычисления матриц внимания. В публикациях отмечается, что при авто-регрессии (генерации по одному токену) MLA интегрируется с FlashAttention-подобным ядром для этапов матричного умножения *QK* и *AV*, выполняя их в т.н. «тайлах» для повышения производительности. Кроме того, Kimi-K2 использует *ротари позиционные эмбеддинги* (RoPE) для кодирования положения токенов в последовательности. Благодаря технике **RoPE scaling** (линейному или динамическому масштабированию шага фазовой ротари-функции) модель поддерживает удлинённое окно контекста без потери способности различать близкие позиции. Наконец, в проекциях внимания **не используются смещения (bias)**, и вероятно отключён dropout в слоях внимания – эти упрощения часто применяются в крупных LLM для экономии параметров и стабильности. В активационных функциях модель задействует **SwiGLU** (Swish + Gated Linear Unit) в позиционно-независимых MLP слоёв – эта функция активации доказала эффективность в больших трансформерах (применялась, например, в PaLM).

## Процесс обучения

### Датасеты и объёмы данных:

Kimi-K2 обучена на *огромном корпусе текстовых данных* объёмом **15,5 триллионов токенов** – это один из самых больших датасетов, использованных для обучения LLM на сегодняшний день. По сути модель «прочитала» практически весь доступный интернет-контент (включая множество источников на английском, китайском и других языках, кодовые репозитории, научные тексты и пр.) многократно. Такая задача обучения заставляет модель формировать обобщённое представление о языке и знаниях, содержащихся в данных. Обучение производилось на последовательностях с переменной длиной и, вероятно, с увеличением максимальной длины контекста по мере обучения (чтобы эффективно задействовать 128k контекст к концу обучения). В состав предобучающего корпуса вошли тексты разнообразных доменов: от энциклопедий и новостей до кода и математических решений. Особый упор, судя по результатам, делался на данные для программирования и математические задачи – модель демонстрирует выдающиеся результаты в кодинге и математике, результаты бенчов будут чуть ниже.

### Оптимизатор Muon и его ограничения

Muon — это алгоритм оптимизации, основанный на принципах ортогонализации матриц, в частности, использующий итерацию Ньютона-Шульца для ортогонализации матриц градиентов. Основная идея заключается в поощрении разнообразных направлений обновления, предотвращая коллапс весовых матриц в низкоранговые структуры, что может ограничивать выразительность модели.

Исходный алгоритм Muon применяет следующее правило обновления:

![Image_01](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/Image_01.png)

#### Ограничения Muon

**Проблемы масштабируемости в исходной версии**

- Изначально Muon демонстрировал хорошие результаты на малых моделях, но его эффективность при масштабировании на крупные модели (с миллиардами параметров) оставалась под вопросом.

**Нестабильность логитов внимания в MoE-моделях**

- Muon, оптимизируя матрицы проекций Query и Key, мог генерировать веса с аномально большими значениями, особенно на поздних этапах обучения. Это приводило к взрывным логитам внимания (до 10³–10⁵), что ломало softmax и вызывало дивергенцию loss.

- В отличие от AdamW, где learning rate и моменты косвенно ограничивают шаг обновления, Muon (особенно в сочетании с техниками вроде weight decay) иногда слишком агрессивно масштабировал веса.

### Оптимизатор **MuonClip**

Обучение столь большой MoE-модели сопряжено с серьёзными трудностями – прежде всего, **неустойчивость обучения**, проявляющаяся во взрывающихся логитах внимания. Стандартно для LLM применяется **AdamW**, но команда **Moonshot AI** разработала более токен-эффективный оптимизатор — [**Muon**](https://arxiv.org/abs/2502.16982), который показал превосходство над AdamW при обучении больших языковых моделей. Однако при масштабировании (например, в модели **Kimi K2**, построенной по архитектуре, схожей с DeepSeek-V3) возникла проблема нестабильности — логиты внимания становились чрезмерно высокими, особенно на поздних этапах обучения. Это приводило к "дивергенции" — резкому скачку значений loss и остановке обучения.

Чтобы устранить эту проблему, была предложена модификация под названием **MuonClip**, ключевым элементом которой является техника **QK-clip**. Её суть — в **прямом масштабировании весов проекций Query и Key после обновления оптимизатором**. Тем самым логиты внимания контролируются «на источнике» — ещё до применения softmax. Это оказалось более устойчивым решением по сравнению с логит-клиппингом, нормализациями query/key и другими эвристиками.

![loss_vs_tokens](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/loss_vs_tokens.png)

Формально, MuonClip вводит адаптивный масштабирующий множитель $\eta$ и балансирующий гиперпараметр $\alpha$, по следующим формулам:

![Image_02](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/Image_02.png)

где $t$ — заранее заданный порог. Это позволяет гарантировать, что никакой логит не превысит допустимого значения, даже в случае накопленных градиентов. Такая адаптация предотвращает взрывы в softmax, сохраняя стабильность градиентов и управление энергией внимания.

На практике, **Kimi K2** была успешно предобучена на **15.5 триллионов токенов** с использованием MuonClip — **без единого сбоя, всплеска loss или остановки обучения**. Это стало возможным благодаря точному контролю за логитами внимания и адаптивному масштабированию весов. Отметим, что обучение, вероятнее всего, происходило в формате **BF16** или **FP16** с динамическим скейлингом потерь, что позволило эффективно использовать память GPU. На этапе инференса веса были переведены в формат **FP8 с блоковым квантованием**, но сама тренировка проводилась в высокой точности.

Таким образом, **MuonClip** представляет собой не просто очередной оптимизатор, а **инженерное решение проблемы масштабируемости LLM**. Он объединяет преимущества токен-эффективности Muon с точной стабилизацией механизма внимания — и становится одним из ключевых факторов, позволивших обучить модель такого масштаба без сбоев.

### Распределённое обучение

Триллион параметров – запредельно много для памяти одного устройства, поэтому обучение Kimi-K2 проводилось распределённо на большом количестве GPU. Moonshot не раскрывает точной конфигурации, но оценки экспертов предполагают сотни высокопроизводительных карт (например, NVIDIA A100/H100) и затраты порядка десятков миллионов долларов. Для эффективной параллелизации модели использовался стек решений на базе [**DeepSpeed**](https://github.com/deepspeedai/DeepSpeed) и техник [Zero Redundancy Optimizer (**ZeRO**)](https://arxiv.org/abs/1910.02054). В частности, применялась по крайней мере **ZeRO Stage-1** или Stage-2, при которых градиенты и стейты оптимизатора распределяются между узлами, уменьшая требование к памяти на каждую карту. Вероятно, модель также разрезалась по экспертам между различными узлами (естественное решение для MoE – разные эксперты хранятся на разных девайсах, а токены роутятся к ним). Такой подход масштабируется почти линейно – добавление новых GPU позволяет вместить больше экспертов. Кроме того, применялись стандартные приёмы вроде **градиентного чекпоутинга** (checkpointing activations) – промежуточные активации не сохраняются полностью, а пересчитываются при обратном проходе, что существенно экономит память при обучении на длинных последовательностях ценой небольшого дополнительного времени. Все эти инженерные решения вместе сделали возможным обучение модели невероятного размера.

### Fine-tuning и RLHF:

После окончания предобучения, разработчики провели дополнительную этапную донастройку модели для придания ей *agentic*-возможностей и пользовательского интерфейса. Было выпущено две версии: **Kimi-K2-Base** – базовая модель после предобучения (предназначена для исследователей, можно самостоятельно дообучать), и **Kimi-K2-Instruct** – модель, прошедшая специальный пост-тюнинг, готовая для интерактивного использования в качестве чат-бота или агентной системы.

В пост-тюнинге особое внимание уделялось обучению модели выполнять *действия*, а не только отвечать текстом. Этот этап можно условно разделить на **supervised fine-tuning на синтетических задачах** и **подкрепляющее обучение с обратной связью**.

* **Имитирование использования инструментов:** команда Moonshot сгенерировала обширный набор задач, требующих взаимодействия с внешними инструментами (API, базы данных, shell-команды, веб-поиск и т.д.), чтобы научить модель последовательности действий. Вместо ручной разметки был применён метод *Large-Scale Agentic Data Synthesis*: с помощью вспомогательных ИИ-агентов симулировались тысячи сценариев из сотен доменов, где агент (модель) должен был пользоваться различными инструментами для достижения цели. Все шаги (запросы к инструментам, полученные ответы, финальные решения) фиксировались в виде псевдодиалогов. Затем отдельная модель-судья (LLM-critic) оценивала эти сгенерированные эпизоды по заданным рубрикам качества, отбирая только лучшие, наиболее успешные попытки. Отфильтрованные таким образом высококачественные последовательности действий были использованы для *обучения с учителем* – Kimi-K2 дообучалась повторять такие многошаговые решения, фактически впитывая шаблоны, как планировать и вызывать инструменты. Этот процесс заложил фундамент «агентного мышления» уже в веса базовой модели.

![workflow-agent](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/workflow-agent.png)

* **RL с самооценкой (в духе RLHF):** помимо имитационного обучения, разработчики внедрили механизм *Reinforcement Learning* для дальнейшего повышения навыков модели решать задачи, особенно те, где нет однозначного правильного ответа. Главная проблема классического RLHF (обучения с подкреплением от отклика человека) – ограниченность и узость сигналов вознаграждения для творческих или аналитических задач. В Kimi-K2 подошли творчески: модель обучалась *самостоятельно оценивать свои ответы* по заданным критериям. Реализована система **самокритики (self-critique)**: модель генерирует ответ и параллельно (или последующим шагом) генерирует оценку этому ответу на основе заранее заданных “рубрик” качества. Поскольку такой критик сам может быть несовершенным, его регулярно улучшали на заданиях, где успех легко проверяется (например, решение матем. задач или кодовых тестов) – эти *верифицируемые задачи* использовались для *обучения критика* более точному прогнозированию качества. Затем этот улучшенный критик применялся к неверифицируемым заданиям (написание эссе, анализ) и давал сигнал награды/штрафа основной модели. Таким образом, шло итеративное обучение с подкреплением без непосредственного участия человека: модель училась улучшать свои действия, опираясь на внутреннюю «судейскую систему», калиброванную на решаемых задачах. Такой подход родственен RLHF, но заменяет человеческий фидбэк на масштабируемый AI-фидбэк. В результате **Kimi-K2-Instruct** получила «рефлекторные» навыки: она сразу выдаёт действие или ответ, близкий к оптимальному, без необходимости в длинных раздумьях (т.н. *reflex-grade model* без длительного chain-of-thought).

Итогом финального обучения стала модель, способная **следовать инструкциям**, поддерживать диалог и *автономно выполнять сложные последовательности действий*. Отметим, что на текущий момент Kimi-K2-Instruct не является мультимодальной – в отличие от предыдущей версии (Kimi k1.5) она не умеет обрабатывать визуальные данные напрямую и не имеет отдельного «режима раздумий». Команда сконцентрировалась на текстовых и агентных возможностях, планируя добавить поддержку изображений и более продвинутые механизмы рассуждения («длительное размышление») в будущих версиях.

## Дополнительные детали и сравнение с предшественниками

### Эволюция по сравнению с Kimi k1.5 

Новая модель Kimi-K2 знаменует существенный шаг вперёд относительно предыдущих моделей Moonshot AI. Предшественник (Kimi k1.5) был выпущен ранее в 2025 году и представлял собой мультимодальный LLM с поддержкой изображений и расширенным контекстом 128k. Kimi k1.5 также использовала RL-подходы в обучении и имела внушительный размер, однако значительно уступающий Kimi-K2: около **389 млрд параметров (52 млрд активных)** при архитектуре MoE, то есть была почти втрое меньше нынешней модели. Kimi-K2 расширила масштабы: 1 трлн параметров (+157% к K1.5) и внедрила новые технологические решения – в частности, механизм MLA для внимания, тогда как Kimi k1.5 в своих длинноконтекстных способностях опиралась на более традиционные подходы (позиционное интерполирование). Кроме того, K1.5 была ориентирована на мультимодальность и диалог, тогда как K2 сделала упор на **agentic-возможности** (автономное выполнение задач). Kimi-K2 в текущей версии не поддерживает обработку изображений или аудио (мультимодальные аспекты планируются позднее), но заметно превосходит K1.5 по *текстовым* и *кодовым* задачам, а также по способности пользоваться инструментами. Еще одно отличие – **открытость**: Kimi-K2 выпущена с открытым исходным кодом и весами (Modified MIT License), тогда как Kimi k1.5 была скорее свободно доступна через API/интерфейс, но без полноценной открытой модели. Таким образом, Kimi-K2 представляет собой более масштабную, узкоспециализированную на агентности эволюцию семейства Kimi.

### Производительность на бенчмарках

![Figure_02](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-29/assets/Figure_02.png)

Kimi-K2 на момент выпуска демонстрирует *state-of-the-art* результаты среди открытых моделей и вплотную подбирается к закрытым лидерам. На академическом тесте знаний и мышления **MMLU** (57 предметов) модель набирает около **87,8%** точности, что превышает результаты всех предшествующих open-source LLM (для сравнения, OpenAI GPT-4 оценивался \~86.4% на MMLU). На конкурсе **C-Eval** (китайский аналог MMLU) Kimi-K2 показала \~**92,5%**, существенно опередив предыдущие модели на китайском языке – подтверждение её глубокого понимания китайских данных. В сложных математических задачах (**MATH** комплексные задачи школьной программы) достигнуто **70,2%** правильных решений – это заметный скачок относительно моделей прошлого поколения (для сравнения, GPT-4 – около 85%, Llama-2 70B – \~50%). На арифметических задачах начальной школы (**GSM8K**) модель верно решает **92,1%** задач, практически устранив ранее характерные ошибки в многошаговых вычислениях.

Особенно впечатляют показатели в программировании. В тестах генерации кода Kimi-K2 устанавливает новые рекорды среди открытых моделей. Например, **LiveCodeBench v6** (реалистичный бенчмарк по конкурентному программированию) – базовая модель Kimi-K2-Base достигает \~**26,3%** точности *pass\@1*, а финальная инструкционная версия Kimi-K2-Instruct – **53,7%** *pass\@1*, **опережая** даже GPT-4.1 (около 44.7%) на этих задачах. В мульти-язычном программировании (**MultiPL-E** бенчмарк) модель приближается к верхним строчкам с \~85-86% точности, а на внутреннем тесте **SWE-bench (Software Engineering)** показала **65,8%** успешных решений, что сравнимо с некоторыми проприетарными моделями Anthropic и значительно лучше большинства open-source моделей.

Также Kimi-K2 лидирует на специализированных агентных бенчмарках: так, на наборах **Tau** и **AceBench** (оценка умения модели использовать инструменты) она заняла первые места среди открытых моделей. Например, в сценариях Tau (поиск решения в доменах ритейла, авиабилетов, телеком и пр. с помощью инструментов) Kimi-K2-Instruct показывает 70-75% успеха, приближаясь к уровням Claude 2 и превосходя другие открытые аналоги.

В совокупности эти результаты свидетельствуют, что **Kimi-K2 установила новый уровень качества** для открытых моделей. По многим метрикам она **догоняет, а порой и превосходит** крупнейшие закрытые системы. Например, разработчики отмечают, что Kimi-K2-Instruct обходит версии Claude 4 (Anthropic) и даже обновлённый GPT-4.1 на ряде ключевых тестов. VentureBeat также подчёркивает, что Kimi-K2 превзошла GPT-4 в некоторых «болевых точках» вроде математических доказательств и сложного кода.

Конечно, модель не идеальна – разработчики указывают, что Kimi-K2 всё ещё может ошибаться в очень длинных цепочках рассуждений, может давать избыточно подробные ответы на простые вопросы, и пока *не* обладает мультимодальными способностями (не «видит» изображения). Однако эти недостатки признаны и активно прорабатываются (планируется улучшение «долгого мышления» и добавление зрения в следующих версиях).

### Вывод

Kimi-K2 представляет собой выдающуюся в техническом плане LLM: инновационная архитектура MoE с топ-8 экспертизой и QK-клип оптимизацией позволили создать *открытую* модель с 1 трлн параметров, обученную на беспрецедентном объёме данных без сбоев. Процесс обучения включал передовые методы устойчивой оптимизации (MuonClip, BF16), распределения нагрузки (ZeRO) и имитационного/подкрепляющего обучения для формирования агентных навыков. Получившаяся модель задаёт новый стандарт качества среди open-source AI, особенно блистая в программировании, математике и автономном выполнении задач. Kimi-K2-Base предоставляет исследователям мощную базу для собственных экспериментов и дообучения, а Kimi-K2-Instruct уже сейчас доступна для прямого использования – её можно запустить локально или через API без каких-либо платных подписок. Модель Kimi-K2 демонстрирует, что открытые инициативы могут конкурировать с лидерами индустрии, и открывает путь к созданию ещё более продвинутых и доступных ИИ-систем в ближайшем будущем.