
# Введение

Современные **фундаментальные модели** (большие предобученные нейросети, например большие языковые модели – LLM) обладают широкой *общей* функциональностью, но для конкретных задач их обычно необходимо *специализировать*. Традиционный подход – дополнительное обучение модели на новом датасете (fine-tuning), – требует аккуратной подготовки данных и длительного вычислительного процесса с подбором гиперпараметров. Это создает «узкое место» в применении ИИ: адаптация под каждую новую задачу стоит дорого и отнимает много времени. Одно из решений – **параметр-эффективное дообучение**. В 2022 году был предложен метод **LoRA** (Low-Rank Adaptation), при котором в большую модель добавляются лишь небольшие низкоранговые весовые матрицы-адаптеры, обучаемые на новой задаче, тогда как исходные веса “замораживаются”. Однако и метод LoRA требует провести оптимизацию под каждую задачу, пусть и меньшим числом параметров. В обсуждаемой работе авторы предлагают подход, позволяющий **выполнять адаптацию модели “на лету”** – сразу по текстовому описанию новой задачи, без явного fine-tuning. Эта система называется **Text-to-LoRA (T2L)**. Модель T2L представляет собой специальную нейросеть – **гиперсеть** (hypernetwork), которая по входному описанию задачи **генерирует веса адаптера LoRA** для большой модели за один прямой проход сети. Иными словами, вместо того чтобы обучать адаптер под новую задачу градиентным спуском, T2L *вычисляет* подходящий адаптер с помощью другой нейросети. В данной статье T2L обучена на ряде задач, после чего способна сразу порождать адаптеры для *невиданных ранее задач* на основании одного лишь текстового описания. Такой подход резко снижает затраты на специализацию моделей и приближает нас к “демократизации” адаптации ИИ-моделей.

Настоящий обзор подробно рассматривает архитектуру Text-to-LoRA, мат.аппарат и механизмы её работы, а также экспериментальные результаты авторов.

## Предпосылки: адаптация LLM и LoRA

### **Постановка задачи адаптации:** 

Предположим, у нас есть базовая языковая модель (LLM) с весами $\Psi$ и имеется набор из $T$ датасетов для дообучения $D = {D_1, \dots, D_T}$, каждый из которых соответствует новой задаче $t_i$. Каждый датасет $D_i$ содержит обучающие пары вход-ответ $(X_i, Y_i)$, а также **описание задачи на естественном языке** $z_i$ (или несколько вариантов описаний). Адаптация модели под задачу $t_i$ означает нахождение таких дополнительных параметров $\Delta W_i$ (например, целый адаптер), которые при совместной работе с исходной моделью минимизируют функцию потерь на данных задачи. Формально оптимальные веса адаптера $\Delta W_i$ определяются как минимум функции **supervised fine-tuning loss** $L_{\text{SFT}}$ на датасете $D_i$:  

$$
\Delta W_i \;=\; \arg\min_{\Delta W} \; L_{\text{SFT}}(D_i,\, \Psi,\, \Delta W)\,. 
\tag{1}
$$  

Здесь $L_{\text{SFT}}(D_i, \Psi, \Delta W)$ – например, стандартная кросс-энтропийная потеря предсказаний модели с добавкой $\Delta W$ на обучающем наборе $D_i$. В классическом полном fine-tuning роль $\Delta W$ играют **все веса модели** (то есть $\Psi$ обновляется полностью). В параметр-эффективном подходе вводятся отдельные слои или матрицы адаптера $\Delta W$, число параметров которых значительно меньше, чем у полной модели, – только их и обучают, оставляя $\Psi$ фиксированными.  

### **Метод LoRA** 

Давайте спустимся на самый простой уровень. У нас есть один линейный слой без функции активации. Если на вход мы подадим $x$, на выходе получим $y = Wx$, где $W$ — матрица весов. Мы хотим немного изменить принцип работы этого слоя, дообучив модель, скорректировав веса на $\Delta W$ (которые обычно ищут обычным градиентным спуском), так что бы новый выход был:

$$
y' = W'x = (W + \Delta W)x = y + \Delta W x
$$

Как мы видим, новый $y$ отличается от старого на $\Delta W x$, что можно интерпретировать как результат работы еще одного, отдельного, полносвязного слоя.

![Image_01](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-27/assets/Image_01.png)

Таким образом, мы можем зафиксировать веса матрицы $W$, а вместо этого учить $\Delta W$ — модель, предсказывающую отличие результата обычной модели, от дообученой. Это отдаленно напоминает градиентный бустинг, где мы учим каждое следующее решающее дерево исправлять ошибки прошлого.

У читателя, помнящего линейную алгебру с далекого первого курса, сразу возникнет вопрос — а где тут выигрыш? Ведь размеры матриц $W$ и $\Delta W$ должны быть одинаковыми, так что в них одинаковое количество обучаемых параметров, и никакого выигрыша в этом нет.

Вот тут и включается в игру слова Low Rank — матрицу маленького ранга можно представить как произведение двух меньшей размерности. Наша матрица может быть размером 100 на 70, но ранг, то есть количество линейно независимых строк или столбцов (если совсем нестрого — таких столбцов которые действительно содержат новую информацию о модели, а не действуют на вектор параметров аналогично соседям) может быть меньше чем 70 — например 4 или 20.

![Image_02](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-27/assets/Image_02.png)

Пример для размерность входного х выходного пространства 100 х 70. Мы можем представить матрицу $\Delta W$ как произведение двух матриц $A$ и $B$, при этом сильно выиграем в количестве обучаемых параметров (для примера на картинке, матрица 100 х 70 содержит 7000 чисел, а две в левой части неравенства $140 + 200 = 340$, в общем случае потребуется обучать в

$$
\frac{nr + rn}{n^2} = \frac{2r}{n}
$$

меньше параметров. $r$ выбирается маленьким порядка 2–8, что делает это значение очень маленьким $\approx 10^{-2}$, однако немного потеряем в общности, так как теперь мы автоматически постулируем, что у $\Delta W$ низкий ранг.

![Image_03](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-27/assets/Image_03.png)

Однако в этом нет ничего страшного, разработчики LoRA, утверждают, что «внутренняя размерность» (intrinsic rank) больших текстовых моделей очень низкая, и большинство параметров, проще говоря, «не работают».

Таким образом, во время обучения нам необходимо хранить в памяти веса $W$ исходной модели и $\Delta W = B\cdot A$ дообучаемой, а считать градиенты только для «новых» маленьких матриц $A$ и $B$. При инициализации модели мы создаем матрицу $B$ случайным образом (например из $N(0, \sigma^2)$), а матрицу $A$ инициализируем нулями, чтобы изначально $\Delta W = 0$.

**Плюсы этого подхода**

* Значительно менее ресурсозатратное дообучение. Теперь модель уровня LLaMA / GPT-3* / … может дообучить под свои задачи любой обладатель массовой видеокарты или вообще с использованием Google Colab, с телефона.
* Снижение числа обучаемых параметров понижает требования к датасету.
* LoRA-модели занимают значительно меньше места на диске. Мы храним одну «базовую» модель, которая действительно весит много, и большое количество LoRA-модулей (например стилей для Stable Diffusion или дообучений под разные языки для Copilot), которые почти ничего не весят. Из-за этого такие модели проще хранить и распространять. Для GPT-3, с 350 ГБ весами, матрицы $A$ и $B$ для всех линейных слоев суммарно занимали 35 Мб!
* Отсутствие задержки вывода. Перед использованием мы можем рассчитать $W' = W + BA$, таким образом новая модель будет требовать столько же вычислений, как и модель без файнтюна.
* Можно менять матрицы $A$ и $B$ прямо налету, посреди диалога, спрашивая у пользователя, например, в каком стиле ему ответить.

## Архитектура модели Text-to-LoRA

T2L представляет собой **гиперсеть $h_{\theta}$**, генерирующую веса адаптера LoRA для произвольной задачи по ее описанию. Архитектура T2L организована следующим образом:

![Figure_01](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-27/assets/Figure_01.jpeg)

*Обзор фреймворка Text-to-LoRA (T2L), показывающий процесс обучения с помощью функции потерь при реконструкции или контролируемого дообучения (SFT), а также анализ производительности при различных коэффициентах сжатия и размерах обучающего набора данных.*

- **Представление задачи:** текстовое описание задачи $z_i$ сначала преобразуется в вектор фиксированной размерности $f(z_i)$. В статье для этой цели используют предобученную модель эмбеддинга **GTE-large-en-v1.5** (Alibaba General Text Embeddings), которая выдаёт семантическое представление строки текста. Заметим, функция $f$ не обучается заново – это фиксированный энкодер. 

- **Эмбеддинги модуля и слоя:** помимо содержательного описания задачи, для однозначного определения конкретного параметра адаптера нужны технические координаты – **какому слою** модели и какому **модулю** (матрице) в этом слое предназначается выход гиперсети. В работе применяются два обучаемых словаря эмбеддингов: $E[m]$ – для типов модулей (например, отдельно для матрицы проекции Query и для Value в self-attention слое) и $E[l]$ – для индексов слоя трансформера. Размерность этих эмбеддингов $d_{\text{emb}}$ выбирается относительно небольшой (в приложении указано, что они способствуют стабилизации обучения).

- **Вектор запроса $\phi_{m,l}^i$:** на вход гиперсети подается конкатенация трёх векторов – описания задачи и указателей модуля и слоя: 

$$\phi_{m,l}^i = (f(z_i), E[m], E[l]). \tag{2}$$ 
 
Этот вектор $\phi_{m,l}^i$ служит _описанием_ того, какие веса нужно сгенерировать – то есть «для задачи $i$, для слоя $l$, для модуля типа $m$». 

- **Генерация весов адаптера:** гиперсеть – это многослойный перцептрон (MLP) с параметрами $\theta$, который принимает $\phi_{m,l}^i$ и выдает матрицы адаптера $\Delta W_{m,l}^i$. В простейшем случае (большая архитектура T2L) гиперсеть сразу выдает обе матрицы LoRA – $A$ и $B$ – для указанного слоя, то есть полный сдвиг $\Delta W_{m,l}^i$. Формально:

$$\Delta W_{m,l}^i = h_{\theta}(\phi_{m,l}^i). \tag{3}$$

Повторяя генерацию для всех требуемых слоёв и модулей $(m,l)$, мы получаем полный набор $\{\Delta W_{m,l}^i\}$ – то есть **адаптер LoRA для задачи $t_i$**, обозначаемый совокупно как $\Delta W^i = h_{\theta}(f(z_i), E[*])$. Поскольку разные $(m,l)$ можно подавать батчем, все веса адаптера для задачи действительно получаются одним запуском гиперсети (одним *forward pass*), только на разных входных эмбеддингах.

### **Варианты архитектуры T2L (L, M, S)** 

Авторы исследовали три модификации выхода гиперсети, вводя разные компромиссы между размером модели и качеством результатов:

![Figure_02](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-27/assets/Figure_02.jpeg)

*Три архитектурных варианта T2L (L, M, S), показывающие различные подходы к генерации параметров с различными компромиссами между выразительностью и эффективностью.*

- **T2L-L (Large):** максимальная модель гиперсети. Её выходной слой сразу порождает *две* матрицы ($A$ и $B$) для заданного $(m,l)$. Таким образом, размер выхода равен $2 \times r \times d$ (где $d$ – соответствующий размерность матрицы $W_0$, а $r$ – ранг LoRA). Число параметров выходного слоя гиперсети, соответственно, $|θ_{\text{head}}| = d_{\text{out}} \times 2 r d$, где $d_{\text{out}}$ – размерность последнего скрытого слоя MLP (в работе взято $d_{\text{out}}=512$). Такой вариант содержит больше всего параметров (в экспериментах ~55 млн.) и наименее накладывает ограничений на форму генерируемых весов. 

- **T2L-M (Medium):** модель среднего размера (~34M параметров). В этом варианте выходной слой **разделяется между ($A$ и $B$)** – по сути, гиперсеть выдаёт одну матрицу размера $r \times d$ за раз, а какая именно (A или B) – определяется дополнительным признаком на входе. Авторы реализовали это как добавочный бинарный эмбеддинг, сигнализирующий тип матрицы (A либо B). То есть гиперсеть одна и та же, но чтобы получить полный адаптер, её нужно вызвать дважды – с индикатором «генерируй A» и «генерируй B» (причём эти вызовы можно объединить батчем). Размер выхода и параметры последнего слоя при этом вдвое меньше, чем в L: $|θ_{\text{head}}| = d_{\text{out}} \times r d$.

- **T2L-S (Small):** самая компактная модель (~5M параметров), вводящая сильные априорные ограничения. Идея в том, чтобы генерировать **по одному ранговому компоненту** за раз. Гиперсеть T2L-S выдаёт вектор длины $d$ – фактически строку матрицы $A$ или $B$. С помощью двух допэмбеддингов она условно знает, **какой ранг $j$ и какая матрица (A/B)** сейчас требуется. Для получения полного $\Delta W_{m,l}^i$ нужно сделать $2r$ проходов (по $r$ раз для A и B) – но, опять же, всё это можно распараллелить в одном батче. Размер выхода T2L-S – $d$, и количество параметров выходного слоя $|θ_{\text{head}}| = d_{\text{out}} \times d_{\text{emb}}$ (по сути, матрица преобразования из внутреннего пространства размерности $d_{\text{out}}$ к вектору длины $d_{\text{emb}}$, равного, видимо, размерности эмбеддингов ранга). Такая модель достигает самой сильной экономии параметров (почти на порядок меньше, чем M), но накладывает жесткую структуру на генерируемые адаптеры. Тем не менее, важно, что все три варианта архитектур **в функциональном отношении эквивалентны** – они способны генерировать адаптеры для любых слоёв и модулей, просто с разным числом проходов и скрытых параметров.