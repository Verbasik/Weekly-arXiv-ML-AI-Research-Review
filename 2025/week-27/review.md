
# Введение

Современные **фундаментальные модели** (большие предобученные нейросети, например большие языковые модели – LLM) обладают широкой *общей* функциональностью, но для конкретных задач их обычно необходимо *специализировать*. Традиционный подход – дополнительное обучение модели на новом датасете (fine-tuning), – требует аккуратной подготовки данных и длительного вычислительного процесса с подбором гиперпараметров. Это создает «узкое место» в применении ИИ: адаптация под каждую новую задачу стоит дорого и отнимает много времени. Одно из решений – **параметр-эффективное дообучение**. В 2022 году был предложен метод **LoRA** (Low-Rank Adaptation), при котором в большую модель добавляются лишь небольшие низкоранговые весовые матрицы-адаптеры, обучаемые на новой задаче, тогда как исходные веса “замораживаются”. Однако и метод LoRA требует провести оптимизацию под каждую задачу, пусть и меньшим числом параметров. В обсуждаемой работе авторы предлагают подход, позволяющий **выполнять адаптацию модели “на лету”** – сразу по текстовому описанию новой задачи, без явного fine-tuning. Эта система называется **Text-to-LoRA (T2L)**. Модель T2L представляет собой специальную нейросеть – **гиперсеть** (hypernetwork), которая по входному описанию задачи **генерирует веса адаптера LoRA** для большой модели за один прямой проход сети. Иными словами, вместо того чтобы обучать адаптер под новую задачу градиентным спуском, T2L *вычисляет* подходящий адаптер с помощью другой нейросети. В данной статье T2L обучена на ряде задач, после чего способна сразу порождать адаптеры для *невиданных ранее задач* на основании одного лишь текстового описания. Такой подход резко снижает затраты на специализацию моделей и приближает нас к “демократизации” адаптации ИИ-моделей.

Настоящий обзор подробно рассматривает архитектуру Text-to-LoRA, мат.аппарат и механизмы её работы, а также экспериментальные результаты авторов.

## Предпосылки: адаптация LLM и LoRA

### **Постановка задачи адаптации:** 

Предположим, у нас есть базовая языковая модель (LLM) с весами $\Psi$ и имеется набор из $T$ датасетов для дообучения $D = {D_1, \dots, D_T}$, каждый из которых соответствует новой задаче $t_i$. Каждый датасет $D_i$ содержит обучающие пары вход-ответ $(X_i, Y_i)$, а также **описание задачи на естественном языке** $z_i$ (или несколько вариантов описаний). Адаптация модели под задачу $t_i$ означает нахождение таких дополнительных параметров $\Delta W_i$ (например, целый адаптер), которые при совместной работе с исходной моделью минимизируют функцию потерь на данных задачи. Формально оптимальные веса адаптера $\Delta W_i$ определяются как минимум функции **supervised fine-tuning loss** $L_{\text{SFT}}$ на датасете $D_i$:  

$$
\Delta W_i \;=\; \arg\min_{\Delta W} \; L_{\text{SFT}}(D_i,\, \Psi,\, \Delta W)\,. 
\tag{1}
$$  

Здесь $L_{\text{SFT}}(D_i, \Psi, \Delta W)$ – например, стандартная кросс-энтропийная потеря предсказаний модели с добавкой $\Delta W$ на обучающем наборе $D_i$. В классическом полном fine-tuning роль $\Delta W$ играют **все веса модели** (то есть $\Psi$ обновляется полностью). В параметр-эффективном подходе вводятся отдельные слои или матрицы адаптера $\Delta W$, число параметров которых значительно меньше, чем у полной модели, – только их и обучают, оставляя $\Psi$ фиксированными.  

### **Метод LoRA** 

Давайте спустимся на самый простой уровень. У нас есть один линейный слой без функции активации. Если на вход мы подадим $x$, на выходе получим $y = Wx$, где $W$ — матрица весов. Мы хотим немного изменить принцип работы этого слоя, дообучив модель, скорректировав веса на $\Delta W$ (которые обычно ищут обычным градиентным спуском), так что бы новый выход был:

$$
y' = W'x = (W + \Delta W)x = y + \Delta W x
$$

Как мы видим, новый $y$ отличается от старого на $\Delta W x$, что можно интерпретировать как результат работы еще одного, отдельного, полносвязного слоя.

![Image_01](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-27/assets/Image_01.png)

Таким образом, мы можем зафиксировать веса матрицы $W$, а вместо этого учить $\Delta W$ — модель, предсказывающую отличие результата обычной модели, от дообученой. Это отдаленно напоминает градиентный бустинг, где мы учим каждое следующее решающее дерево исправлять ошибки прошлого.

У читателя, помнящего линейную алгебру с далекого первого курса, сразу возникнет вопрос — а где тут выигрыш? Ведь размеры матриц $W$ и $\Delta W$ должны быть одинаковыми, так что в них одинаковое количество обучаемых параметров, и никакого выигрыша в этом нет.

Вот тут и включается в игру слова Low Rank — матрицу маленького ранга можно представить как произведение двух меньшей размерности. Наша матрица может быть размером 100 на 70, но ранг, то есть количество линейно независимых строк или столбцов (если совсем нестрого — таких столбцов которые действительно содержат новую информацию о модели, а не действуют на вектор параметров аналогично соседям) может быть меньше чем 70 — например 4 или 20.

![Image_02](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-27/assets/Image_02.png)

Пример для размерность входного х выходного пространства 100 х 70. Мы можем представить матрицу $\Delta W$ как произведение двух матриц $A$ и $B$, при этом сильно выиграем в количестве обучаемых параметров (для примера на картинке, матрица 100 х 70 содержит 7000 чисел, а две в левой части неравенства $140 + 200 = 340$, в общем случае потребуется обучать в

$$
\frac{nr + rn}{n^2} = \frac{2r}{n}
$$

меньше параметров. $r$ выбирается маленьким порядка 2–8, что делает это значение очень маленьким $\approx 10^{-2}$, однако немного потеряем в общности, так как теперь мы автоматически постулируем, что у $\Delta W$ низкий ранг.

![Image_03](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-27/assets/Image_03.png)

Однако в этом нет ничего страшного, разработчики LoRA, утверждают, что «внутренняя размерность» (intrinsic rank) больших текстовых моделей очень низкая, и большинство параметров, проще говоря, «не работают».

Таким образом, во время обучения нам необходимо хранить в памяти веса $W$ исходной модели и $\Delta W = B\cdot A$ дообучаемой, а считать градиенты только для «новых» маленьких матриц $A$ и $B$. При инициализации модели мы создаем матрицу $B$ случайным образом (например из $N(0, \sigma^2)$), а матрицу $A$ инициализируем нулями, чтобы изначально $\Delta W = 0$.

**Плюсы этого подхода**

* Значительно менее ресурсозатратное дообучение. Теперь модель уровня LLaMA / GPT-3* / … может дообучить под свои задачи любой обладатель массовой видеокарты или вообще с использованием Google Colab, с телефона.
* Снижение числа обучаемых параметров понижает требования к датасету.
* LoRA-модели занимают значительно меньше места на диске. Мы храним одну «базовую» модель, которая действительно весит много, и большое количество LoRA-модулей (например стилей для Stable Diffusion или дообучений под разные языки для Copilot), которые почти ничего не весят. Из-за этого такие модели проще хранить и распространять. Для GPT-3, с 350 ГБ весами, матрицы $A$ и $B$ для всех линейных слоев суммарно занимали 35 Мб!
* Отсутствие задержки вывода. Перед использованием мы можем рассчитать $W' = W + BA$, таким образом новая модель будет требовать столько же вычислений, как и модель без файнтюна.
* Можно менять матрицы $A$ и $B$ прямо налету, посреди диалога, спрашивая у пользователя, например, в каком стиле ему ответить.

## Архитектура модели Text-to-LoRA

T2L представляет собой **гиперсеть $h_{\theta}$**, генерирующую веса адаптера LoRA для произвольной задачи по ее описанию. Архитектура T2L организована следующим образом:

![Figure_01](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-27/assets/Figure_01.jpeg)

*Обзор фреймворка Text-to-LoRA (T2L), показывающий процесс обучения с помощью функции потерь при реконструкции или контролируемого дообучения (SFT), а также анализ производительности при различных коэффициентах сжатия и размерах обучающего набора данных.*

- **Представление задачи:** текстовое описание задачи $z_i$ сначала преобразуется в вектор фиксированной размерности $f(z_i)$. В статье для этой цели используют предобученную модель эмбеддинга **GTE-large-en-v1.5** (Alibaba General Text Embeddings), которая выдаёт семантическое представление строки текста. Заметим, функция $f$ не обучается заново – это фиксированный энкодер. 

- **Эмбеддинги модуля и слоя:** помимо содержательного описания задачи, для однозначного определения конкретного параметра адаптера нужны технические координаты – **какому слою** модели и какому **модулю** (матрице) в этом слое предназначается выход гиперсети. В работе применяются два обучаемых словаря эмбеддингов: $E[m]$ – для типов модулей (например, отдельно для матрицы проекции Query и для Value в self-attention слое) и $E[l]$ – для индексов слоя трансформера. Размерность этих эмбеддингов $d_{\text{emb}}$ выбирается относительно небольшой (в приложении указано, что они способствуют стабилизации обучения).

- **Вектор запроса $\phi_{m,l}^i$:** на вход гиперсети подается конкатенация трёх векторов – описания задачи и указателей модуля и слоя: 

$$\phi_{m,l}^i = (f(z_i), E[m], E[l]). \tag{2}$$ 
 
Этот вектор $\phi_{m,l}^i$ служит _описанием_ того, какие веса нужно сгенерировать – то есть «для задачи $i$, для слоя $l$, для модуля типа $m$». 

- **Генерация весов адаптера:** гиперсеть – это многослойный перцептрон (MLP) с параметрами $\theta$, который принимает $\phi_{m,l}^i$ и выдает матрицы адаптера $\Delta W_{m,l}^i$. В простейшем случае (большая архитектура T2L) гиперсеть сразу выдает обе матрицы LoRA – $A$ и $B$ – для указанного слоя, то есть полный сдвиг $\Delta W_{m,l}^i$. Формально:

$$\Delta W_{m,l}^i = h_{\theta}(\phi_{m,l}^i). \tag{3}$$

Повторяя генерацию для всех требуемых слоёв и модулей $(m,l)$, мы получаем полный набор $\{\Delta W_{m,l}^i\}$ – то есть **адаптер LoRA для задачи $t_i$**, обозначаемый совокупно как $\Delta W^i = h_{\theta}(f(z_i), E[*])$. Поскольку разные $(m,l)$ можно подавать батчем, все веса адаптера для задачи действительно получаются одним запуском гиперсети (одним *forward pass*), только на разных входных эмбеддингах.

### **Варианты архитектуры T2L (L, M, S)** 

Авторы исследовали три модификации выхода гиперсети, вводя разные компромиссы между размером модели и качеством результатов:

![Figure_02](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-27/assets/Figure_02.jpeg)

*Три архитектурных варианта T2L (L, M, S), показывающие различные подходы к генерации параметров с различными компромиссами между выразительностью и эффективностью.*

- **T2L-L (Large):** максимальная модель гиперсети. Её выходной слой сразу порождает *две* матрицы ($A$ и $B$) для заданного $(m,l)$. Таким образом, размер выхода равен $2 \times r \times d$ (где $d$ – соответствующий размерность матрицы $W_0$, а $r$ – ранг LoRA). Число параметров выходного слоя гиперсети, соответственно, $|θ_{\text{head}}| = d_{\text{out}} \times 2 r d$, где $d_{\text{out}}$ – размерность последнего скрытого слоя MLP (в работе взято $d_{\text{out}}=512$). Такой вариант содержит больше всего параметров (в экспериментах ~55 млн.) и наименее накладывает ограничений на форму генерируемых весов. 

- **T2L-M (Medium):** модель среднего размера (~34M параметров). В этом варианте выходной слой **разделяется между ($A$ и $B$)** – по сути, гиперсеть выдаёт одну матрицу размера $r \times d$ за раз, а какая именно (A или B) – определяется дополнительным признаком на входе. Авторы реализовали это как добавочный бинарный эмбеддинг, сигнализирующий тип матрицы (A либо B). То есть гиперсеть одна и та же, но чтобы получить полный адаптер, её нужно вызвать дважды – с индикатором «генерируй A» и «генерируй B» (причём эти вызовы можно объединить батчем). Размер выхода и параметры последнего слоя при этом вдвое меньше, чем в L: $|θ_{\text{head}}| = d_{\text{out}} \times r d$.

- **T2L-S (Small):** самая компактная модель (~5M параметров), вводящая сильные априорные ограничения. Идея в том, чтобы генерировать **по одному ранговому компоненту** за раз. Гиперсеть T2L-S выдаёт вектор длины $d$ – фактически строку матрицы $A$ или $B$. С помощью двух допэмбеддингов она условно знает, **какой ранг $j$ и какая матрица (A/B)** сейчас требуется. Для получения полного $\Delta W_{m,l}^i$ нужно сделать $2r$ проходов (по $r$ раз для A и B) – но, опять же, всё это можно распараллелить в одном батче. Размер выхода T2L-S – $d$, и количество параметров выходного слоя $|θ_{\text{head}}| = d_{\text{out}} \times d_{\text{emb}}$ (по сути, матрица преобразования из внутреннего пространства размерности $d_{\text{out}}$ к вектору длины $d_{\text{emb}}$, равного, видимо, размерности эмбеддингов ранга). Такая модель достигает самой сильной экономии параметров (почти на порядок меньше, чем M), но накладывает жесткую структуру на генерируемые адаптеры. Тем не менее, важно, что все три варианта архитектур **в функциональном отношении эквивалентны** – они способны генерировать адаптеры для любых слоёв и модулей, просто с разным числом проходов и скрытых параметров. 

### **Сравнение с хранением отдельных LoRA** 

Стоит отметить масштабируемость подхода T2L: одна гиперсеть может кодировать сразу очень много разных адаптеров. Если обучить отдельные LoRA для $N$ задач, общее число их параметров $\sim N \times (2 r d L |M|)$ (где $L$ – число слоев, $|M|$ – число модулей с адаптацией) растет линейно с $N$. Гиперсеть же имеет фиксированный размер (например, 55M), и при увеличении числа задач её параметры не растут – она лишь обучается их *компрессировать*. В работе приведено сравнение: адаптер LoRA (ранг 8, на два модуля во всех 32 слоях) содержит около **3.4 млн** параметров, библиотека из 479 таких адаптеров имела бы $>1.6$ млрд параметров, тогда как гиперсеть T2L-L, покрывающая все эти задачи, имела **55 млн** параметров. Таким образом, гиперсеть достигает сжатия знаний примерно на порядок-другой. Авторский термин – **“indirect adaptation encoding”** – отражает то, что специализация под задачу задаётся не явными весами, а через промежуточное скрытое представление (эмбеддинг задачи).

## Обучение гиперсети T2L

Чтобы гиперсеть T2L научилась выдавать полезные адаптеры, её сами веса $\theta$ необходимо оптимизировать на наборе задач. Рассмотрены две стратегии: через **восстановление заранее обученных LoRA-адаптеров** (“distillation”) и через **непосредственное обучение на данных задач** (multi-task fine-tuning). Оба подхода имеют свои плюсы, ниже мы их разберем.

### Обучение через реконструкцию LoRA

Прямой способ – *показать* гиперсети, какими должны быть адаптеры. Для этого сначала собирается библиотека готовых адаптеров LoRA для ряда задач. Можно взять уже существующие предобученные адаптеры (например, опубликованные в репозиториях) или самостоятельно обучить “орракульные” LoRA для некоторых датасетов. Авторы используют 9 бенчмарковых задач (ARC, GSM8K, и др.), для которых обучили отдельные LoRA-адаптеры – будем называть их **oracle LoRA**. Затем T2L обучается приближать эти веса по **реконструктивному** лоссу: 

$$
L_{\text{recon}}(\Omega, \theta) \;=\; \mathbb{E}_{\Delta W_i \sim \Omega} \; \|\,\Delta W_i \;-\; h_{\theta}(\phi_i)\,\|\,,
\tag{4}
$$

где:
- $\Omega = \{\Delta W_1, \dots, \Delta W_N\}$ – набор целевых адаптеров (орракулов). 

На практике используется L1-норма разности (сумма модулей разностей весов). Гиперсети подаются соответствующие эмбеддинги задач (либо one-hot идентификаторы задач, либо текстовые описания – см. ниже), и она настраивается так, чтобы **восстановить веса** известных адаптеров. Если использовать только one-hot кодировки задач, T2L научится сжимать и воспроизводить только эти $N$ адаптеров (максимально эффективно, если $N$ невелик). Однако тогда гиперсеть не сможет выдавать новые адаптеры для *невиданных* задач – ведь для них нет one-hot вектора. Поэтому для способности к **zero-shot генерации** авторы вводят условие на **текстовое описание задачи**. В обучении реконструкцией T2L получает на вход **комбинированный эмбеддинг**: описание задачи $z_i$ (через $f(z_i)$) *и* (в некоторых вариантах) совместно с уникальным ID задачи. Выяснилось, что добавление текстового описания практически не влияет на качество воспроизведения известных адаптеров (см. результаты), но зато принципиально важно – открывает возможность обобщать на новые задачи по их описанию. Процесс обучения в этом режиме – относительно дешевый: не требуются метки конечной задачи, только сравнение весов (то есть по сути обучение без учителя, оптимизируется (4)). 

### **Ограничения реконструктивного подхода** 

Обучая T2L таким образом, мы требуем от неё лишь **воспроизвести** заданные LoRA-адаптеры. Она не знает, к какому качеству на задаче ведут эти веса, – ее задача минимизировать среднюю ошибку реконструкции. Это успешно сожмёт информацию, но не гарантирует хорошую *обобщающую способность*. Например, могут быть две родственные задачи $t_1$ и $t_2$, для которых хорошие (отдельно обученные) адаптеры $\Delta W_1$ и $\Delta W_2$ оказались довольно различны (они могли сойтись в разные локальные минимумы). Гиперсеть вынуждена их запомнить раздельно. Если теперь дать **новую задачу** $t_{\text{new}}$, похожую по сути на $t_1$ и $t_2$, гиперсеть реконструктивного типа не сможет сгенерировать правильный адаптер – она не обучена улавливать **сходство функций** задач, только сходство самих матриц. Авторы отмечают, что действительно модель T2L, обученная только реконструкцией, проваливается в эксперименте zero-shot. Она полезна лишь как компрессор для известного набора задач. Но есть и плюс: реконструктивный подход позволяет **прямо оценить вместимость** гиперсети – сколько разных адаптеров она может выучить без существенной деградации. Мы вернемся к этому при обсуждении результатов сжатия.

### Обучение через мультизадачный fine-tuning (SFT)

Альтернативный способ – *встроить* генератор адаптера в процесс обучения на данных. В этом режиме T2L оптимизируется как часть модели, решающей сразу множество задач. Конкретно, для каждого minibatch обучающих данных из некоторой задачи $t_i$, мы:

1. Берём описание $z_i$, генерируем адаптер $\Delta W^i = h_{\theta}(f(z_i), E[*])$; 
2. Применяем базовую LLM с добавкой $\Delta W^i$ на примерах из $D_i$; 
3. Вычисляем loss (например, кросс-энтропию по правильным ответам); 
4. Дифференцируем этот loss по параметрам $\theta$ гиперсети и обновляем их (параметры $\Psi$ LLM заморожены). 

Таким образом, **целевая функция** для гиперсети – средний SFT-loss по всем задачам, аналогичный (1), но с $\Delta W$ выдаваемым $h_{\theta}$: 

$$
\theta = \arg\min_{\theta} \; \mathbb{E}_{t_i \sim D} \; L_{\text{SFT}}(D_i,\, \Psi,\, h_{\theta}(f(z_i), E[*]))\,.
\tag{5}
$$

Иначе говоря, T2L пытается своими генерируемыми адаптерами непосредственно максимально повысить качество модели на каждом из тренировочных датасетов. Этот подход сложнее (внутри стоит полноценный цикл оптимизации с градиентами по выходам гиперсети), но он позволяет **гиперсети самой искать общие черты задач**. Если две задачи требуют похожего навыка от модели, гиперсеть может научиться выдавать близкие адаптеры – ведь так ей проще одновременно минимизировать loss по обеим. Такой _implicit clustering_ задач, по наблюдениям, улучшает и zero-shot адаптацию новых задач. По сути, T2L в режиме SFT становится мета-моделью, которая учится **мета-обобщению**: она должна быть способна синтезировать адаптер под произвольную задачу из распределения обучающих задач. Благодаря включению текстового описания, “распределение” охватывает любые формулировки в естественном языке, что и делает возможной последующую генерацию для новых формулировок.

### **Преимущество SFT-подхода.** 

Главный плюс – способность обобщать. Как уже отмечалось, **T2L-SFT выучивает пространство задач**, опираясь на содержательные описания и данные. Авторы показывают, что такая модель успешно работает для unseen задач, особенно если они похожи на виды, представленные при обучении (например, новые тесты на мульти-классовый вопрос с выбором ответа). Другой плюс – нет необходимости готовить библиотеку LoRA заранее. Это полезно, когда целевой набор задач очень широк (например, сотни разнообразных задач, как в Super Natural Instructions), и проще сразу учить генератор на них всех, чем сначала индивидуально дообучать сотни LoRA, а потом обучать гиперсеть их реконструировать. К тому же, как упомянули авторы, готовые адаптеры из внешних источников могут быть **разнородны** и не сгруппированы по функционалу, что затрудняет гиперсети поиск закономерностей. SFT-тренировка напротив “поощряет” кластеризацию схожих задач, влияя на формирование внутреннего пространства эмбеддингов и выходных весов. Из минусов: обучение T2L-SFT требует больше вычислений (по сути, это одновременное fine-tuning по многим задачам, хотя и с фиксированной большой моделью). Авторы также столкнулись с проблемами *обучения гиперсети* – при слишком большом выходном пространстве оптимизация могла расходиться. Пришлось применять специальные приёмы и инициализацию весов для стабилизации, особенно для большой архитектуры. Тем не менее итоговый результат стоит того: T2L-SFT показала лучшую обобщающую способность.

### **Ключевые различия стратегий обучения**

| **Аспект**                  | **Реконструкция LoRA**                                    | **Мультизадачный Fine-Tuning (SFT)**                         |
|-----------------------------|----------------------------------------------------------|-------------------------------------------------------------|
| **Цель обучения**           | Воспроизведение заданных адаптеров с минимальной ошибкой | Максимизация качества решения задач напрямую через адаптеры |
| **Данные для обучения**     | Библиотека готовых адаптеров (oracle LoRA)               | Сырые данные задач (датасеты ($D_i$))                       |
| **Целевая функция**         | $(L_{\text{recon}} = \|\Delta W_i - h_{\theta}(\phi_i)\|)$ (L1/L2-норма) | $(L_{\text{SFT}})$ (кросс-энтропия по ответам модели)       |
| **Вычислительная сложность** | Низкая (не требует обратных проходов через LLM)          | Высокая (полный цикл оптимизации с градиентами через LLM)   |
| **Обобщение на новые задачи** | Требует текстовых описаний для zero-shot, слабое обобщение | Сильное (через неявную кластеризацию задач на основе данных) |
| **Преимущества**            | • Дешевое обучение <br> • Прямая оценка ёмкости гиперсети <br> • Эффективная компрессия адаптеров | • Лучшее zero-shot обобщение <br> • Не нужны претренированные адаптеры <br> • Улавливает семантику задач |
| **Недостатки**              | • Игнорирует функциональность адаптеров <br> • Не способна к мета-обобщению <br> • Зависит от качества oracle LoRA | • Сложная оптимизация (риск расходимости) <br> • Высокие вычислительные затраты <br> • Риск переобучения на тренировочных задачах |
| **Тип обучения**            | Без учителя (unsupervised)                               | С учителем (supervised, мета-обучение)                      |
| **Ключевая способность**    | Сжатие и воспроизведение известных адаптеров             | Синтез адаптеров для неизвестных задач через описания       |


### **Резюме различий**

1. **Суть обучения**  
   Реконструкция учит гиперсеть *копировать матрицы весов*, SFT учит *решать задачи* через адаптеры.  
   
2. **Обобщающая способность**  
   SFT превосходит в zero-shot сценариях, так как выучивает *семантические связи* между задачами. Реконструкция обобщает плохо без явных описаний.  

3. **Практическая применимость**  
   - **Реконструкция**: оптимальна для сжатия предобученных адаптеров (напр., в edge-устройствах).  
   - **SFT**: предпочтительна для динамической генерации адаптеров под новые задачи (напр., в сервисах с широким спектром запросов).  

4. **Зависимость от данных**  
   Реконструкция требует существующих адаптеров, SFT работает напрямую с датасетами, что гибче для масштабирования.  

> **Итог**: Реконструктивный подход — эффективный «компрессор», SFT — мета-алгоритм для генерации адаптеров, максимизирующий производительность на распределении задач. Выбор зависит от цели: компрессия vs. универсальная адаптация.

## Экспериментальная оценка

Авторы провели обширные эксперименты, чтобы оценить эффективность Text-to-LoRA в двух основных сценариях:
1. Сжатие и замена множества вручную обученных адаптеров (T2L должна воспроизвести их поведение);
2. zero-shot генерация адаптеров для новых задач по описанию. 

Также изучалось, как масштаб охвата задач и различные архитектурные решения влияют на качество. Ниже суммируем настройку экспериментов и ключевые результаты.

### Настройка и бенчмарки

**Базовые LLM:** 

Основная часть опытов проводилась на модели **Mistral-7B-Instruct** (7 млрд параметров, обученная на беседах). В отдельных сравнениях также проверяли переносимость подхода на другие LLM: Llama-3.1-8B и Gemma-2-2B – эти результаты вынесены в приложение (Таблицы 7, 8), и в целом подтверждают общий вывод. Базовые модели во время обучения T2L *не дообучаются* – их веса заморожены, вся адаптация осуществляется только порождаемыми LoRA.

![Image_04](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-27/assets/Image_04.png)

**Датасеты задач:** 

Для обучения гиперсети в режиме SFT использован набор из **479 задач** из коллекции **Super-Natural Instructions (SNI)**. SNI – это обширный корпус разнообразных NLP заданий с инструкциями (Wang et al., 2022). Авторы взяли 500 задач, удалили 10 задач, пересекающихся с тестовыми бенчмарками (чтобы проверка была truly zero-shot), и ещё 11 задач отложили для валидации гиперсети, оставив 479 на обучение. Каждая задача снабжена *описанием на английском* (цель, формат ввода-вывода и т.д.), причём могли быть несколько переформулировок описания – авторы использовали до 128 вариантов формулировки на задачу, случайно выбирая одно в каждом обучающем шаге. Таким образом, T2L училась на большом разнообразии инструкций, что повысило её способность воспринимать любые новые описания.

**Бенчмарки для оценки:** 

Для итогового сравнения выбрано **10 известных задач**: две версии теста ARC (Easy/Challenge), BoolQ, GSM8K (математика), HellaSwag (дополнение истории), OpenBookQA, PIQA (физика в быту), Winogrande (общий смысл), MBPP (генерация кода по описанию) и HumanEval (кодинг-тесты). Эти задачи охватывают множество типов умений LLM: от школьных знаний и логики до программирования. Отметим, некоторые из них схожи с задачами в обучающем множестве (например, ARC – это вопросы с выбором, таких много в SNI), а некоторые довольно новые для модели (в SNI не было задач генерации кода, поэтому MBPP и HumanEval – совершенно *out-of-domain* для T2L). Метрика оценки – стандартная для каждого бенчмарка (accuracy для QA, pass@1 для задач кода и т.п.). Также для этих 10 задач авторы самостоятельно обучили «oracle» LoRA-адаптеры (где были данные для обучения: для HumanEval обучить нельзя, так как это исключительно тестовый набор, поэтому для него oracle отсутствует). Таким образом, сравнение ведется между: 

- **Base model:** исходная LLM без адаптации (но с возможными подсказками). 
- **Task-specific LoRA (oracle):** отдельный LoRA, дообученный именно на эту задачу (эталон качества адаптации). 
- **T2L:** гиперсеть, генерирующая LoRA по описанию (в разных вариантах, обученных ранее). 
- **Multi-task LoRA:** единый LoRA-адаптер, дообученный сразу на **всех обучающих задачах** вместе (в нашем случае – на 479 задачах SNI). Это сильный базовый подход, показывающий, как далеко можно уйти, имея один “универсальный” адаптер для многих задач. 
- **Average LoRA:** простейший способ комбинировать адаптеры – усреднить их значения параметров. Авторы берут все oracle LoRA (для задач бенчмарка) и вычисляют поэлементное среднее матриц $\Delta W$. Получается некий “средний” адаптер, который применяют на всех задачах как fixed. Это скорее искусственный бенчмарк для интуиции (ожидается, что он хуже, чем любой целенаправленный). 
- **HyperDecoder (per-instance):** реализованный авторами вариант гиперсети, генерирующей адаптер **под каждый конкретный вход** (а не задачу в целом). Это повторяет подход Ivison & Peters (2022): для каждого запроса LLM’у генерируется свой LoRA. Гиперсеть HyperDecoder обучается в составе модели, минимизируя потерю на конкретных примерах (т.е. зависит не только от описания задачи, но и от текущего входа). Ожидается, что HyperDecoder может тоньше подстроиться под нюансы конкретного вопроса, но его адаптация не переносима между примерами. 
- **Arrow Routing:** метод Ostapenko et al. (2024) – **zero-shot маршрутизация LoRA**. В этом подходе предполагается библиотека различных LoRA (обученных заранее). Для новой задачи алгоритм Arrow подбирает либо *комбинацию* нескольких существующих адаптеров, либо путь их активации (детали за рамками обзора). Важное отличие: Arrow **не генерирует новых весов**, а лишь выбирает среди имеющихся. Тем не менее, это один из лучших на сегодня способов быстро адаптировать модель без допобучения, поэтому его результаты приводятся для сравнения. Авторы позаимствовали показатели Arrow Routing из их статьи. Учтём, что они основаны на другом наборе обучающих задач и немного иных промптах, поэтому сравнение с T2L условное. 

Кроме адаптеров, рассматривались и трюки без обучения: **Prepending task description** – просто подача описания задачи перед каждым запросом как контекст (некоторые LLM могут воспринять инструкцию и чуть улучшить решение без адаптации); **In-Context Learning (ICL)** – дача трёх примеров решения (3-shot) в качестве демонстрации модели. Эти методы часто улучшают результат base model, но опять же требуют дополнительного ручного труда по подготовке промпта. В нашем случае описание задачи у нас уже есть – его как раз можно использовать для prompt-based адаптации.

