# Group Sequence Policy Optimization

## Содержание
1. Введение
2. Проблема существующих методов
3. Методология GSPO
4. Ключевые алгоритмические различия
5. Результаты экспериментов
6. Практическое применение и преимущества для инфраструктуры
7. Значение и будущие последствия

## 1. Введение

Обучение с подкреплением (RL) стало важным инструментом для масштабирования больших языковых моделей (LLM) для решения сложных задач по рассуждению в математике и программировании. Однако применение RL к моделям с миллиардами параметров сталкивается с критической проблемой: нестабильность обучения, которая может привести к катастрофическому коллапсу модели. Эта статья представляет Group Sequence Policy Optimization (GSPO), новый алгоритм RL, разработанный для устранения фундаментальных недостатков существующих методов, таких как Group Relative Policy Optimization (GRPO).

Основное новшество GSPO заключается в переходе от сэмплирования по важности на уровне токенов к сэмплированию на уровне последовательностей, что согласовывает единицу оптимизации с тем, как на самом деле назначаются награды. Это, казалось бы, простое изменение устраняет серьезные проблемы стабильности, которые преследовали крупномасштабное обучение RL, особенно для моделей "Смесь экспертов" (MoE). Работа демонстрирует, что GSPO не только достигает превосходной стабильности, но также улучшает эффективность и производительность обучения, способствуя замечательным улучшениям, наблюдаемым в последних моделях Alibaba Qwen3.

!["Сравнение производительности обучения, показывающее превосходную стабильность и производительность GSPO по сравнению с GRPO по нескольким бенчмаркам, включая награду за обучение, AIME'24, LiveCodeBench и CodeForces"](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-01.jpeg)

## 2. Проблема существующих методов
Современные передовые алгоритмы RL для LLM страдают от критических проблем стабильности. Proximal Policy Optimization (PPO), хотя и широко используется, требует отдельной модели значений такого же размера, как и модель политики, что создает значительные накладные расходы на память. Group Relative Policy Optimization (GRPO) решил эту проблему, устранив зависимость от модели значений, но ввел более фундаментальную проблему.

Авторы определяют, что нестабильность GRPO проистекает из неправильного применения сэмплирования по важности. GRPO применяет веса важности на уровне токенов, в то время как награды назначаются на уровне последовательностей. Это создает несоответствие, при котором отдельные токены внутри последовательности получают разные веса важности, несмотря на то, что сигнал награды применяется ко всему ответу.

**Давайте немного вспомним как работает GRPO:**

!["Group Relative Policy Optimization (GRPO)"](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-02.jpg)

Самым распространенным для языковых моделей алгоритмом RL является Proximal Policy Optimization (PPO), и GRPO как раз является его вариацией. Суть: 

➖ У агента есть начальная политика (стратегия), по которой он действует.

➖ Агент выполняет действия в среде (отвечает на вопросы), следуя своей текущей политике

➖ PPO оценивает действие агента. Для этого обычно используется три модели: 
reference model – модель, которая выступает эталоном и позволяет измерять, насколько текущая политика изменилась по сравнению с исходной, 
reward model – оценивает награду, которую агент получает за выполнение действия прямо сейчас, 
value model – оценивает ожидаемую долгосрочную выгоду от действия, предсказывая будущие награды. 

➖ На основе этих оценок агент меняет свою политику. Здесь заключена основная особенность алгоритма: функция потерь в PPO устроена так, что слишком резкие изменения политики не допускаются.  Это помогает агенту постепенно улучшать свою стратегию, не делая слишком резких шагов сразу, что делает процесс обучения более стабильным и эффективным. 

Но есть в PPO и недостатки. В частности, value model, которая играет ключевую роль в PPO, тащит на себя очень много ресурсов, потому что обычно сопоставима по размерам с моделью, которую мы обучаем. Это делает обучение дорогим. Так что из GRPO (Group Relative Policy Optimization) value model вообще выкинули. Вместо value model в GRPO мы используем среднюю награду от группы ответов на один и тот же вопрос, и так определяем, насколько "хороши" действия модели. То есть в GRPO оценка качества ответа основана на сравнении с другими ответами в группе, а не на абсолютных значениях наград. Если ответ лучше среднего по группе, политика усиливает вероятность его выбора. Если хуже — ослабляет. Это компенсирует оценку value model и делает обучение более эффективным и менее ресурсоемким. 

Кстати, GRPO работает хорошо даже если пропустить этап файнтюнинга. Так обучали R1-Zero, младшую сестренку R1. Для нее вообще не использовали никакой разметки, и GRPO вытащил все ее качество исключительно на себе.

Математически GRPO использует веса важности на уровне токенов:

!["GRPO"](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-03.png)

Это взвешивание на уровне токенов вводит высокодисперсионный шум, который накапливается в длинных последовательностях. Дисперсия дополнительно усиливается механизмами обрезки, что в конечном итоге приводит к наблюдаемой нестабильности обучения и коллапсу модели, особенно при крупномасштабных развертываниях.

## 3. Методология GSPO

GSPO решает эти проблемы, фундаментально перестраивая сэмплирование по важности в соответствии со структурой вознаграждения. Ключевая идея состоит в определении коэффициентов важности на уровне последовательности, где фактически назначаются награды.

Функция цели GSPO:

!["GSPO"](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-04.png)

Коэффициент важности на уровне последовательности определяется как:

!["sequence-level"](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-05.png)

Ключевым элементом является член нормализации по длине 1/|y_i| в показателе степени. Без такой нормализации коэффициенты правдоподобия на уровне последовательности могли бы значительно варьироваться для ответов разной длины, что потребовало бы использования переменных диапазонов отсечения. Благодаря нормализации значение s_i(theta) остаётся в постоянном числовом диапазоне независимо от длины последовательности.

Для оценки преимущества A_i применяется тот же групповой подход без модели значений, что и в GRPO.

!["advantage estimation"](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-06.png)

Это сохраняет преимущества вычислительной эффективности GRPO, одновременно решая его фундаментальные проблемы стабильности.

## 4. Ключевые алгоритмические различия

Основное различие между GSPO и GRPO заключается во взвешивании градиентов. GRPO применяет колеблющиеся веса важности на уровне токенов к градиентам отдельных токенов, создавая высокую дисперсию. GSPO применяет единый, стабильный вес важности на уровне последовательности ко всем токенам в ответе, устраняя проблемную дисперсию.

В терминах градиентов, GSPO взвешивает все токены в последовательности одинаково после применения коэффициента важности на уровне последовательности, в то время как GRPO создает неравномерное взвешивание между токенами в одной и той же последовательности. Это выравнивание со структурой вознаграждения (на уровне последовательности) создает более стабильную и теоретически обоснованную динамику обучения.

Авторы также представляют GSPO-token, вариант, который позволяет настраивать преимущества по токенам для сценариев, таких как многошаговое обучение с подкреплением (RL), сохраняя при этом основной принцип взвешивания важности на уровне последовательности посредством операций stop-gradient.

## 5. Результаты экспериментов

Эмпирическая оценка демонстрирует значительные преимущества GSPO по нескольким параметрам:

**Стабильность обучения:** GSPO поддерживает стабильное обучение на протяжении всего процесса, в то время как GRPO демонстрирует нестабильность и потенциальный коллапс. Эта стабильность обеспечивает непрерывное улучшение производительности с увеличением вычислительной мощности обучения, регулярными обновлениями набора запросов и увеличенной длиной генерации.

**Производительность и эффективность:** при идентичных вычислительных бюджетах GSPO последовательно достигает лучшей точности обучения и производительности в бенчмарках на сложных задачах, включая AIME'24, LiveCodeBench и CodeForces. Это демонстрирует, что GSPO не только более стабилен, но и более эффективен по выборке.

**Обучение смешанных экспертов:** особо значительным открытием является способность GSPO разрешать проблемы стабильности в обучении RL больших моделей MoE (Mixture-of-Experts). GRPO требовал сложных обходных путей, таких как "Routing Replay" (передача маршрутизации), для обработки изменчивости активации экспертов. Подход GSPO на уровне последовательности по своей природе нечувствителен к этой изменчивости, что позволяет стабильно обучать MoE без дополнительных накладных расходов или ограничений по мощности.

!['Сравнение долей отсечения, показывающее, что GSPO отсекает значительно больше токенов, чем GRPO, при этом достигая лучшей производительности'](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-07.jpeg)

**Контринтуитивное поведение отсечения:** примечательно, что GSPO отсекает примерно на два порядка больше токенов, чем GRPO, но при этом достигает превосходной эффективности обучения. Это подтверждает гипотезу о том, что оценки градиентов GRPO на уровне токенов по своей природе зашумлены и неэффективны. Подход GSPO на уровне последовательности обеспечивает более качественные сигналы обучения даже при отсечении большего количества данных.

!['Производительность GRPO с и без Routing Replay, демонстрирующая необходимость этого сложного обходного пути для стабильности MoE'](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-08.jpeg)

## 6. Практическое применение и преимущества для инфраструктуры

Помимо улучшения производительности, GSPO предлагает практические преимущества для инфраструктуры RL. Его зависимость от вероятностей на уровне последовательности делает его более толерантным к расхождениям в точности между движками обучения и вывода. Это позволяет напрямую использовать вероятности из движков вывода, избегая дорогостоящих перерасчетов и упрощая конвейеры RL.

Специально для моделей MoE GSPO устраняет необходимость в "Routing Replay" и аналогичных стратегиях стабилизации, снижая накладные расходы на память и связь, а также позволяя моделям использовать всю свою архитектурную мощность без искусственных ограничений.

## 7. Значение и будущие последствия

GSPO представляет собой эволюцию наверно центрального алгоритма RL для LLM китов, поскольку он устраняет основные теоретические и практические ограничения существующих методов. Его вклад в "заметные улучшения в последних моделях Qwen3" демонстрирует реальное влияние, выходящее за рамки теоретических достижений.

Способность алгоритма обеспечивать стабильное, крупномасштабное обучение с подкреплением (ОП) открывает новые возможности для дальнейшего масштабирования возможностей посредством обучения с подкреплением. Решая проблему узкого места в стабильности, GSPO позволяет исследователям инвестировать больше вычислительных ресурсов в обучение с подкреплением с уверенностью в сходимости.

Для более широкой области GSPO предоставляет шаблон для проектирования алгоритмов, который тщательно согласует механизмы оптимизации со структурами вознаграждения. Этот принцип может лечь в основу будущих разработок алгоритмов ОП, особенно по мере того, как модели продолжают масштабироваться и усложняться.

Эта работа позиционирует область для исследования более сложных применений ОП для больших языковых моделей (БЯМ), потенциально обеспечивая прорывы в сложном рассуждении, многошаговом решении задач и других возможностях, которые выигрывают от более глубокого исследования, обеспечиваемого стабильным, крупномасштабным обучением с подкреплением.