# Group Sequence Policy Optimization

## Содержание
1. Введение
2. Проблема существующих методов
3. Методология GSPO
4. Ключевые алгоритмические различия
5. Результаты экспериментов
6. Практическое применение и преимущества для инфраструктуры
7. Значение и будущие последствия

## 1. Введение

Обучение с подкреплением (RL) стало важным инструментом для масштабирования больших языковых моделей (LLM) для решения сложных задач по рассуждению в математике и программировании. Однако применение RL к моделям с миллиардами параметров сталкивается с критической проблемой: нестабильность обучения, которая может привести к катастрофическому коллапсу модели. Эта статья представляет Group Sequence Policy Optimization (GSPO), новый алгоритм RL, разработанный для устранения фундаментальных недостатков существующих методов, таких как Group Relative Policy Optimization (GRPO).

Основное новшество GSPO заключается в переходе от сэмплирования по важности на уровне токенов к сэмплированию на уровне последовательностей, что согласовывает единицу оптимизации с тем, как на самом деле назначаются награды. Это, казалось бы, простое изменение устраняет серьезные проблемы стабильности, которые преследовали крупномасштабное обучение RL, особенно для моделей "Смесь экспертов" (MoE). Работа демонстрирует, что GSPO не только достигает превосходной стабильности, но также улучшает эффективность и производительность обучения, способствуя замечательным улучшениям, наблюдаемым в последних моделях Alibaba Qwen3.

!["Сравнение производительности обучения, показывающее превосходную стабильность и производительность GSPO по сравнению с GRPO по нескольким бенчмаркам, включая награду за обучение, AIME'24, LiveCodeBench и CodeForces"](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-01.png)

## 2. Проблема существующих методов
Современные передовые алгоритмы RL для LLM страдают от критических проблем стабильности. Proximal Policy Optimization (PPO), хотя и широко используется, требует отдельной модели значений такого же размера, как и модель политики, что создает значительные накладные расходы на память. Group Relative Policy Optimization (GRPO) решил эту проблему, устранив зависимость от модели значений, но ввел более фундаментальную проблему.

Авторы определяют, что нестабильность GRPO проистекает из неправильного применения сэмплирования по важности. GRPO применяет веса важности на уровне токенов, в то время как награды назначаются на уровне последовательностей. Это создает несоответствие, при котором отдельные токены внутри последовательности получают разные веса важности, несмотря на то, что сигнал награды применяется ко всему ответу.

**Давайте немного вспомним как работает GRPO:**

!["Group Relative Policy Optimization (GRPO)"](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-02.png)

Самым распространенным для языковых моделей алгоритмом RL является Proximal Policy Optimization (PPO), и GRPO как раз является его вариацией. Суть: 

➖ У агента есть начальная политика (стратегия), по которой он действует.

➖ Агент выполняет действия в среде (отвечает на вопросы), следуя своей текущей политике

➖ PPO оценивает действие агента. Для этого обычно используется три модели: 
reference model – модель, которая выступает эталоном и позволяет измерять, насколько текущая политика изменилась по сравнению с исходной, 
reward model – оценивает награду, которую агент получает за выполнение действия прямо сейчас, 
value model – оценивает ожидаемую долгосрочную выгоду от действия, предсказывая будущие награды. 

➖ На основе этих оценок агент меняет свою политику. Здесь заключена основная особенность алгоритма: функция потерь в PPO устроена так, что слишком резкие изменения политики не допускаются.  Это помогает агенту постепенно улучшать свою стратегию, не делая слишком резких шагов сразу, что делает процесс обучения более стабильным и эффективным. 

Но есть в PPO и недостатки. В частности, value model, которая играет ключевую роль в PPO, тащит на себя очень много ресурсов, потому что обычно сопоставима по размерам с моделью, которую мы обучаем. Это делает обучение дорогим. 

Так что из GRPO (Group Relative Policy Optimization) value model вообще выкинули. Вместо value model в GRPO мы используем среднюю награду от группы ответов на один и тот же вопрос, и так определяем, насколько "хороши" действия модели. 

То есть в GRPO оценка качества ответа основана на сравнении с другими ответами в группе, а не на абсолютных значениях наград. Если ответ лучше среднего по группе, политика усиливает вероятность его выбора. Если хуже — ослабляет. Это компенсирует оценку value model и делает обучение более эффективным и менее ресурсоемким. 

Кстати, GRPO работает хорошо даже если пропустить этап файнтюнинга. Так обучали R1-Zero, младшую сестренку R1. Для нее вообще не использовали никакой разметки, и GRPO вытащил все ее качество исключительно на себе.

Математически GRPO использует веса важности на уровне токенов:

$$w_{i,t}(\theta) = \frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\pi_{\theta_{old}}(y_{i,t}|x,y_{i,<t})}$$

Это взвешивание на уровне токенов вводит высокодисперсионный шум, который накапливается в длинных последовательностях. Дисперсия дополнительно усиливается механизмами обрезки, что в конечном итоге приводит к наблюдаемой нестабильности обучения и коллапсу модели, особенно при крупномасштабных развертываниях.