# Group Sequence Policy Optimization

## Содержание
1. Введение
2. Проблема существующих методов
3. Методология GSPO
4. Ключевые алгоритмические различия
5. Результаты экспериментов
6. Практическое применение и преимущества для инфраструктуры
7. Значение и будущие последствия

## 1. Введение

Обучение с подкреплением (RL) стало важным инструментом для масштабирования больших языковых моделей (LLM) для решения сложных задач по рассуждению в математике и программировании. Однако применение RL к моделям с миллиардами параметров сталкивается с критической проблемой: нестабильность обучения, которая может привести к катастрофическому коллапсу модели. Эта статья представляет Group Sequence Policy Optimization (GSPO), новый алгоритм RL, разработанный для устранения фундаментальных недостатков существующих методов, таких как Group Relative Policy Optimization (GRPO).

Основное новшество GSPO заключается в переходе от сэмплирования по важности на уровне токенов к сэмплированию на уровне последовательностей, что согласовывает единицу оптимизации с тем, как на самом деле назначаются награды. Это, казалось бы, простое изменение устраняет серьезные проблемы стабильности, которые преследовали крупномасштабное обучение RL, особенно для моделей "Смесь экспертов" (MoE). Работа демонстрирует, что GSPO не только достигает превосходной стабильности, но также улучшает эффективность и производительность обучения, способствуя замечательным улучшениям, наблюдаемым в последних моделях Alibaba Qwen3.

!["Сравнение производительности обучения, показывающее превосходную стабильность и производительность GSPO по сравнению с GRPO по нескольким бенчмаркам, включая награду за обучение, AIME'24, LiveCodeBench и CodeForces"](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-01.jpeg)

## 2. Проблема существующих методов
Современные передовые алгоритмы RL для LLM страдают от критических проблем стабильности. Proximal Policy Optimization (PPO), хотя и широко используется, требует отдельной модели значений такого же размера, как и модель политики, что создает значительные накладные расходы на память. Group Relative Policy Optimization (GRPO) решил эту проблему, устранив зависимость от модели значений, но ввел более фундаментальную проблему.

Авторы определяют, что нестабильность GRPO проистекает из неправильного применения сэмплирования по важности. GRPO применяет веса важности на уровне токенов, в то время как награды назначаются на уровне последовательностей. Это создает несоответствие, при котором отдельные токены внутри последовательности получают разные веса важности, несмотря на то, что сигнал награды применяется ко всему ответу.

**Давайте немного вспомним как работает GRPO:**

!["Group Relative Policy Optimization (GRPO)"](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-02.jpg)

Самым распространенным для языковых моделей алгоритмом RL является Proximal Policy Optimization (PPO), и GRPO как раз является его вариацией. Суть: 

➖ У агента есть начальная политика (стратегия), по которой он действует.

➖ Агент выполняет действия в среде (отвечает на вопросы), следуя своей текущей политике

➖ PPO оценивает действие агента. Для этого обычно используется три модели: 
reference model – модель, которая выступает эталоном и позволяет измерять, насколько текущая политика изменилась по сравнению с исходной, 
reward model – оценивает награду, которую агент получает за выполнение действия прямо сейчас, 
value model – оценивает ожидаемую долгосрочную выгоду от действия, предсказывая будущие награды. 

➖ На основе этих оценок агент меняет свою политику. Здесь заключена основная особенность алгоритма: функция потерь в PPO устроена так, что слишком резкие изменения политики не допускаются.  Это помогает агенту постепенно улучшать свою стратегию, не делая слишком резких шагов сразу, что делает процесс обучения более стабильным и эффективным. 

Но есть в PPO и недостатки. В частности, value model, которая играет ключевую роль в PPO, тащит на себя очень много ресурсов, потому что обычно сопоставима по размерам с моделью, которую мы обучаем. Это делает обучение дорогим. Так что из GRPO (Group Relative Policy Optimization) value model вообще выкинули. Вместо value model в GRPO мы используем среднюю награду от группы ответов на один и тот же вопрос, и так определяем, насколько "хороши" действия модели. То есть в GRPO оценка качества ответа основана на сравнении с другими ответами в группе, а не на абсолютных значениях наград. Если ответ лучше среднего по группе, политика усиливает вероятность его выбора. Если хуже — ослабляет. Это компенсирует оценку value model и делает обучение более эффективным и менее ресурсоемким. 

Кстати, GRPO работает хорошо даже если пропустить этап файнтюнинга. Так обучали R1-Zero, младшую сестренку R1. Для нее вообще не использовали никакой разметки, и GRPO вытащил все ее качество исключительно на себе.

Математически GRPO использует веса важности на уровне токенов:

!["GRPO"](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-03.png)

Это взвешивание на уровне токенов вводит высокодисперсионный шум, который накапливается в длинных последовательностях. Дисперсия дополнительно усиливается механизмами обрезки, что в конечном итоге приводит к наблюдаемой нестабильности обучения и коллапсу модели, особенно при крупномасштабных развертываниях.

## 3. Методология GSPO

GSPO решает эти проблемы, фундаментально перестраивая сэмплирование по важности в соответствии со структурой вознаграждения. Ключевая идея состоит в определении коэффициентов важности на уровне последовательности, где фактически назначаются награды.

Функция цели GSPO:

!["GSPO"](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-04.png)

Коэффициент важности на уровне последовательности определяется как:

!["sequence-level"](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-05.png)

Ключевым элементом является член нормализации по длине 1/|y_i| в показателе степени. Без такой нормализации коэффициенты правдоподобия на уровне последовательности могли бы значительно варьироваться для ответов разной длины, что потребовало бы использования переменных диапазонов отсечения. Благодаря нормализации значение s_i(theta) остаётся в постоянном числовом диапазоне независимо от длины последовательности.

Для оценки преимущества A_i применяется тот же групповой подход без модели значений, что и в GRPO.

!["advantage estimation"](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-32/assets/Image-06.png)

Это сохраняет преимущества вычислительной эффективности GRPO, одновременно решая его фундаментальные проблемы стабильности.

## 4. Ключевые алгоритмические различия

Основное различие между GSPO и GRPO заключается во взвешивании градиентов. GRPO применяет колеблющиеся веса важности на уровне токенов к градиентам отдельных токенов, создавая высокую дисперсию. GSPO применяет единый, стабильный вес важности на уровне последовательности ко всем токенам в ответе, устраняя проблемную дисперсию.

В терминах градиентов, GSPO взвешивает все токены в последовательности одинаково после применения коэффициента важности на уровне последовательности, в то время как GRPO создает неравномерное взвешивание между токенами в одной и той же последовательности. Это выравнивание со структурой вознаграждения (на уровне последовательности) создает более стабильную и теоретически обоснованную динамику обучения.

Авторы также представляют GSPO-token, вариант, который позволяет настраивать преимущества по токенам для сценариев, таких как многошаговое обучение с подкреплением (RL), сохраняя при этом основной принцип взвешивания важности на уровне последовательности посредством операций stop-gradient.
