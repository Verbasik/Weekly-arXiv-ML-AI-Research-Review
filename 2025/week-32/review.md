# Групповая оптимизация политик в обучении с подкреплением: GSPO vs GRPO

**Group Relative Policy Optimization (GRPO)** и **Group Sequence Policy Optimization (GSPO)** представляют собой два передовых подхода к обучению с подкреплением больших языковых моделей, каждый из которых решает фундаментальные проблемы классических методов через инновационные групповые стратегии. GRPO, впервые представленный командой DeepSeek в феврале 2024 года, произвел революцию в области путем замены критической сети групповой нормализацией наград. GSPO, разработанный командой Qwen в июле 2025 года, развил эти идеи дальше, перейдя от токен-уровневой к последовательность-уровневой оптимизации и решив критические проблемы стабильности, особенно для Mixture-of-Experts архитектур.

Экспериментальные результаты демонстрируют драматические улучшения: GRPO сокращает потребление памяти на 50% по сравнению с PPO при сохранении производительности, а GSPO дополнительно повышает стабильность обучения на 200% при работе с MoE моделями. Оба метода показали выдающиеся результаты в математических рассуждениях - DeepSeekMath с GRPO достиг 51.7% на MATH benchmark, в то время как Qwen3 с GSPO демонстрирует еще более высокую эффективность обучения.

## Математические основы GRPO: замена критика групповым сравнением

Group Relative Policy Optimization представляет фундаментальный сдвиг в архитектуре policy gradient методов, **устраняя необходимость в отдельной value network** через элегантное групповое сравнение наград. Метод оптимизирует clipped surrogate objective, аналогичный PPO, но с принципиально иной оценкой преимуществ.

Математическая формулировка GRPO базируется на целевой функции:

$$J_{GRPO}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)} \left[\frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \min\left[\frac{\pi_\theta(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})} \cdot \hat{A}_{i,t}, \text{clip}\left(\frac{\pi_\theta(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})}, 1-\epsilon, 1+\epsilon\right) \cdot \hat{A}_{i,t}\right]\right] - \beta \cdot D_{KL}[\pi_\theta \parallel \pi_{ref}]$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$J_{GRPO}(\theta)$** — целевая функция (objective function), которую алгоритм максимизирует для оптимизации политики.
- **$\theta$** — параметры **текущей политики** (нейронной сети), которые обновляются в процессе обучения.
- **$\theta_{old}$** — параметры **старой политики**, зафиксированной на момент сбора данных (генерации ответов).
- **$q$** — входной промпт (query), для которого генерируются ответы.
- **$\mathbb{E}_{...}$** — математическое ожидание, взятое по промптам из набора данных и по группам ответов, сгенерированных старой политикой.
- **$G$** — размер группы, т.е. количество ответов, сгенерированных для одного промпта $q$.
- **$\{o_i\}_{i=1}^G$** — группа из $G$ ответов (outputs), сгенерированных на промпт $q$.
- **$o_{i,t}$** — $t$-й токен в $i$-м ответе.
- **$|o_i|$** — длина $i$-го ответа в токенах.
- **$\pi_\theta(o_{i,t}|q,o_{i,<t})$** — вероятность генерации токена $o_{i,t}$ **текущей политикой** $\pi_\theta$ при условии промпта $q$ и предыдущих токенов $o_{i,<t}$.
- **$\pi_{\theta_{old}}(...)$** — аналогичная вероятность, но для **старой политики** $\pi_{\theta_{old}}$.
- **$\frac{\pi_\theta}{\pi_{\theta_{old}}}$** — **коэффициент важности (importance weighting)** на уровне токена. Он корректирует градиенты, позволяя обучать новую политику на данных, собранных старой.
- **$\hat{A}_{i,t}$** — **преимущество (advantage)**, присвоенное $t$-му токену в $i$-м ответе. В GRPO это значение одинаково для всех токенов одной последовательности и равно $\hat{A}_i$.
- **$\text{clip}(...)$** — функция отсечения, которая ограничивает коэффициент важности диапазоном $[1-\epsilon, 1+\epsilon]$, предотвращая слишком большие обновления политики.
- **$\epsilon$** — гиперпараметр клиппинга (типично 0.1–0.2).
- **$\beta$** — коэффициент, регулирующий силу KL-регуляризации.
- **$D_{KL}[\pi_\theta \parallel \pi_{ref}]$** — KL-дивергенция между текущей и референсной политикой.

</details> 

---

Ключевая инновация заключается в **групповой оценке преимуществ**, которая заменяет традиционную функцию ценности:

$$\hat{A}_i = \frac{r_i - \text{mean}(\{r_1, r_2, \ldots, r_G\})}{\text{std}(\{r_1, r_2, \ldots, r_G\})}$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>
где:
- **$\hat{A}_i$** — нормализованное преимущество для $i$-го ответа в группе. Это значение показывает, насколько $i$-й ответ лучше или хуже "среднего" ответа в группе.
- **$r_i$** — награда (reward) для $i$-го ответа, полученная от внешней модели-оценщика (reward model).
- **$\{r_1, r_2, \ldots, r_G\}$** — множество наград для всех $G$ ответов в группе.
- **$\text{mean}(\{...\})$** — среднее арифметическое наград по всей группе. Это значение служит **динамической базовой линией (baseline)**, которая заменяет выходные данные сети критика (value network).
- **$\text{std}(\{...\})$** — стандартное отклонение наград по группе. Нормализация на эту величину стабилизирует градиенты и делает обучение менее чувствительным к масштабу наград.
</details> 

---

где G представляет размер группы (обычно 32-64 ответа), а нормализация по стандартному отклонению обеспечивает стабильность градиентов. Эта формулировка теоретически обоснована принципом снижения дисперсии: групповое среднее служит естественной базовой линией, а относительные сравнения менее чувствительны к абсолютным значениям наград.

Алгоритм GRPO работает итеративно: для каждого вопроса q генерируется группа из G ответов, вычисляются награды через модель оценки, затем **токен-уровневые преимущества** распространяются на всю последовательность. KL-дивергенция с референсной политикой обеспечивает регуляризацию:

$$
D_{KL}[\pi_\theta \parallel \pi_{ref}] = \mathbb{E} \left[ \log\frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_\theta(o_{i,t}|q,o_{i,<t})} \right]
$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$D_{KL}[\pi_\theta \parallel \pi_{ref}]$** — KL-дивергенция, которая измеряет, насколько сильно текущая политика $\pi_\theta$ отклонилась от **референсной политики** $\pi_{ref}$.
- **$\pi_{ref}$** — референсная модель (часто исходная SFT-модель), которая служит "якорем". Регуляризация не позволяет оптимизируемой модели слишком сильно отойти от изначального распределения, что помогает сохранить ее общие языковые способности и предотвратить "катастрофическое забывание".
</details>

---

Давайте детально, шаг за шагом, разберем, как работают GRPO и GSPO на конкретном примере. Представим, что мы обучаем модель давать краткие и точные ответы на научные вопросы.

### Сценарий для примера

*   **Промпт (Query):** `q = "Сформулируйте теорему Пифагора в одном предложении."`
*   **Размер группы (G):** Для простоты возьмем $G = 4$. Это значит, что на один промпт модель сгенерирует 4 разных ответа.
*   **Политики:**
    *   $\pi_{\theta_{old}}$ — "старая" версия модели, которую мы используем для генерации ответов.
    *   $\pi_{\theta}$ — "новая" версия модели, которую мы обучаем.
*   **Модель-оценщик (Reward Model):** Внешняя система, которая оценивает качество каждого ответа по шкале от 0 до 10, где 10 — идеально.

<details> 
    <summary><em><strong>пример</strong></em></summary>

### Пример №1: Как работает GRPO "под капотом"

GRPO работает на уровне **токенов**. Он вычисляет общее "преимущество" для всей последовательности, а затем применяет его к каждому токену отдельно, взвешивая по вероятности генерации этого токена.

#### **Шаг 1: Генерация ответов и оценка**

Наша "старая" модель $\pi_{\theta_{old}}$ генерирует 4 ответа на промпт. Модель-оценщик выставляет им награды ($r_i$):

1.  $o_1$: "В прямоугольном треугольнике квадрат гипотенузы равен сумме квадратов катетов."
    *   **Награда ($r_1$): 10.0** (Идеальный ответ)
2.  $o_2$: "Сумма квадратов двух сторон равна квадрату третьей."
    *   **Награда ($r_2$): 6.0** (Неточно, так как не указано, что треугольник прямоугольный)
3.  $o_3$: "$a^2 + b^2 = c^2$"
    *   **Награда ($r_3$): 8.0** (Точно, но менее полно, чем первый ответ)
4.  $o_4$: "Теорема о треугольниках."
    *   **Награда ($r_4$): 1.0** (Очень плохой, неинформативный ответ)

#### **Шаг 2: Групповая нормализация (Вычисление преимущества $\hat{A}_i$)**

Теперь ключевая инновация GRPO: вместо того чтобы предсказывать "ценность" ответа с помощью отдельной нейросети (критика), мы сравниваем каждый ответ со средним по группе.

*   **Формула:** $\hat{A}_i = (r_i - \text{mean}(r)) / \text{std}(r)$

1.  **Считаем среднее (mean):**
    $\text{mean} = (10.0 + 6.0 + 8.0 + 1.0) / 4 = 25.0 / 4 = 6.25$
    *   *Интерпретация:* "средний" ответ в этой группе имеет качество 6.25.

2.  **Считаем стандартное отклонение (std):**
    $\text{std} = \sqrt{((10-6.25)^2 + (6-6.25)^2 + (8-6.25)^2 + (1-6.25)^2)/4} = \sqrt{(14.06 + 0.06 + 3.06 + 27.56)/4} = \sqrt{11.185} \approx 3.34$
    *   *Интерпретация:* это мера разброса наград. Большая величина означает, что ответы очень разного качества.

3.  **Вычисляем преимущество ($\hat{A}_i$) для каждого ответа:**
    *   $\hat{A}_1 = (10.0 - 6.25) / 3.34 \approx +1.12$ (Значительно лучше среднего)
    *   $\hat{A}_2 = (6.0 - 6.25) / 3.34 \approx -0.07$ (Чуть хуже среднего)
    *   $\hat{A}_3 = (8.0 - 6.25) / 3.34 \approx +0.52$ (Лучше среднего)
    *   $\hat{A}_4 = (1.0 - 6.25) / 3.34 \approx -1.57$ (Значительно хуже среднего)

**Ключевой момент:** положительное преимущество $\hat{A}$ будет поощрять модель, а отрицательное — наказывать.

#### **Шаг 3: Токен-уровневая оптимизация**

Теперь самое главное. GRPO "разбирает" каждый ответ на токены и применяет обновление к каждому из них. Давайте посмотрим на ответ $o_1$: "В", "прямоугольном", "треугольнике", "...", "катетов", ".".

Для **каждого токена** $o_{1,t}$ в этой последовательности мы делаем следующее:

1.  **Вычисляем коэффициент важности (importance weight):**

    $w_{1,t} = \pi_\theta(o_{1,t} | ...) / \pi_{\theta_{old}}(o_{1,t} | ...)$

    Допустим, для токена "гипотенузы" старая модель была не очень уверена и дала вероятность 0.4, а новая, обучаемая модель, стала более уверенной и дала вероятность 0.6.

    $w_{\text{гипотенузы}} = 0.6 / 0.4 = 1.5$

2.  **Вычисляем вклад этого токена в общую функцию потерь:**

    $\text{loss\_contribution} = \min(w_{1,t} \cdot \hat{A}_1, \text{clip}(w_{1,t}) \cdot \hat{A}_1)$

    Используя наше преимущество $\hat{A}_1 = +1.12$ и вес $w = 1.5$:

    $\text{loss\_contribution} = \min(1.5 \cdot 1.12, \text{clip}(1.5, 0.8, 1.2) \cdot 1.12)$

    $\text{loss\_contribution} = \min(1.68, 1.2 \cdot 1.12) = \min(1.68, 1.344) = 1.344$

**Что здесь произошло?**
Алгоритм увидел, что новая политика $\pi_\theta$ стала генерировать токен "гипотенузы" с большей вероятностью. Поскольку вся последовательность была "хорошей" ($\hat{A}_1 > 0$), это изменение поощряется. Клиппинг ($\text{clip}$) не дает этому поощрению быть слишком большим, чтобы обучение было стабильным.

Этот процесс повторяется для **каждого токена во всех четырех ответах**. Преимущество $\hat{A}_1 = +1.12$ будет одинаковым для всех токенов ответа $o_1$, а $\hat{A}_4 = -1.57$ — для всех токенов ответа $o_4$.

#### **Итог GRPO:**

Модель получает сигнал обратной связи на уровне отдельных токенов. Если последовательность в целом была хорошей ($\hat{A} > 0$), вероятность **каждого** ее токена будет увеличена (пропорционально $w_t$). Если плохой ($\hat{A} < 0$), вероятность каждого токена будет уменьшена. Это просто, но, как говорят ребята из Qwen, теоретически не совсем корректно и может создавать "шум".

</details>

---

## Революционный подход GSPO: переход к последовательность-уровневой оптимизации

Group Sequence Policy Optimization представляет следующую эволюцию групповых методов, решая **фундаментальные проблемы токен-уровневого подхода GRPO** через переход к последовательность-уровневой оптимизации. Команда Qwen выявила критический недостаток GRPO: токен-уровневые важностные веса основаны на единственной выборке для каждой позиции токена, что не выполняет корректную коррекцию распределения и вносит высокодисперсионный шум.

GSPO оптимизирует модифицированную целевую функцию:

$$J_{GSPO}(\theta) = \mathbb{E}_{x \sim D, \{y_i\}_{i=1}^G \sim \pi_{\theta_{old}}(\cdot|x)} \left[\frac{1}{G} \sum_{i=1}^{G} \min(s_i(\theta)\hat{A}_i, \text{clip}(s_i(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_i)\right]$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$J_{GSPO}(\theta)$** — целевая функция GSPO, которая оптимизируется на уровне целых последовательностей.
- **$x$** — входной промпт (аналогично $q$ в GRPO).
- **$D$** — набор данных с промптами.
- **$\{y_i\}_{i=1}^G$** — группа из $G$ сгенерированных ответов (последовательностей).
- **$s_i(\theta)$** — **коэффициент важности на уровне последовательности (sequence-level importance weight)**. Это ключевое отличие от GRPO.
- **$\hat{A}_i$** — **преимущество на уровне последовательности**, вычисляемое так же, как в GRPO, но применяемое ко всей последовательности как единому целому.
- **$\min(...)$** и **$\text{clip}(...)$** — механизм отсечения, аналогичный PPO и GRPO, но применяемый к коэффициенту важности всей последовательности $s_i(\theta)$.
</details> 

---

**Ключевая инновация** - определение коэффициента важности на уровне последовательности с нормализацией по длине:

$$s_i(\theta) = \left(\frac{\pi_\theta(y_i|x)}{\pi_{\theta_{old}}(y_i|x)}\right)^{1/|y_i|} = \exp\left(\frac{1}{|y_i|} \sum_{t=1}^{|y_i|} \log\left[\frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\pi_{\theta_{old}}(y_{i,t}|x,y_{i,<t})}\right]\right)$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>
где:
- **$s_i(\theta)$** — отношение вероятностей генерации всей последовательности $y_i$ текущей и старой политиками.
- **$\pi_\theta(y_i|x)$** — вероятность генерации **всей последовательности** $y_i$ текущей политикой. Вычисляется как произведение вероятностей всех ее токенов.
- **$\pi_{\theta_{old}}(y_i|x)$** — аналогичная вероятность для старой политики.
- **$|y_i|$** — длина последовательности $y_i$ в токенах.
- **$(\cdot)^{1/|y_i|}$** — **нормализация по длине**. Это критически важный шаг, который, по сути, вычисляет геометрическое среднее токен-уровневых отношений. Он выполняет три функции:
    1.  **Снижает дисперсию**: предотвращает экспоненциальный рост или затухание значения $s_i(\theta)$ с увеличением длины последовательности.
    2.  **Унифицирует диапазон**: приводит коэффициенты важности для коротких и длинных последовательностей к сопоставимому числовому диапазону.
    3.  **Предотвращает доминирование**: не позволяет длинным последовательностям (с большим количеством множителей в произведении вероятностей) оказывать непропорционально большое влияние на градиент.
</details> 

---

Нормализация по длине через показатель степени `1/|y_i|` выполняет три критические функции: снижает дисперсию градиентов, унифицирует численный диапазон коэффициентов важности и предотвращает доминирование длинных последовательностей над короткими.

Групповое относительное вычисление преимущества остается аналогичным GRPO:

$$\hat{A}_i = \frac{r(x,y_i) - \text{mean}\{r(x,y_i)\}_{i=1}^G}{\text{std}\{r(x,y_i)\}_{i=1}^G}$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>
где:
- **$\hat{A}_i$** — нормализованное преимущество для $i$-й последовательности.
- **$r(x, y_i)$** — награда для ответа $y_i$ на промпт $x$.
- **$\text{mean}\{...\}$** и **$\text{std}\{...\}$** — среднее и стандартное отклонение наград по группе.
- **Ключевое отличие от GRPO**: хотя формула идентична, здесь преимущество $\hat{A}_i$ умножается на единый коэффициент важности $s_i(\theta)$ для всей последовательности. Это обеспечивает соответствие между единицей оптимизации (последовательность) и единицей награждения (также последовательность), что **устраняет токен-уровневый шум** и делает обучение более стабильным и теоретически обоснованным.
</details> 

---

Но применяется на уровне всей последовательности, что **устраняет токен-уровневый шум** и обеспечивает правильное выравнивание между единицей оптимизации (последовательность) и единицей награждения (также последовательность).

<details> 
    <summary><em><strong>пример</strong></em></summary>

### Пример №2: Как работает GSPO "под капотом"

GSPO устраняет главную проблему GRPO, переходя к оптимизации на уровне **целых последовательностей**. Это более логично, так как награда дается за весь ответ, а не за отдельные слова.

#### **Шаги 1 и 2: Генерация, оценка и вычисление преимущества**

Эти шаги **абсолютно идентичны** GRPO. Мы используем тот же промпт, те же 4 ответа и получаем те же самые значения преимущества:
*   $\hat{A}_1 \approx +1.12$
*   $\hat{A}_2 \approx -0.07$
*   $\hat{A}_3 \approx +0.52$
*   $\hat{A}_4 \approx -1.57$

#### **Шаг 3: Ключевое отличие — вычисление коэффициента важности на уровне последовательности ($s_i(\theta)$)**

Вместо того чтобы смотреть на каждый токен отдельно, GSPO вычисляет единый коэффициент важности для всей последовательности.

*   **Формула:** $s_i(\theta) = (\pi_\theta(y_i) / \pi_{\theta_{old}}(y_i))^{1/|y_i|}$

Давайте снова рассмотрим наш лучший ответ $o_1$ (в GSPO он называется $y_1$), который состоит, скажем, из 12 токенов ($|y_1| = 12$).

1.  **Считаем вероятность всей последовательности:**
    *   $\pi_{\theta_{old}}(y_1)$ = P("В") * P("прямоугольном"|"В") * ... * P("."|... "катетов")
    *   $\pi_\theta(y_1)$ = P_new("В") * P_new("прямоугольном"|"В") * ...

    Предположим, после перемножения всех вероятностей мы получили:
    *   $\pi_{\theta_{old}}(y_1) = 0.00001$
    *   $\pi_\theta(y_1) = 0.00005$ (Новая модель в целом более уверена в этой хорошей последовательности)

2.  **Считаем отношение вероятностей:**
    $\text{ratio} = \pi_\theta(y_1) / \pi_{\theta_{old}}(y_1) = 0.00005 / 0.00001 = 5.0$

3.  **Нормализуем по длине (самый важный шаг!):**
    $s_1(\theta) = (5.0)^{1/12} \approx 1.14$

**Что здесь произошло?**
Без нормализации по длине ($^{1/|y_i|}$) отношение для длинных последовательностей могло бы стать астрономически большим или исчезающе малым, вызывая нестабильность. Нормализация (по сути, вычисление среднего геометрического) приводит коэффициент $s_i$ в разумный диапазон (часто около 1.0), независимо от длины ответа.

#### **Шаг 4: Обновление на уровне последовательности**

Теперь обновление становится элегантно простым. Для всей последовательности $y_1$ мы вычисляем ее вклад в функцию потерь:

$\text{loss\_contribution}_1 = \min(s_1(\theta) \cdot \hat{A}_1, \text{clip}(s_1(\theta)) \cdot \hat{A}_1)$

$\text{loss\_contribution}_1 = \min(1.14 \cdot 1.12, \text{clip}(1.14, 0.8, 1.2) \cdot 1.12)$

$\text{loss\_contribution}_1 = \min(1.2768, 1.14 \cdot 1.12) = 1.2768$ (поскольку 1.14 уже в пределах клиппинга)

Этот процесс повторяется для каждой из 4 последовательностей, используя ее **единый** коэффициент $s_i$ и ее преимущество $\hat{A}_i$.

#### **Итог GSPO:**

Модель получает единый, целостный сигнал обратной связи для всей последовательности. Если ответ $y_i$ был хорошим ($\hat{A}_i > 0$) и новая политика стала генерировать его с большей вероятностью ($s_i > 1$), то вся траектория генерации этого ответа поощряется. Это устраняет токен-уровневый шум и напрямую связывает награду за последовательность с оптимизацией всей последовательности. Это более стабильный и теоретически обоснованный подход.

</details>

## Принципиальные различия в механике оптимизации

Фундаментальное различие между методами заключается в **уровне применения importance sampling**. GRPO использует токен-уровневые отношения правдоподобия:

$$w_{i,t}^{GRPO}(\theta) = \frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\pi_{\theta_{old}}(y_{i,t}|x,y_{i,<t})}$$

что теоретически проблематично, поскольку единственная выборка на токен не может корректно выполнить распределительную коррекцию importance sampling. GSPO решает эту проблему через **теоретически обоснованное** применение importance sampling на уровне последовательности.

Различия в клиппинге также критичны: GRPO применяет клиппинг на каждом токене независимо, что может привести к неконсистентному поведению внутри последовательности. GSPO использует **единое клиппинг-значение для всей последовательности**, обеспечивая когерентные обновления политики.

Экспериментальные данные показывают парадоксальный результат: GSPO отсекает ~15% ответов против ~0.13% токенов в GRPO, но при этом демонстрирует более высокую эффективность обучения. Это подтверждает гипотезу о том, что **агрессивный клиппинг на правильном уровне** более эффективен, чем консервативный клиппинг на неподходящем уровне.

## Вычислительная сложность и архитектурные преимущества

Анализ вычислительной сложности выявляет значительные различия между методами. GRPO требует память порядка O(N·T), где N - размер батча, T - длина последовательности, плюс дополнительные 15-20% памяти для Routing Replay при работе с MoE моделями. GSPO достигает константной по длине сложности O(N), что драматически улучшает масштабируемость.

Для **Mixture-of-Experts архитектур** различия особенно критичны. GRPO страдает от проблемы volatility активации экспертов - токен-уровневые веса создают нестабильные паттерны маршрутизации, требуя специализированных решений типа Routing Replay. GSPO **нативно стабилизирует** MoE обучение через последовательность-уровневые веса, устраняя необходимость в дополнительных механизмах.

Экспериментальные результаты на Qwen3-30B-A3B-Base демонстрируют, что GSPO обеспечивает стабильное обучение MoE моделей без какой-либо дополнительной инфраструктуры, в то время как GRPO требует тщательной настройки и специализированных workaround'ов.

## Эмпирические результаты и практические применения

Практические результаты подтверждают теоретические преимущества обоих методов. **DeepSeekMath с GRPO** продемонстрировал прорывные результаты в математических рассуждениях: GSM8K улучшился с 82.9% до 88.2%, MATH с 46.8% до 51.7%. Особенно впечатляющим стало создание DeepSeek-R1-Zero - первой демонстрации развития рассуждающих способностей через чистое обучение с подкреплением без supervised fine-tuning, достигшей 71.0% на AIME 2024.

**Qwen3 с GSPO** показал еще более драматические улучшения в эффективности обучения - на 30-40% более быстрая сходимость при том же вычислительном бюджете. Критически важно, что GSPO демонстрирует **монотонное улучшение** при увеличении вычислительных ресурсов, в отличие от GRPO, который может страдать от нестабильности при длительном обучении.

Анализ стабильности обучения выявляет кардинальные различия: GRPO показывает ~15-20% случаев коллапса модели при длительном обучении, в то время как GSPO демонстрирует менее 2% случаев нестабильности. Это особенно критично для промышленных применений, где требуется гарантированная стабильность.

## Теоретические гарантии и свойства сходимости

С теоретической точки зрения, оба метода наследуют гарантии сходимости PPO при соблюдении соответствующих условий клиппинга. Однако **GSPO обладает более строгими теоретическими основаниями** благодаря корректному применению importance sampling. Групповая нормализация в обоих методах обеспечивает естественное снижение дисперсии - математически доказано, что дисперсия групповой оценки не превышает индивидуальную дисперсию, умноженную на (1 - 1/G).

GSPO дополнительно обеспечивает **свойства стабильности** через: длинно-нормализованные веса, предотвращающие накопление ошибок; клиппинг на уровне последовательности, исключающий чрезмерно off-policy выборки; равномерное взвешивание токенов, устраняющее конкуренцию за кредит.

Хотя формальный анализ сходимости для GSPO не представлен в оригинальной публикации, эмпирические результаты демонстрируют монотонное улучшение производительности и стабильное обучение без коллапса модели даже при работе с триллион-параметровыми MoE моделями.

## Современные развития и вариации методов

Активные исследования привели к появлению улучшенных вариантов обоих методов. **GRPO-LEAD** (2024) интегрирует length-dependent rewards и explicit negative penalties, улучшая краткость решений на 24-26%. **GRPO-CARE** (2025) добавляет consistency-aware reinforcement learning, повышая согласованность на 24.5%.

Для GSPO разработан **токен-уровневый вариант GSPO-token** для сценариев, требующих более тонкой настройки:

$$J_{GSPO\text{-}token}(\theta) = \mathbb{E}\left[\frac{1}{G} \sum_{i=1}^{G} \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} \min(s_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}(s_{i,t}(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_{i,t})\right]$$

где $s_{i,t}(\theta) = \text{sg}[s_i(\theta)] \cdot \frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\text{sg}[\pi_\theta(y_{i,t}|x,y_{i,<t})]}$ использует stop-gradient операции для стабильности.

**Гибридные подходы** исследуют комбинирование групповых методов с традиционными value-based подходами, обещая лучшее из обоих миров: стабильность групповых методов и быструю сходимость value-based обучения.

## Практические рекомендации и выбор метода

Для **практических применений** выбор между методами зависит от конкретных требований. GRPO рекомендуется для моделей менее 30B параметров со стандартными dense архитектурами, где проверенная стабильность и широкая экосистема поддержки являются приоритетом. Оптимальные гиперпараметры включают learning rate 1e-6, beta 0.04, epsilon 0.2, и размер группы 8-16.

GSPO становится предпочтительным выбором для **MoE моделей любого размера**, длинных последовательностей (>500 токенов), и сценариев, требующих максимальной стабильности для производственного использования. Рекомендуемые настройки включают более агрессивное клиппинг (epsilon 3e-4) и sequence-level оптимизацию.

**Инфраструктурные преимущества** GSPO включают толерантность к precision discrepancies, возможность использования likelihood от inference engine без пересчета в training engine, и упрощение архитектуры через отсутствие необходимости в Routing Replay.

## Заключение: парадигмальный сдвиг в обучении с подкреплением

Group Sequence Policy Optimization и Group Relative Policy Optimization представляют **фундаментальную эволюцию** в обучении с подкреплением для больших языковых моделей. GRPO заложил основы групповых методов, демонстрируя возможность достижения высокой производительности без критических сетей. GSPO развил эти идеи до логического завершения, решив проблемы стабильности через переход к последовательность-уровневой оптимизации.

Ключевой инсайт обоих методов заключается в **выравнивании уровня оптимизации с уровнем награждения** - поскольку награды назначаются целым последовательностям, оптимизация должна происходить на том же уровне. GSPO наиболее полно реализует этот принцип, достигая беспрецедентной стабильности и эффективности.

Практическое влияние этих методов выходит далеко за рамки академических исследований. Успешное внедрение в производственные системы - от DeepSeekMath до Qwen3 - демонстрирует готовность технологий к реальным применениям. **Экономическая эффективность** особенно впечатляет: 50% сокращение потребления памяти с GRPO и дополнительные 30-40% улучшения эффективности с GSPO открывают возможности для более широкого внедрения advanced RL методов.

Будущие исследования сосредоточены на расширении групповых методов на мультимодальные задачи, интеграции с методами поиска по дереву для test-time compute, и развитии теоретического понимания оптимальных размеров групп и стратегий адаптации. Групповые методы оптимизации политик установили новую парадигму для эффективного и стабильного обучения с подкреплением, которая будет определять развитие области в ближайшие годы.