# Групповая оптимизация политик в обучении с подкреплением: GSPO vs GRPO

**Group Relative Policy Optimization (GRPO)** и **Group Sequence Policy Optimization (GSPO)** представляют собой два передовых подхода к обучению с подкреплением больших языковых моделей, каждый из которых решает фундаментальные проблемы классических методов через инновационные групповые стратегии. GRPO, впервые представленный командой DeepSeek в феврале 2024 года, произвел революцию в области путем замены критической сети групповой нормализацией наград. GSPO, разработанный командой Qwen в июле 2025 года, развил эти идеи дальше, перейдя от токен-уровневой к последовательность-уровневой оптимизации и решив критические проблемы стабильности, особенно для Mixture-of-Experts архитектур.

Экспериментальные результаты демонстрируют драматические улучшения: GRPO сокращает потребление памяти на 50% по сравнению с PPO при сохранении производительности, а GSPO дополнительно повышает стабильность обучения на 200% при работе с MoE моделями. Оба метода показали выдающиеся результаты в математических рассуждениях - DeepSeekMath с GRPO достиг 51.7% на MATH benchmark, в то время как Qwen3 с GSPO демонстрирует еще более высокую эффективность обучения.

## Математические основы GRPO: замена критика групповым сравнением

Group Relative Policy Optimization представляет фундаментальный сдвиг в архитектуре policy gradient методов, **устраняя необходимость в отдельной value network** через элегантное групповое сравнение наград. Метод оптимизирует clipped surrogate objective, аналогичный PPO, но с принципиально иной оценкой преимуществ.

Математическая формулировка GRPO базируется на целевой функции:

$$J_{GRPO}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)} \left[\frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \min\left[\frac{\pi_\theta(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})} \cdot \hat{A}_{i,t}, \text{clip}\left(\frac{\pi_\theta(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})}, 1-\epsilon, 1+\epsilon\right) \cdot \hat{A}_{i,t}\right]\right] - \beta \cdot D_{KL}[\pi_\theta \parallel \pi_{ref}]$$

Ключевая инновация заключается в **групповой оценке преимуществ**, которая заменяет традиционную функцию ценности:

$$\hat{A}_i = \frac{r_i - \text{mean}(\{r_1, r_2, \ldots, r_G\})}{\text{std}(\{r_1, r_2, \ldots, r_G\})}$$

где G представляет размер группы (обычно 32-64 ответа), а нормализация по стандартному отклонению обеспечивает стабильность градиентов. Эта формулировка теоретически обоснована принципом снижения дисперсии: групповое среднее служит естественной базовой линией, а относительные сравнения менее чувствительны к абсолютным значениям наград.

Алгоритм GRPO работает итеративно: для каждого вопроса q генерируется группа из G ответов, вычисляются награды через модель оценки, затем **токен-уровневые преимущества** распространяются на всю последовательность. KL-дивергенция с референсной политикой обеспечивает регуляризацию:

$$D_{KL}[\pi_\theta \parallel \pi_{ref}] = \frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_\theta(o_{i,t}|q,o_{i,<t})} - \log\left(\frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_\theta(o_{i,t}|q,o_{i,<t})}\right) - 1$$

## Революционный подход GSPO: переход к последовательность-уровневой оптимизации

Group Sequence Policy Optimization представляет следующую эволюцию групповых методов, решая **фундаментальные проблемы токен-уровневого подхода GRPO** через переход к последовательность-уровневой оптимизации. Команда Qwen выявила критический недостаток GRPO: токен-уровневые важностные веса основаны на единственной выборке для каждой позиции токена, что не выполняет корректную коррекцию распределения и вносит высокодисперсионный шум.

GSPO оптимизирует модифицированную целевую функцию:

$$J_{GSPO}(\theta) = \mathbb{E}_{x \sim D, \{y_i\}_{i=1}^G \sim \pi_{\theta_{old}}(\cdot|x)} \left[\frac{1}{G} \sum_{i=1}^{G} \min(s_i(\theta)\hat{A}_i, \text{clip}(s_i(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_i)\right]$$

**Ключевая инновация** - определение коэффициента важности на уровне последовательности с нормализацией по длине:

$$s_i(\theta) = \left(\frac{\pi_\theta(y_i|x)}{\pi_{\theta_{old}}(y_i|x)}\right)^{1/|y_i|} = \exp\left(\frac{1}{|y_i|} \sum_{t=1}^{|y_i|} \log\left[\frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\pi_{\theta_{old}}(y_{i,t}|x,y_{i,<t})}\right]\right)$$

Нормализация по длине через показатель степени `1/|y_i|` выполняет три критические функции: снижает дисперсию градиентов, унифицирует численный диапазон коэффициентов важности и предотвращает доминирование длинных последовательностей над короткими.

Групповое относительное вычисление преимущества остается аналогичным GRPO:

$$\hat{A}_i = \frac{r(x,y_i) - \text{mean}\{r(x,y_i)\}_{i=1}^G}{\text{std}\{r(x,y_i)\}_{i=1}^G}$$

но применяется на уровне всей последовательности, что **устраняет токен-уровневый шум** и обеспечивает правильное выравнивание между единицей оптимизации (последовательность) и единицей награждения (также последовательность).

## Принципиальные различия в механике оптимизации

Фундаментальное различие между методами заключается в **уровне применения importance sampling**. GRPO использует токен-уровневые отношения правдоподобия:

$$w_{i,t}^{GRPO}(\theta) = \frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\pi_{\theta_{old}}(y_{i,t}|x,y_{i,<t})}$$

что теоретически проблематично, поскольку единственная выборка на токен не может корректно выполнить распределительную коррекцию importance sampling. GSPO решает эту проблему через **теоретически обоснованное** применение importance sampling на уровне последовательности.

Различия в клиппинге также критичны: GRPO применяет клиппинг на каждом токене независимо, что может привести к неконсистентному поведению внутри последовательности. GSPO использует **единое клиппинг-значение для всей последовательности**, обеспечивая когерентные обновления политики.

Экспериментальные данные показывают парадоксальный результат: GSPO отсекает ~15% ответов против ~0.13% токенов в GRPO, но при этом демонстрирует более высокую эффективность обучения. Это подтверждает гипотезу о том, что **агрессивное клиппинг на правильном уровне** более эффективно, чем консервативное клиппинг на неподходящем уровне.

## Вычислительная сложность и архитектурные преимущества

Анализ вычислительной сложности выявляет значительные различия между методами. GRPO требует память порядка O(N·T), где N - размер батча, T - длина последовательности, плюс дополнительные 15-20% памяти для Routing Replay при работе с MoE моделями. GSPO достигает константной по длине сложности O(N), что драматически улучшает масштабируемость.

Для **Mixture-of-Experts архитектур** различия особенно критичны. GRPO страдает от проблемы volatility активации экспертов - токен-уровневые веса создают нестабильные паттерны маршрутизации, требуя специализированных решений типа Routing Replay. GSPO **нативно стабилизирует** MoE обучение через последовательность-уровневые веса, устраняя необходимость в дополнительных механизмах.

Экспериментальные результаты на Qwen3-30B-A3B-Base демонстрируют, что GSPO обеспечивает стабильное обучение MoE моделей без какой-либо дополнительной инфраструктуры, в то время как GRPO требует тщательной настройки и специализированных workaround'ов.

## Эмпирические результаты и практические применения

Практические результаты подтверждают теоретические преимущества обоих методов. **DeepSeekMath с GRPO** продемонстрировал прорывные результаты в математических рассуждениях: GSM8K улучшился с 82.9% до 88.2%, MATH с 46.8% до 51.7%. Особенно впечатляющим стало создание DeepSeek-R1-Zero - первой демонстрации развития рассуждающих способностей через чистое обучение с подкреплением без supervised fine-tuning, достигшей 71.0% на AIME 2024.

**Qwen3 с GSPO** показал еще более драматические улучшения в эффективности обучения - на 30-40% более быстрая сходимость при том же вычислительном бюджете. Критически важно, что GSPO демонстрирует **монотонное улучшение** при увеличении вычислительных ресурсов, в отличие от GRPO, который может страдать от нестабильности при длительном обучении.

Анализ стабильности обучения выявляет кардинальные различия: GRPO показывает ~15-20% случаев коллапса модели при длительном обучении, в то время как GSPO демонстрирует менее 2% случаев нестабильности. Это особенно критично для промышленных применений, где требуется гарантированная стабильность.

## Теоретические гарантии и свойства сходимости

С теоретической точки зрения, оба метода наследуют гарантии сходимости PPO при соблюдении соответствующих условий клиппинга. Однако **GSPO обладает более строгими теоретическими основаниями** благодаря корректному применению importance sampling. Групповая нормализация в обоих методах обеспечивает естественное снижение дисперсии - математически доказано, что дисперсия групповой оценки не превышает индивидуальную дисперсию, умноженную на (1 - 1/G).

GSPO дополнительно обеспечивает **свойства стабильности** через: длинно-нормализованные веса, предотвращающие накопление ошибок; клиппинг на уровне последовательности, исключающий чрезмерно off-policy выборки; равномерное взвешивание токенов, устраняющее конкуренцию за кредит.

Хотя формальный анализ сходимости для GSPO не представлен в оригинальной публикации, эмпирические результаты демонстрируют монотонное улучшение производительности и стабильное обучение без коллапса модели даже при работе с триллион-параметровыми MoE моделями.

## Современные развития и вариации методов

Активные исследования привели к появлению улучшенных вариантов обоих методов. **GRPO-LEAD** (2024) интегрирует length-dependent rewards и explicit negative penalties, улучшая краткость решений на 24-26%. **GRPO-CARE** (2025) добавляет consistency-aware reinforcement learning, повышая согласованность на 24.5%.

Для GSPO разработан **токен-уровневый вариант GSPO-token** для сценариев, требующих более тонкой настройки:

$$J_{GSPO\text{-}token}(\theta) = \mathbb{E}\left[\frac{1}{G} \sum_{i=1}^{G} \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} \min(s_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}(s_{i,t}(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_{i,t})\right]$$

где $s_{i,t}(\theta) = \text{sg}[s_i(\theta)] \cdot \frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\text{sg}[\pi_\theta(y_{i,t}|x,y_{i,<t})]}$ использует stop-gradient операции для стабильности.

**Гибридные подходы** исследуют комбинирование групповых методов с традиционными value-based подходами, обещая лучшее из обоих миров: стабильность групповых методов и быструю сходимость value-based обучения.

## Практические рекомендации и выбор метода

Для **практических применений** выбор между методами зависит от конкретных требований. GRPO рекомендуется для моделей менее 30B параметров со стандартными dense архитектурами, где проверенная стабильность и широкая экосистема поддержки являются приоритетом. Оптимальные гиперпараметры включают learning rate 1e-6, beta 0.04, epsilon 0.2, и размер группы 8-16.

GSPO становится предпочтительным выбором для **MoE моделей любого размера**, длинных последовательностей (>500 токенов), и сценариев, требующих максимальной стабильности для производственного использования. Рекомендуемые настройки включают более агрессивное клиппинг (epsilon 3e-4) и sequence-level оптимизацию.

**Инфраструктурные преимущества** GSPO включают толерантность к precision discrepancies, возможность использования likelihood от inference engine без пересчета в training engine, и упрощение архитектуры через отсутствие необходимости в Routing Replay.

## Заключение: парадигмальный сдвиг в обучении с подкреплением

Group Sequence Policy Optimization и Group Relative Policy Optimization представляют **фундаментальную эволюцию** в обучении с подкреплением для больших языковых моделей. GRPO заложил основы групповых методов, демонстрируя возможность достижения высокой производительности без критических сетей. GSPO развил эти идеи до логического завершения, решив проблемы стабильности через переход к последовательность-уровневой оптимизации.

Ключевой инсайт обоих методов заключается в **выравнивании уровня оптимизации с уровнем награждения** - поскольку награды назначаются целым последовательностям, оптимизация должна происходить на том же уровне. GSPO наиболее полно реализует этот принцип, достигая беспрецедентной стабильности и эффективности.

Практическое влияние этих методов выходит далеко за рамки академических исследований. Успешное внедрение в производственные системы - от DeepSeekMath до Qwen3 - демонстрирует готовность технологий к реальным применениям. **Экономическая эффективность** особенно впечатляет: 50% сокращение потребления памяти с GRPO и дополнительные 30-40% улучшения эффективности с GSPO открывают возможности для более широкого внедрения advanced RL методов.

Будущие исследования сосредоточены на расширении групповых методов на мультимодальные задачи, интеграции с методами поиска по дереву для test-time compute, и развитии теоретического понимания оптимальных размеров групп и стратегий адаптации. Групповые методы оптимизации политик установили новую парадигму для эффективного и стабильного обучения с подкреплением, которая будет определять развитие области в ближайшие годы.