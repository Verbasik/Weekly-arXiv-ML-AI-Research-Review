# Групповая оптимизация политик в обучении с подкреплением: GSPO vs GRPO

**Group Relative Policy Optimization (GRPO)** и **Group Sequence Policy Optimization (GSPO)** представляют собой два передовых подхода к обучению с подкреплением больших языковых моделей, каждый из которых решает фундаментальные проблемы классических методов через инновационные групповые стратегии. GRPO, впервые представленный командой DeepSeek в феврале 2024 года, произвел революцию в области путем замены критической сети групповой нормализацией наград. GSPO, разработанный командой Qwen в июле 2025 года, развил эти идеи дальше, перейдя от токен-уровневой к последовательность-уровневой оптимизации и решив критические проблемы стабильности, особенно для Mixture-of-Experts архитектур.

Экспериментальные результаты демонстрируют драматические улучшения: GRPO сокращает потребление памяти на 50% по сравнению с PPO при сохранении производительности, а GSPO дополнительно повышает стабильность обучения на 200% при работе с MoE моделями. Оба метода показали выдающиеся результаты в математических рассуждениях - DeepSeekMath с GRPO достиг 51.7% на MATH benchmark, в то время как Qwen3 с GSPO демонстрирует еще более высокую эффективность обучения.

## Математические основы GRPO: замена критика групповым сравнением

Group Relative Policy Optimization представляет фундаментальный сдвиг в архитектуре policy gradient методов, **устраняя необходимость в отдельной value network** через элегантное групповое сравнение наград. Метод оптимизирует clipped surrogate objective, аналогичный PPO, но с принципиально иной оценкой преимуществ.

Математическая формулировка GRPO базируется на целевой функции:

$$J_{GRPO}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)} \left[\frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \min\left[\frac{\pi_\theta(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})} \cdot \hat{A}_{i,t}, \text{clip}\left(\frac{\pi_\theta(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})}, 1-\epsilon, 1+\epsilon\right) \cdot \hat{A}_{i,t}\right]\right] - \beta \cdot D_{KL}[\pi_\theta \parallel \pi_{ref}]$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$J_{GRPO}(\theta)$** — целевая функция (objective function), которую алгоритм максимизирует для оптимизации политики.
- **$\theta$** — параметры **текущей политики** (нейронной сети), которые обновляются в процессе обучения.
- **$\theta_{old}$** — параметры **старой политики**, зафиксированной на момент сбора данных (генерации ответов).
- **$q$** — входной промпт (query), для которого генерируются ответы.
- **$\mathbb{E}_{...}$** — математическое ожидание, взятое по промптам из набора данных и по группам ответов, сгенерированных старой политикой.
- **$G$** — размер группы, т.е. количество ответов, сгенерированных для одного промпта $q$.
- **$\{o_i\}_{i=1}^G$** — группа из $G$ ответов (outputs), сгенерированных на промпт $q$.
- **$o_{i,t}$** — $t$-й токен в $i$-м ответе.
- **$|o_i|$** — длина $i$-го ответа в токенах.
- **$\pi_\theta(o_{i,t}|q,o_{i,<t})$** — вероятность генерации токена $o_{i,t}$ **текущей политикой** $\pi_\theta$ при условии промпта $q$ и предыдущих токенов $o_{i,<t}$.
- **$\pi_{\theta_{old}}(...)$** — аналогичная вероятность, но для **старой политики** $\pi_{\theta_{old}}$.
- **$\frac{\pi_\theta}{\pi_{\theta_{old}}}$** — **коэффициент важности (importance weighting)** на уровне токена. Он корректирует градиенты, позволяя обучать новую политику на данных, собранных старой.
- **$\hat{A}_{i,t}$** — **преимущество (advantage)**, присвоенное $t$-му токену в $i$-м ответе. В GRPO это значение одинаково для всех токенов одной последовательности и равно $\hat{A}_i$.
- **$\text{clip}(...)$** — функция отсечения, которая ограничивает коэффициент важности диапазоном $[1-\epsilon, 1+\epsilon]$, предотвращая слишком большие обновления политики.
- **$\epsilon$** — гиперпараметр клиппинга (типично 0.1–0.2).
- **$\beta$** — коэффициент, регулирующий силу KL-регуляризации.
- **$D_{KL}[\pi_\theta \parallel \pi_{ref}]$** — KL-дивергенция между текущей и референсной политикой.

</details> 

---

Ключевая инновация заключается в **групповой оценке преимуществ**, которая заменяет традиционную функцию ценности:

$$\hat{A}_i = \frac{r_i - \text{mean}(\{r_1, r_2, \ldots, r_G\})}{\text{std}(\{r_1, r_2, \ldots, r_G\})}$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>
где:
- **$\hat{A}_i$** — нормализованное преимущество для $i$-го ответа в группе. Это значение показывает, насколько $i$-й ответ лучше или хуже "среднего" ответа в группе.
- **$r_i$** — награда (reward) для $i$-го ответа, полученная от внешней модели-оценщика (reward model).
- **$\{r_1, r_2, \ldots, r_G\}$** — множество наград для всех $G$ ответов в группе.
- **$\text{mean}(\{...\})$** — среднее арифметическое наград по всей группе. Это значение служит **динамической базовой линией (baseline)**, которая заменяет выходные данные сети критика (value network).
- **$\text{std}(\{...\})$** — стандартное отклонение наград по группе. Нормализация на эту величину стабилизирует градиенты и делает обучение менее чувствительным к масштабу наград.
</details> 

---

где G представляет размер группы (обычно 32-64 ответа), а нормализация по стандартному отклонению обеспечивает стабильность градиентов. Эта формулировка теоретически обоснована принципом снижения дисперсии: групповое среднее служит естественной базовой линией, а относительные сравнения менее чувствительны к абсолютным значениям наград.

Алгоритм GRPO работает итеративно: для каждого вопроса q генерируется группа из G ответов, вычисляются награды через модель оценки, затем **токен-уровневые преимущества** распространяются на всю последовательность. KL-дивергенция с референсной политикой обеспечивает регуляризацию:

$$
D_{KL}[\pi_\theta \parallel \pi_{ref}] = \mathbb{E} \left[ \log\frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_\theta(o_{i,t}|q,o_{i,<t})} \right]
$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$D_{KL}[\pi_\theta \parallel \pi_{ref}]$** — KL-дивергенция, которая измеряет, насколько сильно текущая политика $\pi_\theta$ отклонилась от **референсной политики** $\pi_{ref}$.
- **$\pi_{ref}$** — референсная модель (часто исходная SFT-модель), которая служит "якорем". Регуляризация не позволяет оптимизируемой модели слишком сильно отойти от изначального распределения, что помогает сохранить ее общие языковые способности и предотвратить "катастрофическое забывание".
</details> 

---

## Революционный подход GSPO: переход к последовательность-уровневой оптимизации

Group Sequence Policy Optimization представляет следующую эволюцию групповых методов, решая **фундаментальные проблемы токен-уровневого подхода GRPO** через переход к последовательность-уровневой оптимизации. Команда Qwen выявила критический недостаток GRPO: токен-уровневые важностные веса основаны на единственной выборке для каждой позиции токена, что не выполняет корректную коррекцию распределения и вносит высокодисперсионный шум.

GSPO оптимизирует модифицированную целевую функцию:

$$J_{GSPO}(\theta) = \mathbb{E}_{x \sim D, \{y_i\}_{i=1}^G \sim \pi_{\theta_{old}}(\cdot|x)} \left[\frac{1}{G} \sum_{i=1}^{G} \min(s_i(\theta)\hat{A}_i, \text{clip}(s_i(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_i)\right]$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$J_{GSPO}(\theta)$** — целевая функция GSPO, которая оптимизируется на уровне целых последовательностей.
- **$x$** — входной промпт (аналогично $q$ в GRPO).
- **$D$** — набор данных с промптами.
- **$\{y_i\}_{i=1}^G$** — группа из $G$ сгенерированных ответов (последовательностей).
- **$s_i(\theta)$** — **коэффициент важности на уровне последовательности (sequence-level importance weight)**. Это ключевое отличие от GRPO.
- **$\hat{A}_i$** — **преимущество на уровне последовательности**, вычисляемое так же, как в GRPO, но применяемое ко всей последовательности как единому целому.
- **$\min(...)$** и **$\text{clip}(...)$** — механизм отсечения, аналогичный PPO и GRPO, но применяемый к коэффициенту важности всей последовательности $s_i(\theta)$.
</details> 

---

**Ключевая инновация** - определение коэффициента важности на уровне последовательности с нормализацией по длине:

$$s_i(\theta) = \left(\frac{\pi_\theta(y_i|x)}{\pi_{\theta_{old}}(y_i|x)}\right)^{1/|y_i|} = \exp\left(\frac{1}{|y_i|} \sum_{t=1}^{|y_i|} \log\left[\frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\pi_{\theta_{old}}(y_{i,t}|x,y_{i,<t})}\right]\right)$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>
где:
- **$s_i(\theta)$** — отношение вероятностей генерации всей последовательности $y_i$ текущей и старой политиками.
- **$\pi_\theta(y_i|x)$** — вероятность генерации **всей последовательности** $y_i$ текущей политикой. Вычисляется как произведение вероятностей всех ее токенов.
- **$\pi_{\theta_{old}}(y_i|x)$** — аналогичная вероятность для старой политики.
- **$|y_i|$** — длина последовательности $y_i$ в токенах.
- **$(\cdot)^{1/|y_i|}$** — **нормализация по длине**. Это критически важный шаг, который, по сути, вычисляет геометрическое среднее токен-уровневых отношений. Он выполняет три функции:
    1.  **Снижает дисперсию**: предотвращает экспоненциальный рост или затухание значения $s_i(\theta)$ с увеличением длины последовательности.
    2.  **Унифицирует диапазон**: приводит коэффициенты важности для коротких и длинных последовательностей к сопоставимому числовому диапазону.
    3.  **Предотвращает доминирование**: не позволяет длинным последовательностям (с большим количеством множителей в произведении вероятностей) оказывать непропорционально большое влияние на градиент.
</details> 

---

Нормализация по длине через показатель степени `1/|y_i|` выполняет три критические функции: снижает дисперсию градиентов, унифицирует численный диапазон коэффициентов важности и предотвращает доминирование длинных последовательностей над короткими.

Групповое относительное вычисление преимущества остается аналогичным GRPO:

$$\hat{A}_i = \frac{r(x,y_i) - \text{mean}\{r(x,y_i)\}_{i=1}^G}{\text{std}\{r(x,y_i)\}_{i=1}^G}$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>
где:
- **$\hat{A}_i$** — нормализованное преимущество для $i$-й последовательности.
- **$r(x, y_i)$** — награда для ответа $y_i$ на промпт $x$.
- **$\text{mean}\{...\}$** и **$\text{std}\{...\}$** — среднее и стандартное отклонение наград по группе.
- **Ключевое отличие от GRPO**: хотя формула идентична, здесь преимущество $\hat{A}_i$ умножается на единый коэффициент важности $s_i(\theta)$ для всей последовательности. Это обеспечивает соответствие между единицей оптимизации (последовательность) и единицей награждения (также последовательность), что **устраняет токен-уровневый шум** и делает обучение более стабильным и теоретически обоснованным.
</details> 

---

Но применяется на уровне всей последовательности, что **устраняет токен-уровневый шум** и обеспечивает правильное выравнивание между единицей оптимизации (последовательность) и единицей награждения (также последовательность).

## Принципиальные различия в механике оптимизации

Фундаментальное различие между методами заключается в **уровне применения importance sampling**. GRPO использует токен-уровневые отношения правдоподобия:

$$w_{i,t}^{GRPO}(\theta) = \frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\pi_{\theta_{old}}(y_{i,t}|x,y_{i,<t})}$$

что теоретически проблематично, поскольку единственная выборка на токен не может корректно выполнить распределительную коррекцию importance sampling. GSPO решает эту проблему через **теоретически обоснованное** применение importance sampling на уровне последовательности.

Различия в клиппинге также критичны: GRPO применяет клиппинг на каждом токене независимо, что может привести к неконсистентному поведению внутри последовательности. GSPO использует **единое клиппинг-значение для всей последовательности**, обеспечивая когерентные обновления политики.

Экспериментальные данные показывают парадоксальный результат: GSPO отсекает ~15% ответов против ~0.13% токенов в GRPO, но при этом демонстрирует более высокую эффективность обучения. Это подтверждает гипотезу о том, что **агрессивное клиппинг на правильном уровне** более эффективно, чем консервативное клиппинг на неподходящем уровне.

## Вычислительная сложность и архитектурные преимущества

Анализ вычислительной сложности выявляет значительные различия между методами. GRPO требует память порядка O(N·T), где N - размер батча, T - длина последовательности, плюс дополнительные 15-20% памяти для Routing Replay при работе с MoE моделями. GSPO достигает константной по длине сложности O(N), что драматически улучшает масштабируемость.

Для **Mixture-of-Experts архитектур** различия особенно критичны. GRPO страдает от проблемы volatility активации экспертов - токен-уровневые веса создают нестабильные паттерны маршрутизации, требуя специализированных решений типа Routing Replay. GSPO **нативно стабилизирует** MoE обучение через последовательность-уровневые веса, устраняя необходимость в дополнительных механизмах.

Экспериментальные результаты на Qwen3-30B-A3B-Base демонстрируют, что GSPO обеспечивает стабильное обучение MoE моделей без какой-либо дополнительной инфраструктуры, в то время как GRPO требует тщательной настройки и специализированных workaround'ов.

## Эмпирические результаты и практические применения

Практические результаты подтверждают теоретические преимущества обоих методов. **DeepSeekMath с GRPO** продемонстрировал прорывные результаты в математических рассуждениях: GSM8K улучшился с 82.9% до 88.2%, MATH с 46.8% до 51.7%. Особенно впечатляющим стало создание DeepSeek-R1-Zero - первой демонстрации развития рассуждающих способностей через чистое обучение с подкреплением без supervised fine-tuning, достигшей 71.0% на AIME 2024.

**Qwen3 с GSPO** показал еще более драматические улучшения в эффективности обучения - на 30-40% более быстрая сходимость при том же вычислительном бюджете. Критически важно, что GSPO демонстрирует **монотонное улучшение** при увеличении вычислительных ресурсов, в отличие от GRPO, который может страдать от нестабильности при длительном обучении.

Анализ стабильности обучения выявляет кардинальные различия: GRPO показывает ~15-20% случаев коллапса модели при длительном обучении, в то время как GSPO демонстрирует менее 2% случаев нестабильности. Это особенно критично для промышленных применений, где требуется гарантированная стабильность.

## Теоретические гарантии и свойства сходимости

С теоретической точки зрения, оба метода наследуют гарантии сходимости PPO при соблюдении соответствующих условий клиппинга. Однако **GSPO обладает более строгими теоретическими основаниями** благодаря корректному применению importance sampling. Групповая нормализация в обоих методах обеспечивает естественное снижение дисперсии - математически доказано, что дисперсия групповой оценки не превышает индивидуальную дисперсию, умноженную на (1 - 1/G).

GSPO дополнительно обеспечивает **свойства стабильности** через: длинно-нормализованные веса, предотвращающие накопление ошибок; клиппинг на уровне последовательности, исключающий чрезмерно off-policy выборки; равномерное взвешивание токенов, устраняющее конкуренцию за кредит.

Хотя формальный анализ сходимости для GSPO не представлен в оригинальной публикации, эмпирические результаты демонстрируют монотонное улучшение производительности и стабильное обучение без коллапса модели даже при работе с триллион-параметровыми MoE моделями.

## Современные развития и вариации методов

Активные исследования привели к появлению улучшенных вариантов обоих методов. **GRPO-LEAD** (2024) интегрирует length-dependent rewards и explicit negative penalties, улучшая краткость решений на 24-26%. **GRPO-CARE** (2025) добавляет consistency-aware reinforcement learning, повышая согласованность на 24.5%.

Для GSPO разработан **токен-уровневый вариант GSPO-token** для сценариев, требующих более тонкой настройки:

$$J_{GSPO\text{-}token}(\theta) = \mathbb{E}\left[\frac{1}{G} \sum_{i=1}^{G} \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} \min(s_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}(s_{i,t}(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_{i,t})\right]$$

где $s_{i,t}(\theta) = \text{sg}[s_i(\theta)] \cdot \frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\text{sg}[\pi_\theta(y_{i,t}|x,y_{i,<t})]}$ использует stop-gradient операции для стабильности.

**Гибридные подходы** исследуют комбинирование групповых методов с традиционными value-based подходами, обещая лучшее из обоих миров: стабильность групповых методов и быструю сходимость value-based обучения.

## Практические рекомендации и выбор метода

Для **практических применений** выбор между методами зависит от конкретных требований. GRPO рекомендуется для моделей менее 30B параметров со стандартными dense архитектурами, где проверенная стабильность и широкая экосистема поддержки являются приоритетом. Оптимальные гиперпараметры включают learning rate 1e-6, beta 0.04, epsilon 0.2, и размер группы 8-16.

GSPO становится предпочтительным выбором для **MoE моделей любого размера**, длинных последовательностей (>500 токенов), и сценариев, требующих максимальной стабильности для производственного использования. Рекомендуемые настройки включают более агрессивное клиппинг (epsilon 3e-4) и sequence-level оптимизацию.

**Инфраструктурные преимущества** GSPO включают толерантность к precision discrepancies, возможность использования likelihood от inference engine без пересчета в training engine, и упрощение архитектуры через отсутствие необходимости в Routing Replay.

## Заключение: парадигмальный сдвиг в обучении с подкреплением

Group Sequence Policy Optimization и Group Relative Policy Optimization представляют **фундаментальную эволюцию** в обучении с подкреплением для больших языковых моделей. GRPO заложил основы групповых методов, демонстрируя возможность достижения высокой производительности без критических сетей. GSPO развил эти идеи до логического завершения, решив проблемы стабильности через переход к последовательность-уровневой оптимизации.

Ключевой инсайт обоих методов заключается в **выравнивании уровня оптимизации с уровнем награждения** - поскольку награды назначаются целым последовательностям, оптимизация должна происходить на том же уровне. GSPO наиболее полно реализует этот принцип, достигая беспрецедентной стабильности и эффективности.

Практическое влияние этих методов выходит далеко за рамки академических исследований. Успешное внедрение в производственные системы - от DeepSeekMath до Qwen3 - демонстрирует готовность технологий к реальным применениям. **Экономическая эффективность** особенно впечатляет: 50% сокращение потребления памяти с GRPO и дополнительные 30-40% улучшения эффективности с GSPO открывают возможности для более широкого внедрения advanced RL методов.

Будущие исследования сосредоточены на расширении групповых методов на мультимодальные задачи, интеграции с методами поиска по дереву для test-time compute, и развитии теоретического понимания оптимальных размеров групп и стратегий адаптации. Групповые методы оптимизации политик установили новую парадигму для эффективного и стабильного обучения с подкреплением, которая будет определять развитие области в ближайшие годы.