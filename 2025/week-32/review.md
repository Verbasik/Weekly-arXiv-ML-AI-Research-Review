# Group Policy Optimization in Reinforcement Learning: GSPO vs GRPO

**Group Relative Policy Optimization (GRPO)** and **Group Sequence Policy Optimization (GSPO)** are two advanced approaches to reinforcement learning of large language models, each addressing fundamental issues of classical methods through innovative group-based strategies. GRPO, first introduced by the DeepSeek team in February 2024, revolutionized the field by replacing the critic network with group-wise reward normalization. GSPO, developed by the Qwen team in July 2025, extended these ideas further by shifting from token-level to sequence-level optimization and resolving critical stability issues, particularly for Mixture-of-Experts architectures.

Experimental results demonstrate dramatic improvements: GRPO reduces memory consumption by 50% compared to PPO while maintaining performance, and GSPO further increases training stability by 200% when working with MoE models. Both methods have shown outstanding results in mathematical reasoning—DeepSeekMath with GRPO achieved 51.7% on the MATH benchmark, while Qwen3 with GSPO demonstrates even higher training efficiency.

## Mathematical Foundations of GRPO: Replacing the Critic with Group Comparison

Group Relative Policy Optimization represents a fundamental shift in policy gradient architectures, **eliminating the need for a separate value network** through an elegant group-wise reward comparison. The method optimizes a clipped surrogate objective similar to PPO, but with a fundamentally different advantage estimation.

The mathematical formulation of GRPO is based on the objective function:

$$J_{GRPO}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)} \left[\frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \min\left[\frac{\pi_\theta(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})} \cdot \hat{A}_{i,t}, \text{clip}\left(\frac{\pi_\theta(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})}, 1-\epsilon, 1+\epsilon\right) \cdot \hat{A}_{i,t}\right]\right] - \beta \cdot D_{KL}[\pi_\theta \parallel \pi_{ref}]$$

<details> 
    <summary><em><strong>variable explanations</strong></em></summary>

where:
- **$J_{GRPO}(\theta)$** — the objective function, which the algorithm maximizes to optimize the policy.
- **$\theta$** — parameters of the **current policy** (neural network), updated during training.
- **$\theta_{old}$** — parameters of the **old policy**, fixed at the time of data collection (response generation).
- **$q$** — the input prompt (query) for which responses are generated.
- **$\mathbb{E}_{...}$** — the mathematical expectation over prompts from the dataset and groups of responses generated by the old policy.
- **$G$** — group size, i.e., the number of responses generated for one prompt $q$.
- **$\{o_i\}_{i=1}^G$** — a group of $G$ responses (outputs) generated on prompt $q$.
- **$o_{i,t}$** — the $t$-th token in the $i$-th response.
- **$|o_i|$** — the length of the $i$-th response in tokens.
- **$\pi_\theta(o_{i,t}|q,o_{i,<t})$** — the probability of generating token $o_{i,t}$ under the **current policy** $\pi_\theta$, given prompt $q$ and previous tokens $o_{i,<t}$.
- **$\pi_{\theta_{old}}(...)$** — the analogous probability under the **old policy** $\pi_{\theta_{old}}$.
- **$\frac{\pi_\theta}{\pi_{\theta_{old}}}$** — the **importance weight** at the token level. It adjusts gradients, enabling the new policy to be trained on data collected by the old one.
- **$\hat{A}_{i,t}$** — the **advantage** assigned to the $t$-th token in the $i$-th response. In GRPO, this value is identical for all tokens of a single sequence and equals $\hat{A}_i$.
- **$\text{clip}(...)$** — the clipping function, which constrains the importance weight to the range $[1-\epsilon, 1+\epsilon]$, preventing overly large policy updates.
- **$\epsilon$** — the clipping hyperparameter (typically 0.1–0.2).
- **$\beta$** — coefficient regulating the strength of KL regularization.
- **$D_{KL}[\pi_\theta \parallel \pi_{ref}]$** — the KL divergence between the current and reference policy.

</details> 

---

The key innovation lies in the **group-wise advantage estimation**, which replaces the traditional value function:

$$\hat{A}_i = \frac{r_i - \text{mean}(\{r_1, r_2, \ldots, r_G\})}{\text{std}(\{r_1, r_2, \ldots, r_G\})}$$

<details> 
    <summary><em><strong>variable explanations</strong></em></summary>
where:
- **$\hat{A}_i$** — the normalized advantage for the $i$-th response in the group. This value indicates how much better or worse the $i$-th response is compared to the "average" response in the group.
- **$r_i$** — the reward for the $i$-th response, obtained from an external reward model.
- **$\{r_1, r_2, \ldots, r_G\}$** — the set of rewards for all $G$ responses in the group.
- **$\text{mean}(\{...\})$** — the arithmetic mean of rewards across the entire group. This serves as a **dynamic baseline**, replacing the output of the critic network.
- **$\text{std}(\{...\})$** — the standard deviation of rewards across the group. Normalization by this value stabilizes gradients and makes training less sensitive to reward scaling.
</details> 

---

where $G$ represents group size (typically 32–64 responses), and normalization by standard deviation ensures gradient stability. This formulation is theoretically grounded in variance reduction: the group mean serves as a natural baseline, and relative comparisons are less sensitive to absolute reward values.

The GRPO algorithm operates iteratively: for each query $q$, a group of $G$ responses is generated, rewards are computed via the reward model, and then **token-level advantages** are propagated across the entire sequence. KL divergence from the reference policy provides regularization:

$$
D_{KL}[\pi_\theta \parallel \pi_{ref}] = \mathbb{E} \left[ \log\frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_\theta(o_{i,t}|q,o_{i,<t})} \right]
$$

<details> 
    <summary><em><strong>variable explanations</strong></em></summary>

where:
- **$D_{KL}[\pi_\theta \parallel \pi_{ref}]$** — the KL divergence, measuring how much the current policy $\pi_\theta$ has deviated from the **reference policy** $\pi_{ref}$.
- **$\pi_{ref}$** — the reference model (often the original SFT model), serving as an "anchor". Regularization prevents the optimized model from straying too far from the initial distribution, helping preserve its general language capabilities and preventing "catastrophic forgetting".
</details>

---

Let us now break down, step by step, how GRPO and GSPO work on a concrete example. Imagine we are training a model to give concise and accurate answers to scientific questions.

### Example Scenario

*   **Prompt (Query):** `q = "Formulate the Pythagorean theorem in one sentence."`
*   **Group size (G):** For simplicity, let $G = 4$. This means the model generates four different responses for one prompt.
*   **Policies:**
    *   $\pi_{\theta_{old}}$ — the "old" version of the model used to generate responses.
    *   $\pi_{\theta}$ — the "new" version of the model being trained.
*   **Reward Model:** An external system that evaluates each response on a scale from 0 to 10, where 10 is perfect.

<details> 
    <summary><em><strong>example</strong></em></summary>

### Example #1: How GRPO Works "Under the Hood"

GRPO operates at the **token level**. It computes a single "advantage" for the entire sequence and applies it uniformly to each token individually, weighted by the probability of generating that token.

#### **Step 1: Generate Responses and Evaluate**

Our "old" model $\pi_{\theta_{old}}$ generates four responses to the prompt. The reward model assigns them rewards ($r_i$):

1.  $o_1$: "In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs."
    *   **Reward ($r_1$): 10.0** (Perfect answer)
2.  $o_2$: "The sum of the squares of two sides equals the square of the third."
    *   **Reward ($r_2$): 6.0** (Inaccurate—does not specify that the triangle is right-angled)
3.  $o_3$: "$a^2 + b^2 = c^2$"
    *   **Reward ($r_3$): 8.0** (Accurate but less complete than the first answer)
4.  $o_4$: "The theorem about triangles."
    *   **Reward ($r_4$): 1.0** (Very poor, uninformative answer)

#### **Step 2: Group Normalization (Computing Advantage $\hat{A}_i$)**

Now comes GRPO’s key innovation: instead of predicting the "value" of an answer using a separate neural network (critic), we compare each response to the group average.

*   **Formula:** $\hat{A}_i = (r_i - \text{mean}(r)) / \text{std}(r)$

1.  **Compute mean:**
    $\text{mean} = (10.0 + 6.0 + 8.0 + 1.0) / 4 = 25.0 / 4 = 6.25$
    *   *Interpretation:* The "average" response in this group has a quality of 6.25.

2.  **Compute standard deviation (std):**
    $\text{std} = \sqrt{((10-6.25)^2 + (6-6.25)^2 + (8-6.25)^2 + (1-6.25)^2)/4} = \sqrt{(14.06 + 0.06 + 3.06 + 27.56)/4} = \sqrt{11.185} \approx 3.34$
    *   *Interpretation:* This measures reward dispersion. A large value indicates responses vary greatly in quality.

3.  **Compute advantage ($\hat{A}_i$) for each response:**
    *   $\hat{A}_1 = (10.0 - 6.25) / 3.34 \approx +1.12$ (Significantly better than average)
    *   $\hat{A}_2 = (6.0 - 6.25) / 3.34 \approx -0.07$ (Slightly worse than average)
    *   $\hat{A}_3 = (8.0 - 6.25) / 3.34 \approx +0.52$ (Better than average)
    *   $\hat{A}_4 = (1.0 - 6.25) / 3.34 \approx -1.57$ (Significantly worse than average)

**Key point:** A positive advantage $\hat{A}$ encourages the model; a negative one penalizes it.

#### **Step 3: Token-Level Optimization**

Now the crucial part. GRPO "decomposes" each response into tokens and applies an update to each one individually. Let’s examine response $o_1$: "In", "a", "right", "triangle", "...", "legs", ".".

For **each token** $o_{1,t}$ in this sequence, we do the following:

1.  **Compute the importance weight:**

    $w_{1,t} = \pi_\theta(o_{1,t} | ...) / \pi_{\theta_{old}}(o_{1,t} | ...)$

    Suppose for the token "hypotenuse", the old model was uncertain and assigned a probability of 0.4, while the new, training model became more confident and assigned 0.6.

    $w_{\text{hypotenuse}} = 0.6 / 0.4 = 1.5$

2.  **Compute the contribution of this token to the overall loss function:**

    $\text{loss\_contribution} = \min(w_{1,t} \cdot \hat{A}_1, \text{clip}(w_{1,t}) \cdot \hat{A}_1)$

    Using our advantage $\hat{A}_1 = +1.12$ and weight $w = 1.5$:

    $\text{loss\_contribution} = \min(1.5 \cdot 1.12, \text{clip}(1.5, 0.8, 1.2) \cdot 1.12)$

    $\text{loss\_contribution} = \min(1.68, 1.2 \cdot 1.12) = \min(1.68, 1.344) = 1.344$

**What happened here?**
The algorithm observed that the new policy $\pi_\theta$ now generates the token "hypotenuse" with higher probability. Since the entire sequence was "good" ($\hat{A}_1 > 0$), this change is encouraged. Clipping ($\text{clip}$) prevents this encouragement from becoming too large, ensuring training stability.

This process repeats for **every token in all four responses**. Advantage $\hat{A}_1 = +1.12$ is identical for all tokens of response $o_1$, and $\hat{A}_4 = -1.57$ is identical for all tokens of response $o_4$.

#### **GRPO Summary:**

The model receives feedback signals at the individual token level. If the sequence as a whole was good ($\hat{A} > 0$), the probability of **every** token is increased (proportional to $w_t$). If it was bad ($\hat{A} < 0$), the probability of every token is decreased. It is simple, but as the Qwen team notes, theoretically somewhat flawed and potentially noisy.

</details>

---

## Revolutionary Approach: GSPO — Transitioning to Sequence-Level Optimization

Group Sequence Policy Optimization represents the next evolution of group-based methods, solving **fundamental problems of token-level GRPO** by shifting to sequence-level optimization. The Qwen team identified a critical flaw in GRPO: token-level importance weights are based on single samples per token position, failing to correctly correct the distribution and introducing high-variance noise.

GSPO optimizes a modified objective function:

$$J_{GSPO}(\theta) = \mathbb{E}_{x \sim D, \{y_i\}_{i=1}^G \sim \pi_{\theta_{old}}(\cdot|x)} \left[\frac{1}{G} \sum_{i=1}^{G} \min(s_i(\theta)\hat{A}_i, \text{clip}(s_i(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_i)\right]$$

<details> 
    <summary><em><strong>variable explanations</strong></em></summary>

where:
- **$J_{GSPO}(\theta)$** — the GSPO objective function, optimized at the level of entire sequences.
- **$x$** — the input prompt (analogous to $q$ in GRPO).
- **$D$** — the dataset of prompts.
- **$\{y_i\}_{i=1}^G$** — a group of $G$ generated responses (sequences).
- **$s_i(\theta)$** — the **sequence-level importance weight**. This is the key distinction from GRPO.
- **$\hat{A}_i$** — the **sequence-level advantage**, computed the same way as in GRPO but applied to the entire sequence as a single unit.
- **$\min(...)$** and **$\text{clip}(...)$** — the clipping mechanism, similar to PPO and GRPO, but applied to the entire sequence's importance coefficient $s_i(\theta)$.
</details> 

---

**The key innovation** is defining the sequence-level importance weight with length normalization:

$$s_i(\theta) = \left(\frac{\pi_\theta(y_i|x)}{\pi_{\theta_{old}}(y_i|x)}\right)^{1/|y_i|} = \exp\left(\frac{1}{|y_i|} \sum_{t=1}^{|y_i|} \log\left[\frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\pi_{\theta_{old}}(y_{i,t}|x,y_{i,<t})}\right]\right)$$

<details> 
    <summary><em><strong>variable explanations</strong></em></summary>
where:
- **$s_i(\theta)$** — the ratio of the probabilities of generating the entire sequence $y_i$ under the current and old policies.
- **$\pi_\theta(y_i|x)$** — the probability of generating the **entire sequence** $y_i$ under the current policy. Computed as the product of probabilities of all its tokens.
- **$\pi_{\theta_{old}}(y_i|x)$** — the analogous probability under the old policy.
- **$|y_i|$** — the length of sequence $y_i$ in tokens.
- **$(\cdot)^{1/|y_i|}$** — **length normalization**. This critical step effectively computes the geometric mean of token-level ratios. It performs three functions:
    1.  **Reduces variance**: Prevents exponential growth or decay of $s_i(\theta)$ with increasing sequence length.
    2.  **Unifies range**: Brings importance coefficients for short and long sequences to a comparable numerical range.
    3.  **Prevents domination**: Stops long sequences (with more multiplicative probability factors) from disproportionately influencing gradients.
</details> 

---

Length normalization via exponent $1/|y_i|$ performs three critical functions: it reduces gradient variance, unifies the numerical range of importance coefficients, and prevents long sequences from dominating short ones.

The group-wise relative advantage computation remains analogous to GRPO:

$$\hat{A}_i = \frac{r(x,y_i) - \text{mean}\{r(x,y_i)\}_{i=1}^G}{\text{std}\{r(x,y_i)\}_{i=1}^G}$$

<details> 
    <summary><em><strong>variable explanations</strong></em></summary>
where:
- **$\hat{A}_i$** — the normalized advantage for the $i$-th sequence.
- **$r(x, y_i)$** — the reward for response $y_i$ to prompt $x$.
- **$\text{mean}\{...\}$** and **$\text{std}\{...\}$** — mean and standard deviation of rewards across the group.
- **Key difference from GRPO**: Although the formula is identical, here advantage $\hat{A}_i$ is multiplied by a single sequence-level importance coefficient $s_i(\theta)$. This ensures alignment between the unit of optimization (sequence) and the unit of reward (also sequence), **eliminating token-level noise** and making training more stable and theoretically sound.
</details> 

---

But applied at the sequence level, this **eliminates token-level noise** and ensures proper alignment between the unit of optimization (sequence) and the unit of reward (also sequence).

<details> 
    <summary><em><strong>example</strong></em></summary>

### Example #2: How GSPO Works "Under the Hood"

GSPO resolves the primary flaw of GRPO by shifting to optimization at the level of **entire sequences**. This is more intuitive, since rewards are assigned to complete responses, not individual words.

#### **Steps 1 and 2: Generation, Evaluation, and Advantage Computation**

These steps are **identical** to GRPO. We use the same prompt, the same four responses, and obtain the exact same advantage values:
*   $\hat{A}_1 \approx +1.12$
*   $\hat{A}_2 \approx -0.07$
*   $\hat{A}_3 \approx +0.52$
*   $\hat{A}_4 \approx -1.57$

#### **Step 3: Key Difference — Computing Sequence-Level Importance Weight ($s_i(\theta)$)**

Instead of examining each token individually, GSPO computes a single importance weight for the entire sequence.

*   **Formula:** $s_i(\theta) = (\pi_\theta(y_i) / \pi_{\theta_{old}}(y_i))^{1/|y_i|}$

Let’s revisit our best response $o_1$ (called $y_1$ in GSPO), which consists of, say, 12 tokens ($|y_1| = 12$).

1.  **Compute the probability of the entire sequence:**
    *   $\pi_{\theta_{old}}(y_1)$ = P("In") * P("right"|"In") * ... * P("."|... "legs")
    *   $\pi_\theta(y_1)$ = P_new("In") * P_new("right"|"In") * ...

    Suppose after multiplying all token probabilities we obtain:
    *   $\pi_{\theta_{old}}(y_1) = 0.00001$
    *   $\pi_\theta(y_1) = 0.00005$ (The new model is overall more confident in this good sequence)

2.  **Compute the probability ratio:**
    $\text{ratio} = \pi_\theta(y_1) / \pi_{\theta_{old}}(y_1) = 0.00005 / 0.00001 = 5.0$

3.  **Normalize by length (the most critical step!):**
    $s_1(\theta) = (5.0)^{1/12} \approx 1.14$

**What happened here?**
Without length normalization ($^{1/|y_i|}$), the ratio for long sequences could become astronomically large or vanishingly small, causing instability. Normalization (essentially computing the geometric mean) brings the coefficient $s_i$ into a reasonable range (often near 1.0), regardless of response length.

#### **Step 4: Sequence-Level Update**

Now the update becomes elegantly simple. For the entire sequence $y_1$, we compute its contribution to the loss function:

$\text{loss\_contribution}_1 = \min(s_1(\theta) \cdot \hat{A}_1, \text{clip}(s_1(\theta)) \cdot \hat{A}_1)$

$\text{loss\_contribution}_1 = \min(1.14 \cdot 1.12, \text{clip}(1.14, 0.8, 1.2) \cdot 1.12)$

$\text{loss\_contribution}_1 = \min(1.2768, 1.14 \cdot 1.12) = 1.2768$ (since 1.14 is already within the clipping range)

This process repeats for each of the four sequences, using its **single** importance coefficient $s_i$ and its advantage $\hat{A}_i$.

#### **GSPO Summary:**

The model receives a unified, holistic feedback signal for the entire sequence. If response $y_i$ was good ($\hat{A}_i > 0$) and the new policy generates it with higher probability ($s_i > 1$), the entire generation trajectory is encouraged. This eliminates token-level noise and directly links sequence-level reward to sequence-level optimization. This is a more stable and theoretically grounded approach.

</details>

## Fundamental Differences in Optimization Mechanics

The core distinction between the methods lies in the **level at which importance sampling is applied**. GRPO employs token-level likelihood ratios:

$$w_{i,t}^{GRPO}(\theta) = \frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\pi_{\theta_{old}}(y_{i,t}|x,y_{i,<t})}$$

which is theoretically problematic, since a single sample per token cannot correctly perform distributional correction for importance sampling. GSPO resolves this through **theoretically sound** application of importance sampling at the sequence level.

Differences in clipping are also critical: GRPO applies clipping independently at each token, potentially causing inconsistent behavior within a sequence. GSPO uses a **single clipping value for the entire sequence**, ensuring coherent policy updates.

Experimental data reveals a paradoxical result: GSPO clips ~15% of responses versus ~0.13% of tokens in GRPO, yet demonstrates higher training efficiency. This confirms the hypothesis that **aggressive clipping at the correct level** is more effective than conservative clipping at an inappropriate level.

## Computational Complexity and Architectural Advantages

Analysis of computational complexity reveals significant differences between the methods. GRPO requires memory of order O(N·T), where N is batch size and T is sequence length, plus an additional 15–20% memory for Routing Replay when working with MoE models. GSPO achieves constant-in-length complexity O(N), dramatically improving scalability.

For **Mixture-of-Experts architectures**, the differences are especially critical. GRPO suffers from expert activation volatility—token-level weights create unstable routing patterns, requiring specialized solutions like Routing Replay. GSPO **natively stabilizes** MoE training through sequence-level weights, eliminating the need for additional mechanisms.

Experimental results on Qwen3-30B-A3B-Base demonstrate that GSPO enables stable MoE training without any additional infrastructure, whereas GRPO requires careful tuning and specialized workarounds.

## Empirical Results and Practical Applications

Practical results confirm the theoretical advantages of both methods. **DeepSeekMath with GRPO** achieved breakthrough performance in mathematical reasoning: GSM8K improved from 82.9% to 88.2%, and MATH from 46.8% to 51.7%. Particularly impressive was the creation of DeepSeek-R1-Zero—the first demonstration of developing reasoning capabilities through pure reinforcement learning without supervised fine-tuning, achieving 71.0% on AIME 2024.

**Qwen3 with GSPO** showed even stronger improvements in training efficiency—30–40% faster convergence at the same computational budget. Critically, GSPO demonstrates **monotonic improvement** with increased computational resources, unlike GRPO, which can suffer from instability during prolonged training.

Stability analysis reveals stark differences: GRPO exhibits ~15–20% cases of model collapse during prolonged training, while GSPO shows less than 2% instability. This is especially critical for production applications requiring guaranteed stability.

## Theoretical Guarantees and Convergence Properties

Theoretically, both methods inherit PPO convergence guarantees under appropriate clipping conditions. However, **GSPO possesses stronger theoretical foundations** due to its correct application of importance sampling. Group normalization in both methods ensures natural variance reduction—mathematically proven, the variance of group estimates does not exceed individual variance multiplied by (1 - 1/G).

GSPO further ensures **stability properties** through: length-normalized weights preventing error accumulation; sequence-level clipping excluding overly off-policy samples; and uniform token weighting eliminating credit competition.

Although a formal convergence analysis for GSPO is not presented in the original publication, empirical results demonstrate monotonic performance improvement and stable training without model collapse, even when training trillion-parameter MoE models.

## Modern Developments and Method Variants

Active research has led to improved variants of both methods. **GRPO-LEAD** (2024) integrates length-dependent rewards and explicit negative penalties, improving solution conciseness by 24–26%. **GRPO-CARE** (2025) adds consistency-aware reinforcement learning, increasing consistency by 24.5%.

For GSPO, a **token-level variant, GSPO-token**, has been developed for scenarios requiring finer control:

$$J_{GSPO\text{-}token}(\theta) = \mathbb{E}\left[\frac{1}{G} \sum_{i=1}^{G} \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} \min(s_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}(s_{i,t}(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_{i,t})\right]$$

where $s_{i,t}(\theta) = \text{sg}[s_i(\theta)] \cdot \frac{\pi_\theta(y_{i,t}|x,y_{i,<t})}{\text{sg}[\pi_\theta(y_{i,t}|x,y_{i,<t})]}$ uses stop-gradient operations for stability.

**Hybrid approaches** explore combining group methods with traditional value-based approaches, promising the best of both worlds: the stability of group methods and the fast convergence of value-based learning.

## Practical Recommendations and Method Selection

For **practical applications**, the choice between methods depends on specific requirements. GRPO is recommended for models under 30B parameters with standard dense architectures, where proven stability and broad ecosystem support are priorities. Optimal hyperparameters include learning rate 1e-6, beta 0.04, epsilon 0.2, and group size 8–16.

GSPO becomes the preferred choice for **MoE models of any size**, long sequences (>500 tokens), and scenarios requiring maximum stability for production use. Recommended settings include more aggressive clipping (epsilon 3e-4) and sequence-level optimization.

**Infrastructure advantages** of GSPO include tolerance to precision discrepancies, the ability to use likelihood from the inference engine without recomputation in the training engine, and simplified architecture through elimination of Routing Replay.

## Conclusion: A Paradigm Shift in Reinforcement Learning

Group Sequence Policy Optimization and Group Relative Policy Optimization represent a **fundamental evolution** in reinforcement learning for large language models. GRPO laid the groundwork for group-based methods, demonstrating high performance without critic networks. GSPO extended these ideas to their logical conclusion, solving stability issues through transition to sequence-level optimization.

The key insight of both methods is **aligning the level of optimization with the level of reward**—since rewards are assigned to entire sequences, optimization must occur at the same level. GSPO most fully realizes this principle, achieving unprecedented stability and efficiency.

The practical impact of these methods extends far beyond academic research. Successful deployment in production systems—from DeepSeekMath to Qwen3—demonstrates the readiness of these technologies for real-world applications. **Economic efficiency** is especially impressive: 50% memory reduction with GRPO and additional 30–40% efficiency gains with GSPO open possibilities for broader adoption of advanced RL methods.

Future research focuses on extending group methods to multimodal tasks, integrating with tree-search methods for test-time compute, and developing theoretical understanding of optimal group sizes and adaptive strategies. Group policy optimization methods have established a new paradigm for efficient and stable reinforcement learning that will define the field’s development in the coming years.