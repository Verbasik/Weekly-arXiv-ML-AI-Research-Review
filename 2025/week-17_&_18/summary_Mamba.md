# **Mamba: моделирование линейно-временной последовательности с использованием пространств выборочных состояний**

## **Оглавление**
1. [Введение](#введение)
2. [Понимание традиционного моделирования последовательностей](#понимание-традиционного-моделирования-последовательностей)
3. [Архитектура Мамбы](#архитектура-мамбы)
4. [Механизм отбора](#механизм-отбора)
5. [Реализация с учетом аппаратного обеспечения](#реализация-с-учетом-аппаратного-обеспечения)
6. [Анализ производительности](#анализ-производительности)
7. [Универсальность в разных областях](#универсальность-в-разных-областях)
8. [Масштабирование свойств](#масштабирование-свойств)
9. [Эффективность вывода](#эффективность-вывода)
10. [Ограничения и будущая работа](#ограничения-и-будущая-работа)
11. [Заключение](#заключение)

## **Введение**

Моделирование последовательностей стало краеугольным камнем современного глубокого обучения, обеспечив прорывы в обработке естественного языка, компьютернго зрении, обработке звука и геномике. Доминирующей архитектурой для этих задач был **Transformer**, который использует механизмы внутреннего внимания для моделирования отношений между элементами в последовательности. Однако **Transformers** сталкиваются со значительным ограничением: их вычислительная сложность масштабируется квадратично с длиной последовательности, что делает их неэффективными для обработки длинных последовательностей.

В статье «**Мамба: моделирование линейно-временной последовательности с использованием пространств выборочных состояний**» авторы **Альберт Гу** (Университет Карнеги-Меллона) и **Три Дао** (Принстонский университет) представляют новую архитектуру, которая устраняет это ограничение, сохраняя при этом мощные возможности моделирования **Transformers**.

![Модель селективного пространства состояний с аппаратно-зависимым расширением состояний](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_01.png)  
*Рисунок 1: Архитектура модели пространства селективных состояний с расширением состояния, учитывающим аппаратные средства. Входные данные влияют на параметры модели через механизм выбора, что позволяет делать обоснованные рассуждения на основе содержания.*

**Mamba** представляет собой фундаментальный сдвиг в моделировании последовательностей, объединяя свойства линейного масштабирования моделей пространства состояний (**SSM**) с новым механизмом выбора, который позволяет делать обоснованные рассуждения на основе содержания — способность, ранее присущую только механизмам внимания. Эта разработка имеет глубокие последствия для эффективности и возможностей систем ИИ, обрабатывающих последовательные данные.

## **Понимание традиционного моделирования последовательностей**

Чтобы оценить новаторство **Mamba**, важно понимать эволюцию подходов моделирования последовательностей и их ограничения.

**Трансформеры** произвели революцию в моделировании последовательностей с их механизмом самовнимания, который создает прямые связи между всеми позициями в последовательности. Это обеспечивает превосходную мощность моделирования, но достигается ценой квадратичной вычислительной сложности относительно длины последовательности — проблемы, известной как «узкое место внимания».

Для устранения этого узкого места было разработано несколько подходов:

- **Линейное внимание**: приближения внутреннего внимания с линейной сложностью.
- **Сверточные алгоритмы**: расширенные сверточные модели с механизмами стробирования.
- **Рекуррентные нейронные сети (RNN)**: последовательная обработка со скрытыми обновлениями состояния.
- **Модели структурированного пространства состояний (SSM)**: непрерывные системы, дискретизированные для моделирования последовательностей.

Хотя эти альтернативы обеспечивают линейное масштабирование с длиной последовательности, им, как правило, не хватает возможностей содержательного рассуждения, присущих вниманию, что существенно влияет на их эффективность в задачах языкового моделирования.

**State Space Models (SSM)** в частности показали многообещающие результаты, эффективно моделируя долгосрочные зависимости, но традиционные **SSM** используют фиксированные параметры независимо от входного содержимого. Эта инвариантность во времени ограничивает их способность выполнять такие задачи, как выборочное копирование и индукционные головки — фундаментальные операции для понимания языка.

![Сравнение задач копирования](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_02.png)
*Рисунок 2: Сравнение задач копирования. Слева: Стандартная задача копирования, решаемая с помощью моделей, не зависящих от времени. Справа: Выборочное копирование и индукционные головки требуют содержательного обоснования.*

## **Архитектура Мамбы**

**Mamba** вводит концепцию «выборочных моделей пространства состояний» (выборочных SSM), которые позволяют параметрам модели быть функциями входных данных. Это позволяет модели динамически решать, какую информацию хранить и распространять на основе содержимого текущих входных данных.

Основная инновация заключается в том, что параметры SSM (**A**, **B**, **C** и **D**) становятся функциями, зависящими от входных данных, а не фиксированными значениями. Это достигается с помощью механизма выбора, который проецирует входные данные для определения значений параметров на каждом шаге.

### **Непрерывный во времени SSM**

Линейная непрерывная SSM описывается следующими дифференциальными уравнениями:

$$
\begin{align}
h'(t) &= Ah(t) + Bx(t) \\
y(t) &= Ch(t) + Dx(t)
\end{align}
$$

Где:
- $h'(t)$ — производная скрытого состояния по времени
- $x(t)$ — входной сигнал в момент времени $t$
- $h(t)$ — скрытое состояние в момент времени $t$
- $y(t)$ — выходной сигнал в момент времени $t$

Уравнение состояния с помощью матриц A и B описывает, как состояние изменяется под влиянием входных данных.

<div align="center">
  <img src="https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/SSM/Image_02.png" alt="Визуализация уравнения состояния">
</div>

Уравнение выхода описывает, как состояние переводится в выход (через матрицу C) и как вход влияет на выход (через матрицу D).

<div align="center">
  <img src="https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/SSM/Image_03.png" alt="Визуализация уравнения выхода">
</div>

> Примечание: Матрицы A, B, C и D являются обучаемыми параметрами.

**Интерпретация компонентов:**

- **Матрица $A$** определяет собственную динамику системы. Её собственные значения указывают на устойчивость системы:
  - Отрицательные действительные части собственных значений → устойчивая система
  - Положительные действительные части → неустойчивая система
  - Мнимые части → колебательное поведение

- **Матрица $B$** определяет, как входной сигнал влияет на изменение скрытого состояния.

- **Матрица $C$** определяет, как скрытое состояние влияет на выходной сигнал.

- **Матрица $D$** (если используется) позволяет входному сигналу напрямую влиять на выходной сигнал.

<div align="center">
  <img src="https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/SSM/Image_05.png" alt="Итоговая схема работы SSM">
</div>

Таким образом, вся система работает так:

- Входной сигнал сначала умножается на матрицу B, которая описывает, как входные сигналы влияют на систему;

- Происходит обновление скрытого состояния. Мы умножаем состояние на матрицу A, которая описывает, как связаны все внутренние состояния. Матрица A применяется перед созданием представлений состояний и обновляется после того, как представление было обновлено;

- Затем, мы используем матрицу C, чтобы описать перевод в выходной сигнал;

- Матрица D — это Skip Connection, который используется, для борьбы с затуханием градиентов внутри сети.

**В селективной формулировке SSM эти параметры становятся зависимыми от входных данных:**

$$
\begin{align}
h'(t) &= A(x)h(t) + B(x)x(t) \\
y(t) &= C(x)h(t) + D(x)x(t)
\end{align}
$$

Где $A(x)$, $B(x)$, $C(x)$ и $D(x)$ — функции от входа $x(t)$, обычно реализуемые через нейронные сети.

**Общее количество параметров в стандартном SSM слое:**
- Матрица $A$: $d_h \times d_h$ (или $d_h$ для диагональной параметризации)
- Матрица $B$: $d_h \times d_x$
- Матрица $C$: $d_y \times d_h$
- Матрица $D$ (если используется): $d_y \times d_x$
- Итого: $d_h \times d_h + d_h \times d_x + d_y \times d_h + d_y \times d_x$ параметров

В селективных SSM количество параметров увеличивается за счет дополнительных проекционных слоев, которые генерируют параметры в зависимости от входных данных.

### **Проекционные слои для динамической генерации параметров**

Чтобы параметры SSM-слоя (матрицы $A$, $B$, $C$ и $D$) стали функциями от входных данных $x(t)$, используют **проекционные слои**. По сути, это небольшие нейронные сети (иногда их называют гиперсетями или hypernetworks), которые на каждом шаге «смотрят» на текущий вход и выдают новые значения параметров.  

1. **Идея проекционного слоя**  
   Вместо того чтобы хранить фиксированную матрицу $A$, мы учим дополнительную сеть $f_A$, которая по входному вектору $x$ выдаёт «развёрнутый» (flattened) вектор параметров $\theta_A$. Затем этот вектор приводят к форме матрицы того же размера, что и $A$. Аналогично действуют сети $f_B$, $f_C$, $f_D$.  
   $$
     \theta_A = f_A(x), \quad A(x) = \mathrm{reshape}(\theta_A)
   $$

2. **Структура одного проекционного слоя**  
   Чаще всего $f_A$ — это одно- или двухслойный MLP (полносвязная сеть):  
   $$
   \begin{aligned}
   z_1 &= W_1 x + b_1,\\
   a_1 &= \sigma(z_1),\\
   \theta_A &= W_2 a_1 + b_2,
   \end{aligned}
   $$
   где  
   - $W_1, b_1$ — параметры первого слоя (размерности $d_{\text{прок}}\times d_x$ и $d_{\text{прок}}$),  
   - $W_2, b_2$ — параметры выходного слоя ($d_h^2\times d_{\text{прок}}$ и $d_h^2$),  
   - $\sigma$ — нелинейность (ReLU, GELU и т.д.),  
   - $d_{\text{прок}}$ — «скрытая» размерность проекционного слоя.  

   После этого вектор $\theta_A\in\mathbb{R}^{d_h^2}$ растягивают до матрицы $A(x)\in\mathbb{R}^{d_h\times d_h}$.

3. **Преимущества и накладные расходы**  
   - **Гибкость.** Сеть $f_A$ «учится» выдавать разные динамики системы в зависимости от содержания $x$.  
   - **Локальная адаптация.** Модель может сразу реагировать на новые события в входе, изменяя свою внутреннюю механику.  
   - **Накладные расходы.** Вместо одного набора параметров мы храним параметры гиперсети:  
     $$
       \underbrace{d_{\text{прок}}\cdot d_x + d_{\text{прок}}}_{\text{первый слой}}
       \;+\;
       \underbrace{d_h^2\cdot d_{\text{прок}} + d_h^2}_{\text{второй слой}}
     $$
     Но зачастую $d_{\text{прок}}\ll d_h^2$, так что рост числа параметров остаётся умеренным.

4. **Пример на практике**  
   Пусть $d_x = 128$, $d_h = 64$, а мы выбрали $d_{\text{прок}} = 32$.  
   - Первый слой гиперсети: $32\times128 + 32 = 4128$ параметров.  
   - Второй слой: $64^2\times32 + 64^2 = 131\,072 + 4096 = 135\,168$.  
   - Всего ≈139 296 параметров вместо фиксированного $A$ размером $64^2 = 4096$.

<u>Таким образом, проекционные слои (гиперсети) превращают входной сигнал $x(t)$ в настройки внутренних параметров SSM-слоя, обеспечивая динамическую, содержательно-зависимую обработку последовательностей. Это и есть ключ к объединению линейного масштабирования SSM с «разумным» рассуждением, присущим attention-механизмам.</u>

### **Дискретизированная версия**

Дискретизированная версия, используемая в **Mamba**, выражается следующим образом:

![Переход от непрерывной SSM к дискретной. Теперь мы подаём на вход дискретные значения и получаем дискретный выход.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/SSM/Image_06.png)

$$
h_t = \bar A_t\,h_{t-1} + \bar B_t\,x_t
$$

$$
y_t = C_t\,h_t + D_t\,x_t
$$

Где параметры $\bar A_t$, $\bar B_t$, $C_t$ и $D_t$ на каждом шаге вычисляются проекционными слоями (гиперсетями) на основе текущего входного сигнала $x_t$.

### Интеграция в модель

Архитектура **Mamba** интегрирует эти селективные SSM в оптимизированную модельную структуру, которая является удивительно простой по сравнению с **Transformers**. Модель состоит из чередующихся слоев селективных SSM и простых проекций, не требуя отдельного внимания или блоков MLP.

![Архитектурная эволюция до Мамбы](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_03.png)
*Рисунок 3: Сравнение архитектур, показывающее эволюцию от H3 (вариант SSM) до Mamba через закрытые блоки MLP.*

## **Механизм отбора**

Механизм выбора — это то, что позволяет Mamba выполнять основанные на содержании рассуждения, подобные вниманию, сохраняя при этом линейную сложность. Он берет текущий входной токен x_t и вычисляет параметры для слоя SSM, которые определяют, как информация проходит через модель.

Этот механизм позволяет Mamba решать фундаментальные задачи, с которыми не справляются другие модели линейной сложности:

1. **Выборочное копирование:** возможность выборочного извлечения и копирования определенной информации из входной последовательности.
2. **Головы индукции:** способность определять закономерности и экстраполировать их на новые контексты

Эмпирическая проверка этих возможностей демонстрируется с помощью синтетических задач, где Mamba значительно превосходит другие модели. Самое поразительное, что в то время как другие модели испытывают трудности с экстраполяцией на более длинные последовательности, Mamba сохраняет идеальную точность даже при масштабировании до последовательностей в миллион раз длиннее, чем те, которые наблюдались во время обучения.

![Экстраполяционная производительность в задаче индукционных головок](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_04.png)
*Рисунок 4: Экстраполяционная производительность в задаче индукционных головок. Mamba сохраняет идеальную точность в последовательностях до 1 миллиона токенов, в то время как другие модели скатываются к случайной производительности.*

## **Реализация с учетом аппаратного обеспечения**

Одной из ключевых проблем при внедрении селективных SSM является вычислительная эффективность. Механизм выбора не позволяет использовать быстрые алгоритмы на основе свертки, обычно используемые для стандартных SSM, поскольку параметры изменяются на каждом временном шаге.

Авторы решают эту проблему с помощью аппаратно-ориентированного алгоритма параллельного сканирования, который эффективно реализует рекуррентные вычисления. Алгоритм тщательно управляет использованием памяти, сохраняя расширенное состояние в быстрой памяти графического процессора (SRAM), а не в более медленной памяти с высокой пропускной способностью (HBM), что значительно снижает затраты на доступ к памяти.

Полученная реализация значительно превосходит существующие подходы:

![Сравнение времени вычислений для различных операций с последовательностями](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_05.png)
*Рисунок 5: Сравнение времени вычислений для различных операций с последовательностями. Операция сканирования Mamba (красная линия) масштабируется линейно с длиной последовательности и превосходит даже оптимизированные реализации внимания, такие как FlashAttention-2 для более длинных последовательностей.*

Эта аппаратно-ориентированная реализация имеет решающее значение для практической жизнеспособности Mamba, поскольку она позволяет модели эффективно обрабатывать последовательности, несмотря на более сложную вычислительную схему, вводимую механизмом выбора.

## **Анализ производительности**

Mamba демонстрирует исключительную производительность в различных тестах и ​​задачах:

### **Моделирование языка**

В задачах моделирования языка Mamba достигает показателей перплексии, сопоставимых или превосходящих Transformers аналогичного размера. Примечательно, что Mamba-3B превосходит Transformers того же размера и даже достигает Transformers, которые вдвое больше его, в некоторых тестах.

![Законы масштабирования для различных архитектур](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_06.png)
*Законы масштабирования при моделировании языка, демонстрирующие высокую производительность Mamba по сравнению с различными архитектурами Transformer и альтернативными архитектурами.*

Модель демонстрирует постоянное улучшение по мере масштабирования, следуя тем же законам масштабирования, что и Transformers, но с более высокой эффективностью параметров.

### **Обработка звука**

Mamba достигает передовых результатов в задачах моделирования аудиосигналов, превосходя специализированные аудиомодели:

![Сравнение производительности моделирования аудиосигнала при различных параметризациях модели](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_07.png)
*Сравнение производительности моделирования аудиосигнала при различных параметризациях модели. Mamba демонстрирует последовательные улучшения по мере увеличения длины последовательности.*

Избирательный механизм оказывается особенно ценным для аудио, где модель должна фокусироваться на различных частотных компонентах в разное время.

### **Геномика**

Что касается моделирования геномной последовательности, Mamba демонстрирует высокие результаты как в задачах моделирования языка ДНК, так и в задачах классификации видов:

![Законы масштабирования при моделировании последовательности генома человека](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_08.png)
*Законы масштабирования при моделировании последовательности генома человека. Mamba превосходит другие архитектуры по мере увеличения числа параметров.*

Особенно впечатляет способность Mamba повышать производительность с увеличением длины последовательности, что позволяет ей использовать долгосрочные зависимости, присутствующие в геномных данных.

## **Универсальность в разных областях**

Одной из самых убедительных особенностей Mamba является ее универсальность в различных модальностях данных. В то время как многие архитектуры специализируются на определенных доменах, Mamba достигает высокой производительности в области языка, аудио и геномики без доменно-специфических модификаций.

Эта универсальность предполагает, что Mamba охватывает фундаментальные возможности моделирования последовательностей, которые выходят за рамки конкретных типов данных. Избирательный механизм, по-видимому, обеспечивает универсальный инструмент для идентификации и распространения соответствующей информации независимо от характера последовательности.

Особенно примечательна способность модели обрабатывать как дискретные (язык, ДНК), так и непрерывные (аудио) последовательности с использованием одной и той же архитектуры, поскольку для них обычно требуются разные подходы к моделированию.

## **Масштабирование свойств**

Mamba демонстрирует благоприятные масштабные свойства в нескольких измерениях:

### **Масштабирование длины последовательности**

В отличие от «Трансформеров», которые становятся непомерно дорогими из-за длинных сцен, эффективность «Мамбы» продолжает улучшаться с увеличением продолжительности контекста:

![Законы масштабирования относительно длины последовательности](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_09.png)
*Законы масштабирования относительно длины последовательности в данных YouTubeMix. Mamba последовательно превосходит S4+FFN по мере увеличения длины последовательности.*

Это свойство имеет решающее значение для таких приложений, как геномика, понимание документов и обработка аудио, где соответствующая информация может быть распределена по очень длинным последовательностям.

### **Масштабирование размера модели**

Производительность Mamba плавно масштабируется с размером модели, следуя предсказуемым законам масштабирования:

![Масштабирование сложности с вычислительными показателями FLOP для различных вариантов Mamba](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_10.png)
*Масштабирование сложности с вычислительными показателями FLOP для различных вариантов Mamba, демонстрирующее последовательное улучшение по мере увеличения вычислительной мощности.*

Различные варианты (стандартная Mamba, Mamba-MLP и Mamba-MHA) демонстрируют схожие тенденции масштабирования, указывая на то, что производительность модели определяется основным селективным механизмом SSM, а не конкретными архитектурными решениями.

### **Эффективность вывода**

Помимо эффективности обучения, Mamba демонстрирует замечательные преимущества в скорости вывода:

![Сравнение пропускной способности вывода](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_11.png)
*Сравнение пропускной способности вывода между моделями Mamba и Transformer. Mamba достигает значительно более высокой пропускной способности для всех размеров партий.*

На стандартном оборудовании (A100 GPU) модели Mamba достигают примерно в 5 раз более высокой пропускной способности, чем Transformers сопоставимого размера. Эта эффективность напрямую трансформируется в практические преимущества для развертывания, делая Mamba особенно привлекательной для приложений реального времени и крупномасштабных сценариев вывода.

Линейное масштабирование с длиной последовательности означает, что по мере увеличения контекстных окон преимущество Мамбы над Трансформерами становится еще более выраженным.

## **Ограничения и будущая работа**

Несмотря на впечатляющие результаты, Mamba имеет ряд ограничений и областей для будущего изучения:

- **Двунаправленность:** текущая архитектура Mamba в первую очередь предназначена для каузального (слева направо) моделирования. Расширение ее до двунаправленных или конфигураций кодер-декодер расширило бы ее применимость.

- **Интерпретируемость:** селективный механизм повышает производительность, но усложняет внутреннюю динамику модели, потенциально затрудняя ее интерпретацию по сравнению со стандартными SSM.

- **Мультимодальная интеграция:** хотя Mamba хорошо работает в различных модальностях независимо друг от друга, ее применение для мультимодальных задач, объединяющих различные типы данных, еще предстоит изучить.

- **Эффективность параметров:** хотя Mamba является вычислительно эффективной, могут существовать возможности для дальнейшего повышения эффективности параметров с помощью таких методов, как распределение веса или структурированная параметризация.

Авторы предполагают, что в будущих работах можно будет изучить более сложные механизмы отбора, альтернативные параметризации модели пространства состояний и приложения к конкретным областям, таким как обучение с подкреплением и распознавание речи.

## **Заключение**

Mamba представляет собой значительный прогресс в моделировании последовательностей, объединяя свойства линейного масштабирования моделей пространства состояний с возможностями рассуждений на основе контента, ранее эксклюзивными для механизмов внимания. Эта комбинация устраняет фундаментальное ограничение в существующих подходах к моделированию последовательностей и открывает новые возможности для эффективных, высокопроизводительных моделей.

Высокая производительность архитектуры в различных областях предполагает, что ее селективный механизм охватывает универсальные возможности моделирования последовательностей. Ее линейное масштабирование с длиной последовательности делает ее особенно перспективной для приложений, включающих длинные контексты, где Transformers становятся вычислительно невыгодными.

Возможно, самое важное, что Mamba предлагает оптимизированный, унифицированный подход к моделированию последовательностей, который снижает потребность в архитектурах, специфичных для домена. Эта простота в сочетании с ее вычислительной эффективностью позиционирует Mamba как многообещающую основу для следующего поколения языковых, аудио, геномных и других систем ИИ на основе последовательностей.

Поскольку глубокое обучение продолжает развиваться, инновационный подход Mamba к достижению баланса между вычислительной эффективностью и мощностью моделирования может оказаться решающим фактором в поиске более эффективного и ресурсоэффективного ИИ.
