# Nemotron-H: Семейство точных и эффективных гибридных моделей Mamba-Transformer  

## Содержание  
1. [Введение](#введение)  
2. [Дизайн гибридной архитектуры](#дизайн-гибридной-архитектуры)  
3. [Методология обучения](#методология-обучения)  
4. [Сжатие модели с помощью MiniPuzzle](#сжатие-модели-с-помощью-minipuzzle)  
5. [Повышение производительности и эффективности](#повышение-производительности-и-эффективности)  
6. [Применение и универсальность](#применение-и-универсальность)  
7. [Заключение](#заключение)  

## **1. Введение**
Большие языковые модели (LLM) продемонстрировали замечательные возможности в различных задачах, но их вычислительные потребности во время логического вывода остаются серьезной проблемой, особенно для обработки длинных последовательностей. Семейство моделей Nemotron-H решает это ограничение, представляя гибридную архитектуру, которая сочетает в себе сильные стороны Transformer с эффективностью слоев Mamba.

![Figure_01](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Figure_01.jpeg)

*Сравнение пропускной способности и точностиРисунок 1: Сравнение моделей Nemotron-H с другими современными LLM с точки зрения пропускной способности (токенов/с/GPU) и точности на эталонном тесте MMLU. Nemotron-H-56B предлагает в 2,4 раза более высокую пропускную способность, чем Llama-3.1-70B, при более высоких уровнях точности.*

Разработанные NVIDIA модели Nemotron-H стратегически заменяют большую часть слоев самовнимания в Transformer слоями Mamba, которые основаны на моделях пространства состояний (SSM). В отличие от самовнимания, вычислительная и объемная сложность которого масштабируются квадратично с длиной последовательности, слои Mamba предлагают постоянную вычислительную и объемную сложность на токен, что делает их особенно эффективными для генерации длинных последовательностей.

Ключевое новшество Nemotron-H заключается в тщательном балансировании этих двух архитектурных парадигм для поддержания или улучшения точности при значительном увеличении скорости логического вывода. Этот подход отвечает критической потребности в сообществе LLM в моделях, которые могут эффективно обрабатывать длинные контексты без ущерба для производительности.

## **2. Дизайн гибридной архитектуры**

Архитектура Nemotron-H сочетает в себе слои Mamba-2 с традиционными компонентами Transformer для создания сбалансированной гибридной конструкции. Структура модели стратегически располагает слои самовнимания, чтобы использовать их сильные стороны в захвате глобальных отношений, в то время как слои Mamba используются для эффективной обработки последовательностей.

![Figure_02](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Figure_02.png)

*Рисунок 2 | Архитектуры моделей Nemotron-H-8B/56B. Примерно 8% от общего числа слоев в модели составляют слои самовнимания (self-attention); эти слои равномерно распределены по всей модели. Остальная часть модели состоит из чередующихся слоев Mamba-2 и FFN.*

Как показано на рисунке 2, как Nemotron-H-8B, так и Nemotron-H-56B следуют схожей схеме: серия начальных пар слоев Mamba-2 и FFN, за которой следует средняя секция, которая включает в себя один слой внимания среди нескольких пар Mamba-2 и FFN, и завершается дополнительными слоями Mamba-2 и FFN. Ключевое различие заключается в количестве повторений — в то время как Nemotron-H-8B имеет 4 повторения в своей средней секции, Nemotron-H-56B имеет 10.

Эта архитектура была тщательно разработана посредством обширных экспериментов, чтобы найти оптимальный баланс между вычислительной эффективностью и возможностями модели. Сохраняя некоторые слои самовнимания, Nemotron-H сохраняет способность моделировать определенные глобальные отношения, с которыми слои Mamba могут испытывать трудности, при этом используя эффективность Mamba для большинства операций обработки последовательностей.

В архитектуре используются несколько ключевых компонентов:

1. **Слои Mamba-2:** обеспечивают эффективное моделирование последовательностей с постоянной вычислительной сложностью на токен;
2. **Слои самовнимания:** стратегически расположены для захвата глобальных отношений;
3. **Прямые нейронные сети (FFN):** обрабатывают выходы слоев Mamba и внимания.

Этот гибридный подход позволяет Nemotron-H обрабатывать последовательности более эффективно, чем чистые модели Transformer, при этом поддерживая сопоставимую или лучшую точность в широком диапазоне задач.

## **3. Методология обучения**

Модели Nemotron-H обучались с использованием комбинации инновационных подходов для обеспечения высокой производительности и эффективности:

### **3.1 Курирование и подготовка данных**

Обучающие данные состояли из разнообразной смеси источников, тщательно сбалансированных для развития различных возможностей:

![Figure_03](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Figure_05.jpeg)

*Рисунок 3: Распределение источников данных предварительного обучения для моделей Nemotron-H, показывающее баланс между веб-сканированием, кодом, академическими и другими типами данных.*

Модель 56B была обучена примерно на 20 триллионах токенов, при этом данные веб-сканирования составляли наибольшую часть (59%), за ними следовали код (20%) и академический контент (8,8%). Смесь данных была разработана для обеспечения всестороннего охвата общих знаний при одновременном развитии сильных возможностей в специализированных областях, таких как кодирование и математика.

Для этапов постобработки распределение данных было скорректировано с упором на примеры контролируемой тонкой настройки (SFT), как показано на последующих графиках распределения данных.

### **3.2 Рецепт обучения FP8**

Значительным нововведением в разработке Nemotron-H является использование 8-битной арифметики с плавающей запятой (FP8) для обучения, что снижает требования к памяти и вычислительные затраты при сохранении качества модели:

![Относительная разница в потерях при обучении между FP8 и BF16](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Figure_06.jpeg)

*Рисунок 4: Относительная разница в потерях при обучении между FP8 и BF16, показывающая сходимость по мере прогресса обучения.*

Рецепт обучения FP8 включает в себя:

- Текущее масштабирование для каждого тензора для повышения стабильности
- Сохранение первых и последних четырех GEMM модели в точности BF16
- Постепенную сходимость с обучением BF16 с течением времени

Результаты показывают, что обучение FP8 может соответствовать или превосходить производительность обучения BF16 по различным тестам:

![Сравнение обучения FP8 и BF16](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Figure_07.jpeg)

*Рисунок 5: Сравнение обучения FP8 и BF16 по различным тестам, показывающее сопоставимую или лучшую производительность при обучении FP8.*

Этот метод позволяет проводить более эффективное обучение, сохраняя или улучшая качество модели, о чем свидетельствуют показатели производительности на тестах MMLU, понимания здравого смысла, генерации кода и GSM8K.