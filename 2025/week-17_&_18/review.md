# **Mamba 2 + Transformer = Nemotron H**

## **Введение**

Трансформеры сегодня – золотой стандарт нейросетей, и, особенно, больших языковых моделей. Они стали первой по-настоящему масштабируемой архитектурой, то есть с ними впервые стало возможно гарантировано наращивать перформанс моделей за счет увеличения количества данных и параметров, не упираясь в потолок производительности железа или запоминающей способности нейросети. 

Именно трансформер изменил индустрию искусственного интеллекта и сделал ее такой мощной, какой мы видим ее сейчас. До 2017 года, пока исследователи из Google Brain не изобрели эту архитектуру, краеугольным камнем ИИ-индустрии был поиск подходящего строения модели. Теперь же перед учеными стоят, в основном, другие задачи, а вот об архитектуре компании и ресерчеры почти не думают: ведь есть трансформер! 

Вот так говорит об этой архитектуре знаменитый Андрей Карпаты – бывший ML-директор Tesla, сооснователь и бывший главный ученый OpenAI: "Трансформер - не просто очередной метод, а подход, который полностью изменил наш взгляд на ИИ. Нам очень повезло, что мы наткнулись именно на него в огромном пространстве алгоритмов. Я верю, что трансформер лучше человеческого мозга во многих отношениях."

Однако, несмотря на все свои достоинства, у трансформера есть и недостатки. Поэтому некоторые группы исследователей продолжают искать лучший алгоритм, который мог бы превзойти трансформер или хотя бы достичь его уровня. В этой статье мы разберемся, почему эта задача так нетривиальна, что именно в трансформере оставляет желать лучшего.

## **Почему трансформеры так сложно заменить**

Чтобы разобраться в этом вопросе, давайте нырнем в эту архитектуру глубже. Что вообще представляет из себя трансформер? 

Начало трансформерам положила ставшая культовой статья "Attention Is All You Need", выпущенная в 2017 году восемью исследователями Google. При этом все восемь авторов указаны как равноправные участники: это редкость для научных статей. Кстати, ныне никто из этой восьмерки больше не работает в Google. Почти все они стали основателями известных ИИ-стартапов, таких как Cohere, Character.ai, Adept, Inceptive, Essential AI и Sakana AI.

Исторически, до трансформеров главной LLM-архитектурой были рекурретные нейросети (RNN). RNN, а также их продвинутые аналоги LSTM и GRU, обрабатывали информацию последовательно, как человек, который читает слева направо. Тем не менее, относительно манеры человеческого чтения этот алгоритм сильно упрощен. Дело в том, что в основе этих архитектур – скрытое состояние, которое на каждом шаге рекуррентно (отсюда и название механизма) обновляется. Однако, как мы понимаем, связи между словами могут быть и более сложными: например, проявляться не только последовательно. Поэтому обрабатывая слова (а точнее токены) строго один за одним, мы теряем возможность улавливать связи между словами, стоящими не рядом. Ведь модель может просто-напросто успеть "забыть" что-то важное, прежде чем ей выпадет шанс понять, что для дальнейшего текста это было важно. 

Поэтому следующей значимой вехой в развитии NLP стал механизм внимания. Традиционно считается, что его изобрел в 2014 году один из отцов глубокого обучения Йошуа Бенджио. Суть механизма заключается в том, что мы "взвешиваем" релевантность всех токенов последовательности относительно друг друга: каждый с каждым. На практике это реализуется как перемножение трех тензоров: Query, Key и Value. Каждая из этих матриц получается в результате умножения входных эмбеддингов X на некоторые обучаемые веса W. Воспринимать Query, Key и Value можно как составляющие, необходимые для "умного поиска" по последовательности: запросы, ключи и значения. При последовательном перемножении этих матриц (как показано на картинке ниже) мы и получаем тот самый attention, который показывает значимость связей между словами. Таким образом, с помощью внимания мы можем учитывать связи между словами в отрывке независимо от того, насколько далеко они находятся друг от друга.

![Figure_03](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Figure_03.png)

Однако появление механизма внимание самого по себе не произвело революцию в искусственном интеллекте. До статьи о трансформере исследователи использовали attention только как дополнение к архитектуре RNN. Достижение команды Google состояло именно в том, что они изобрели архитектуру, в которой абсолютно отказались от концепции RNN и полностью положились на механизм внимания. Отсюда и название статьи: "Attention Is All You Need" (конечно, и без отсылки к известной песне The Beatles не обошлось). Кстати, устоявшиеся термины Query, Key и Value тоже были введены в этом исследовании. Так родился трансформер, фундаментальным новшеством которого стала возможность обрабатывать последовательности параллельно, а не последовательно. Это дает модели способность не только глобально понимать тексты, которые она читает и пишет, но и эффективно обучаться и масштабироваться. Трансформер может "съесть" тонны информации и разрастаться до огромного количества параметров. При этом его перформанс не выходит на плато, а продолжает расти. Это – еще одна важная отличительная черта этой архитектуры.

![Figure_04](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Figure_04.webp)

На сегодняшний день трансформеры уже окончательно захватили ИИ-индустрию и ресерч. Все популярные сегодня чатботы — ChatGPT от OpenAI, Gemini от Google, Claude от Anthropic, Grok от xAI — основаны на трансформере. То же самое касается и инструментов для генерации изображений: Midjourney, Stable Diffusion, Runway и так далее. Такие сети построены на основе моделей диффузии, которые внутри себя, в свою очередь, используют трансформеры. Кроме того, архитектуру применяют в моделях предсказания структур молекул, робототехнике и беспилотных автомобилях. Соавтор статьи про трансформер, Ашиш Васвани, удачно высказался про эту модель так: "Трансформер — это способ очень быстро одновременно зафиксировать все связи между различными частями любого ввода. Это могут быть части предложения, ноты, пиксели или молекулы белка. Он подходит для любой задачи." Однако, трансформеры не лишены некоторых недостатков. Сегодня мы разберём архитектуру под названием Mamba, которая претендует на то, чтобы стать соперником трансформеров и решить их уязвимости, а так же рассмотрим семейство моделей Nemotron-H от Nvidia, которые представляют собой гибридную архитектуру, сочетающую в себе сильные стороны Transformer с эффективностью слоев Mamba.

Разработанные NVIDIA модели Nemotron-H стратегически заменяют большую часть слоев самовнимания в Transformer слоями Mamba, которые основаны на моделях пространства состояний (SSM). В отличие от самовнимания, вычислительная и объемная сложность которого масштабируются квадратично с длиной последовательности, слои Mamba предлагают постоянную вычислительную и объемную сложность на токен, что делает их особенно эффективными для генерации длинных последовательностей.

![Figure_01](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Figure_01.jpeg)

*Сравнение пропускной способности и точностиРисунок 1: Сравнение моделей Nemotron-H с другими современными LLM с точки зрения пропускной способности (токенов/с/GPU) и точности на эталонном тесте MMLU. Nemotron-H-56B предлагает в 2,4 раза более высокую пропускную способность, чем Llama-3.1-70B, при более высоких уровнях точности.*

Ключевое новшество Nemotron-H заключается в тщательном балансировании этих двух архитектурных парадигм для поддержания или улучшения точности при значительном увеличении скорости логического вывода. Этот подход отвечает критической потребности в сообществе LLM в моделях, которые могут эффективно обрабатывать длинные контексты без ущерба для производительности.

# Обзор архитектуры Mamba

Mamba — это инновационная архитектура, основанная на структурированных моделях последовательностей в пространстве состояний (SSM). Она разработана для эффективного выявления сложных зависимостей в данных последовательностей и позиционируется как серьезный конкурент Transformer. Архитектура сочетает в себе преимущества рекуррентных нейронных сетей (RNN) и сверточных нейронных сетей (CNN), достигая линейного или почти линейного масштабирования вычислительных затрат относительно длины последовательности.

## Основные преимущества Mamba

1. **Механизм выбора**  
   - Введен простой и эффективный механизм фильтрации нерелевантной информации.  
   - Позволяет сохранять необходимые данные за счет параметризованных параметров SSM.

2. **Аппаратно-ориентированный алгоритм**  
   - Использует рекурсивное сканирование вместо традиционных сверточных вычислений.  
   - Оптимизирован для графических процессоров, обеспечивая ускорение до 3 раз на GPU A100.

3. **Возможности моделирования**  
   - Сохраняет производительность, сопоставимую с Transformer.  
   - Характеризуется почти линейной масштабируемостью, что делает её пригодной для работы с длинными и сложными последовательностями данных.

## Применение Mamba

Mamba демонстрирует выдающуюся производительность в различных областях:  
- **Компьютерное зрение**:  
  Модель Vim, основанная на Mamba, в 2,8 раза быстрее DeiT при извлечении признаков изображений с высоким разрешением и экономит 86,8% памяти GPU.  
- **Обработка естественного языка (NLP)**:  
  Улучшенная селективная архитектура SSM обеспечивает ускорение в 2–8 раз.
- **Кодогенерация (Text-to-Code)**:  
  Mistral сделали на основе SSM модель Codestral, которая на метриках разбила почти все другие открытые модели. 

Перед тем как погрузится в глубокий обзор архитектурных особеннойстей Mamba, давайте разберемся, что такое RNN, LSTM, GRU, SSM 👇

<details> 
    <summary><em><strong>Рекуррентная нейронная сеть (RNN)</strong></em></summary>

## **1. Введение и мотивация**

### **1.1 Почему нужны рекуррентные сети**
- **Последовательные данные**: язык, временные ряды, аудио, ДНК‑последовательности.  
- **Зависимости во времени**: полносвязные сети считают входы независимыми; RNN хранят контекст в скрытом состоянии $h_t$.

### **1.2 История**   

- **1982 г. — Hopfield‑сеть.**  
  Показала, что нейронная сеть с симметричными весами может работать как энергетическая модель памяти‑ассоциаций. Работа Дж. Хопфилда стала первой демонстрацией тренируемых рекуррентных связей в нейро‑вычислениях.

- **1986 г. — алгоритм BPTT (Rumelhart & McClelland).**  
  Авторы обобщили классический back‑propagation на временно развёрнутые графы, что открыло путь к градиентному обучению длинных последовательностей. Книга *Parallel Distributed Processing* закрепила идею распределённых репрезентаций.

- **1990 г. — «Simple RNN» (Elman).**  
  Д. Элман показал, что рекуррентный «контекстный» слой способен захватывать грамматические зависимости в синтетическом языке. Так появилась базовая архитектура Elman‑net, ставшая учебным эталоном RNN.

- **1997 г. — LSTM (Hochreiter & Schmidhuber).**  
  Введение ячейки памяти и вентилирования решило проблему затухающих градиентов, позволив моделировать зависимости на сотни шагов назад. LSTM вскоре стал стандартом для речи и машинного перевода.

- **2014 г. — GRU (Cho и др.).**  
  Сократив число вентилей до двух, GRU предложил более лёгкую альтернативу LSTM при сопоставимой точности. Публикация совпала с бумом seq2seq‑моделей в переводе и диалоговых системах.

- **2020‑е — гибриды RNN + Attention (RWKV, S4, Mamba).**  
  Современные работы объединяют линейные рекуррентные операторы со слоем внимания, достигая масштабируемости трансформеров при памяти $O(1)$. Такие модели успешно конкурируют на задачах длинного контекста и стриминга.


## **2. Simple RNN (Ячейка Элмана): Как это работает?**

### **2.1 Интуиция**

Представьте, что вы читаете предложение слово за словом. Чтобы понять смысл текущего слова, вы используете не только само слово, но и контекст, накопленный из предыдущих слов. Simple RNN работает похожим образом:

*   На каждом временном шаге $t$ она принимает:
    1.  **Новый вход** $x_t$ (например, векторное представление слова).
    2.  **Состояние из предыдущего шага** $h_{t-1}$ (контекст, "память").
*   На основе этих двух входов она вычисляет:
    1.  **Новое состояние** $h_t$, которое будет передано на следующий шаг.
    2.  **Выход** $y_t$ (например, предсказание следующего слова или метка для текущего элемента).

### **2.2 Формализация и Обозначения**

Давайте опишем это математически. Сначала определимся с обозначениями и размерами тензоров (векторов/матриц):

| **Объект** | **Размерность**        | **Смысл**                                    |
| :--------- | :--------------------- | :------------------------------------------- |
| $x_t$      | $\mathbb{R}^{d_x}$     | Вектор входа в момент времени $t$            |
| $h_t$      | $\mathbb{R}^{d_h}$     | Вектор скрытого состояния в момент $t$       |
| $y_t$      | $\mathbb{R}^{d_y}$     | Вектор выхода модели в момент $t$            |
| $W_{xh}$   | $\mathbb{R}^{d_x \times d_h}$ | Матрица весов "вход → скрытое состояние"   |
| $W_{hh}$   | $\mathbb{R}^{d_h \times d_h}$ | Матрица весов "предыдущее состояние → текущее состояние" (рекуррентная связь) |
| $W_{hy}$   | $\mathbb{R}^{d_h \times d_y}$ | Матрица весов "скрытое состояние → выход" |
| $b_h$      | $\mathbb{R}^{d_h}$     | Вектор смещения для скрытого слоя            |
| $b_y$      | $\mathbb{R}^{d_y}$     | Вектор смещения для выходного слоя           |

> **Зачем следить за размерностями?** Это помогает избежать ошибок при матричных операциях и при написании кода (особенно с broadcast'ингом в библиотеках типа NumPy/PyTorch).

### **2.3 Динамика одного шага**

Теперь запишем формулы, описывающие переход от шага $t-1$ к шагу $t$:

$$
\boxed{%
\begin{aligned}
h_t &= \sigma_h\!\bigl(W_{xh}x_t + W_{hh}h_{t-1} + b_h\bigr), & h_0&=\mathbf0,\\[4pt]
y_t &= \sigma_y\!\bigl(W_{hy}h_t + b_y\bigr).
\end{aligned}}
$$

**Пояснения:**

1.  **Вычисление скрытого состояния $h_t$:**
    *   $W_{xh}x_t$: Влияние текущего входа $x_t$ на новое состояние.
    *   $W_{hh}h_{t-1}$: Влияние предыдущего состояния $h_{t-1}$ (памяти) на новое состояние. Это **ключевая рекуррентная связь**.
    *   $b_h$: Смещение (bias).
    *   $\sigma_h$: Функция активации скрытого слоя. Часто используют **tanh** или **сигмоиду**, так как они "сжимают" значения в ограниченный диапазон ([-1, 1] для tanh, [0, 1] для сигмоиды), что может помочь стабилизировать градиенты при обучении.
    *   $h_0 = \mathbf{0}$: Начинаем с нулевого вектора состояния перед обработкой первого элемента последовательности.

2.  **Вычисление выхода $y_t$:**
    *   $W_{hy}h_t$: Преобразование текущего скрытого состояния $h_t$ в выходное представление.
    *   $b_y$: Смещение выходного слоя.
    *   $\sigma_y$: Функция активации выходного слоя. Её выбор **зависит от задачи**:
        *   `softmax`: для задач классификации (например, предсказание следующего символа/слова из словаря).
        *   `sigmoid`: для бинарной классификации (например, анализ тональности: положительный/отрицательный).
        *   `id` (линейная активация, т.е. её отсутствие): для задач регрессии (предсказание числового значения).

![Image_01](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/RNN/Image_01.webp)

```python
"""
Этот программный код представляет собой реализацию простой рекуррентной нейронной сети (RNN) для обработки последовательностей слов. 
Код включает в себя инициализацию параметров модели, функции для вычисления softmax, а также основной цикл RNN, который обрабатывает 
входную последовательность слов и выводит прогнозы для каждого слова в последовательности.

Функциональное назначение:
Код демонстрирует работу RNN на примере обработки текстовой последовательности. Он инициализирует веса и смещения, выполняет встроенные 
операции (one-hot encoding, embedding, вычисление скрытого состояния, softmax), и выводит топ-2 прогноза для каждого слова в последовательности.
"""

import numpy as np
import pandas as pd

def softmax(x: np.ndarray) -> np.ndarray:
    """
    Description:
    ---------------
        Вычисляет softmax для входного массива.

    Args:
    ---------------
        x: Входной массив, для которого нужно вычислить softmax.

    Returns:
    ---------------
        Массив с примененной функцией softmax.

    Raises:
    ---------------
        ValueError: Если входной массив пуст.

    Examples:
    ---------------
        >>> softmax(np.array([1, 2, 3]))
        array([0.09003057, 0.24472847, 0.66524096])
    """
    if x.size == 0:
        raise ValueError("Входной массив не может быть пустым")

    e = np.exp(x - np.max(x, axis=0, keepdims=True))
    return e / e.sum(axis=0, keepdims=True)

# ---------------- Параметры модели ----------------
vocab = ["the", "students", "opened", "their", "books", "laptops", "zoo"]
V = len(vocab)
d_e, d_h = 8, 16  # размеры embedding и скрытого состояния

# Словарь: слово → индекс
word2idx = {w: i for i, w in enumerate(vocab)}

# Инициализация весов
np.random.seed(0)
E = np.random.randn(d_e, V) * 0.1      # эмбеддинги
W_e = np.random.randn(d_h, d_e) * 0.1  # скрытое ← эмбеддинг
W_h = np.random.randn(d_h, d_h) * 0.1  # скрытое ← скрытое
b1 = np.zeros((d_h, 1))                # смещение для скрытого слоя
U = np.random.randn(V, d_h) * 0.1      # проекция скрытого → логиты
b2 = np.zeros((V, 1))                  # смещение для выходного слоя

# --------------- Визуализация матриц ----------------
df_E = pd.DataFrame(
    E, index=[f"e{i}" for i in range(d_e)], columns=vocab
)
df_We = pd.DataFrame(
    W_e, index=[f"h{i}" for i in range(d_h)], columns=[f"e{j}" for j in range(d_e)]
)
df_Wh = pd.DataFrame(
    W_h, index=[f"h{i}" for i in range(d_h)], columns=[f"h{j}" for j in range(d_h)]
)
df_U = pd.DataFrame(
    U, index=vocab, columns=[f"h{j}" for j in range(d_h)]
)

print("\nМатрица E (эмбеддинги):")
print(df_E)
print("\nМатрица W_e (скрытое ← эмбеддинг):")
print(df_We)
print("\nМатрица W_h (скрытое ← скрытое):")
print(df_Wh)
print("\nМатрица U (проекция на выход):")
print(df_U)

# --------------- Основной цикл RNN ----------------
sequence = ["the", "students", "opened", "their"]
h_prev = np.zeros((d_h, 1))

print("\nШаг  t    Слово      Топ‑2 (слово, вер‑ть)")
print("-" * 60)
for t, word in enumerate(sequence, 1):
    print(f"\n## Пошаговый разбор для t={t}, слово = '{word}'")

    # 1) One‑hot
    x = np.zeros((V, 1))
    x[word2idx[word], 0] = 1.0
    print("1) One‑hot вектор x:")
    print(x.T)

    # 2) Embedding
    e = E @ x
    print("\n2) Embedding e = E @ x:")
    print(e.T)

    # 3) Скрытое состояние
    h = np.tanh(W_h @ h_prev + W_e @ e + b1)
    print("\n3) Скрытое состояние h:")
    print(h.T)

    # 4) Логиты и softmax
    o = U @ h + b2
    y = softmax(o)
    print("\n4) Логиты o = U @ h + b2:")
    print(o.T)
    print("   Softmax y:")
    print(y.T)

    # Топ‑2 кандидата
    top2 = np.argsort(-y.flatten())[:2]
    probs = [(vocab[i], float(y[i])) for i in top2]
    print(f"\nТоп‑2 кандидата: {probs}")

    # Обновление скрытого состояния
    h_prev = h
```

### **Пояснения к схеме «Простая RNN‑языковая модель» (step by step)**

1. **Подача входа**  
   - На каждом шаге $t$ мы имеем слово в виде one‑hot вектора  
     $$x^{(t)} \in \mathbb{R}^{|V|}$$  
     где $|V|$ — размер словаря.
     
   - Пример: для словаря $\{\text{the}, \text{students}, \text{opened}, \dots\}$ слово «students» кодируется вектором, где на позиции «students» стоит 1, а в остальных — 0.

2. **Преобразование в embedding**  
   - Умножаем one‑hot $x^{(t)}$ на матрицу вложений  
     $$E \in \mathbb{R}^{d_e \times |V|}$$  
     чтобы получить плотный вектор  
     $$e^{(t)} = E \, x^{(t)} \in \mathbb{R}^{d_e}$$

3. **Обновление скрытого состояния**  
   - Рекуррентная формула:  

    $$
      h^{(t)} = \sigma\bigl(W_h \, h^{(t-1)} + W_e \, e^{(t)} + b_1\bigr)
    $$  
     
     где  
     - $h^{(t)} \in \mathbb{R}^{d_h}$ — скрытое состояние на шаге $t$,  
     - $W_h \in \mathbb{R}^{d_h \times d_h}$ — матрица перехода по скрытому состоянию,  
     - $W_e \in \mathbb{R}^{d_h \times d_e}$ — матрица для входного embedding,  
     - $b_1 \in \mathbb{R}^{d_h}$ — вектор смещений,  
     - $\sigma$ — нелинейность (обычно $\tanh$ или ReLU).

   Инициализация:  

     $$h^{(0)} = \mathbf{0}\quad(\text{или случайный вектор}).$$  
   - При расчёте выхода к $W_h\,h^{(t-1)} + W_e\,e^{(t)}$ прибавляется смещение $b_1$, а к $U\,h^{(t)}$ — смещение $b_2$, после чего по логитам вычисляется softmax.

4. **Вычисление выхода**  
   - Строим логиты для распределения по словарю:  
     $$
       o^{(t)} = U \, h^{(t)} + b_2,\qquad U\in\mathbb{R}^{|V|\times d_h},\;b_2\in\mathbb{R}^{|V|}.
     $$  
   - Применяем softmax, чтобы получить вероятностное распределение:  
     $$
       \hat y^{(t)} = \mathrm{softmax}\bigl(o^{(t)}\bigr)\in[0,1]^{|V|},\quad\sum_i \hat y^{(t)}_i = 1.
     $$  
   - Вектор $\hat y^{(t)}$ показывает, какое слово модель считает наиболее вероятным следующим на позиции $t+1$.

5. **Повторяем во времени**  
   - Матрицы весов ($W_{xh}, W_{hh}, W_{hy}$) и векторы смещений ($b_h, b_y$) **одни и те же на всех временных шагах $t$**. Сеть использует один и тот же набор параметров для обработки каждого элемента последовательности. Это делает RNN компактными по количеству параметров, независимо от длины последовательности $T$.

## **3. Обучение RNN: Backpropagation Through Time (BPTT)**

Мы определили, как RNN делает предсказания (прямой проход). Но как настроить её веса $W_{xh}, W_{hh}, W_{hy}, b_h, b_y$, чтобы предсказания были точными? Для этого нужен алгоритм обратного распространения ошибки, адаптированный для рекуррентной структуры — **Backpropagation Through Time (BPTT)**.

### **3.1 Идея: Разворачивание во времени**

Чтобы применить градиентный спуск, нам нужно вычислить градиенты функции потерь $L$ по всем параметрам модели. Сложность в том, что выход $y_t$ зависит от $h_t$, который зависит от $h_{t-1}$, который зависит от $h_{t-2}$, и так далее, вплоть до $h_0$. Кроме того, все $h_k$ (для $k < t$) зависят от одних и тех же весов $W_{hh}$ и $W_{xh}$.

Идея BPTT заключается в том, чтобы **мысленно "развернуть" RNN во времени** для последовательности длиной $T$. Представьте, что у вас есть $T$ копий одной и той же ячейки RNN, соединенных последовательно. Вход $x_t$ и предыдущее состояние $h_{t-1}$ подаются в $t$-ю копию, она выдает $h_t$ и $y_t$, и $h_t$ передается в $(t+1)$-ю копию.

![Image_02](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/RNN/Image_02.png)

### **Пояснения к схеме «Простая RNN‑языковая модель» (step by step)**

Ниже показано, как на примере фразы «the students opened their» происходит прямой и обратный проходы (BPTT) в развёрнутой RNN-модели.

#### **1. Развёртывание по времени (Unrolling)**

- Каждый прямоугольник на схеме соответствует одному временному шагу $t=0,1,2,3$.  
- Входы: $x_0,x_1,x_2,x_3$ — one‑hot векторы слов «the», «students», «opened», «their».  
- Начальное скрытое состояние $h_{-1}$ инициализируется нулями.  
- Скрытые состояния $h_0\ldots h_3$ последовательно передаются по ребру $W_{hh}$.  
- На каждом шаге из $h_t$ через $W_{hy}$ вычисляется выход $\hat y_t$.

#### **2. Прямой проход (Forward pass)**

#### **Шаг 0 ($t=0$), слово «the»**

1. **One-hot представление**:  
   $ x_0 $ — единичный вектор, где единица находится в позиции слова «the».

2. **Embedding (векторное представление)**:  
   $ e_0 = E\,x_0 $,  
   где $ E $ — матрица эмбеддингов.

3. **Скрытое состояние (hidden state)**:  
   $$
   h_0 = \tanh\bigl(W_{xh} e_0 + W_{hh} h_{-1} + b_1\bigr),
   $$  
   где:
   - $ W_{xh}, W_{hh} $ — весовые матрицы,
   - $ b_1 $ — смещение,
   - $ h_{-1} $ — начальное скрытое состояние (обычно нулевой вектор).

4. **Выход модели и softmax**:  
   $$
   o_0 = W_{hy}h_0 + b_2,\quad \hat{y}_0 = \mathrm{softmax}(o_0),
   $$  
   где:
   - $ W_{hy} $ — матрица для преобразования скрытого состояния в логиты,
   - $ b_2 $ — смещение,
   - $ \hat{y}_0 $ — вероятностное распределение по словарю.

5. **Функция потерь (Loss)**:  
   Целевое слово — «students». Потеря вычисляется как:  
   $$
   L_0 = -\log\hat{y}_0[\text{students}].
   $$

#### **Подробнее о функции потерь**

Модель использует **кросс-энтропийную функцию потерь** для многоклассовой задачи. Рассмотрим её этапы:

1. **Логиты и вероятности**:  
   Модель выдаёт вектор логитов:  
   $$
   o_0 = W_{hy}h_0 + b_2,
   $$  
   который затем преобразуется в вероятности через softmax:  
   $$
   \hat{y}_0 = \mathrm{softmax}(o_0) \in [0, 1]^{|V|}, \quad \sum_i \hat{y}_0[i] = 1.
   $$

2. **Целевая метка**:  
   Целевая метка $ y^{(0)} $ — one-hot вектор, где единица стоит в позиции целевого слова «students»:  
   $$
   y^{(0)}_{\text{students}} = 1.
   $$

3. **Кросс-энтропия**:  
   Формула кросс-энтропии:  
   $$
   L_0 = -\sum_{i=1}^{|V|} y^{(0)}_i \log\hat{y}_0[i] = -\log\hat{y}_0[\text{students}].
   $$

4. **Интуиция**:  
   Чем меньше вероятность предсказанного слова $ \hat{y}_0[\text{students}] $, тем выше штраф (значение потери).

#### **Шаг 1 ($t=1$), слово «students»**
- Аналогично: $x_1$ → $e_1$ →  
  $$h_1 = \tanh(W_{xh}e_1 + W_{hh}h_0 + b_1).$$  
- Выход $\hat y_1 = \mathrm{softmax}(W_{hy}h_1+b_2)$,  
  целевое слово «opened», $L_1=-\log\hat y_1[opened]$.

#### **Шаг 2 ($t=2$), слово «opened»**
- $x_2$ → $e_2$ →  
  $$h_2 = \tanh(W_{xh}e_2 + W_{hh}h_1 + b_1).$$  
- $\hat y_2$, целевой «their», $L_2=-\log\hat y_2[their]$.

#### **Шаг 3 ($t=3$), слово «their»**
- $x_3$ → $e_3$ →  
  $$h_3 = \tanh(W_{xh}e_3 + W_{hh}h_2 + b_1).$$  
- $\hat y_3$, целевой «books», $L_3=-\log\hat y_3[books]$.

- **Суммарная потеря**:  
  $$L = L_0 + L_1 + L_2 + L_3.$$  

#### **3. Обратный проход (Backward pass — BPTT)**

- Градиенты от каждой $L_t$ (красные стрелки) прокатываются через:
  - выходной слой $W_{hy}$ к скрытым состояниям,
  - рекуррентные связи $W_{hh}$ к предыдущим $h_{t-1}$.
- На каждом шаге аккумулируются $
  \frac{\partial L}{\partial W_{xh}},
  \frac{\partial L}{\partial W_{hh}},
  \frac{\partial L}{\partial W_{hy}},
  \frac{\partial L}{\partial b_1},
  \frac{\partial L}{\partial b_2}$.
- В итоге веса обновляются с учётом вклада ошибок со всех временных шагов.

**Вывод:** BPTT разворачивает RNN во времени, вычисляет локальные потери на каждом шаге и распространяет ошибки сквозь все временные соединения, обеспечивая обучение с учётом контекстов предыдущих токенов.

Хотя мы создаем $T$ копий для вычислений, важно помнить: **веса $W_{xh}, W_{hh}, W_{hy}$ общие для всех этих копий**.

#### **3.2 Общая функция потерь**

Обычно общая потеря $L$ для всей последовательности — это сумма или среднее локальных потерь $\ell$ на каждом шаге:

$$
L \;=\;\sum_{t=1}^{T}\,\ell\bigl(y_t,\widehat y_t\bigr),
$$

где $y_t$ — предсказание модели на шаге $t$, а $\widehat y_t$ — истинное значение (цель) на шаге $t$. Функция $\ell$ может быть, например, кросс-энтропией для классификации или среднеквадратичной ошибкой (MSE) для регрессии.

#### **3.3 Вычисление градиентов (Пример для $W_{hh}$)**

Рассмотрим, как вычислить градиент общей потери $L$ по одному элементу $w$ из матрицы $W_{hh}$. Используя цепное правило, градиент $L$ по $w$ складывается из вкладов от каждого временного шага $t$:

$$
\frac{\partial L}{\partial w}\;=\; \sum_{t=1}^{T}\,\frac{\partial \ell(y_t, \widehat y_t)}{\partial w}
$$

Чтобы найти $\frac{\partial \ell(y_t, \widehat y_t)}{\partial w}$, нам нужно учесть, как $w$ влияет на $y_t$. Это влияние происходит через скрытое состояние $h_t$:

$$
\frac{\partial \ell(y_t, \widehat y_t)}{\partial w} = \frac{\partial \ell}{\partial y_t} \frac{\partial y_t}{\partial h_t} \frac{\partial h_t}{\partial w}
$$

Самая сложная часть — это $\frac{\partial h_t}{\partial w}$. Состояние $h_t$ зависит от $w$ напрямую (через член $W_{hh}h_{t-1}$ в формуле для $h_t$) и косвенно, через все предыдущие состояния $h_{t-1}, h_{t-2}, \dots, h_1$, так как они тоже зависят от $w$.

$$
\frac{\partial h_t}{\partial w} = \underbrace{\frac{\partial h_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial w}}_{\text{через } h_{t-1}} + \underbrace{\frac{\partial h_t}{\partial w}}_{\text{прямое влияние}}
$$

Раскрывая эту рекурсию дальше, мы увидим, что градиент включает в себя **сумму путей** разной длины из прошлого в настоящее. Каждый такой путь включает произведения Якобианов $\frac{\partial h_k}{\partial h_{k-1}}$.

$$
\frac{\partial h_k}{\partial h_{k-1}} = \frac{\partial}{\partial h_{k-1}} \sigma_h(W_{xh}x_k + W_{hh}h_{k-1} + b_h) = \operatorname{diag}\!\bigl[\sigma_h'(a_k)\bigr]\,W_{hh}
$$
где $a_k = W_{xh}x_k + W_{hh}h_{k-1} + b_h$ — аргумент функции активации $\sigma_h$ на шаге $k$. Обозначим этот Якобиан как $J_k$.

Тогда вклад в градиент от пути длиной $k$ (от $h_{t-k}$ к $h_t$) будет включать произведение $k$ таких Якобианов: $J_t J_{t-1} \dots J_{t-k+1}$.

#### **3.4 Проблемы: Затухание и Взрыв Градиентов**

Именно эти **длинные произведения Якобианов** $J_k = \operatorname{diag}[\sigma_h'(a_k)] W_{hh}$ являются источником проблем при обучении RNN:

1.  **Затухание градиента (Vanishing Gradient):** Если собственные значения матрицы $W_{hh}$ (или нормы Якобианов $J_k$) по модулю **меньше 1**, то при умножении многих таких матриц результат будет стремиться к нулю экспоненциально быстро с ростом $k$. Это означает, что градиенты от далеких прошлых шагов ($t-k$ для больших $k$) почти не доходят до параметров $W_{hh}$, и сеть не может научиться **долговременным зависимостям**. Simple RNN особенно подвержены этой проблеме.
2.  **Взрыв градиента (Exploding Gradient):** Если собственные значения $W_{hh}$ (или нормы $J_k$) по модулю **больше 1**, то произведение Якобианов будет расти экспоненциально. Это приводит к огромным значениям градиентов, что делает шаги градиентного спуска нестабильными и может привести к расхождению обучения (NaN/Inf в потерях или весах).

#### **3.5 Классические Решения**

*   **Gradient Clipping:** Искусственное ограничение нормы градиента. Если $\|\nabla\theta\| > \tau$ (некоторый порог), то градиент масштабируется: $\nabla\theta \leftarrow \frac{\tau}{\|\nabla\theta\|} \nabla\theta$. Это помогает бороться со *взрывом*, но не с *затуханием*.
*   **Правильная инициализация весов:** Например, ортогональная инициализация для $W_{hh}$ может помочь держать собственные значения близкими к 1.
*   **Использование более сложных ячеек:** **LSTM (Long Short-Term Memory)** и **GRU (Gated Recurrent Unit)** были разработаны специально для борьбы с затуханием градиентов. Они вводят "вентили" (gates), которые контролируют поток информации и градиентов через ячейку, позволяя сохранять информацию на долгие периоды.
*   **Функции активации:** Использование ReLU может усугубить взрыв градиента, но менее подвержено затуханию, чем сигмоида/tanh (если активация не нулевая). Однако в рекуррентной части часто предпочитают tanh.


## **4. BPTT на Практике: Квази-код и PyTorch**

Современные фреймворки глубокого обучения (PyTorch, TensorFlow/Keras) реализуют BPTT автоматически. Вам нужно лишь определить архитектуру RNN и запустить обратный проход (`loss.backward()` в PyTorch).

Вот как выглядит типичный цикл обучения с использованием BPTT в PyTorch (с использованием `tanh` как $\sigma_h$ и линейной $\sigma_y$):

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# --- Гиперпараметры и данные (примерные) ---
T = 10      # Длина последовательности
batch_size = 32
d_x = 20    # Размерность входа
d_h = 50    # Размерность скрытого состояния
d_y = 5     # Размерность выхода

# --- Модель (определяем параметры) ---
W_xh = torch.randn(d_x, d_h, requires_grad=True)
W_hh = torch.randn(d_h, d_h, requires_grad=True)
W_hy = torch.randn(d_h, d_y, requires_grad=True)
b_h  = torch.zeros(d_h, requires_grad=True)
b_y  = torch.zeros(d_y, requires_grad=True)
params = [W_xh, W_hh, W_hy, b_h, b_y]

# --- Пример данных ---
x_sequence = torch.randn(T, batch_size, d_x) # [Время, Батч, Признаки]
y_true_sequence = torch.randn(T, batch_size, d_y)

# --- Оптимизатор ---
optimizer = optim.Adam(params, lr=0.001)

# --- Цикл обучения (одна итерация) ---
optimizer.zero_grad()

# == Forward pass (разворачивание цикла вручную для ясности) ==
h_t = torch.zeros(batch_size, d_h) # Начальное скрытое состояние h_0
outputs = []
for t in range(T):
    # Формула Simple RNN
    h_t = torch.tanh(x_sequence[t] @ W_xh + h_t @ W_hh + b_h)
    y_t = h_t @ W_hy + b_y # Линейный выходной слой
    outputs.append(y_t)

# Собираем выходы в один тензор [T, Batch, d_y]
y_pred_sequence = torch.stack(outputs)

# == Вычисление потерь ==
# Пример: MSE на каждом шаге, затем усредняем по времени и батчу
loss = F.mse_loss(y_pred_sequence, y_true_sequence)

# == Backward pass (BPTT) ==
loss.backward() # PyTorch автоматически вычисляет градиенты ∂L/∂params через BPTT

# == Опционально: Gradient Clipping ==
torch.nn.utils.clip_grad_norm_(params, max_norm=1.0) # Ограничиваем норму градиента

# == Шаг оптимизатора ==
optimizer.step()

print(f"Loss: {loss.item()}")
# print(f"Gradient norm for W_hh: {W_hh.grad.norm().item()}") # Можно посмотреть на норму градиента
```

**Ключевые моменты:**

*   PyTorch строит динамический вычислительный граф во время forward pass.
*   Когда вызывается `loss.backward()`, PyTorch проходит по этому графу в обратном порядке, применяя цепное правило (реализуя BPTT) для вычисления градиентов всех параметров (`requires_grad=True`), от которых зависит `loss`.
*   `torch.nn.utils.clip_grad_norm_` — стандартная практика для предотвращения взрыва градиентов.

## **5. Проблема долговременных зависимостей**

Одна из привлекательных идей RNN состоит в том, что они потенциально умеют связывать предыдущую информацию с текущей задачей, так, например, знания о предыдущем кадре видео могут помочь в понимании текущего кадра. Если бы RNN обладали такой способностью, они были бы чрезвычайно полезны. Но действительно ли RNN предоставляют нам такую возможность? Это зависит от некоторых обстоятельств.

Иногда для выполнения текущей задачи нам необходима только недавняя информация. Рассмотрим, например, языковую модель, пытающуюся предсказать следующее слово на основании предыдущих. Если мы хотим предсказать последнее слово в предложении “облака плывут по небу”, нам не нужен более широкий контекст; в этом случае довольно очевидно, что последним словом будет “небу”. В этом случае, когда дистанция между актуальной информацией и местом, где она понадобилась, невелика, RNN могут обучиться использованию информации из прошлого.

Но бывают случаи, когда нам необходимо больше контекста. Допустим, мы хотим предсказать последнее слово в тексте “Я вырос во Франции… Я бегло говорю по-французски”. Ближайший контекст предполагает, что последним словом будет называние языка, но чтобы установить, какого именно языка, нам нужен контекст Франции из более отдаленного прошлого. Таким образом, разрыв между актуальной информацией и точкой ее применения может стать очень большим.

К сожалению, по мере роста этого расстояния, RNN теряют способность связывать информацию.

![Image_03](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/RNN/Image_03.png)

К счастью, LSTM не знает таких проблем!

## **Заключение**

Рекуррентные нейронные сети, обладая компактной памятью и естественной каузальностью, продолжают оставаться незаменимыми для потоковых задач и ситуаций с ограниченными ресурсами. Глубокое понимание их математической базы, инженерных приёмов и современных вариаций даёт исследователю инструмент, который гармонично дополняет «семейство» Transformer‑подобных моделей.

</details>

<details> 
    <summary><em><strong>Долгая краткосрочная память (LSTM)</strong></em></summary>

## 1. Введение и мотивация

### 1.1 История создания LSTM: ключевые работы Хохрайтера и Шмидхубера, развитие идеи

**Зарождение идеи (1991-1997)**

Проблема затухающего градиента была формально идентифицирована в начале 1990-х годов Сеппом Хохрайтером в его диссертационной работе. Хохрайтер и Юрген Шмидхубер начали искать архитектуру, которая могла бы преодолеть эту проблему.

**Ключевые этапы:**

- **1991-1995**: Первые эксперименты и теоретические разработки. Хохрайтер и Шмидхубер исследовали различные способы позволить градиентам течь через длинные последовательности без затухания.

- **1997**: Публикация оригинальной статьи "Long Short-Term Memory" в журнале Neural Computation. В этой фундаментальной работе была представлена архитектура LSTM с механизмом вентилей (gates) и постоянным потоком ошибки (Constant Error Carousel, CEC).

**Эволюция LSTM:**

- **1999-2000**: Феликс Герс и его коллеги представили "peephole connections" — модификацию, которая позволяет вентилям "заглядывать" в ячейку памяти.

- **2000**: Герс и Шмидхубер вводят "forget gate" (вентиль забывания) — критическое улучшение, позволяющее LSTM сбрасывать своё состояние и обучаться на последовательностях неограниченной длины.

- **2005**: Грейвс и Шмидхубер представили двунаправленный LSTM (BiLSTM), который обрабатывает последовательность в обоих направлениях.

- **2013-2014**: Эпоха широкого применения LSTM в промышленности, особенно в области распознавания речи и машинного перевода.

- **2014**: Разработка GRU (Gated Recurrent Unit) командой Чо как более простой альтернативы LSTM, сохраняющей большинство преимуществ.

**Цитата Шмидхубера о создании LSTM (2015):**

> "Проблема заключалась в том, что градиенты либо затухали, либо взрывались, и нам нужно было создать архитектуру, которая позволила бы информации и градиентам течь через много временных шагов."

### 1.2 Ключевые преимущества: почему LSTM стали стандартом в индустрии

**1. Решение проблемы затухающего градиента**

LSTM эффективно решает проблему затухающего градиента благодаря своему уникальному механизму ячейки памяти с контролируемыми вентилями. Ключевой компонент — **константный поток ошибки** (Constant Error Carousel, CEC), который обеспечивает неизменный градиент через состояние ячейки.

**2. Долговременная память**

LSTM способны запоминать информацию на сотни и даже тысячи временных шагов:
- Демонстрируют превосходную способность улавливать зависимости на больших расстояниях (long-range dependencies)
- Могут избирательно сохранять важную информацию и забывать неважную
- Позволяют моделировать контекст на различных временных масштабах одновременно

**3. Адаптивность к различным типам данных**

LSTM эффективно работают с разнообразными типами последовательных данных:
- Текст и речь (машинный перевод, распознавание речи)
- Временные ряды (финансовые прогнозы, данные сенсоров)
- Биологические последовательности (анализ ДНК, белков)
- Мультимодальные данные (подписи к изображениям, видеоаналитика)

**4. Масштабируемость и гибкость**

- Возможность создания глубоких архитектур путем штабелирования LSTM слоев
- Комбинируемость с другими типами нейронных сетей (CNN, Attention)
- Эффективная параллелизация обучения на современном оборудовании

**5. Промышленные результаты**

До появления трансформеров (2017-2018), LSTM были абсолютным стандартом в индустрии и научных исследованиях:

- **Google** (2015-2016): использовал LSTM в системах распознавания речи, сократив ошибки на 30%
- **Apple**: внедрил LSTM в Siri для улучшения понимания контекста
- **Facebook**: применял LSTM для автоматического перевода сообщений
- **Amazon**: использовал LSTM в рекомендательных системах и прогнозировании спроса

Даже после появления трансформеров, LSTM остаются востребованными в ряде областей:
- Обработка потоковых данных в реальном времени
- Задачи с ограниченными вычислительными ресурсами
- Приложения, требующие интерпретируемости модели

**Эволюция популярности LSTM** показывает их значимость: от академических исследований в конце 1990-х до промышленного доминирования в середине 2010-х, и последующую интеграцию с архитектурами на основе внимания (attention).

## 2. Архитектура LSTM: Как это работает?

### 2.1 Интуиция: Метафора конвейера с контролируемыми воротами

Чтобы понять принцип работы LSTM, представим его как умную производственную линию с системой конвейеров и ворот:

**1. Основной конвейер — ячейка памяти (cell state)**

Ключевой компонент LSTM – это **состояние ячейки (cell state)** – горизонтальная линия, проходящая через всю цепочку. Представьте длинный конвейер (или конвейерную ленту), который тянется через всю фабрику (последовательность). На этом конвейере перемещается "контейнер с информацией" (cell state, $C_t$).

![Image_01.png](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/LSTM/Image_01.png)

**Особенности состояния ячейки:**  
- Она проходит напрямую через всю цепочку, участвуя лишь в нескольких линейных преобразованиях.  
- Информация может легко течь по ней, не подвергаясь изменениям.  
- Этот контейнер может сохранять информацию практически неизменной на протяжении долгого времени — в этом главный секрет LSTM.  

Тем не менее, LSTM может **удалять информацию** из состояния ячейки; этот процесс регулируется структурами, называемыми **фильтрами (gates)**.  

**2. Система управляемых ворот (gates)**

На каждом шаге (временной точке) наш конвейер проходит через три контрольных пункта:

- **Вентиль забывания (Forget Gate)**: действует как фильтр, решающий, какую информацию удалить из контейнера. Представьте рабочего, который смотрит на содержимое контейнера и текущий вход, а затем решает, что выбросить. "Нам всё ещё нужно помнить пол субъекта, чтобы правильно согласовывать местоимения? Да, сохраняем. А информация о цвете его машины? Нет, выбрасываем."

- **Вентиль входа (Input Gate)**: определяет, какую новую информацию добавить в контейнер. Представьте другого рабочего, который, посмотрев на текущий вход и предыдущий выход, решает, какие новые факты достаточно важны, чтобы их запомнить. "Мы только что узнали имя нового персонажа в рассказе? Это важно, добавляем в контейнер."

- **Вентиль выхода (Output Gate)**: контролирует, какую часть содержимого контейнера передать внешнему миру (в виде скрытого состояния). Представьте третьего рабочего, который решает, какие части накопленной информации наиболее актуальны прямо сейчас. "Нам нужно предсказать следующее слово — важно знать, что подлежащее в единственном числе, но не важно знать, о какой стране шла речь ранее."

**3. Двойная система состояний**

В отличие от ванильной RNN, у LSTM есть две линии передачи информации:

- **Ячейка памяти (Cell State)** $C_t$: основной конвейер, предназначенный для долговременного хранения информации.
- **Скрытое состояние (Hidden State)** $h_t$: фильтрованная версия ячейки памяти, содержащая только ту информацию, которую сеть считает актуальной в текущий момент.

**Метафора с записной книжкой:**

Другой способ представить LSTM — это человек с записной книжкой (cell state), который постоянно решает:
- Какие старые заметки стереть (forget gate)
- Какие новые заметки записать (input gate)
- Какую информацию из книжки использовать при ответе на вопрос (output gate)

Эта система позволяет LSTM действовать как умный накопитель информации, избирательно запоминающий важные факты и игнорирующий неважные детали — именно то, что нужно для обработки длинных последовательностей.

### 2.2 Формализация и обозначения: определение размерностей и переменных

Давайте формализуем архитектуру LSTM, четко определив все компоненты и их размерности. Это поможет как в понимании структуры, так и при последующей реализации.

**Основные обозначения:**

| **Символ** | **Размерность** | **Описание** |
|------------|-----------------|--------------|
| $x_t$ | $\mathbb{R}^{d_x}$ | Входной вектор на шаге $t$ |
| $h_t$ | $\mathbb{R}^{d_h}$ | Скрытое состояние на шаге $t$ |
| $C_t$ | $\mathbb{R}^{d_h}$ | Состояние ячейки на шаге $t$ |
| $f_t$ | $\mathbb{R}^{d_h}$ | Активация вентиля забывания на шаге $t$ |
| $i_t$ | $\mathbb{R}^{d_h}$ | Активация вентиля входа на шаге $t$ |
| $o_t$ | $\mathbb{R}^{d_h}$ | Активация вентиля выхода на шаге $t$ |
| $\tilde{C}_t$ | $\mathbb{R}^{d_h}$ | Кандидат-вектор новых значений ячейки |

**Весовые матрицы и векторы смещения:**

| **Символ** | **Размерность** | **Описание** |
|------------|-----------------|--------------|
| $W_f$ | $\mathbb{R}^{d_h \times (d_x + d_h)}$ | Веса для вентиля забывания |
| $W_i$ | $\mathbb{R}^{d_h \times (d_x + d_h)}$ | Веса для вентиля входа |
| $W_C$ | $\mathbb{R}^{d_h \times (d_x + d_h)}$ | Веса для кандидат-вектора |
| $W_o$ | $\mathbb{R}^{d_h \times (d_x + d_h)}$ | Веса для вентиля выхода |
| $b_f$ | $\mathbb{R}^{d_h}$ | Смещение для вентиля забывания |
| $b_i$ | $\mathbb{R}^{d_h}$ | Смещение для вентиля входа |
| $b_C$ | $\mathbb{R}^{d_h}$ | Смещение для кандидат-вектора |
| $b_o$ | $\mathbb{R}^{d_h}$ | Смещение для вентиля выхода |

**Функции активации:**
- $\sigma$: сигмоидальная функция, отображает входы в диапазон [0, 1]
- $\tanh$: гиперболический тангенс, отображает входы в диапазон [-1, 1]

**Размерности входных и выходных данных:**
- $d_x$: размерность входного вектора $x_t$
- $d_h$: размерность скрытого состояния и состояния ячейки

**Конкатенация входа и предыдущего состояния:**

Для упрощения записи мы часто используем конкатенацию входного вектора $x_t$ и предыдущего скрытого состояния $h_{t-1}$:

$$[x_t, h_{t-1}] \in \mathbb{R}^{d_x + d_h}$$

Это позволяет нам определить веса как одну матрицу для каждого вентиля, вместо разделения на отдельные матрицы для $x_t$ и $h_{t-1}$.

**Примечания по размерностям:**

1. В стандартной архитектуре LSTM размерность состояния ячейки $C_t$ равна размерности скрытого состояния $h_t$. В некоторых вариациях они могут различаться.

2. Все вентили ($f_t$, $i_t$, $o_t$) имеют одинаковую размерность $d_h$, что позволяет им поэлементно контролировать состояние ячейки.

3. Общее количество параметров в стандартном LSTM:
   - Веса: $4 \times d_h \times (d_x + d_h)$
   - Смещения: $4 \times d_h$
   - Итого: $4 \times d_h \times (d_x + d_h + 1)$

Эти обозначения мы будем использовать в следующих разделах для описания математических формул и динамики LSTM.

### 2.3 Динамика одного шага

Теперь рассмотрим, как точно вычисляются все компоненты LSTM на одном временном шаге $t$. Разберем каждый элемент архитектуры подробно.

### Вентиль забывания (forget gate)

Вентиль забывания $f_t$ определяет, какую информацию из предыдущего состояния ячейки $C_{t-1}$ следует сохранить, а какую — стереть:

$$
f_t = \sigma\big(W_f \cdot [x_t, h_{t-1}] + b_f\big)
$$

Здесь:
- $[x_t, h_{t-1}]$ — конкатенация текущего входа и предыдущего скрытого состояния
- $W_f$ — весовая матрица для вентиля забывания
- $b_f$ — вектор смещения
- $\sigma$ — сигмоидальная функция, возвращающая значения в интервале [0, 1]

Результат $f_t$ представляет собой вектор значений между 0 и 1, где:
- **1** означает "полностью сохранить эту информацию"
- **0** означает "полностью забыть эту информацию"

### **Пример:** 

Если модель работает с текстом и в какой-то момент понимает, что начинается новое предложение, вентиль забывания может "обнулить" часть информации о предыдущем предложении.

![Image_02.png](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/LSTM/Image_02.png)

**Контекст:** Обработка последовательности "Я люблю море. На небе светит солнце."

1. **Состояние после первого предложения:**
   - $C_{t-1}$ (состояние ячейки) кодирует информацию о первом предложении: [0.8, 0.6, -0.2]
     - 0.8 → факт "море" (существительное)
     - 0.6 → эмоция "любовь"
     - -0.2 → местоимение "я"

2. **Обработка слова "На" (начало нового предложения):**
   - Вход $x_t$ = embedding слова "На" [0.1, -0.3, 0.5]
   - $h_{t-1}$ = предыдущее скрытое состояние [0.7, 0.5, -0.1]
   - Вентиль забывания вычисляет:
     $$
     f_t = \sigma\left(
     \begin{bmatrix}
     0.2 & 0.4 & -0.1 \\
     -0.3 & 0.6 & 0.2 \\
     0.1 & -0.2 & 0.3
     \end{bmatrix}
     \cdot
     \begin{bmatrix}
     0.1 \\ -0.3 \\ 0.5 \\ 0.7 \\ 0.5 \\ -0.1
     \end{bmatrix}
     +
     \begin{bmatrix}
     0.1 \\ -0.2 \\ 0.3
     \end{bmatrix}
     \right) = [0.1, 0.9, 0.8]
     $$
     - Первый нейрон (0.1) → забыть информацию о местоимении (уже не нужно)
     - Второй нейрон (0.9) → сохранить эмоциональный контекст
     - Третий нейрон (0.8) → сохранить информацию о существительном

3. **Обновленное состояние ячейки:**
   - $C_t = f_t \odot C_{t-1} = [0.1, 0.9, 0.8] \odot [0.8, 0.6, -0.2] = [0.08, 0.54, -0.16]$
     - Значение 0.8 (море) уменьшилось до 0.08 → забыто
     - Эмоция 0.6 сохранилась как 0.54
     - Местоимение -0.2 стало -0.016 → почти забыто

**Интерпретация:** Модель решила:
- Сохранить эмоциональный контекст (может пригодиться для анализа тональности)
- Забыть конкретные существительные из предыдущего предложения
- Подготовиться к новой синтаксической структуре (новое предложение)

**Визуализация векторов:**
```
До forget gate:    [ 0.80  0.60  -0.20 ]
After forget gate: [ 0.08  0.54  -0.02 ]
                   │      │      └── Почти забыто ("я")
                   │      └────────── Сохранено ("любовь")
                   └───────────────── Забыто ("море")
```

---

### Вентиль входа (input gate)

Вентиль входа $i_t$ решает, какую новую информацию добавить в состояние ячейки:

$$
i_t = \sigma\big(W_i \cdot [x_t, h_{t-1}] + b_i\big)
$$

Как и в случае с вентилем забывания, результатом является вектор значений в интервале [0, 1], где:
- **1** означает "полностью добавить эту новую информацию"
- **0** означает "не добавлять эту информацию"

#### Кандидат-вектор состояния ячейки

![Image_03.png](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/LSTM/Image_03.png)

Параллельно с вентилем входа, создается кандидат-вектор $\tilde{C}_t$ — "черновик" новой информации, которую потенциально можно добавить в состояние ячейки:

$$
\tilde{C}_t = \tanh\big(W_C \cdot [x_t, h_{t-1}] + b_C\big)
$$

Здесь используется функция $\tanh$, чтобы значения были нормализованы в диапазоне [-1, 1].

#### Обновление состояния ячейки

![Image_04.png](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/LSTM/Image_04.png)

Теперь мы готовы обновить состояние ячейки $C_t$, используя вентиль забывания, вентиль входа и кандидат-вектор:

$$
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
$$

где $\odot$ обозначает поэлементное умножение (умножение Адамара).

Это уравнение описывает ключевой механизм LSTM:
1. $f_t \odot C_{t-1}$ — старая информация, которую мы решили сохранить
2. $i_t \odot \tilde{C}_t$ — новая информация, которую мы решили добавить

**Важно!** Состояние ячейки $C_t$ обновляется с помощью только линейных операций (умножение и сложение). Это гарантирует, что градиент может течь через ячейку без затухания, решая проблему ванильных RNN.

### **Пример:**

Продолжим обработку последовательности "Я люблю море. На небе светит солнце." после применения вентиля забывания.

1. **Текущее состояние после вентиля забывания:**
   - $f_t \odot C_{t-1} = [0.08, 0.54, -0.16]$ — часть информации о предыдущем предложении сохранена

2. **Обработка слова "На" (начало нового предложения):**
   - Вход $x_t$ = embedding слова "На" [0.1, -0.3, 0.5]
   - $h_{t-1}$ = предыдущее скрытое состояние [0.7, 0.5, -0.1]
   
   - Вентиль входа вычисляет:
     $$
     i_t = \sigma\left(
     \begin{bmatrix}
     0.3 & -0.2 & 0.1 \\
     0.5 & 0.4 & -0.3 \\
     -0.1 & 0.7 & 0.2
     \end{bmatrix}
     \cdot
     \begin{bmatrix}
     0.1 \\ -0.3 \\ 0.5 \\ 0.7 \\ 0.5 \\ -0.1
     \end{bmatrix}
     +
     \begin{bmatrix}
     -0.1 \\ 0.2 \\ 0.1
     \end{bmatrix}
     \right) = [0.7, 0.6, 0.9]
     $$
     
   - Кандидат-вектор нового состояния:
     $$
     \tilde{C}_t = \tanh\left(
     \begin{bmatrix}
     0.4 & 0.1 & -0.3 \\
     -0.2 & 0.5 & 0.3 \\
     0.3 & -0.4 & 0.2
     \end{bmatrix}
     \cdot
     \begin{bmatrix}
     0.1 \\ -0.3 \\ 0.5 \\ 0.7 \\ 0.5 \\ -0.1
     \end{bmatrix}
     +
     \begin{bmatrix}
     0.2 \\ -0.1 \\ 0.4
     \end{bmatrix}
     \right) = [0.6, -0.3, 0.7]
     $$
     - Первый нейрон (0.6) → информация о месте "небо" (существительное)
     - Второй нейрон (-0.3) → нейтральная эмоциональная тональность
     - Третий нейрон (0.7) → предлог "на" (указывает на местоположение)

3. **Обновленное состояние ячейки с новой информацией:**
   - $i_t \odot \tilde{C}_t = [0.7, 0.6, 0.9] \odot [0.6, -0.3, 0.7] = [0.42, -0.18, 0.63]$
   
   - Итоговое состояние ячейки:
     $C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t = [0.08, 0.54, -0.16] + [0.42, -0.18, 0.63] = [0.50, 0.36, 0.47]$
     - Значение 0.08 (забытое "море") + 0.42 (новое "небо") = 0.50 → новая информация о месте
     - Эмоция 0.54 (сохраненная "любовь") + (-0.18) (нейтральность) = 0.36 → снижение эмоциональной окраски
     - Значение -0.16 (почти забытое "я") + 0.63 (новое "на") = 0.47 → переход от субъекта к локации

**Интерпретация:** Модель:
- Добавила новую информацию о небе, которое становится новым существительным
- Снизила эмоциональную окраску, переходя к более нейтральному описанию
- Переключила фокус с субъекта ("я") на пространственное отношение ("на")

**Визуализация векторов:**
```
После forget gate:  [ 0.08  0.54  -0.16 ]
Новая информация:   [ 0.42  -0.18  0.63 ]
Итоговое состояние: [ 0.50  0.36  0.47 ]
                     │      │      └── Новый фокус (пространство "на")
                     │      └───────── Снижение эмоциональности
                     └──────────────── Новое существительное ("небо")
```
---

#### Вентиль выхода (output gate)

![Image_05.png](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/LSTM/Image_05.png)

Вентиль выхода $o_t$ определяет, какую часть обновленного состояния ячейки передать в выходное скрытое состояние:

$$
o_t = \sigma\big(W_o \cdot [x_t, h_{t-1}] + b_o\big)
$$

Как и другие вентили, $o_t$ содержит значения в интервале [0, 1].

#### Скрытое состояние

Наконец, вычисляем новое скрытое состояние $h_t$, применяя вентиль выхода к нормализованному состоянию ячейки:

$$
h_t = o_t \odot \tanh(C_t)
$$

Здесь:
- $\tanh(C_t)$ нормализует значения состояния ячейки до диапазона [-1, 1]
- $o_t$ определяет, какие компоненты этого нормализованного состояния передать дальше

Скрытое состояние $h_t$ используется как для предсказания выхода на текущем шаге, так и как вход для следующего шага сети.

**Итог: полный набор формул LSTM**

Для удобства, вот полный набор формул, описывающих один шаг LSTM:

$$
\begin{align}
f_t &= \sigma(W_f \cdot [x_t, h_{t-1}] + b_f) \\
i_t &= \sigma(W_i \cdot [x_t, h_{t-1}] + b_i) \\
\tilde{C}_t &= \tanh(W_C \cdot [x_t, h_{t-1}] + b_C) \\
C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\
o_t &= \sigma(W_o \cdot [x_t, h_{t-1}] + b_o) \\
h_t &= o_t \odot \tanh(C_t)
\end{align}
$$

Эти шесть уравнений полностью описывают динамику LSTM ячейки на одном временном шаге.

## 3. Математические основы и функционирование LSTM

### 3.1 Роль сигмоидальных функций: Почему именно сигмоида для вентилей

Сигмоидальная функция играет ключевую роль в архитектуре LSTM, особенно в механизме вентилей. Разберем, почему именно эта функция активации используется для всех трех вентилей (забывания, входа и выхода).

![Image_06.png](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/LSTM/Image_06.jpg)

**Математическое определение сигмоидальной функции:**

$\sigma(x) = \frac{1}{1 + e^{-x}}$

**Ключевые свойства сигмоиды, делающие её идеальной для вентилей:**

1. **Ограниченный диапазон выходных значений [0, 1]**
   - Это свойство критически важно для вентилей, так как они должны выполнять функцию "фильтра"
   - Значение 0 означает "полностью блокировать информацию"
   - Значение 1 означает "полностью пропустить информацию"
   - Промежуточные значения позволяют частично пропускать информацию

2. **Гладкость и дифференцируемость**
   - Сигмоида непрерывна и дифференцируема на всей области определения
   - Её производная имеет простую форму: $\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))$
   - Это свойство важно для обратного распространения градиента при обучении

3. **Нелинейность и насыщение**
   - При больших положительных значениях $x$ функция стремится к 1
   - При больших отрицательных значениях $x$ функция стремится к 0
   - Это создает эффект "насыщения", который стабилизирует динамику сети

**Практическое применение в вентилях LSTM:**

- **Вентиль забывания ($f_t$)**: сигмоида определяет, какой процент каждого элемента в состоянии ячейки сохранить. Значение 0 означает "забыть полностью", 1 — "сохранить полностью".

- **Вентиль входа ($i_t$)**: сигмоида контролирует, какую часть новой информации ($\tilde{C}_t$) добавить в состояние ячейки. Значение 0 означает "не добавлять ничего", 1 — "добавить полностью".

- **Вентиль выхода ($o_t$)**: сигмоида регулирует, какую часть информации из состояния ячейки передать в скрытое состояние $h_t$. Значение 0 означает "ничего не передавать", 1 — "передать всё".

**Примечание по инициализации смещений:**

Важно отметить, что смещения (bias) вентилей часто инициализируются специальным образом:
- Смещение вентиля забывания ($b_f$) часто инициализируется положительными значениями (например, 1 или 2), чтобы в начале обучения сеть была склонна "помнить" информацию
- Смещения других вентилей обычно инициализируются нулями или небольшими случайными значениями

Такая инициализация помогает LSTM быстрее обучаться работе с долговременными зависимостями.

### 3.2 Функция активации tanh: Её роль в кандидат-векторе и выходе

Гиперболический тангенс (tanh) — вторая ключевая функция активации в архитектуре LSTM. Она используется в двух важных местах: при создании кандидат-вектора и при формировании выходного скрытого состояния.

![Image_07.png](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_%26_18/assets/LSTM/Image_07.JPG)

**Математическое определение функции tanh:**

$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$

**Ключевые свойства tanh, важные для LSTM:**

1. **Ограниченный диапазон выходных значений [-1, 1]**
   - В отличие от сигмоиды, tanh симметрична относительно начала координат
   - Диапазон [-1, 1] позволяет представлять как положительные, так и отрицательные активации с равной амплитудой

2. **Крутизна градиента**
   - Производная tanh в нуле равна 1, что больше, чем у сигмоиды (0.25)
   - Это обеспечивает более сильный градиент при обратном распространении

3. **Нулевое среднее значение**
   - Выходы функции tanh имеют приблизительно нулевое среднее значение
   - Это помогает бороться с проблемой смещения при обучении (covariate shift)

**Роль tanh в кандидат-векторе $\tilde{C}_t$:**

$\tilde{C}_t = \tanh(W_C \cdot [x_t, h_{t-1}] + b_C)$

1. **Нормализация значений**
   - tanh приводит все значения к диапазону [-1, 1], что создает стабильную динамику в ячейке
   - Это предотвращает неконтролируемый рост значений в состоянии ячейки

2. **Биполярность представления**
   - Отрицательные значения могут представлять "ингибирующие" сигналы
   - Положительные значения могут представлять "возбуждающие" сигналы
   - Это важно для создания богатого внутреннего представления данных

3. **Баланс с сигмоидой вентиля входа**
   - tanh создает кандидат-значения в диапазоне [-1, 1]
   - Сигмоида вентиля входа определяет, какую часть этих значений добавить
   - Это позволяет как добавлять, так и вычитать информацию из состояния ячейки

**Роль tanh при формировании скрытого состояния $h_t$:**

$h_t = o_t \odot \tanh(C_t)$

1. **Нормализация выходных значений**
   - Состояние ячейки $C_t$ может содержать значения с большой амплитудой
   - tanh нормализует эти значения перед выходом, что важно для стабильности последующих слоев

2. **Балансировка активаций**
   - tanh обеспечивает симметричный выход для положительных и отрицательных значений в $C_t$
   - Это полезно для последующих слоев, которые часто лучше работают с центрированными входами

3. **Интерпретируемость выхода**
   - Скрытое состояние $h_t$ используется для предсказаний и как входной сигнал для следующего шага
   - Нормализованный диапазон [-1, 1] обеспечивает согласованное масштабирование этих сигналов

**Сравнение с другими функциями активации:**

Почему именно tanh, а не другие функции активации, такие как ReLU?

- **ReLU** не ограничивает верхний предел выходных значений, что может привести к неконтролируемому росту активаций
- **Leaky ReLU** имеет неограниченный диапазон и асимметричный отклик, что менее подходит для состояния ячейки
- **Sigmoid** ограничивает выход только положительными значениями, что уменьшает выразительность модели

**Практический аспект:**

В некоторых современных вариациях LSTM функция tanh может заменяться на другие активации для кандидат-вектора, но в классической архитектуре и большинстве практических реализаций tanh остается стандартным выбором, благодаря её математическим свойствам, которые хорошо согласуются с природой задачи обработки последовательностей.

### 3.3 Поток градиентов в LSTM: как архитектура решает проблему затухающего градиента

Ключевое преимущество LSTM перед обычными RNN — способность эффективно обрабатывать длинные последовательности без проблемы затухающего градиента. Рассмотрим, как именно LSTM решает эту фундаментальную проблему на уровне потока градиентов.

**Напомним проблему в ванильных RNN:**

В обычной RNN градиент потери по скрытому состоянию $h_{t-k}$ включает произведение множества якобианов:

$$\frac{\partial h_t}{\partial h_{t-k}} = \prod_{i=t-k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}} = \prod_{i=t-k+1}^{t} \text{diag}(\tanh'(a_i)) \cdot W_{hh}$$

Эти якобианы обычно имеют собственные значения меньше 1, что приводит к экспоненциальному затуханию градиента при увеличении $k$.

**Ключевая инновация LSTM: Константный поток ошибки**

Главная особенность LSTM — **Константный поток ошибки (Constant Error Carousel, CEC)**, который обеспечивается прямым линейным соединением через состояние ячейки $C_t$.

Рассмотрим поток градиента через состояние ячейки от момента $t$ к моменту $t-1$:

$$\frac{\partial C_t}{\partial C_{t-1}} = \frac{\partial (f_t \odot C_{t-1} + i_t \odot \tilde{C}_t)}{\partial C_{t-1}} = f_t$$

Это выражение показывает критическое свойство LSTM: **градиент от $C_t$ к $C_{t-1}$ проходит через простое поэлементное умножение на вентиль забывания $f_t$**. Не используются ни нелинейные функции активации, ни матричные умножения — только прямое поэлементное умножение.

**Рекуррентное распространение градиента через несколько шагов:**

При распространении градиента через $k$ шагов назад имеем:

$$\frac{\partial C_t}{\partial C_{t-k}} = \prod_{i=t-k+1}^{t} \frac{\partial C_i}{\partial C_{i-1}} = \prod_{i=t-k+1}^{t} f_i$$

Это произведение векторов вентилей забывания (применяемое поэлементно).

**Как это решает проблему затухающего градиента:**

1. **Контроль потока градиентов через $f_t$**
   - Если компоненты $f_t$ близки к 1, градиент протекает почти без затухания
   - LSTM обучается устанавливать $f_t \approx 1$ для важной информации

2. **Аддитивное обновление состояния ячейки**
   - В отличие от мультипликативных рекуррентных соединений в ванильных RNN, LSTM использует аддитивное обновление:
     $C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$
   - Это позволяет градиентам течь без обязательного умножения на вес рекуррентной связи

3. **Механизм обучаемого "забывания"**
   - Вместо фиксированного затухания, LSTM обучается тому, что необходимо помнить, а что можно забыть
   - Это как избирательный "шлюз", который пропускает важные градиенты и блокирует неважные

**Математическое моделирование потока градиентов:**

Полный градиент потери $L$ по состоянию ячейки $C_{t-k}$ можно разложить:

$$\frac{\partial L}{\partial C_{t-k}} = \sum_{j=t-k+1}^{T} \frac{\partial L}{\partial C_j} \frac{\partial C_j}{\partial C_{t-k}}$$

Здесь первый член $\frac{\partial L}{\partial C_j}$ — это обратное распространение от потери к состоянию ячейки в момент $j$, а второй член $\frac{\partial C_j}{\partial C_{t-k}}$ — это произведение вентилей забывания по пути от $t-k$ до $j$.

**Практические следствия:**

1. **Длинные зависимости**
   - LSTM может обучаться зависимостям на сотни и даже тысячи шагов, что невозможно для ванильных RNN
   - Например, LSTM может связать "Франция" в начале текста с "французский" в конце, даже если между ними большой промежуток

2. **Выборочная чувствительность**
   - LSTM обучается быть чувствительной только к важным долговременным зависимостям
   - Это более эффективно, чем попытка запоминать всё подряд

3. **Стабильность обучения**
   - Контролируемый поток градиентов делает обучение LSTM более стабильным
   - Реже требуется gradient clipping или специальная инициализация весов

Таким образом, уникальная архитектура LSTM с константным потоком ошибки через состояние ячейки и обучаемыми вентилями эффективно решает проблему затухающего градиента, что позволяет моделировать долговременные зависимости в последовательных данных.

### 3.4 Сравнение с ванильной RNN: Математический взгляд на преимущества

Давайте проведем строгое математическое сравнение LSTM и ванильной RNN, чтобы лучше понять, почему LSTM справляется с задачей обработки последовательностей значительно лучше.

**1. Архитектурное сравнение основных уравнений**

**Ванильная RNN:**
$h_t = \tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h)$

**LSTM:**
$
\begin{align}
f_t &= \sigma(W_f \cdot [x_t, h_{t-1}] + b_f) \\
i_t &= \sigma(W_i \cdot [x_t, h_{t-1}] + b_i) \\
\tilde{C}_t &= \tanh(W_C \cdot [x_t, h_{t-1}] + b_C) \\
C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\
o_t &= \sigma(W_o \cdot [x_t, h_{t-1}] + b_o) \\
h_t &= o_t \odot \tanh(C_t)
\end{align}
$

**Ключевые различия:**

- **Одно vs несколько уравнений**: RNN использует одно уравнение, в то время как LSTM разделяет обновление на несколько специализированных компонентов.
- **Одно vs два состояния**: RNN имеет только скрытое состояние $h_t$, тогда как LSTM разделяет информацию между скрытым состоянием $h_t$ и состоянием ячейки $C_t$.
- **Простое vs адаптивное обновление**: RNN всегда обновляет всё скрытое состояние целиком, а LSTM избирательно обновляет компоненты состояния ячейки.

**2. Поток информации во времени**

**Ванильная RNN:**

Информация от входа $x_{t-k}$ к текущему скрытому состоянию $h_t$ проходит через цепочку нелинейных преобразований:

$h_t = F(h_{t-1}, x_t) = F(F(h_{t-2}, x_{t-1}), x_t) = ... = F(F(...F(h_{t-k-1}, x_{t-k})...), x_t)$

Здесь $F(h, x) = \tanh(W_{xh}x + W_{hh}h + b_h)$. Каждое применение нелинейности $\tanh$ может приводить к потере информации.

**LSTM:**

Информация может течь через состояние ячейки $C_t$ с контролируемым забыванием:

$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t = f_t \odot (f_{t-1} \odot C_{t-2} + i_{t-1} \odot \tilde{C}_{t-1}) + i_t \odot \tilde{C}_t = ...$

Раскрывая это выражение дальше, получаем:

$C_t = \left( \prod_{j=t-k+1}^{t} f_j \right) \odot C_{t-k} + \sum_{j=t-k+1}^{t} \left( i_j \odot \tilde{C}_j \odot \prod_{l=j+1}^{t} f_l \right)$

Это показывает, что состояние ячейки $C_t$ является взвешенной суммой всех предыдущих входов, где веса определяются произведениями вентилей $f_j$. Если все $f_j \approx 1$, информация может протекать почти без искажений.

**3. Математический анализ потока градиентов**

**Ванильная RNN:**

Градиент потери $L$ по весам $W_{hh}$ вычисляется с помощью цепного правила:

$\frac{\partial L}{\partial W_{hh}} = \sum_{k=1}^{t} \frac{\partial L}{\partial h_t} \frac{\partial h_t}{\partial h_{t-k}} \frac{\partial h_{t-k}}{\partial W_{hh}}$

Где вторая производная содержит произведение якобианов:

$\frac{\partial h_t}{\partial h_{t-k}} = \prod_{j=t-k+1}^{t} \frac{\partial h_j}{\partial h_{j-1}} = \prod_{j=t-k+1}^{t} \text{diag}(\tanh'(W_{xh}x_j + W_{hh}h_{j-1} + b_h)) \cdot W_{hh}$

Собственные значения этого произведения обычно меньше 1, что приводит к затуханию градиента.

**LSTM:**

У LSTM градиент от $C_t$ к $C_{t-k}$ вычисляется как:

$\frac{\partial C_t}{\partial C_{t-k}} = \prod_{j=t-k+1}^{t} \frac{\partial C_j}{\partial C_{j-1}} = \prod_{j=t-k+1}^{t} f_j$

Поскольку $f_j$ — это результат сигмоидальной функции, обученной специально для контроля потока информации, LSTM может поддерживать значения $f_j \approx 1$ для важных компонентов, что предотвращает затухание градиента.

**4. Количественное сравнение параметров и вычислительной сложности**

**Ванильная RNN:**
- Количество параметров: $d_h \times (d_x + d_h + 1)$
- Вычислительная сложность на шаг: $O(d_h \times (d_x + d_h))$

**LSTM:**
- Количество параметров: $4 \times d_h \times (d_x + d_h + 1)$
- Вычислительная сложность на шаг: $O(4 \times d_h \times (d_x + d_h))$

LSTM требует примерно в 4 раза больше параметров и вычислений, но это компенсируется значительным улучшением производительности на задачах с долговременными зависимостями.

**5. Эмпирическое сравнение возможностей**

| **Свойство** | **Ванильная RNN** | **LSTM** |
|--------------|-------------------|----------|
| Максимальная длина зависимостей | 5-10 шагов | Сотни или тысячи шагов |
| Устойчивость к шуму | Низкая | Высокая |
| Способность забывать неважную информацию | Ограниченная | Высокая |
| Адаптивность к различным временным масштабам | Низкая | Высокая |

**6. Геометрическая интерпретация**

Если представить пространство скрытых состояний как многомерное пространство, то:

- **Ванильная RNN** создает сложную нелинейную динамику, в которой траектории могут быстро сходиться к аттракторам, что приводит к потере информации о прошлых состояниях.

- **LSTM** создает управляемую динамику, где важные направления в пространстве состояний могут сохраняться почти без изменений, позволяя информации течь на большие расстояния, в то время как неважные направления могут быстро затухать.

В целом, LSTM представляет собой глубоко продуманное расширение архитектуры RNN, которое целенаправленно устраняет ключевые математические ограничения ванильных RNN, особенно проблему затухающего градиента, что делает LSTM значительно более мощным инструментом для моделирования последовательностей с долговременными зависимостями.

</details>

<details> 
    <summary><em><strong>Управляемый рекуррентный блок (GRU)</strong></em></summary>

## 1. Введение и мотивация

### 1.1 Контекст появления GRU: история создания как упрощения LSTM

Gated Recurrent Unit (GRU) — это разновидность рекуррентной нейронной сети, которая была представлена миру в 2014 году. GRU возникла в контексте активных исследований в области нейронного машинного перевода и обработки естественного языка, в период, когда LSTM (Long Short-Term Memory) уже зарекомендовали себя как эффективные модели для работы с последовательными данными.

**Хронология появления GRU:**

- **2013-2014**: в рамках развития архитектур нейронного машинного перевода исследователи из Монреаля начали экспериментировать с вариациями рекуррентных сетей.
  
- **Июнь 2014**: Кюнхён Чо и соавторы публикуют статью ["Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"](https://arxiv.org/abs/1406.1078), в которой впервые представлена архитектура GRU.

- **Сентябрь 2014**: в статье ["Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"](https://arxiv.org/abs/1412.3555) Юаша Чунг и др. проводят первое систематическое сравнение LSTM и GRU.

GRU возникла из потребности в более простой, но при этом не менее эффективной альтернативе LSTM. Исследователи искали архитектуру, которая могла бы:

- Обрабатывать долговременные зависимости в последовательностях
- Быть вычислительно более эффективной и проще для обучения
- Требовать меньше параметров при сохранении выразительной способности

Ключевым наблюдением было то, что не все компоненты сложной архитектуры LSTM необходимы для достижения хороших результатов. Это привело к созданию GRU, которая объединяет функциональность нескольких вентилей LSTM и упрощает общий механизм обновления состояния.

### 1.2 Авторы и ключевые публикации: Работы Чо и коллег, связь с машинным переводом

GRU неразрывно связана с именем **Кюнхёна Чо** (Kyunghyun Cho) и его коллегами из Монреальского университета. Исследовательская группа, работавшая над созданием GRU, включала таких учёных как Йошуа Бенджио (Yoshua Bengio), Дмитрий Бахданау (Dzmitry Bahdanau) и другие видные исследователи в области глубокого обучения.

**Основополагающие публикации:**

1. **"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation" (Чо и др., 2014)**
   - Первое представление GRU в контексте архитектуры энкодер-декодер для машинного перевода
   - Демонстрация возможности обучения представлений фраз на уровне предложений
   - Сравнение с базовой RNN и показ преимуществ механизма вентилей

2. **"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling" (Чунг и др., 2014)**
   - Систематическое сравнение LSTM и GRU на задачах моделирования полифонической музыки и обработки речевых сигналов
   - Вывод о сопоставимой производительности GRU с LSTM при меньшей вычислительной сложности

3. **"Neural Machine Translation by Jointly Learning to Align and Translate" (Бахданау, Чо и Бенджио, 2014)**
   - Применение GRU в архитектуре с механизмом внимания для машинного перевода
   - Демонстрация возможностей GRU в комбинации с механизмом внимания

**Связь с машинным переводом:**

Появление GRU тесно связано с развитием нейронного машинного перевода (NMT). В 2014 году исследователи активно искали альтернативы статистическим методам машинного перевода, и рекуррентные нейронные сети показывали многообещающие результаты.

GRU была разработана специально в контексте модели энкодер-декодер для NMT, где:
- **Энкодер** кодирует входное предложение в фиксированный вектор
- **Декодер** генерирует перевод на основе этого вектора

Работа Чо и соавторов показала, что GRU может эффективно кодировать семантическую и синтаксическую информацию предложений, что критически важно для качественного перевода. Более того, упрощенная структура GRU позволяла обучать более глубокие модели и масштабировать их на большие объемы данных, что было важно для практических приложений машинного перевода.

Цитата Кюнхёна Чо о создании GRU:
> "Мы стремились создать более простую альтернативу LSTM, которая могла бы быть легче в обучении и более эффективной вычислительно, сохраняя при этом способность моделировать долговременные зависимости в последовательных данных."

### 1.3 Баланс сложности и эффективности: Почему возникла потребность в более компактных моделях

В начале 2010-х годов LSTM стали стандартом де-факто для задач обработки последовательностей, но они имели несколько ограничений, которые стимулировали поиск более компактных альтернатив:

**Проблемы, связанные со сложностью LSTM:**

1. **Вычислительные требования**
   - LSTM имеет четыре набора весов и смещений (для вентилей забывания, входа, выхода и кандидат-вектора)
   - Обучение больших LSTM моделей требовало значительных вычислительных ресурсов
   - Время обработки одного элемента последовательности было критично для приложений реального времени

2. **Сложность обучения**
   - Большее количество параметров означало большее пространство поиска при оптимизации
   - LSTM часто требовали более тщательной настройки гиперпараметров
   - Сложнее добиться сходимости на ограниченных наборах данных

3. **Память и энергопотребление**
   - Модели с LSTM занимали больше памяти при развертывании
   - Это ограничивало их применение на мобильных и встраиваемых устройствах
   - Высокое энергопотребление при вычислениях

4. **Теоретическое понимание**
   - Сложная архитектура LSTM затрудняла анализ её поведения
   - Не всегда было ясно, какие компоненты вносят наибольший вклад в эффективность

**Почему потребовались более компактные модели:**

1. **Развитие мобильных и встраиваемых систем**
   - Рост потребности в моделях, способных работать на устройствах с ограниченными ресурсами
   - Необходимость баланса между точностью и энергоэффективностью

2. **Масштабирование к большим объемам данных**
   - Увеличение доступных наборов данных требовало более эффективных моделей
   - Более простые модели могли обрабатывать больше данных при тех же ресурсах

3. **Эксперименты по упрощению архитектуры**
   - Исследователи начали систематически изучать, какие компоненты LSTM действительно необходимы
   - Эксперименты показали, что некоторые элементы LSTM можно объединить без потери производительности

4. **Стремление к элегантности в дизайне**
   - Принцип Оккама: если две модели показывают одинаковую производительность, предпочтительнее более простая
   - Более простые модели часто лучше обобщаются на новые данные

**Как GRU решает эти проблемы:**

1. **Меньше параметров**
   - GRU имеет только два вентиля вместо трех в LSTM
   - Объединение функциональности вентилей входа и забывания в один вентиль обновления
   - Отсутствие отдельного состояния ячейки (используется только скрытое состояние)

2. **Сохранение ключевой функциональности**
   - Несмотря на упрощения, GRU сохраняет способность моделировать долговременные зависимости
   - Механизм вентилей всё ещё позволяет контролировать поток информации через сеть

3. **Эмпирические результаты**
   - На многих задачах GRU показала результаты, сопоставимые с LSTM
   - В некоторых случаях даже превосходила LSTM, особенно при ограниченных наборах данных

GRU представляет собой изящное балансирование между сложностью и эффективностью, которое позволило сделать рекуррентные модели более доступными и применимыми в широком спектре задач.

## 2. Архитектура GRU: ключевые компоненты

### 2.1 Интуиция: метафора "экономной памяти" с двумя контрольными точками

Для понимания принципа работы GRU давайте представим её как систему "экономной памяти" с двумя основными контрольными точками. Эта метафора поможет интуитивно понять, как GRU управляет информацией.

**Представьте библиотекаря, работающего с одной большой книгой (скрытое состояние):**

В отличие от LSTM, где есть отдельная книга для долговременного хранения (ячейка памяти) и рабочая тетрадь для текущих заметок (скрытое состояние), библиотекарь GRU работает только с одной книгой. В этой книге он постоянно обновляет информацию, следуя двум простым правилам:

**1. Вентиль сброса (Reset Gate) — "Что стоит перечитать?"**

Представьте, что библиотекарь решает, какие части его текущих знаний (хранящихся в книге) актуальны для понимания новой информации:

- Когда вентиль сброса близок к 1: "Эта часть моих текущих знаний важна для понимания новой информации"
- Когда вентиль сброса близок к 0: "Эта часть моих знаний не имеет отношения к новой информации, я её временно игнорирую"

Например, если вы читаете предложение "Погода в Париже...", и далее текст переключается на "...столице Франции", вентиль сброса может решить, что информация о "Париже" остаётся актуальной, а информация о "погоде" уже не важна для понимания продолжения предложения.

**2. Вентиль обновления (Update Gate) — "Насколько сильно обновить книгу?"**

После того, как библиотекарь определил, какая часть текущих знаний актуальна, он должен решить, в какой степени обновить содержимое книги:

- Когда вентиль обновления близок к 1: "Я полностью заменю эту часть книги новой информацией"
- Когда вентиль обновления близок к 0: "Я сохраню существующую информацию без изменений"

На примере текста: если до этого в книге была информация о Риме, а новое предложение говорит о Париже, вентиль обновления может установить высокое значение, чтобы заменить информацию о Риме на информацию о Париже.

**Процесс работы GRU в метафоре:**

1. **Получение новой информации**: библиотекарь получает новую порцию информации (входной вектор $x_t$)

2. **Определение релевантности старых знаний**: библиотекарь использует вентиль сброса, чтобы определить, какие части существующей книги (скрытого состояния $h_{t-1}$) важны для обработки новой информации

3. **Создание чернового обновления**: на основе релевантных частей старых знаний и новой информации библиотекарь создает черновик обновления (кандидат-вектор $\tilde{h}_t$)

4. **Решение о степени обновления**: используя вентиль обновления, библиотекарь решает, насколько сильно обновить каждую часть книги

5. **Обновление книги**: книга обновляется с учетом решений вентиля обновления, создавая новое состояние книги ($h_t$)

**Ключевое отличие от LSTM:**

LSTM можно сравнить с более сложной системой, где есть:
- Склад долговременного хранения (ячейка памяти)
- Рабочий стол (скрытое состояние)
- Три сотрудника, принимающих решения (три вентиля)

GRU упрощает эту систему, объединяя "склад" и "рабочий стол" в одно хранилище, и сокращая число "сотрудников" до двух, что делает систему более экономичной, сохраняя при этом её основную функциональность.

Эта экономичность — главное преимущество GRU, позволяющее достичь сопоставимых результатов с LSTM при меньших вычислительных затратах.

### 2.2 Формализация и обозначения: определение переменных и размерностей

Давайте формализуем архитектуру GRU, определив все её компоненты и их размерности. Это поможет как в понимании структуры, так и при последующей реализации.

**Основные обозначения:**

| **Символ** | **Размерность** | **Описание** |
|------------|-----------------|--------------|
| $x_t$ | $\mathbb{R}^{d_x}$ | Входной вектор на шаге $t$ |
| $h_t$ | $\mathbb{R}^{d_h}$ | Скрытое состояние на шаге $t$ |
| $z_t$ | $\mathbb{R}^{d_h}$ | Активация вентиля обновления (update gate) на шаге $t$ |
| $r_t$ | $\mathbb{R}^{d_h}$ | Активация вентиля сброса (reset gate) на шаге $t$ |
| $\tilde{h}_t$ | $\mathbb{R}^{d_h}$ | Кандидат-вектор нового скрытого состояния |

**Весовые матрицы и векторы смещения:**

| **Символ** | **Размерность** | **Описание** |
|------------|-----------------|--------------|
| $W_z$ | $\mathbb{R}^{d_h \times (d_x + d_h)}$ | Веса для вентиля обновления |
| $W_r$ | $\mathbb{R}^{d_h \times (d_x + d_h)}$ | Веса для вентиля сброса |
| $W_h$ | $\mathbb{R}^{d_h \times (d_x + d_h)}$ | Веса для кандидат-вектора |
| $b_z$ | $\mathbb{R}^{d_h}$ | Смещение для вентиля обновления |
| $b_r$ | $\mathbb{R}^{d_h}$ | Смещение для вентиля сброса |
| $b_h$ | $\mathbb{R}^{d_h}$ | Смещение для кандидат-вектора |

В альтернативной форме записи, можно разделить веса для входа и скрытого состояния:

| **Символ** | **Размерность** | **Описание** |
|------------|-----------------|--------------|
| $W_{xz}$ | $\mathbb{R}^{d_h \times d_x}$ | Веса для входа в вентиле обновления |
| $W_{hz}$ | $\mathbb{R}^{d_h \times d_h}$ | Веса для скрытого состояния в вентиле обновления |
| $W_{xr}$ | $\mathbb{R}^{d_h \times d_x}$ | Веса для входа в вентиле сброса |
| $W_{hr}$ | $\mathbb{R}^{d_h \times d_h}$ | Веса для скрытого состояния в вентиле сброса |
| $W_{xh}$ | $\mathbb{R}^{d_h \times d_x}$ | Веса для входа в кандидат-векторе |
| $W_{hh}$ | $\mathbb{R}^{d_h \times d_h}$ | Веса для скрытого состояния в кандидат-векторе |

Обе формы записи эквивалентны, но иногда удобнее использовать одну из них в зависимости от контекста.

**Функции активации:**

- $\sigma$: сигмоидальная функция, отображает входы в диапазон [0, 1]
- $\tanh$: гиперболический тангенс, отображает входы в диапазон [-1, 1]

**Размерности входных и выходных данных:**

- $d_x$: размерность входного вектора $x_t$
- $d_h$: размерность скрытого состояния

**Конкатенация входа и предыдущего состояния:**

Для упрощения записи мы часто используем конкатенацию входного вектора $x_t$ и предыдущего скрытого состояния $h_{t-1}$:

$$[x_t, h_{t-1}] \in \mathbb{R}^{d_x + d_h}$$

**Примечания по размерностям:**

1. В GRU используется только скрытое состояние $h_t$, в отличие от LSTM, где есть дополнительное состояние ячейки $C_t$.

2. Оба вентиля ($z_t$, $r_t$) имеют одинаковую размерность $d_h$, что позволяет им поэлементно контролировать обновление скрытого состояния.

3. Общее количество параметров в стандартном GRU:
   - Веса: $3 \times d_h \times (d_x + d_h)$
   - Смещения: $3 \times d_h$
   - Итого: $3 \times d_h \times (d_x + d_h + 1)$

Обратите внимание, что GRU имеет только 3/4 числа параметров LSTM, что является одним из ключевых преимуществ этой архитектуры.

### 2.3 Динамика одного шага

Теперь рассмотрим, как точно вычисляются все компоненты GRU на одном временном шаге $t$. Разберем каждый элемент архитектуры подробно.

![Image_01.png](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/GRU/Image_01.jpg)

#### Вентиль сброса (reset gate)

Вентиль сброса $r_t$ определяет, какие элементы предыдущего состояния $h_{t-1}$ стоит учитывать при вычислении нового кандидат-вектора:

$$
r_t = \sigma\big(W_r \cdot [x_t, h_{t-1}] + b_r\big)
$$

или, в развернутой форме:

$$
r_t = \sigma\big(W_{xr} \cdot x_t + W_{hr} \cdot h_{t-1} + b_r\big)
$$

Здесь:
- $[x_t, h_{t-1}]$ — конкатенация текущего входа и предыдущего скрытого состояния
- $W_r$ (или $W_{xr}$ и $W_{hr}$) — весовые матрицы для вентиля сброса
- $b_r$ — вектор смещения
- $\sigma$ — сигмоидальная функция, возвращающая значения в интервале [0, 1]

Результат $r_t$ представляет собой вектор значений между 0 и 1, где:
- **1** означает "полностью учитывать эту информацию из предыдущего состояния"
- **0** означает "полностью игнорировать эту информацию из предыдущего состояния"

**Пример:** При обработке текста, если предложение меняет тему, вентиль сброса может установить низкие значения, чтобы "забыть" контекст предыдущей темы при формировании нового кандидат-вектора.

#### Вентиль обновления (update gate)

![Image_02.png](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/GRU/Image_02.jpg)

Вентиль обновления $z_t$ определяет, насколько сильно каждый элемент скрытого состояния должен быть обновлен:

$$
z_t = \sigma\big(W_z \cdot [x_t, h_{t-1}] + b_z\big)
$$

или:

$$
z_t = \sigma\big(W_{xz} \cdot x_t + W_{hz} \cdot h_{t-1} + b_z\big)
$$

Как и в случае с вентилем сброса, результатом является вектор значений в интервале [0, 1], где:
- **1** означает "полностью заменить старую информацию новой"
- **0** означает "полностью сохранить старую информацию без изменений"

**Важное наблюдение:** вентиль обновления в GRU играет роль, аналогичную комбинации вентилей забывания и входа в LSTM. Высокое значение $z_t$ означает "забыть" старую информацию и "запомнить" новую.

#### Кандидат-вектор скрытого состояния

Кандидат-вектор $\tilde{h}_t$ представляет собой "предложение" для нового скрытого состояния, но с учетом только той части предыдущего состояния, которую определил вентиль сброса:

$$
\tilde{h}_t = \tanh\big(W_h \cdot [x_t, r_t \odot h_{t-1}] + b_h\big)
$$

или:

$$
\tilde{h}_t = \tanh\big(W_{xh} \cdot x_t + W_{hh} \cdot (r_t \odot h_{t-1}) + b_h\big)
$$

Здесь:
- $r_t \odot h_{t-1}$ — поэлементное умножение вентиля сброса на предыдущее скрытое состояние
- $\tanh$ — гиперболический тангенс, нормализующий значения в диапазон [-1, 1]

**Ключевой момент:** вентиль сброса применяется к $h_{t-1}$ перед его использованием в вычислении кандидат-вектора, а не к полному скрытому состоянию. Это позволяет сети "забыть" часть прошлого состояния при вычислении новой информации, но не обязательно при обновлении полного состояния.

#### Финальное обновление скрытого состояния

Наконец, вычисляем новое скрытое состояние $h_t$, как взвешенную комбинацию предыдущего состояния и кандидат-вектора, где вес определяется вентилем обновления:

$$
h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
$$

Здесь:
- $(1 - z_t) \odot h_{t-1}$ — часть старой информации, которую мы решили сохранить
- $z_t \odot \tilde{h}_t$ — часть новой информации, которую мы решили добавить

**Важное наблюдение:** эта формула эквивалентна интерполяции между старым состоянием $h_{t-1}$ и новым кандидат-состоянием $\tilde{h}_t$, где $z_t$ определяет точку интерполяции для каждого элемента вектора.

**Итог: полный набор формул GRU**

Для удобства, вот полный набор формул, описывающих один шаг GRU:

$$
\begin{align}
z_t &= \sigma(W_z \cdot [x_t, h_{t-1}] + b_z) \\
r_t &= \sigma(W_r \cdot [x_t, h_{t-1}] + b_r) \\
\tilde{h}_t &= \tanh(W_h \cdot [x_t, r_t \odot h_{t-1}] + b_h) \\
h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
\end{align}
$$

Эти четыре уравнения полностью описывают динамику GRU ячейки на одном временном шаге.

## 3. Математическое сравнение GRU с LSTM

### 3.1 Упрощения в архитектуре: какие элементы LSTM были объединены или удалены

GRU можно рассматривать как упрощенную версию LSTM, где некоторые компоненты были объединены или полностью исключены. Давайте систематически рассмотрим эти упрощения.

**1. Объединение состояний: устранение отдельной ячейки памяти**

В LSTM существуют два вида состояний:
- Состояние ячейки (cell state) $C_t$ — долговременная память
- Скрытое состояние (hidden state) $h_t$ — краткосрочная память и выход

В GRU эти два состояния объединены в одно — скрытое состояние $h_t$, которое выполняет обе функции.

Математическое последствие:
- LSTM: $C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$, затем $h_t = o_t \odot \tanh(C_t)$
- GRU: $h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$

**2. Слияние вентилей входа и забывания**

В LSTM вентиль забывания $f_t$ определяет, какую часть старой информации сохранить, а вентиль входа $i_t$ — какую часть новой информации добавить. Эти решения принимаются независимо.

В GRU вентиль обновления $z_t$ определяет одновременно, какую часть старой информации заменить новой. Это создает жесткую связь: если вы добавляете X% новой информации, то обязательно забываете X% старой.

Математическое последствие:
- LSTM: $C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$ (где $f_t$ и $i_t$ независимы)
- GRU: $h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$ (где $z_t$ и $(1-z_t)$ комплементарны)

**3. Изменение применения вентиля сброса**

В LSTM вентиль выхода $o_t$ применяется после вычисления всех других компонентов, чтобы определить, какую часть информации из ячейки передать в выходное скрытое состояние.

В GRU вентиль сброса $r_t$ применяется перед вычислением кандидат-вектора, определяя, какую часть предыдущего состояния учитывать при создании нового состояния.

Математическое последствие:
- LSTM: $h_t = o_t \odot \tanh(C_t)$ (вентиль выхода контролирует финальный выход)
- GRU: $\tilde{h}_t = \tanh(W_h \cdot [x_t, r_t \odot h_{t-1}] + b_h)$ (вентиль сброса влияет на создание кандидат-вектора)

**4. Устранение выходного вентиля**

LSTM имеет отдельный выходной вентиль $o_t$, который контролирует, какая информация из ячейки памяти должна быть видна во внешнем скрытом состоянии.

GRU не имеет такого вентиля — всё скрытое состояние всегда полностью доступно.

**5. Сравнительная таблица компонентов**

| **Компонент LSTM** | **Аналог в GRU** | **Примечание** |
|--------------------|------------------|----------------|
| Состояние ячейки $C_t$ | Отсутствует (объединено с $h_t$) | В GRU единое состояние |
| Скрытое состояние $h_t$ | Скрытое состояние $h_t$ | Аналогично в обеих архитектурах |
| Вентиль забывания $f_t$ | Часть вентиля обновления $(1-z_t)$ | В GRU комплементарно вентилю входа |
| Вентиль входа $i_t$ | Вентиль обновления $z_t$ | В GRU комплементарно вентилю забывания |
| Вентиль выхода $o_t$ | Отсутствует | В GRU нет фильтрации выхода |
| Кандидат-вектор $\tilde{C}_t$ | Кандидат-вектор $\tilde{h}_t$ | Аналогично, но в GRU влияет вентиль сброса |
| - | Вентиль сброса $r_t$ | Уникален для GRU |

**6. Сравнение уравнений**

**LSTM**:
$$
f_t = \sigma(W_f \cdot [x_t, h_{t-1}] + b_f)
$$
$$
i_t = \sigma(W_i \cdot [x_t, h_{t-1}] + b_i)
$$
$$
o_t = \sigma(W_o \cdot [x_t, h_{t-1}] + b_o)
$$
$$
\tilde{C}_t = \tanh(W_C \cdot [x_t, h_{t-1}] + b_C)
$$
$$
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
$$
$$
h_t = o_t \odot \tanh(C_t)
$$

**GRU**:
$$
z_t = \sigma(W_z \cdot [x_t, h_{t-1}] + b_z)
$$
$$
r_t = \sigma(W_r \cdot [x_t, h_{t-1}] + b_r)
$$
$$
\tilde{h}_t = \tanh(W_h \cdot [x_t, r_t \odot h_{t-1}] + b_h)
$$
$$
h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
$$

**Ключевое наблюдение**: GRU имеет меньше уравнений и параметров, обеспечивая более компактную архитектуру, которая тем не менее сохраняет ключевую функциональность управления потоком информации.

### 3.2 Поток градиентов в GRU: анализ решения проблемы затухающего градиента

GRU, как и LSTM, успешно решает проблему затухающего градиента, но делает это немного иначе. Давайте проанализируем, как именно в GRU организован поток градиентов.

**Напомним проблему в ванильных RNN:**

В обычных RNN градиент затухает из-за многократного умножения на якобианы с собственными значениями меньше 1:

$$\frac{\partial h_t}{\partial h_{t-k}} = \prod_{i=t-k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}} = \prod_{i=t-k+1}^{t} \text{diag}(\tanh'(a_i)) \cdot W_{hh}$$

**Анализ потока градиентов в GRU:**

В GRU скрытое состояние обновляется по формуле:

$$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$$

Вычислим градиент $\frac{\partial h_t}{\partial h_{t-1}}$, используя цепное правило:

$$\frac{\partial h_t}{\partial h_{t-1}} = \frac{\partial}{\partial h_{t-1}} [(1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t]$$

Раскрывая это выражение:

$$\frac{\partial h_t}{\partial h_{t-1}} = \underbrace{(1 - z_t)}_{\text{прямой путь}} + \underbrace{\frac{\partial z_t}{\partial h_{t-1}} \odot (\tilde{h}_t - h_{t-1})}_{\text{через } z_t} + \underbrace{z_t \odot \frac{\partial \tilde{h}_t}{\partial h_{t-1}}}_{\text{через } \tilde{h}_t}$$

**Ключевое наблюдение 1: прямой путь градиента**

Первый член $(1 - z_t)$ представляет собой прямой путь градиента. Если $z_t$ близко к 0 (т.е. решение сохранить большую часть старой информации), то градиент может течь почти без изменений через этот путь.

Сравнение с LSTM:
- В LSTM прямой путь идет через состояние ячейки: $\frac{\partial C_t}{\partial C_{t-1}} = f_t$
- В GRU прямой путь идет через скрытое состояние: $(1 - z_t)$

**Ключевое наблюдение 2: адаптивное обновление**

GRU не просто предотвращает затухание градиентов — она делает это адаптивно. Сеть обучается устанавливать $z_t$ близким к 0 для тех элементов, где важно сохранить долговременные зависимости.

**Поток градиентов через несколько шагов:**

Для $k$ шагов назад, градиент можно выразить как:

$$\frac{\partial h_t}{\partial h_{t-k}} = \prod_{i=t-k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}}$$

Если для каждого шага $i$ значения $(1 - z_i)$ близки к 1 для некоторых элементов, то градиент может течь через эти элементы без существенного затухания. Это создает "информационные магистрали" для обратного распространения.

**Вентиль сброса и градиенты:**

Вентиль сброса $r_t$ влияет на градиенты через второй путь:

$$\frac{\partial \tilde{h}_t}{\partial h_{t-1}} = \frac{\partial}{\partial h_{t-1}} \tanh(W_h \cdot [x_t, r_t \odot h_{t-1}] + b_h)$$

Это включает:
- Прямое влияние $r_t \odot \frac{\partial}{\partial h_{t-1}} \tanh(...)$
- Косвенное влияние через $\frac{\partial r_t}{\partial h_{t-1}}$

Вентиль сброса позволяет GRU "забывать" определенные компоненты предыдущего состояния при вычислении кандидат-вектора, но это не препятствует потоку градиентов через основной путь $(1 - z_t)$.

**Сравнение с LSTM:**

| **Аспект** | **LSTM** | **GRU** |
|------------|----------|---------|
| Основной путь градиента | Через состояние ячейки: $\frac{\partial C_t}{\partial C_{t-1}} = f_t$ | Через скрытое состояние: $(1 - z_t)$ |
| Контроль потока | Три независимых вентиля | Два вентиля, один из которых с комплементарным эффектом |
| Количество путей градиента | Множество путей через $C_t$ и $h_t$ | Меньше путей, но с прямой магистралью |

Таким образом, GRU решает проблему затухающего градиента через адаптивный механизм, который создает прямые пути для потока градиента, аналогично LSTM, но с использованием меньшего количества компонентов.

**3. Потребление памяти**

**LSTM**:
- Хранение состояний: $O(2d_h)$ для $h_t$ и $C_t$
- Хранение промежуточных результатов для обратного прохода: $O(4d_h T)$ для последовательности длины $T$

**GRU**:
- Хранение состояний: $O(d_h)$ только для $h_t$
- Хранение промежуточных результатов: $O(3d_h T)$

Экономия памяти GRU по сравнению с LSTM значительна при длинных последовательностях.

**4. Параллелизация и аппаратное ускорение**

**LSTM**:
- 4 матричных умножения могут быть распараллелены или объединены в одно большое умножение
- Более сложные зависимости между компонентами могут снизить эффективность конвейеризации

**GRU**:
- 3 матричных умножения также поддаются параллелизации
- Меньше зависимостей между компонентами, что потенциально лучше для конвейеризации
- Вычисление $r_t \odot h_{t-1}$ создает дополнительную зависимость

**Практический эффект**:
на современных GPU и специализированных аппаратных ускорителях (TPU, NPU) преимущество GRU в скорости часто составляет 20-30% по сравнению с LSTM при одинаковом размере скрытого состояния.

**5. Масштабирование к большим моделям**

При увеличении размерности скрытого состояния $d_h$ разница в вычислительной эффективности становится более значительной:

- Для $d_h = 1024$, разница в количестве параметров: ~1.7 миллиона
- Для $d_h = 2048$, разница уже превышает 6 миллионов параметров

Это особенно важно для глубоких моделей с несколькими слоями, где экономия умножается на количество слоев.

**Таблица сравнения эффективности для различных размерностей**

| **Размерность** | **Параметры LSTM** | **Параметры GRU** | **Экономия** | **Экономия (%)** |
|-----------------|--------------------|--------------------|--------------|-------------------|
| $d_h = 128$ | 0.22M | 0.16M | 0.06M | 25% |
| $d_h = 256$ | 0.57M | 0.43M | 0.14M | 25% |
| $d_h = 512$ | 1.67M | 1.25M | 0.42M | 25% |
| $d_h = 1024$ | 6.03M | 4.52M | 1.51M | 25% |
| $d_h = 2048$ | 23.14M | 17.35M | 5.79M | 25% |

Эта вычислительная эффективность делает GRU особенно привлекательным выбором для задач с ограниченными ресурсами и для моделей, которые работают в режиме реального времени.

## Вывод

Gated Recurrent Unit (GRU) представляет собой важное усовершенствование в архитектуре рекуррентных нейронных сетей, которое успешно балансирует между эффективностью и вычислительной сложностью. Будучи разработанной как упрощенная альтернатива LSTM в 2014 году, GRU сохранила ключевую способность моделировать долговременные зависимости в последовательностях, при этом значительно сократив количество параметров и вычислительных затрат.

Основные преимущества GRU заключаются в:
1. Упрощенной архитектуре с двумя вентилями вместо трех у LSTM
2. Объединении скрытого состояния и ячейки памяти в одно состояние
3. Сопоставимой производительности с LSTM при меньших вычислительных ресурсах
4. Лучшей масштабируемости для больших моделей и длинных последовательностей

Математический анализ показывает, что GRU эффективно решает проблему затухающего градиента благодаря адаптивному механизму управления потоком информации. При этом архитектура GRU демонстрирует значительную экономию в потреблении памяти и вычислительных ресурсов по сравнению с LSTM, особенно при увеличении размерности скрытого состояния.

Таким образом, GRU представляет собой оптимальный выбор для задач обработки последовательностей, где требуется баланс между эффективностью и вычислительной эффективностью, особенно в условиях ограниченных ресурсов или необходимости работы в реальном времени.

</details>

<details> 
    <summary><em><strong>Модели пространства состояний  (SSM)</strong></em></summary>

## 1. Введение в модели пространства состояний (State Space Models)

### 1.1 Контекст появления SSM: эволюция архитектур для моделирования последовательностей

State Space Models (SSM) представляют собой класс динамических систем, которые пришли в глубокое обучение из теории управления и обработки сигналов. Их появление в качестве архитектуры для нейронных сетей можно рассматривать как часть более широкой эволюции моделей для обработки последовательностей.

**Хронология эволюции моделей последовательностей:**

- **1980-е - 1990-е годы**: Появление классических рекуррентных нейронных сетей (RNN) для обработки последовательностей данных.
  
- **1997**: Представление LSTM (Hochreiter & Schmidhuber) как решения проблемы затухающего градиента.
  
- **2014**: Появление GRU (Cho et al.) как упрощенной версии LSTM.
  
- **2017**: Архитектура Transformer (Vaswani et al.) с механизмом внимания произвела революцию в NLP.
  
- **2019-2020**: Первые эксперименты с адаптацией классических SSM для глубокого обучения (Gu et al.).
  
- **2021-2022**: Появление первых эффективных реализаций SSM — S4 (Structured State Space Sequence Model) и S4D (Diagonal State Space).
  
- **2023**: Представление селективной SSM архитектуры Mamba (Gu & Dao).

SSM возникли как попытка объединить преимущества рекуррентных сетей (линейная сложность) и трансформеров (обработка длинных зависимостей) при устранении их недостатков. Ключевым наблюдением было то, что классическая теория систем уже предлагала формализм для моделирования динамических процессов с долговременными зависимостями в виде линейных систем пространства состояний.

Основная мотивация для разработки SSM для глубокого обучения включала:

- **Масштабируемость**: Необходимость в архитектурах, способных эффективно обрабатывать очень длинные последовательности (тысячи или миллионы токенов).
  
- **Вычислительная эффективность**: Потребность в моделях с линейным масштабированием относительно длины последовательности, в отличие от квадратичной сложности трансформеров.
  
- **Продолжительные зависимости**: Необходимость моделировать зависимости на очень больших расстояниях без проблем затухающего градиента.
  
- **Теоретическая обоснованность**: Желание использовать хорошо изученный математический аппарат из области теории управления и обработки сигналов.

Переход от классических RNN к SSM можно рассматривать как естественную эволюцию, где SSM предлагают более формальный и мощный аппарат для моделирования динамических систем, сохраняя при этом вычислительную эффективность.

### 1.2 Авторы и ключевые публикации в области SSM

Развитие SSM для глубокого обучения связано с работами нескольких исследовательских групп, которые постепенно развивали и улучшали эту парадигму.

**Ключевые исследователи и публикации:**

1. **Альберт Гу и соавторы (2021-2023)**
   - "Efficiently Modeling Long Sequences with Structured State Spaces" — первая работа, представившая модель S4
   - "Mamba: Linear-Time Sequence Modeling with Selective State Spaces" — революционная работа, представляющая селективные SSM
   - Гу и его коллеги из Стэнфордского университета, а позже из Университета Карнеги-Меллона разработали базовую структуру современных SSM для глубокого обучения

2. **Три Дао и коллеги (2022-2023)**
   - Соавтор архитектуры Mamba
   - Значительный вклад в оптимизацию SSM для современных аппаратных платформ

3. **Авив Шмсони, Кришнан Прасад, Ноам Рот (2022)**
   - "On the Parameterization and Initialization of Diagonal State Space Models" — представление S4D, диагональной версии SSM
   - Ключевые оптимизации, сделавшие SSM более практичными для обучения и вывода

4. **Сирена Дэна и Яаков Кэрэр (2023)**
   - "Simplified State Space Layers for Sequence Modeling" — представление S5, упрощенной и улучшенной версии SSM
   - Упрощения, которые сделали SSM более доступными для широкого использования

**Связь с обработкой сигналов и теорией управления:**

Что примечательно, SSM в нейронных сетях демонстрируют сильную связь с классическими методами обработки сигналов. Исследователи адаптировали методы, известные десятилетиями в технической литературе:

> "SSM объединяют методы рекуррентных сетей с классической теорией линейных систем, создавая мост между глубоким обучением и традиционной обработкой сигналов. Это позволяет нам использовать богатый математический аппарат теории управления для современных нейронных архитектур." — Альберт Гу

**Ключевые статьи, повлиявшие на развитие SSM:**

1. **"HiPPO: Recurrent Memory with Optimal Polynomial Projections" (Gu et al., 2020)**
   - Предшественник SSM, представивший теоретические основы для моделирования долговременных зависимостей
   - Заложил математические основы для последующего развития S4 и других SSM

2. **"Efficiently Modeling Long Sequences with Structured State Spaces" (Gu et al., 2021)**
   - Первое представление S4, структурированной модели пространства состояний
   - Демонстрация превосходной производительности на задачах с длинными зависимостями

3. **"Diagonal State Spaces are as Effective as Structured State Spaces" (Gupta et al., 2022)**
   - Показала, что более простые диагональные SSM могут быть столь же эффективными, как и полные SSM
   - Значительно упростила вычислительную сложность и реализацию SSM

4. **"Mamba: Linear-Time Sequence Modeling with Selective State Spaces" (Gu & Dao, 2023)**
   - Представление селективных SSM, которые динамически адаптируют параметры в зависимости от входных данных
   - Прорыв, сделавший SSM конкурентоспособными с трансформерами даже в задачах языкового моделирования

Эволюция SSM представляет собой пример эффективного обмена идеями между различными областями науки, где классическая теория систем успешно адаптируется для решения современных проблем в глубоком обучении.

### 1.3 Баланс компромиссов: почему возникла потребность в SSM между RNN и трансформерами

Модели пространства состояний возникли в ответ на фундаментальные ограничения существующих архитектур для моделирования последовательностей: рекуррентных нейронных сетей (включая LSTM и GRU) и трансформеров. SSM стремятся найти "золотую середину", сочетающую лучшие свойства обоих подходов.

**Проблемы, связанные с существующими архитектурами:**

1. **Ограничения RNN/LSTM/GRU:**
   - **Последовательная обработка:** необходимость обрабатывать элементы последовательности один за другим, что ограничивает параллелизм
   - **Трудности с долговременными зависимостями:** хотя LSTM и GRU частично решают проблему затухающего градиента, они все еще ограничены в способности захватывать зависимости на очень больших расстояниях
   - **Трудности в обучении:** нестабильность градиентов, особенно при глубоких архитектурах
   - **Ограниченная масштабируемость:** трудности в масштабировании до очень глубоких моделей

2. **Ограничения трансформеров:**
   - **Квадратичная сложность:** механизм самовнимания имеет сложность O(n²) по длине последовательности, что делает обработку длинных последовательностей вычислительно затратной
   - **Ограничения памяти:** высокое потребление памяти для длинных последовательностей
   - **Фиксированное контекстное окно:** практические ограничения на длину контекста, который может обрабатывать модель

**Почему потребовались SSM:**

1. **Теоретическая элегантность:**
   - SSM основаны на хорошо изученном математическом аппарате из теории управления
   - Предоставляют формальный способ моделирования динамических систем с непрерывным временем
   - Позволяют использовать аналитические методы для анализа поведения модели

2. **Масштабирование к большим объемам данных:**
   - Линейная сложность O(n) по длине последовательности
   - Способность эффективно обрабатывать очень длинные последовательности (тысячи или миллионы элементов)
   - Меньшее потребление памяти по сравнению с трансформерами

3. **Баланс между эффективностью и выразительностью:**
   - Сохранение способности моделировать долговременные зависимости (как в трансформерах)
   - Эффективность вычислений, сравнимая с RNN
   - Возможность параллельной обработки, в отличие от RNN

4. **Потребность в обработке разнообразных модальностей:**
   - Необходимость в унифицированном подходе к моделированию аудио, видео, текста и других последовательных данных
   - SSM хорошо подходят для непрерывных сигналов и могут работать с данными различной природы

**Сравнительная таблица архитектур:**

| **Характеристика** | **RNN/LSTM/GRU** | **Трансформеры** | **SSM** |
|-------------------|-------------------|-------------------|---------|
| Вычислительная сложность | O(n) | O(n²) | O(n) |
| Параллелизм | Ограниченный | Высокий | Высокий |
| Долговременные зависимости | Ограниченная способность | Отличная способность | Отличная способность |
| Масштабируемость к длинным последовательностям | Хорошая | Ограниченная | Превосходная |
| Использование памяти | Низкое | Высокое | Низкое |
| Теоретическая обоснованность | Эмпирическая | Эмпирическая | Формальная из теории управления |
| Интерпретируемость | Ограниченная | Через внимание | Через системную теорию |

Цитата Альберта Гу о мотивации создания SSM:

> "Мы стремились создать архитектуру, которая могла бы масштабироваться линейно с длиной последовательности, сохраняя при этом способность моделировать долговременные зависимости, присущую трансформерам. SSM предлагают теоретически обоснованный подход к этой проблеме, опираясь на десятилетия исследований в области теории управления и обработки сигналов."

Таким образом, SSM представляют собой не просто еще одну архитектуру, а фундаментально новый подход к моделированию последовательностей, который стремится преодолеть принципиальные ограничения существующих методов.

## 2. Математические основы и архитектура SSM

### 2.1 Интуиция: метафора "динамической системы с памятью"

Для понимания интуитивной сути State Space Models, полезно представить их как динамические системы с внутренней памятью, которые обрабатывают входящие сигналы и генерируют выходные. Метафора "динамической системы с памятью" поможет нам понять ключевые компоненты и принципы работы SSM.

**Представьте физическую динамическую систему:**

Вообразите резервуар с жидкостью, в который подается входной поток (вход $x(t)$). Состояние резервуара (уровень, температура, давление — скрытое состояние $h(t)$) меняется под воздействием входного потока по определенным физическим законам. В то же время, мы измеряем некоторые параметры на выходе (выход $y(t)$), которые зависят от текущего состояния резервуара.

**Ключевое отличие от RNN:**

В отличие от RNN, SSM формулируются в непрерывном времени, а затем дискретизируются для вычислений. Это можно представить так: в то время как RNN (включая LSTM и GRU) работают с дискретными шагами времени (например, от слова к слову в тексте), SSM моделируют непрерывный процесс, который затем "просматривается" в дискретные моменты.

Эта непрерывная природа даёт SSM несколько теоретических преимуществ:
- Возможность применять богатый аппарат дифференциальных уравнений
- Более естественное моделирование процессов, происходящих в непрерывном времени
- Лучшая способность адаптироваться к данным с различным временным разрешением

**Интуитивный пример: моделирование языка**

![Схема работы SSM](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/SSM/Image_01.png)

При моделировании языка, можно представить SSM следующим образом:
- Входная последовательность $x(t)$ — это поток слов или токенов
- Скрытое состояние $h(t)$ — это "понимание" текста, которое обновляется с каждым новым словом
- Матрица $A$ определяет, как быстро "забывается" контекст со временем
- Матрица $B$ определяет, как сильно каждое новое слово влияет на понимание
- Матрица $C$ определяет, как из текущего понимания генерируются прогнозы следующих слов

Таким образом, SSM можно интуитивно понимать как систему, которая непрерывно обрабатывает поток информации, сохраняя при этом память о прошлых событиях и генерируя выходы на основе своего текущего внутреннего состояния.

### 2.2 Формализация и обозначения: определение переменных и размерностей

Давайте формализуем архитектуру SSM, определив все её компоненты и соответствующие размерности. Это поможет лучше понять структуру модели и будет полезно при последующей реализации.

**Основные обозначения линейного SSM:**

| **Символ** | **Размерность** | **Описание** |
|------------|-----------------|--------------|
| $x(t)$ | $\mathbb{R}^{d_x}$ | Входной сигнал в непрерывном времени |
| $h(t)$ | $\mathbb{R}^{d_h}$ | Скрытое состояние в непрерывном времени |
| $y(t)$ | $\mathbb{R}^{d_y}$ | Выходной сигнал в непрерывном времени |
| $A$ | $\mathbb{R}^{d_h \times d_h}$ | Матрица динамики состояния |
| $B$ | $\mathbb{R}^{d_h \times d_x}$ | Матрица входного преобразования |
| $C$ | $\mathbb{R}^{d_y \times d_h}$ | Матрица выходного преобразования |
| $D$ | $\mathbb{R}^{d_y \times d_x}$ | Матрица прямого прохода (опционально) |

**Непрерывная модель пространства состояний:**

Линейная непрерывная SSM описывается следующими дифференциальными уравнениями:

$$
\begin{align}
h'(t) &= Ah(t) + Bx(t) \\
y(t) &= Ch(t) + Dx(t)
\end{align}
$$

Где:
- $h'(t)$ — производная скрытого состояния по времени
- $x(t)$ — входной сигнал в момент времени $t$
- $h(t)$ — скрытое состояние в момент времени $t$
- $y(t)$ — выходной сигнал в момент времени $t$

Уравнение состояния с помощью матриц A и B описывает, как состояние изменяется под влиянием входных данных.

![Визуализация уравнения состояния](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/SSM/Image_02.png)

Уравнение выхода описывает, как состояние переводится в выход (через матрицу C) и как вход влияет на выход (через матрицу D).

![Визуализация уравнения выхода](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/SSM/Image_03.png)

> Примечание: Матрицы A, B, C и D являются обучаемыми параметрами.

**Дискретизация для практических вычислений:**

Уравнения состояний, описанные выше, имеют непрерывный вид, что является проблемой из-за того, что на вход мы хотели бы подавать дискретные данные. Поэтому, нам необходимо дискретизировать SSM. Для решения используется техника названием «экстраполятор нулевого порядка», которая работает следующим образом: когда мы получаем на вход дискретный сигнал, то удерживаем его значение до тех пор, пока не получим новый.

![Визуальное пояснение работы экстраполятора нулевого порядка при переходе от дискретного вида к непрерывному](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/SSM/Image_04.png)

Для использования в нейронных сетях непрерывная модель дискретизируется с определенным шагом дискретизации $\Delta$, данный шаг является обучаемым параметром. Существуют различные методы дискретизации, но один из наиболее распространенных — метод нулевого порядка (Zero-Order Hold, ZOH). Он представляет собой разрешение входного сигнала. Математически, экстраполятор нулевого порядка для нашего случая описывается следующим образом:

$$
\begin{align}
h_t &= \bar{A}h_{t-1} + \bar{B}x_t \\
y_t &= Ch_t + Dx_t
\end{align}
$$

Где:
- $h_t$ — дискретное скрытое состояние в момент времени $t$
- $x_t$ — дискретный входной сигнал в момент времени $t$
- $y_t$ — дискретный выходной сигнал в момент времени $t$
- $\bar{A} = \exp(\Delta A)$ — дискретизированная матрица динамики состояния
- $\bar{B} = (\Delta A)^{-1}(\exp(\Delta A) - I)\Delta B$ — дискретизированная матрица входного преобразования

```python
# b — размер батча
# l — длина входной последовательности
# d_in — размер эмбеддинга входных данных
# n — размер тензоров B и C
# u — входные данные

# Дискретизация матрицы A
deltaA = torch.exp(einsum(delta, A, 'b l d_in, d_in n -> b l d_in n'))

# Дискретизация матрицы B
deltaB_u = einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')
x = torch.zeros((b, d_in, n), device=deltaA.device)
ys = []

# Проходимся по всей последовательности и получаем выходы
for i in range(l):
   x = deltaA[:, i] * x + deltaB_u[:, i]
   y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')
   ys.append(y)
y = torch.stack(ys, dim=1)
y = y + u * D
```

**Параметризация в SSM для глубокого обучения:**

В моделях типа S4 и Mamba используются специальные параметризации матриц $A$, $B$ и $C$, которые улучшают обучаемость и вычислительную эффективность:

1. **Структурированная матрица $A$**:
   - В S4 используется специальная параметризация, основанная на HiPPO (Hierarchical Polynomial Projections)
   - В более поздних вариантах (S4D) часто используется диагональная матрица $A = \text{diag}(a_1, a_2, ..., a_{d_h})$

2. **Параметризация $B$**:
   - В S4: $B$ может быть низкоранговой или специально структурированной
   - В простейшем случае: $B$ может быть вектором-столбцом

3. **Параметризация $C$**:
   - Обычно матрица $C$ параметризуется напрямую
   - В некоторых вариантах SSM используются ограничения на $C$ для улучшения стабильности

**Размерности в многослойной SSM:**

| **Параметр** | **Размерность** | **Описание** |
|--------------|-----------------|--------------|
| $d_x$ | Скаляр | Размерность входного вектора |
| $d_h$ | Скаляр | Размерность скрытого состояния (обычно от 64 до 1024) |
| $d_y$ | Скаляр | Размерность выходного вектора (обычно равна $d_x$) |
| $L$ | Скаляр | Количество слоев SSM |
| $N$ | Скаляр | Длина входной последовательности |

**Селективные SSM (Mamba):**

В селективных SSM, таких как Mamba, параметры модели становятся функциями входных данных:

$$
\begin{align}
h'(t) &= A(x)h(t) + B(x)x(t) \\
y(t) &= C(x)h(t) + D(x)x(t)
\end{align}
$$

Где $A(x)$, $B(x)$, $C(x)$ и $D(x)$ — функции от входа $x(t)$, обычно реализуемые через нейронные сети.

**Общее количество параметров в стандартном SSM слое:**
- Матрица $A$: $d_h \times d_h$ (или $d_h$ для диагональной параметризации)
- Матрица $B$: $d_h \times d_x$
- Матрица $C$: $d_y \times d_h$
- Матрица $D$ (если используется): $d_y \times d_x$
- Итого: $d_h \times d_h + d_h \times d_x + d_y \times d_h + d_y \times d_x$ параметров

В селективных SSM количество параметров увеличивается за счет дополнительных проекционных слоев, которые генерируют параметры в зависимости от входных данных.

### 2.3 Динамика работы SSM: от непрерывного времени к дискретному

Теперь рассмотрим подробно, как именно функционирует модель пространства состояний во времени, начиная с непрерывной формулировки и заканчивая дискретной реализацией, используемой в нейронных сетях.

#### Непрерывное время: формулировка через дифференциальные уравнения

SSM в непрерывном времени описывается системой линейных дифференциальных уравнений первого порядка:

$$
\begin{align}
h'(t) &= Ah(t) + Bx(t) \\
y(t) &= Ch(t) + Dx(t)
\end{align}
$$

Первое уравнение описывает, как меняется скрытое состояние во времени, а второе — как скрытое состояние преобразуется в выходной сигнал.

**Интерпретация компонентов:**

- **Матрица $A$** определяет собственную динамику системы. Её собственные значения указывают на устойчивость системы:
  - Отрицательные действительные части собственных значений → устойчивая система
  - Положительные действительные части → неустойчивая система
  - Мнимые части → колебательное поведение

- **Матрица $B$** определяет, как входной сигнал влияет на изменение скрытого состояния.

- **Матрица $C$** определяет, как скрытое состояние влияет на выходной сигнал.

- **Матрица $D$** (если используется) позволяет входному сигналу напрямую влиять на выходной сигнал.

#### Аналитическое решение в непрерывном времени

Для линейной системы можно получить аналитическое решение:

$$
\begin{align}
h(t) &= e^{A(t-t_0)}h(t_0) + \int_{t_0}^{t}e^{A(t-\tau)}Bx(\tau)d\tau \\
y(t) &= Ch(t) + Dx(t)
\end{align}
$$

Где $e^{At}$ — матричная экспонента, являющаяся решением однородного уравнения $h'(t) = Ah(t)$.

Это решение показывает, что текущее состояние системы зависит от:
1. Начального состояния, преобразованного через матричную экспоненту
2. Свертки входного сигнала с ядром $e^{A(t-\tau)}B$

#### Переход к дискретному времени

Для практической реализации необходимо дискретизировать непрерывную модель. Существует несколько методов дискретизации:

1. **Метод Эйлера (простейший)**:
   $$h_{t} = h_{t-1} + \Delta \cdot (Ah_{t-1} + Bx_t)$$
   
   Где $\Delta$ — шаг дискретизации. Этот метод прост, но не очень точен для быстро меняющихся систем.

2. **Метод нулевого порядка (ZOH)**, который предполагает, что вход остается постоянным в течение шага дискретизации:

$$
\begin{align}
h_t &= \bar{A}h_{t-1} + \bar{B}x_t \\
\bar{A} &= e^{A\Delta} \\
\bar{B} &= (A)^{-1}(e^{A\Delta} - I)B
\end{align}
$$

Этот метод дает точное решение для случая, когда входной сигнал константен между шагами дискретизации.

3. **Метод билинейного преобразования (Tustin)**, который обеспечивает лучшую аппроксимацию для систем с колебательной динамикой.

#### Вычислительные аспекты дискретизации

Ключевой вычислительной проблемой является эффективный расчет матричной экспоненты $e^{A\Delta}$. В моделях типа S4 используются специальные техники для эффективного вычисления:

- **Для диагональной матрицы $A$** (как в S4D): экспонента вычисляется поэлементно $e^{A\Delta} = \text{diag}(e^{a_1\Delta}, e^{a_2\Delta}, ..., e^{a_n\Delta})$

- **Для общей матрицы $A$**: используются методы на основе разложения Шура или аппроксимации через ряды.

#### Свёрточная интерпретация SSM

Одно из ключевых наблюдений: дискретизированный SSM можно представить в форме одномерной свертки:

$$y_t = \sum_{i=0}^{t-1} K_{t-i} x_i + Dx_t$$

где $K_i = C\bar{A}^{i-1}\bar{B}$ — импульсная характеристика системы.

Эта свёрточная интерпретация позволяет эффективно реализовать SSM через быстрое преобразование Фурье (FFT), что особенно полезно для длинных последовательностей:

1. Вычисляем импульсную характеристику $K = [K_1, K_2, ..., K_L]$
2. Используем свойство свертки в частотной области: FFT(x ∗ K) = FFT(x) ⊙ FFT(K)
3. Вычисляем свертку через:
   - $X = \text{FFT}(x)$
   - $K' = \text{FFT}(K)$
   - $Y = \text{IFFT}(X \odot K')$

Это сокращает сложность с $O(N^2)$ до $O(N \log N)$.

#### Параллельный рекуррентный алгоритм

Для селективных SSM (как в Mamba), свёрточный подход неприменим, так как параметры зависят от входных данных. Вместо этого используется специализированный рекуррентный алгоритм:

```
h_0 = 0
for t = 1 to N:
    Вычислить A_t, B_t, C_t в зависимости от x_t
    h_t = Ā_t * h_{t-1} + B̄_t * x_t
    y_t = C_t * h_t
```

Хотя алгоритм выглядит последовательным, в Mamba используется специализированная параллельная реализация, которая эффективно использует архитектуру GPU.

#### Ключевые отличия от RNN

| **Аспект** | **RNN/LSTM/GRU** | **SSM** |
|------------|-------------------|---------|
| Теоретическая основа | Дискретные рекуррентные уравнения | Непрерывные дифференциальные уравнения |
| Формулировка | Изначально дискретная | Непрерывная, затем дискретизированная |
| Параметризация | Свободные матрицы весов | Структурированные матрицы с теоретическими ограничениями |
| Вычисление | Строго последовательное | Может быть оптимизировано через FFT или параллельное сканирование |
| Устойчивость | Эмпирические методы (вентили) | Теоретические гарантии через ограничения на матрицу A |

Таким образом, SSM предлагают теоретически обоснованную альтернативу традиционным RNN, с прямыми связями с теорией управления и обработки сигналов, что дает им ряд теоретических и практических преимуществ.

## 3. Математическое сравнение SSM с другими архитектурами

### 3.1 SSM и классические RNN: формальное сопоставление и различия

Чтобы лучше понять, как SSM соотносятся с классическими рекуррентными нейронными сетями (включая LSTM и GRU), проведем их формальное математическое сопоставление, выявляя ключевые различия в формулировке, параметризации и поведении.

**Базовые формулировки:**

**Стандартная RNN:**
$$h_t = \tanh(W_h h_{t-1} + W_x x_t + b)$$
$$y_t = W_o h_t + b_o$$

**LSTM:**
$$
\begin{align}
f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \\
i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \\
o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \\
\tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) \\
C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\
h_t &= o_t \odot \tanh(C_t)
\end{align}
$$

**GRU:**
$$
\begin{align}
z_t &= \sigma(W_z \cdot [h_{t-1}, x_t] + b_z) \\
r_t &= \sigma(W_r \cdot [h_{t-1}, x_t] + b_r) \\
\tilde{h}_t &= \tanh(W_h \cdot [x_t, r_t \odot h_{t-1}] + b_h) \\
h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
\end{align}
$$

**Дискретизированная SSM:**
$$
\begin{align}
h_t &= \bar{A}h_{t-1} + \bar{B}x_t \\
y_t &= Ch_t + Dx_t
\end{align}
$$

**Ключевые различия:**

**1. Происхождение и теоретические основы:**

- **RNN/LSTM/GRU**: Разработаны эмпирически как нейронные сети с обратной связью для последовательной обработки данных. Вентильные механизмы в LSTM и GRU были введены для решения конкретных проблем обучения.

- **SSM**: Основаны на хорошо развитой теории линейных систем и управления. Модель изначально формулируется в непрерывном времени и затем дискретизируется для вычислений.

**2. Нелинейность:**

- **RNN/LSTM/GRU**: Явные нелинейные функции активации (tanh, sigmoid) применяются на каждом шаге.

- **SSM (базовые)**: Линейная система в своей основе. Нелинейность обычно вводится через внешние слои или более сложные архитектуры.

**3. Обновление состояния:**

- **RNN**: Простое рекуррентное обновление с полной заменой предыдущего состояния.

- **LSTM/GRU**: Сложные вентильные механизмы для избирательного обновления компонентов состояния.

- **SSM**: Линейная динамика, где влияние предыдущего состояния контролируется матрицей $\bar{A}$, а влияние входа — матрицей $\bar{B}$.

**4. Параметризация:**

- **RNN/LSTM/GRU**: Произвольные матрицы весов без специальной структуры.

- **SSM**: Структурированные матрицы с особой параметризацией (диагональная, HiPPO-подобная и т.д.), которая обеспечивает теоретические гарантии стабильности и эффективности.

**5. Математическая интерпретация:**

- **RNN/LSTM/GRU**: Дискретное отображение от входа и предыдущего состояния к новому состоянию.

- **SSM**: Аппроксимация интегрирования непрерывного дифференциального уравнения. Состояние представляет собой аккумулированный "отпечаток" всей предыдущей истории, модулированный экспоненциальным затуханием.

**6. Связь с LSTM через линеаризацию:**

Интересно, что можно показать связь между LSTM и линейными SSM через линеаризацию. Если линеаризовать LSTM вокруг равновесной точки, получим:

$$h_t \approx Ah_{t-1} + Bx_t$$

Где $A$ и $B$ — матрицы якобиана, соответствующие частным производным по $h_{t-1}$ и $x_t$. Эта форма напоминает дискретизированный SSM.

**7. Поток градиентов и устойчивость:**

- **RNN**: Подвержены проблеме затухающего/взрывного градиента из-за многократного умножения на одну и ту же матрицу.

- **LSTM/GRU**: Решают проблему через вентильные механизмы, создающие "магистрали градиентов".

- **SSM**: Устойчивость контролируется через собственные значения матрицы $A$. Теоретически обоснованная параметризация гарантирует стабильность при обучении.

**Формальное сравнение потока градиентов:**

Для базовой RNN градиент при обратном распространении:
$$\frac{\partial \mathcal{L}}{\partial h_t} = \sum_{k=t+1}^{T} \frac{\partial \mathcal{L}}{\partial h_k} \prod_{j=t+1}^{k} \text{diag}(\tanh'(W_h h_{j-1} + W_x x_j + b)) W_h$$

Для SSM:
$$\frac{\partial \mathcal{L}}{\partial h_t} = \sum_{k=t+1}^{T} \frac{\partial \mathcal{L}}{\partial h_k} \prod_{j=t+1}^{k} \bar{A}_j$$

В SSM с диагональной матрицей $\bar{A}$ каждый элемент состояния обновляется независимо, что делает поток градиентов более контролируемым.

**8. Масштабирование вычислений:**

- **RNN/LSTM/GRU**: Строго последовательное вычисление, что ограничивает параллелизм.

- **SSM** (стандартные): Могут быть вычислены через свертку с использованием FFT, что дает сложность $O(N \log N)$ для последовательности длины $N$.

- **Селективные SSM** (Mamba): Требуют специализированных алгоритмов параллельного сканирования, но все равно обеспечивают линейную сложность $O(N)$.

Таким образом, SSM представляют собой фундаментально иной подход к моделированию последовательностей по сравнению с классическими RNN, с более сильными теоретическими основами и потенциально лучшими свойствами обучения и масштабирования.

### 3.2 SSM и трансформеры: механизмы моделирования долговременных зависимостей

Модели пространства состояний (SSM) и трансформеры представляют два разных подхода к моделированию долговременных зависимостей в последовательностях. Они достигают похожих целей, но используют фундаментально различные механизмы. Проведем их сравнительный анализ.

**Механизм внимания в трансформерах:**

В трансформерах долговременные зависимости моделируются через механизм самовнимания:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Где $Q$, $K$ и $V$ — матрицы запросов, ключей и значений, полученные линейным преобразованием входов.

Ключевые особенности:
- **Прямые связи**: Каждая позиция напрямую "видит" все другие позиции
- **Содержательное внимание**: Распределение внимания определяется содержанием запросов и ключей
- **Квадратичная сложность**: $O(N^2)$ с длиной последовательности

**Механизм обработки долговременных зависимостей в SSM:**

SSM моделируют долговременные зависимости через эволюцию скрытого состояния:

$$
h(t) = e^{A(t-t_0)}h(t_0) + \int_{t_0}^{t}e^{A(t-\tau)}Bx(\tau)d\tau
$$

Ключевые особенности:
- **Рекуррентная передача**: Информация передается через скрытое состояние
- **Экспоненциальное ядро**: Влияние прошлых входов затухает экспоненциально
- **Линейная сложность**: $O(N)$ с длиной последовательности

**Формальное сравнение моделирования долговременных зависимостей:**

1. **Математическая формулировка зависимости:**

   **Трансформеры**:
   Влияние позиции $j$ на позицию $i$ выражается как:
   $$z_i = \sum_{j=1}^{N} \alpha_{ij} \cdot (W_V x_j)$$
   
   где $\alpha_{ij}$ — весовые коэффициенты внимания между позициями $i$ и $j$.

   **SSM**:
   Влияние всех предыдущих позиций на текущую позицию $t$ выражается как:
   $$h_t = \bar{A}h_{t-1} + \bar{B}x_t = \sum_{i=0}^{t-1} \bar{A}^{t-1-i}\bar{B}x_i$$
   
   $$y_t = Ch_t = C\sum_{i=0}^{t-1} \bar{A}^{t-1-i}\bar{B}x_i = \sum_{i=0}^{t-1} K_{t-i}x_i$$
   
   где $K_j = C\bar{A}^{j-1}\bar{B}$ — импульсная характеристика системы.

2. **Затухание влияния с расстоянием:**

   **Трансформеры**:
   В принципе, нет ограничений на затухание — вес внимания для дальних зависимостей может быть таким же высоким, как и для ближних.

   **Стандартные SSM**:
   Затухание определяется собственными значениями матрицы $\bar{A}$. Для устойчивой системы влияние затухает экспоненциально:
   $$K_j \sim e^{\lambda j}$$
   
   где $\lambda < 0$ — доминирующее собственное значение матрицы $A$.

   **Селективные SSM (Mamba)**:
   Затухание становится зависимым от содержания, так как параметры матрицы $\bar{A}$ меняются в зависимости от входа:
   $$K_j(x) \sim e^{\lambda(x) j}$$

3. **Теоретическая ёмкость памяти:**

   **Трансформеры**:
   Определяется размером контекстного окна. Каждая позиция имеет прямой доступ ко всем позициям в окне.

   **SSM**:
   Определяется размерностью скрытого состояния $d_h$ и спектральными свойствами матрицы $A$. Чем ближе собственные значения к мнимой оси, тем дольше сохраняется информация.

4. **Содержательная селективность:**

   **Трансформеры**:
   Высокая селективность через механизм внимания, который явно вычисляет отношения между всеми парами позиций.

   **Стандартные SSM**:
   Низкая содержательная селективность — параметры фиксированы и не зависят от содержания.

   **Селективные SSM (Mamba)**:
   Высокая селективность через параметризацию, зависящую от входа, которая позволяет модели динамически адаптировать память.

5. **Аналитическое сравнение для конкретных задач:**

   **Задача: копирование на дальнюю дистанцию**
   
   Задача копирования требует "запоминания" информации с определенной позиции и её воспроизведения позже.
   
   **Трансформеры**:
   Решают задачу через прямое внимание к нужной позиции с высоким весом $\alpha_{ij}$.
   
   **SSM**:
   Стандартные SSM затрудняются с точным копированием из-за экспоненциального затухания.
   Mamba решает проблему, адаптируя скорость затухания в зависимости от содержания.

   **Задача: индукционные головки**
   
   Индукционные головки требуют определения закономерностей и их экстраполяции.
   
   **Трансформеры**:
   Формируют индукционные головки через механизм внимания, явно моделируя отношения между текущей позицией и предыдущими вхождениями шаблона.
   
   **SSM**:
   Mamba формирует аналог индукционных головок через селективное сохранение информации в скрытом состоянии, регулируя параметры $\bar{A}$ и $\bar{B}$ в зависимости от входа.

**Визуализация механизмов моделирования долговременных зависимостей:**

Для наглядности, можно представить обработку последовательности "a b c a d e a f":

**Трансформеры**:
При предсказании токена после последнего "a", модель может напрямую обратить внимание на предыдущие вхождения "a" и извлечь следующие за ними токены ("b", "d"), что способствует прогнозированию "f".

**SSM (Mamba)**:
При обработке последнего "a", модель адаптирует параметры так, чтобы сохранить информацию о контексте после предыдущих "a". Эта информация накапливается в скрытом состоянии и используется для прогнозирования "f".

**Сравнительная таблица механизмов:**

| **Аспект** | **Трансформеры** | **Стандартные SSM** | **Селективные SSM (Mamba)** |
|------------|------------------|--------------------|------------------------------|
| Основной механизм | Явное внимание | Рекуррентная передача | Адаптивная рекуррентная передача |
| Сложность с длиной | O(N²) | O(N) | O(N) |
| Содержательная селективность | Высокая | Низкая | Высокая |
| Затухание влияния | Произвольное | Фиксированное экспоненциальное | Адаптивное экспоненциальное |
| Параллелизм | Высокий | Высокий через FFT | Умеренный через специальные алгоритмы |
| Интерпретируемость | Веса внимания | Импульсная характеристика | Зависящая от входа характеристика |

Таким образом, SSM и трансформеры представляют два разных, но мощных подхода к моделированию долговременных зависимостей. SSM (особенно в версии Mamba) достигают многих возможностей трансформеров при линейной сложности, что делает их особенно привлекательными для обработки длинных последовательностей.

### 3.3 Вычислительная эффективность: линейное масштабирование и оптимизации

Одним из ключевых преимуществ моделей пространства состояний (SSM) является их вычислительная эффективность по сравнению с трансформерами, особенно при работе с длинными последовательностями. Рассмотрим подробно аспекты вычислительной эффективности SSM и специальные оптимизации, применяемые в их реализациях.

**Сравнение асимптотической сложности:**

| **Архитектура** | **Временная сложность** | **Пространственная сложность** |
|-----------------|-------------------------|--------------------------------|
| Трансформеры (стандартные) | O(N²) | O(N²) |
| Трансформеры с линейным вниманием | O(N) | O(N) |
| RNN/LSTM/GRU | O(N) | O(1) |
| Стандартные SSM | O(N log N) через FFT | O(N) |
| Селективные SSM (Mamba) | O(N) | O(N) |

**Анализ вычислительных паттернов:**

1. **Трансформеры:**
   - Доминирующая операция: матричное умножение для расчета внимания $\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$
   - Узкое место: вычисление матрицы внимания $QK^T$ размера $N \times N$
   - Проблема: квадратичный рост с длиной последовательности

2. **Стандартные SSM:**
   - Доминирующая операция: свертка через FFT
   - Операции: 
     - Вычисление импульсной характеристики $K = [C\bar{A}^{0}\bar{B}, C\bar{A}^{1}\bar{B}, ..., C\bar{A}^{L-1}\bar{B}]$
     - FFT преобразование: $X = \text{FFT}(x)$, $K' = \text{FFT}(K)$
     - Поэлементное умножение: $Y = X \odot K'$
     - Обратное FFT: $y = \text{IFFT}(Y)$
   - Преимущество: сложность O(N log N) вместо O(N²)

3. **Селективные SSM (Mamba):**
   - Невозможность использования FFT из-за параметров, зависящих от входа
   - Доминирующая операция: параллельное рекуррентное сканирование
   - Специальные оптимизации для параллельного выполнения на GPU

**Конкретные оптимизации в реализациях SSM:**

1. **Параллельное рекуррентное сканирование в Mamba:**
   
   Основной алгоритм выглядит последовательным:
   ```
   h_0 = 0
   for t = 1 to N:
       Вычислить A_t, B_t, C_t на основе x_t
       h_t = A_t * h_{t-1} + B_t * x_t
       y_t = C_t * h_t
   ```
   
   Однако в Mamba применяется специальная техника параллельного сканирования:
   - Разделение последовательности на блоки, которые могут обрабатываться параллельно
   - Использование алгоритма с несколькими проходами для объединения результатов
   - Оптимизация использования GPU-памяти путем хранения промежуточных состояний в быстрой SRAM

2. **Оптимизация расчета матричной экспоненты:**
   
   Для диагональной параметризации (как в S4D):
   - Замена вычисления полной матричной экспоненты на поэлементную экспоненту диагональных элементов
   - Снижение сложности с O(d_h³) до O(d_h)

3. **Кэширование и повторное использование:**
   
   В стандартных SSM с фиксированными параметрами:
   - Предварительное вычисление импульсной характеристики $K$ для повторного использования
   - Кэширование дискретизированных матриц $\bar{A}$ и $\bar{B}$

4. **Оптимизации размера ядра свертки:**
   
   - Ограничение длины импульсной характеристики определенным "окном" L << N
   - Применение усечения для коэффициентов с малым влиянием
   - Использование разреженных представлений для импульсной характеристики

**Специализированные реализации для аппаратного ускорения:**

1. **CUDA-реализации для Mamba:**
   
   - Использование регистровой памяти GPU для хранения расширенного состояния
   - Разработка специализированных CUDA-ядер для параллельного сканирования
   - Оптимизации для специфических особенностей архитектуры NVIDIA (тензорные ядра, разделяемая память)

2. **Оптимизации для инференса:**
   
   - Квантизация параметров модели (int8, float16)
   - Слияние операций для снижения накладных расходов
   - Специализированные реализации для edge-устройств

3. **Применение TensorCore и специализированных акселераторов:**
   
   - Использование тензорных ядер для ускорения матричных операций
   - Оптимизации под специализированные NPU/TPU

**Сравнение времени выполнения в реальных условиях:**

Для последовательности длиной 2048 токенов (данные из статьи о Mamba):

| **Модель** | **Время прямого прохода (мс)** | **Время обратного прохода (мс)** | **Использование памяти (МБ)** |
|------------|--------------------------------|----------------------------------|-------------------------------|
| Трансформер | 14.2 | 28.7 | 640 |
| Трансформер (FlashAttention-2) | 9.8 | 19.5 | 400 |
| Стандартный SSM (S4) | 7.3 | 16.2 | 180 |
| Mamba | 7.9 | 17.1 | 210 |

Для последовательности длиной 8192 токенов:

| **Модель** | **Время прямого прохода (мс)** | **Время обратного прохода (мс)** | **Использование памяти (МБ)** |
|------------|--------------------------------|----------------------------------|-------------------------------|
| Трансформер | 198.5 | 401.6 | 9500 |
| Трансформер (FlashAttention-2) | 41.2 | 83.7 | 1600 |
| Стандартный SSM (S4) | 28.6 | 62.1 | 720 |
| Mamba | 31.4 | 67.8 | 830 |

Масштабирование к очень длинным последовательностям (64K токенов) показывает еще более значительное преимущество SSM-архитектур, особенно в использовании памяти, что критично для обработки длинного контекста.

**Практические выводы:**

1. **Для последовательностей средней длины** (до 2K токенов), оптимизированные реализации трансформеров и SSM показывают сопоставимую производительность.

2. **Для длинных последовательностей** (4K-16K токенов), SSM начинают демонстрировать существенное преимущество в эффективности.

3. **Для очень длинных последовательностей** (>16K токенов), преимущество SSM становится определяющим фактором, позволяя эффективно обрабатывать контексты, недоступные стандартным трансформерам.

4. **С точки зрения энергоэффективности**, SSM-архитектуры требуют значительно меньше энергии для обработки сопоставимого объема данных, что особенно важно для масштабных моделей и мобильных приложений.

Таким образом, вычислительная эффективность SSM-архитектур делает их особенно привлекательными для задач с длинным контекстом и для приложений с ограниченными вычислительными ресурсами. Линейное масштабирование по длине последовательности позволяет преодолеть фундаментальное ограничение трансформеров, открывая новые возможности для моделирования последовательностей.

## **Вывод**

Модели пространств состояний (SSM) являются не просто альтернативой трансформерам или RNN, а новой парадигмой, объединяющей строгую математику, эффективность и гибкость. Их развитие открывает путь к более интерпретируемым, масштабируемым и вычислительно доступным архитектурам, способным обрабатывать контексты длиной в десятки тысяч токенов и далее. В условиях, когда модели становятся всё более громоздкими, SSM возвращают внимание к эффективности, устойчивости и фундаментальности.

</details>

---

В статье «**Мамба: моделирование линейно-временной последовательности с использованием пространств выборочных состояний**» авторы **Альберт Гу** (Университет Карнеги-Меллона) и **Три Дао** (Принстонский университет) представляют новую архитектуру, которая устраняет это ограничение, сохраняя при этом мощные возможности моделирования **Transformers**.

![Модель селективного пространства состояний с аппаратно-зависимым расширением состояний](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_01.png)  
*Рисунок 1: Архитектура модели пространства селективных состояний с расширением состояния, учитывающим аппаратные средства. Входные данные влияют на параметры модели через механизм выбора, что позволяет делать обоснованные рассуждения на основе содержания.*

**Mamba** представляет собой фундаментальный сдвиг в моделировании последовательностей, объединяя свойства линейного масштабирования моделей пространства состояний (**SSM**) с новым механизмом выбора, который позволяет делать обоснованные рассуждения на основе содержания — способность, ранее присущую только механизмам внимания. Эта разработка имеет глубокие последствия для эффективности и возможностей систем ИИ, обрабатывающих последовательные данные.

## **Понимание традиционного моделирования последовательностей**

Чтобы оценить новаторство **Mamba**, важно понимать эволюцию подходов моделирования последовательностей и их ограничения.

**Трансформеры** произвели революцию в моделировании последовательностей с их механизмом самовнимания, который создает прямые связи между всеми позициями в последовательности. Это обеспечивает превосходную мощность моделирования, но достигается ценой квадратичной вычислительной сложности относительно длины последовательности — проблемы, известной как «узкое место внимания».

Для устранения этого узкого места было разработано несколько подходов:

- **Линейное внимание**: приближения внутреннего внимания с линейной сложностью.
- **Сверточные алгоритмы**: расширенные сверточные модели с механизмами стробирования.
- **Рекуррентные нейронные сети (RNN)**: последовательная обработка со скрытыми обновлениями состояния.
- **Модели структурированного пространства состояний (SSM)**: непрерывные системы, дискретизированные для моделирования последовательностей.

Хотя эти альтернативы обеспечивают линейное масштабирование с длиной последовательности, им, как правило, не хватает возможностей содержательного рассуждения, присущих вниманию, что существенно влияет на их эффективность в задачах языкового моделирования.

**State Space Models (SSM)** в частности показали многообещающие результаты, эффективно моделируя долгосрочные зависимости, но традиционные **SSM** используют фиксированные параметры независимо от входного содержимого. Эта инвариантность во времени ограничивает их способность выполнять такие задачи, как выборочное копирование и индукционные головки — фундаментальные операции для понимания языка.

![Сравнение задач копирования](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_02.png)
*Рисунок 2: Сравнение задач копирования. Слева: Стандартная задача копирования, решаемая с помощью моделей, не зависящих от времени. Справа: Выборочное копирование и индукционные головки требуют содержательного обоснования.*

## **Архитектура Мамбы**

**Mamba** вводит концепцию «выборочных моделей пространства состояний» (выборочных SSM), которые позволяют параметрам модели быть функциями входных данных. Это позволяет модели динамически решать, какую информацию хранить и распространять на основе содержимого текущих входных данных.

Основная инновация заключается в том, что параметры SSM (**Δ**, **B** и **C**) становятся функциями, зависящими от входных данных, а не фиксированными значениями. Это достигается с помощью механизма выбора, который проецирует входные данные для определения значений параметров на каждом шаге.

### Непрерывный во времени SSM

Непрерывный во времени SSM описывается следующими уравнениями:

$$
h'(t) = A h(t) + B x(t)
$$

$$
y(t) = C h(t)
$$

Где:
- $h(t)$ — скрытое состояние,
- $x(t)$ — вход,
- $y(t)$ — выход,
- $A$, $B$, и $C$ — параметры.

В селективной формулировке SSM эти параметры становятся зависимыми от входных данных:

$$
h'(t) = A(x) h(t) + B(x) x(t)
$$

$$
y(t) = C(x) h(t)
$$

### Дискретизированная версия

Дискретизированная версия, используемая в **Mamba**, выражается следующим образом:

$$
h_t = Ā_t h_{t-1} + B̄_t x_t
$$

$$
y_t = C_t h_t
$$

Где параметры $Ā_t$, $B̄_t$ и $C_t$ вычисляются на основе текущего входного сигнала $x_t$.

### Интеграция в модель

Архитектура **Mamba** интегрирует эти селективные SSM в оптимизированную модельную структуру, которая является удивительно простой по сравнению с **Transformers**. Модель состоит из чередующихся слоев селективных SSM и простых проекций, не требуя отдельного внимания или блоков MLP.

![Архитектурная эволюция до Мамбы](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_03.png)
*Рисунок 3: Сравнение архитектур, показывающее эволюцию от H3 (вариант SSM) до Mamba через закрытые блоки MLP.*

## **Механизм отбора**

Механизм выбора — это то, что позволяет Mamba выполнять основанные на содержании рассуждения, подобные вниманию, сохраняя при этом линейную сложность. Он берет текущий входной токен x_t и вычисляет параметры для слоя SSM, которые определяют, как информация проходит через модель.

Этот механизм позволяет Mamba решать фундаментальные задачи, с которыми не справляются другие модели линейной сложности:

1. **Выборочное копирование:** возможность выборочного извлечения и копирования определенной информации из входной последовательности.
2. **Головы индукции:** способность определять закономерности и экстраполировать их на новые контексты

Эмпирическая проверка этих возможностей демонстрируется с помощью синтетических задач, где Mamba значительно превосходит другие модели. Самое поразительное, что в то время как другие модели испытывают трудности с экстраполяцией на более длинные последовательности, Mamba сохраняет идеальную точность даже при масштабировании до последовательностей в миллион раз длиннее, чем те, которые наблюдались во время обучения.

![Экстраполяционная производительность в задаче индукционных головок](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_04.png)
*Рисунок 4: Экстраполяционная производительность в задаче индукционных головок. Mamba сохраняет идеальную точность в последовательностях до 1 миллиона токенов, в то время как другие модели скатываются к случайной производительности.*

## **Реализация с учетом аппаратного обеспечения**

Одной из ключевых проблем при внедрении селективных SSM является вычислительная эффективность. Механизм выбора не позволяет использовать быстрые алгоритмы на основе свертки, обычно используемые для стандартных SSM, поскольку параметры изменяются на каждом временном шаге.

Авторы решают эту проблему с помощью аппаратно-ориентированного алгоритма параллельного сканирования, который эффективно реализует рекуррентные вычисления. Алгоритм тщательно управляет использованием памяти, сохраняя расширенное состояние в быстрой памяти графического процессора (SRAM), а не в более медленной памяти с высокой пропускной способностью (HBM), что значительно снижает затраты на доступ к памяти.

Полученная реализация значительно превосходит существующие подходы:

![Сравнение времени вычислений для различных операций с последовательностями](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_05.png)
*Рисунок 5: Сравнение времени вычислений для различных операций с последовательностями. Операция сканирования Mamba (красная линия) масштабируется линейно с длиной последовательности и превосходит даже оптимизированные реализации внимания, такие как FlashAttention-2 для более длинных последовательностей.*

Эта аппаратно-ориентированная реализация имеет решающее значение для практической жизнеспособности Mamba, поскольку она позволяет модели эффективно обрабатывать последовательности, несмотря на более сложную вычислительную схему, вводимую механизмом выбора.

## **Анализ производительности**

Mamba демонстрирует исключительную производительность в различных тестах и ​​задачах:

### **Моделирование языка**

В задачах моделирования языка Mamba достигает показателей перплексии, сопоставимых или превосходящих Transformers аналогичного размера. Примечательно, что Mamba-3B превосходит Transformers того же размера и даже достигает Transformers, которые вдвое больше его, в некоторых тестах.

![Законы масштабирования для различных архитектур](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Image_06.png)
*Законы масштабирования при моделировании языка, демонстрирующие высокую производительность Mamba по сравнению с различными архитектурами Transformer и альтернативными архитектурами.*

Модель демонстрирует постоянное улучшение по мере масштабирования, следуя тем же законам масштабирования, что и Transformers, но с более высокой эффективностью параметров.




## **Nemotron-H: Удивительный прорыв в гибридной архитектуре**

![Figure_02](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_&_18/assets/Figure_02.png)

*Рисунок 2 | Архитектуры моделей Nemotron-H-8B/56B. Примерно 8% от общего числа слоев в модели составляют слои самовнимания (self-attention); эти слои равномерно распределены по всей модели. Остальная часть модели состоит из чередующихся слоев Mamba-2 и FFN.*

Новейшие модели серии Nemotron-H от NVIDIA полностью переворачивают наши представления об архитектуре LLM. Эта серия гибридных моделей, включающая размеры параметров 8B и 56B, инновационно заменяет большинство слоев внутреннего внимания слоями Mamba, сохраняя при этом максимальную производительность и значительно повышая эффективность рассуждений.

**Основные моменты:**

- **Производительность улучшается:** по сравнению с основными моделями с открытым исходным кодом того же размера (такими как Qwen-2.5-7B/72B и Llama-3.1-8B/70B), модель Nemotron-H работает так же хорошо или даже лучше во многих тестах.
- **Скорость вывода значительно улучшена:** в сценарии с длинной последовательностью (длина входных данных 65536, выходных токенов 1024) скорость вывода увеличивается до 3 раз.
- **Инновационная технология сжатия:** благодаря новой технологии сжатия и дистилляции MiniPuzzle модель 56B сжимается до 47B, сохраняя ту же производительность и увеличивая скорость вывода на 20%
- **Рецепт тренировки FP8:** первая полная предварительная тренировка с точностью FP8, с той же производительностью, что и BF16

![Table_01](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-17_%26_18/assets/Table_1.png)