# **Как LLM выучивают факты: Динамика, запоминание, галлюцинации. Новое исследование от Google DeepMind**

## **1. Введение**

### **Ключевая проблема**: 

Несмотря на то, что крупные языковые модели (LLM) в ходе предварительного обучения усваивают огромный объем фактических знаний, внутренние механизмы того, как они изучают, хранят и применяют эти знания, остаются «черным ящиком». Раскрытие этих механизмов критически важно не только для оптимизации обучения моделей, но и для понимания и решения таких ключевых проблем, как «галлюцинации» и трудности обновления знаний после предварительного обучения.  

### **Методология исследования**: 

Для системного изучения этой проблемы в данной статье разработана контролируемая экспериментальная методика. Вместо использования сложных реальных текстов исследователи создали синтетическую задачу на воспроизведение фактов (factual recall), основанную на искусственных биографиях. Такой синтетический подход позволяет точно контролировать свойства данных и эффективно отслеживать процесс усвоения знаний на всех этапах обучения.  

### **Ключевые выводы вкратце**:  

1. **Этапное обучение**: Знания усваиваются не линейно. Модель демонстрирует три уникальные фазы изучения фактов, включая критический «плато-период», когда производительность кажется застывшей, но внутри формируются механизмы представления. **Период плато возникает на этапе предварительного обучения (pre-training)!**
2. **Важность распределения данных**: Статистические свойства обучающих данных, особенно частотное распределение упоминаний разных «индивидуумов», существенно влияют на скорость и динамику обучения, в том числе на длительность плато-периода.  
3. **Сосуществование галлюцинаций и знаний**: Склонность модели к «галлюцинациям» (генерации информации о несуществующих объектах) проявляется почти одновременно с процессом усвоения реальных фактов.  
4. **Сложности дообучения**: Интеграция новых знаний в уже обученную модель через дообучение (fine-tuning) оказывается крайне трудной и часто приводит к быстрому разрушению существующей параметрической памяти (т.н. «катастрофическому забыванию»).

## **2. Отслеживание получения знаний: детали экспериментальной среды**

**Ключевой вопрос:**  
При изучении того, как LLM усваивают факты, мы сталкиваемся с двумя методологическими проблемами:

1. **Отделение знаний:**  
   Как измерить степень владения моделью фактическими знаниями, отделив их от других языковых способностей (грамматика, беглость и т.д.)?

2. **Эффективность и масштабируемость оценки:**  
   Как непрерывно отслеживать уровень знаний в процессе обучения, избегая дорогостоящих комплексных оценок (например, QA-тестов) на каждом этапе?

**Интуиция и общий подход:**  
Идеальная экспериментальная среда должна обладать следующими характеристиками:
- Факты дискретны и атомарны;
- Успех задачи напрямую зависит от воспроизведения конкретных пар "сущность-атрибут";
- Процесс генерации данных контролируем для регистрации статистических свойств;
- Оценка знаний интегрирована в стандартный процесс обучения.

Это естественным образом приводит к использованию структурированных синтетических данных для задач воспроизведения фактов.

### 2.1 Знания vs. Память: ключевое различие

Для точного понимания поведения модели необходимо различать **"знания"** и **"память"**:

| Концепт  | Определение                                                                 | Характеристики                          | Пример (знание "Париж — столица Франции")              |
|----------|-----------------------------------------------------------------------------|-----------------------------------------|-------------------------------------------------------|
| Знания   | Информация, усвоенная моделью, независимая от формы ввода и гибко применяемая | Абстрактность, обобщение, гибкость     | Ответы на вопросы: "Столица Франции?", "Какой стране принадлежит Париж?" |
| Память   | Воспроизведение конкретных примеров обучения, привязанное к форме ввода      | Конкретность, хрупкость                | Завершение предложения: "Париж — это ___ Франции"      |

### 2.2 Синтетический биографический набор данных

**Преимущества дизайна:**
- **Атомарность:** каждый факт (например, место рождения) независим, что отделяет способность "вспоминать" от способности "рассуждать";
- **Синтетичность и контроль:** точный контроль распределения данных (например, частоты появления персонажей) без помех из реальных корпусов;
- **Реалистичная статистика:** использование распространенных имен и мест сохраняет естественное распределение токенов;
- **Релевантность предыдущих исследований:** малые модели на подобных данных демонстрируют механизмы хранения знаний, сравнимые с большими LLM.

**Процесс генерации:**
1. **Создание базы персонажей:** Генерация N виртуальных "персонажей" с уникальными именами и шестью атрибутами;
2. **Заполнение шаблонов:** Для каждого атрибута случайно выбирается шаблон из библиотеки (25 вариантов на тип атрибута), куда подставляется конкретная информация (например, "[Имя] родился в [Место рождения]");
   - *Ключевой момент:* Множество шаблонов создает текстовое разнообразие, вынуждая модель выйти за рамки простого запоминания
3. **Сборка биографии:** Случайное упорядочивание предложений с атрибутами в полную биографи;
   - *Ключевой момент:* Случайный порядок предотвращает использование моделью последовательностных подсказок
4. **Разделение на обучающую/оценочную выборки:** Для каждого персонажа 20 шаблонов идут в обучение, 5 — в оценку. Это гарантирует, что модель сталкивается с новыми формулировками известных фактов, тестируя уровень абстракции знаний.

![Figure_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-15/assets/Figure_01.png)
> Figure_1. Процесс генерации данных, лежащий в основе синтетического набора биографий, на котором мы обучаем модели. Мы измеряем знания, содержащиеся в этих моделях, через величину потерь (loss), которую они достигают при предсказании атрибутных токенов (выделены синим).*

### 2.3 Масштабное измерение знаний: потери и точность атрибутов

**Проблема:** Многократные QA-оценки в процессе обучения требуют больших вычислений  
**Решение:** Специальная структура биографий превращает предсказание значений атрибутов в задачу воспроизведения фактов  

### 2.4 Стандартизированные модели и обучение

Для обеспечения воспроизводимости результатов:

- **Архитектура:** 8-слойный Decoder-only Transformer (44M параметров, на основе Hoffmann et al., 2022)
- **Оптимизатор:** AdamW с косинусным затуханием learning rate (без warmup)
- **Learning rate:** Настраивается индивидуально для каждого эксперимента

## **3. Динамический процесс приобретения знаний о языковой модели**

Основной вопрос: теперь, когда у нас есть способ измерения знаний, какова фактическая динамика приобретения знаний в ходе обучения ? Является ли это плавным, постепенным процессом или наблюдаются явные фазовые изменения и потенциальные сдвиги механизмов?

Основные выводы: Исследование показало, что независимо от того, как изменяются гиперпараметры, приобретение знаний обычно происходит по трехэтапной схеме . Среди них, казалось бы, «застойный» период плато играет решающую роль на уровне механизма.

### **3.1 Трехэтапная модель приобретения знаний**

![Figure_2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-15/assets/Figure_02.png)
> Figure_2. Приобретение знаний происходит в три этапа. (Слева) На очень коротком первом этапе модель изучает общую статистику значений атрибутов. На втором этапе производительность выходит на плато, соответствующее уровню, достижимому идеальной моделью без знаний об отдельных индивидах (это соответствует базовому уровню "без знаний" и почти нулевой точности распознавания). Длительность этого плато почти пропорциональна количеству индивидов (справа). Наконец, модель учится ассоциациям между субъектами и атрибутами: знания формируются по мере продолжения обучения (в центре). Результаты усреднены по 5 запускам (± стандартное отклонение).

Исследователи стабильно выявили следующие три этапа, наблюдая за кривыми изменения потерь атрибутов (Attribute Loss) и точности атрибутов (Attribute Accuracy) в процессе обучения:

| Этап | Название               | Основное поведение                          | Объяснение и механизм                                                                 |
|------|------------------------|---------------------------------------------|---------------------------------------------------------------------------------------|
| 1    | Начальное понимание / Статистическое обучение | Быстрое снижение потерь атрибутов.          | Модель быстро усваивает поверхностные статистические данные, такие как частые значения атрибутов, структуру биографии и т.д. К концу этапа производительность достигает уровня **базовой линии без знаний**. Модель понимает типы информации, но не связывает их с конкретными индивидуумами. |
| 2    | Плато производительности ("Грань усвоения знаний") | Потери атрибутов остаются на уровне **базовой линии без знаний**; точность атрибутов близка к 0. | Почему возникает застой? Две возможные причины: <br> (1) **Оптимизация**: модель попадает в седловую точку или локальный минимум функции потерь. <br> (2) **Статистика (√)**: модели требуется многократно наблюдать одного и того же индивидуума (несмотря на разное описание), чтобы надежно выделить факты, специфичные для него, из статистического шума. <br> **Доказательство**: длина плато линейно зависит от количества индивидуумов $\text{Плато} \propto N^{0.81}$, рис. 2 справа), что сильно поддерживает статистическую гипотезу. |
| 3    | Проявление знаний (Knowledge Emergence) | Потери атрибутов становятся значительно ниже **базовой линии без знаний**; точность атрибутов стабильно превышает 0. | На этом этапе модель активно формирует и укрепляет связи между **именем индивидуума** и его **специфическими атрибутами**. Параметризованные знания сохраняются и успешно извлекаются. |

Устойчивость модели: эта трехэтапная модель стабильно сохраняется при изменении скорости обучения (learning rate), весового затухания (weight decay), размера пакета (batch size), количества индивидуумов, размера модели и даже при замене механизма внимания на рекуррентную сеть (разновидность RNN).

### **Связь с полным циклом обучения языковых моделей**

Понимание трехэтапной модели приобретения знаний помогает переосмыслить весь процесс обучения современных LLM. Рассмотрим, как эти три этапа соотносятся с традиционными фазами обучения языковых моделей:

<details> 
    <summary><em><strong>Полный цикл обучения современных LLM</strong></em></summary>

---

Обучение современных больших языковых моделей — это сложный, многоэтапный и ресурсоемкий процесс. Он включает в себя несколько фаз, каждая из которых преследует свою цель: от формирования базового понимания языка до тонкой настройки поведения модели в соответствии с человеческими ожиданиями. Давайте разберем этот цикл по шагам.

**Полный цикл обучения LLM**

1.  **Подготовка данных**
2.  **Pre-training (Предварительное обучение)**
3.  **Supervised Fine-Tuning (SFT) / Instruction Fine-Tuning**
4.  **Reinforcement Learning from Human Feedback (RLHF)**
5.  **Оценка и Развертывание**

![Этапы обучения LLM](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-15/assets/Image_01.jpeg)
> Этапы обучения LLM

Процесс обучения языковых моделей обычно состоит из трёх показанных ниже этапов. Сначала мы предварительно обучаем языковую модель, и этот этап с большим отрывом является самой вычислительно затратной частью обучения. Дальше мы выполняем выравнивание, обычно при помощи трёхэтапного фреймворка с supervised fine-tuning (SFT) и обучением с подкреплением на основе обратной связи от человека (RLHF).
> Любопытно, что для этого этапа не нужна обратная связь от человека. В последних исследованиях изучается обучение с подкреплением на основе обратной связи ИИ (RLAIF)!

![Этапы обучения LLM](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-15/assets/Image_02.jpeg)

Из перечисленных выше этапов состоит стандартизированный конвейер обучения, применяемый для большинства современных LLM (например, ChatGPT или LLaMA-3). По сравнению с предварительным обучением, SFT и RLHF вычислительно малозатратны, но они требуют курирования датасета (или высококачественных выходных данных LLM, или обратной связи от человека по выходным данным LLM), что может быть сложным и длительным процессом.

![Этапы обучения LLM](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-15/assets/Image_03.jpeg)

Иногда для решения узкой задачи нам нужно сделать чуть больше, чем просто применить LLM. В частности, мы можем ещё больше специализировать языковую модель (при необходимости), применив или fine-tuning под предметную область, или контекстное обучение (см. ниже). Fine-tuning под предметную область просто продолжает обучение модели (обычно при помощи цели языкового моделирования, схожей с предварительным обучением/SFT) на данных, релевантных узкой задачи, а контекстное обучение добавляет больше контекста или примеров в промт языковой модели, который используется в качестве контекста для решения задачи.

![Этапы обучения LLM](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-15/assets/Image_04.jpeg)

Что такое выравнивание? Мы много раз использовали выше термин, который важно понять: выравнивание (alignment). Предварительно обученная языковая модель обычно не особо полезна. Если генерировать выходные данные при помощи этой модели, то результаты, вероятно, будут повторяющимися и малоприменимыми. Для создания более полезной языковой модели нам нужно выровнять эту модель под желания живого пользователя. Иными словами, вместо генерации наиболее вероятной текстовой последовательности наша модель учится генерировать текстовую последовательность, требуемую пользователю.

Такое выравнивание, выполняемое при помощи описанного выше трёхэтапного фреймворка SFT и RLHF, может использоваться для подталкивания LLM к разнообразным поведениям и свойствам. Обычно они обучают модель выбирать множество из одного или нескольких критериев, на которые делается упор в процессе выравнивания. Вот самые распространённые критерии выравнивания: повышение способности следования инструкциям, препятствование вредоносным выходным данным, повышение полезности LLM и многие другие.

---

<details> 
    <summary><em><strong>Этап 1. Подготовка данных</strong></em></summary>

## **Этап 1. Подготовка данных**

### **1.1 Концептуальные основы подготовки данных**

Качество и количество данных представляют собой фундаментальную основу для функционирования крупномасштабных языковых моделей (Large Language Models, LLM). Процесс подготовки данных для предварительного обучения следует рассматривать как многоаспектную задачу, стратегические направления которой формируются в значительной степени под влиянием современных исследований в области масштабирования нейронных архитектур. Комплексный характер данной задачи обусловлен необходимостью обеспечения оптимального баланса между объемом данных, их качеством и вычислительными ресурсами, доступными для обучения модели.

## **2. Систематизация процессов сбора и обработки данных**

### **2.1. Методы аккумуляции текстовых корпусов**

В современной практике разработки LLM аккумуляция данных осуществляется посредством интеграции масштабных текстовых корпусов из разнообразных источников, включая:
* интернет-ресурсы (преимущественно Common Crawl);
* литературные источники (коллекции Project Gutenberg, Google Books);
* научные публикации (репозиторий arXiv);
* программный код (GitHub);
* диалогические корпусы;
* медийные публикации.

Объемы используемых данных в современных проектах исчисляются терабайтами текстовой информации, что соответствует триллионам токенизированных элементов. Данная количественная характеристика обусловлена эмпирически установленными закономерностями масштабирования языковых моделей.

### **2.2. Теоретические и практические аспекты законов масштабирования (Chinchilla Scaling Laws)**

Стратегия сбора данных и планирования обучающего процесса находится в тесной взаимосвязи с эмпирическими законами масштабирования. Исследование DeepMind "Chinchilla" (2022) установило, что для достижения оптимальных показателей производительности модели при фиксированном вычислительном бюджете (FLOPs) необходимо обеспечить сбалансированное соотношение между размером модели (количество параметров, $N$) и объемом обучающих данных (количество токенов, $D$).

#### **2.2.1. Статистические закономерности и их интерпретация**
Согласно закономерностям Chinchilla, оптимальное соотношение выражается как $D \approx 20 \times N$, что указывает на необходимость обеспечения примерно 20 токенов обучающих данных на каждый параметр модели. Данная пропорция основана на эмпирических наблюдениях и статистическом анализе эффективности различных конфигураций моделей.

#### **2.2.2. Практическая значимость исследования в контексте развития LLM**
Открытие, сделанное в рамках проекта Chinchilla, продемонстрировало, что предшествующие крупномасштабные модели (включая GPT-3 и Gopher) характеризовались субоптимальным соотношением объема обучающих данных к размеру модели. В частности, модель Chinchilla (70 миллиардов параметров), обученная на 1,4 триллиона токенов (соотношение ~20:1), продемонстрировала превосходящие показатели по сравнению с более параметризованной моделью Gopher (280 миллиардов параметров), обученной на 300 миллиардах токенов (соотношение ~1:1).

#### **2.2.3. Методологические импликации для подготовки данных**
Установленные закономерности подчеркивают критическую важность не только увеличения параметрической размерности модели, но и опережающего наращивания объема качественных обучающих данных. Данный вывод стимулирует интенсификацию усилий исследовательского сообщества по сбору, фильтрации и обработке триллионов токенов текстовой информации с целью максимизации эффективности использования вычислительных ресурсов при обучении современных LLM.

### **2.3. Методология очистки и нормализации данных**

Процесс очистки данных представляет собой критически важный этап, оказывающий непосредственное влияние на качественные характеристики и безопасность модели. Данный процесс включает следующие компоненты:

* **Удаление дубликатов** на уровне документов и предложений с целью повышения разнообразия данных и предотвращения эффекта переобучения на повторяющихся паттернах.

* **Фильтрация низкокачественного контента**, включая спам, шаблонные тексты и автоматически генерируемую информацию. Данная процедура направлена на повышение общего качества корпуса и снижение риска обучения модели на нерелевантных или малоинформативных данных.

* **Обработка персональных данных** посредством удаления или анонимизации персональной идентифицирующей информации (PII) для обеспечения соответствия нормативам приватности и защиты данных.

* **Фильтрация нежелательного контента**, включая токсичные, предвзятые или потенциально вредоносные материалы. Данная задача представляет собой комплексную проблему, требующую применения как автоматизированных алгоритмов, так и методов ручной модерации.

* **Нормализация текстового материала**, которая может включать стандартизацию регистра, обработку пунктуации, унификацию пробельных символов. При этом следует отметить, что современные архитектуры часто демонстрируют повышенную эффективность при работе с текстом, максимально приближенным к исходной форме, сохраняющим оригинальный регистр и пунктуацию.

### **2.4. Алгоритмические подходы к токенизации текстовых данных**

Токенизация представляет собой процесс декомпозиции текста на элементарные единицы — токены, подлежащие обработке моделью. В контексте современных LLM преимущественно используются субсловные токенизаторы (subword tokenizers), среди которых можно выделить следующие:

* **Byte Pair Encoding (BPE)**: Алгоритм, начинающий процесс с отдельных символов (или байтов) и итеративно объединяющий наиболее частотные пары в новые токены словаря. Данный метод обеспечивает эффективное балансирование между размером словаря и способностью моделировать редкие слова.

* **WordPiece**: Метод, методологически близкий к BPE, но отличающийся критерием объединения пар, который заключается в максимизации правдоподобия данных обучения при заданной модели токенизации. Данный алгоритм нашел применение в моделях BERT и других разработках Google.

* **SentencePiece**: Токенизатор, осуществляющий обработку текста как последовательности Unicode-символов без предварительной сегментации по словам. Данная особенность обеспечивает универсальность применения для различных языковых систем, что особенно актуально в контексте многоязычных моделей. SentencePiece активно используется в современных архитектурах, включая Llama и серию GPT.

#### **2.4.1. Иллюстративный пример и функциональный анализ токенизации**

В качестве примера рассмотрим процесс токенизации слова "масштабирование", которое может быть сегментировано на токены вида `[" мас", "штаб", "ирование"]` или `[" масштаб", "ирован", "ие"]`. Данный механизм обеспечивает модели следующие функциональные возможности:

1. **Обработка неизвестной лексики**: Способность анализировать и генерировать незнакомые или редкие слова посредством их композиции из известных сегментов, что существенно повышает гибкость модели при работе с открытым словарем.

2. **Оптимизация размера словаря**: Возможность эффективного управления размерностью словаря (обычно в диапазоне от 30 до 100 тысяч токенов), что было бы невозможно при использовании целых слов в качестве базовых единиц токенизации.

## **3. Выводы**

Законы масштабирования Chinchilla представляют собой значимый методологический ориентир на этапе подготовки данных, определяя целевые объемы текстовых корпусов в зависимости от планируемой параметрической размерности модели и доступных вычислительных ресурсов. Данные закономерности подчеркивают, что эффективность LLM определяется не исключительно архитектурными и алгоритмическими аспектами обучения, но и в существенной степени стратегическими подходами к работе с данными.

В контексте современных исследований в области искусственного интеллекта и, в частности, обработки естественного языка, глубокое понимание взаимосвязи между характеристиками обучающих данных и производительностью модели представляется фундаментальным для дальнейшего прогресса в разработке все более совершенных языковых моделей. Стратегическое планирование процессов сбора и обработки данных, основанное на эмпирически установленных закономерностях, становится одним из ключевых факторов, определяющих успешность проектов в области разработки крупномасштабных языковых моделей.

</details> 

<details> 
    <summary><em><strong>Этап 2. Pre-training (Предварительное обучение)</strong></em></summary>

## **2. Pre-training (Предварительное обучение)**

### **2.1. Концептуальная характеристика этапа предварительного обучения**

Предварительное обучение (pre-training) представляет собой наиболее вычислительно интенсивный этап в разработке крупномасштабных языковых моделей, в ходе которого осуществляется формирование базового понимания моделью статистических и семантических закономерностей языка. Этот процесс требует значительных вычислительных ресурсов и имеет определяющее значение для функциональных возможностей модели.

Основная целевая задача предварительного обучения заключается в формировании способности модели прогнозировать следующий токен в последовательности на основе контекста предыдущих токенов. Данный подход позволяет модели усваивать грамматические структуры, фактологическую информацию, основы логических взаимосвязей и определенные аспекты рассуждений, представленные в масштабных текстовых корпусах. Процесс обучения основывается на выявлении статистических закономерностей в последовательностях токенов, что обеспечивает формирование обобщенных репрезентаций языковых структур без необходимости в явной аннотации данных.

### **2.2. Архитектурные компоненты современных языковых моделей**

В настоящее время доминирующей архитектурой в области крупномасштабных языковых моделей является архитектура Transformer, представленная в исследовании "Attention Is All You Need" (Vaswani et al., 2017).

На рисунке ниже, изображена архитектура модели Transformer. Она состоит из двух основных частей: **кодера (encoder)** и **декодера (decoder)**.

![Figure_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-04/assets/Figure_1.png)

### Кодер (Encoder)
Кодер обычно находится в левой части архитектуры. Он состоит из нескольких слоев, каждый из которых включает:
1. **Multi-Head Attention** — механизм внимания, который позволяет модели фокусироваться на разных частях входных данных.
2. **Add & Norm** — слой, который добавляет входные данные к результату внимания (residual connection) и применяет нормализацию.
3. **Feed Forward** — полносвязный слой, который применяется к каждому элементу последовательности независимо.
4. **Add & Norm** — снова добавляет входные данные к результату и нормализует.

Эти слои повторяются несколько раз (обычно 6 или более) для создания глубокой модели.

### Декодер (Decoder)
Декодер обычно находится в правой части архитектуры. Он также состоит из нескольких слоев, но имеет дополнительные компоненты:
1. **Masked Multi-Head Attention** — механизм внимания, который маскирует будущие токены, чтобы предотвратить "подглядывание" вперед.
2. **Add & Norm** — слой, который добавляет входные данные к результату внимания и нормализует.
3. **Multi-Head Attention** — механизм внимания, который учитывает выход кодера.
4. **Add & Norm** — снова добавляет входные данные к результату и нормализует.
5. **Feed Forward** — полносвязный слой, аналогичный тому, что используется в кодировщике.
6. **Add & Norm** — завершающий слой добавления и нормализации.

### Входы и выходы
- **Input Embedding** и **Positional Encoding** относятся к входным данным, которые подаются в кодер.
- **Output Embedding** и **Outputs (shifted right)** относятся к выходным данным, которые обрабатываются декодером.

### **2.3. Методология процесса обучения**

#### **2.3.1. Формулировка задачи предварительного обучения**

В контексте крупномасштабных языковых моделей основной задачей предварительного обучения является причинное языковое моделирование (Causal Language Modeling, CLM), заключающееся в последовательном предсказании токенов. Модель получает на вход последовательность токенов $t_1, t_2, ..., t_{k-1}$ и оптимизируется для предсказания следующего токена $t_k$. Эта формулировка задачи позволяет модели осваивать широкий спектр языковых закономерностей без необходимости в специфической разметке данных.

#### **2.3.2. Функция потерь и её обоснование**

В качестве целевой функции оптимизации используется кросс-энтропийная функция потерь (Cross-Entropy Loss), которая количественно оценивает расхождение между предсказанным моделью распределением вероятностей следующего токена и фактическим распределением, представленным истинным следующим токеном.

Математически для последовательности $T = (t_1, ..., t_L)$ функция потерь выражается как:

$$L_{Pretrain}(\theta) = - \sum_{i=1}^{L} \log P(t_i | t_{1}, ..., t_{i-1}; \theta)$$

где: 

- $\theta$ — параметры модели
- $P(t_i | t_{1}, ..., t_{i-1}; \theta)$ — вероятность $i$-го токена, предсказанная моделью на основе предшествующих токенов. 
В практических реализациях вычисление проводится по батчам последовательностей.

Содержательная интерпретация данной функции заключается в том, что модель штрафуется при присвоении низкой вероятности токену, который фактически следует за предшествующей последовательностью в обучающем тексте. Минимизация этой функции стимулирует модель к более точному предсказанию текстовых последовательностей, что приводит к усвоению структурных и семантических закономерностей языка.

#### **2.3.3. Оптимизационные алгоритмы**

В процессе предварительного обучения крупномасштабных языковых моделей преимущественно используются адаптивные алгоритмы оптимизации, такие как Adam (Adaptive Moment Estimation) или его модификация AdamW, характеризующаяся усовершенствованным механизмом регуляризации весов. Ключевым гиперпараметром оптимизационного процесса является скорость обучения (learning rate), определяющая величину обновления параметров на каждой итерации.

#### **2.3.4. Стратегии управления скоростью обучения**

Процесс обучения обычно начинается с низких значений скорости обучения, которая постепенно увеличивается до максимального значения в течение начальных нескольких тысяч итераций (фаза разогрева, warmup). Этот подход способствует стабилизации процесса обучения на начальных этапах. После фазы разогрева скорость обучения постепенно снижается согласно определенному расписанию (например, косинусоидальному), что способствует более точной сходимости модели к оптимуму.

Математическая формулировка фазы разогрева представляется следующим образом:

$$lr(step) = lr_{max} \times \frac{step}{warmup\_steps}$$ 

для $step \leq warmup\_steps$

где: 

- $lr(step)$ — скорость обучения на итерации $step$ 
- $lr_{max}$ — максимальное значение скорости обучения
- $warmup\_steps$ — количество итераций фазы разогрева. На этом этапе скорость обучения линейно возрастает от почти нулевого значения до максимального, что обеспечивает плавное начало процесса оптимизации.

### **2.4. Вычислительные аспекты и стратегии масштабирования**

Предварительное обучение крупномасштабных языковых моделей требует значительных вычислительных ресурсов, включая сотни или тысячи графических (GPU) или тензорных (TPU) процессоров, функционирующих в течение продолжительного времени. Для обеспечения эффективности процесса обучения применяются следующие стратегии распределенного обучения:

* **Параллелизм данных (Data Parallelism)**: Предполагает распределение различных батчей данных между вычислительными устройствами, каждое из которых содержит полную копию модели. После обработки батчей вычисляется средний градиент, который используется для обновления параметров на всех устройствах.

* **Тензорный параллелизм (Tensor Parallelism)**: Обеспечивает распределение отдельных тензоров (матриц весов) между устройствами, что особенно эффективно для моделей, превышающих объем памяти одного устройства. Этот подход позволяет обрабатывать значительно более крупные модели, чем было бы возможно на одном устройстве.

* **Конвейерный параллелизм (Pipeline Parallelism)**: Предполагает распределение различных слоев модели между устройствами с организацией конвейерной обработки последовательных микро-батчей, что обеспечивает эффективное использование вычислительных ресурсов при обучении глубоких архитектур.

## **3. Выводы**

Результатом этапа предварительного обучения является базовая модель (base model), характеризующаяся развитыми способностями к пониманию и генерации текста. Однако, следует отметить, что данная модель еще не оптимизирована для выполнения специфических инструкций или поддержания диалогического взаимодействия, что обуславливает необходимость последующих этапов обучения.

Важно подчеркнуть, что качество базовой модели, полученной на этапе предварительного обучения, в значительной степени определяет потенциал модели для последующих этапов тонкой настройки и обучения с подкреплением. Таким образом, оптимизация процесса предварительного обучения представляет собой критически важную задачу в контексте разработки высокоэффективных крупномасштабных языковых моделей.

</details>

<details> 
    <summary><em><strong>Этап 3. Supervised Fine-Tuning (SFT) / Instruction Fine-Tuning</strong></em></summary>

### 3. Supervised Fine-Tuning (SFT) / Instruction Fine-Tuning

Supervised fine-tuning (SFT) — это первый этап обучения в рамках процесса выравнивания LLM. Во-первых, нам нужно выполнить куририрование датасета высококачественных выходных данных LLM (по сути, это просто примеры правильного поведения LLM) (см. ниже). Затем мы напрямую выполняем fine-tuning модели по этим примерам. Supervised («с учителем») в термине SFT означает, что мы собираем датасет примеров того, чему должна подражать модель. Затем модель учится воспроизводить стиль этих примеров в процессе fine-tuning.

![Image_05](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-15/assets/Image_05.jpeg)

Связь с прогнозированием следующего токена. Любопытно, что SFT не особо отличается от предварительного обучения языковой модели — и в обучении, и в SFT используется в качестве цели обучения прогнозирование следующего токена! Основное различие связано с тем, как применяются данные. Во время предварительного обучения модели мы используем огромные корпуса сырых текстовых данных. В SFT в качестве «учителя» применяется датасет высококачественных выходных данных LLM. На каждой итерации обучения мы сэмплируем множество примеров, затем выполняем fine-tuning модели на этих данных, используя в качестве цели обучения прогнозирование следующего токена. Обычно прогнозирование следующего токена применяется только к части каждого примера, соответствующей выходным данным LLM (например, к ответу на рисунке выше).

#### **Формализуем процесс SFT**

С формальной точки зрения, этап Supervised Fine-Tuning (SFT) можно описать следующим образом. Пусть $\pi_{\theta}$ обозначает предварительно обученную языковую модель с параметрами $\theta$, полученными на этапе pre-training. Цель SFT — адаптировать эту модель для лучшего следования инструкциям и генерации ответов в желаемом стиле, используя курируемый набор данных.

Этот набор данных, $D_{SFT}$, состоит из $M$ пар примеров:
$$D_{SFT} = \{(x^{(i)}, y^{(i)})\}_{i=1}^{M}$$
где:
*   $x^{(i)}$ — это входная последовательность токенов, представляющая собой инструкцию, вопрос или любой другой промпт (например, `Input` на рисунке выше).
*   $y^{(i)}$ — это соответствующая эталонная выходная последовательность токенов, демонстрирующая желаемый ответ или поведение модели (например, `Output` на рисунке выше).

Процесс SFT заключается в дальнейшей оптимизации параметров модели, инициализированных значениями $\theta$, для минимизации функции потерь на датасете $D_{SFT}$. В качестве функции потерь, аналогично этапу предварительного обучения, обычно используется кросс-энтропийная функция потерь, но с важным отличием: потери вычисляются **только** по токенам эталонного ответа $y^{(i)}$. Это ключевой момент, отличающий SFT от pre-training, где потери вычисляются по всей последовательности токенов.

Пусть $y^{(i)} = (y_1^{(i)}, y_2^{(i)}, ..., y_{L_i}^{(i)})$ — последовательность токенов эталонного ответа длиной $L_i$. Модель $\pi_{\phi}$, с параметрами $\phi$ (инициализированными как $\phi_{init} = \theta$), обучается максимизировать вероятность генерации последовательности $y^{(i)}$ при условии входной последовательности $x^{(i)}$. Это эквивалентно минимизации следующей функции потерь $L_{SFT}$:

$$L_{SFT}(\phi) = - \sum_{i=1}^{M} \frac{1}{L_i} \sum_{j=1}^{L_i} \log P_{\pi_{\phi}}(y_j^{(i)} | x^{(i)}, y_1^{(i)}, ..., y_{j-1}^{(i)}; \phi)$$

где:
*   $\phi$ — параметры модели, которые оптимизируются в процессе SFT.
*   $P_{\pi_{\phi}}(y_j^{(i)} | x^{(i)}, y_1^{(i)}, ..., y_{j-1}^{(i)}; \phi)$ — это вероятность $j$-го токена ответа $y_j^{(i)}$, предсказанная моделью $\pi_{\phi}$ на основе входного промпта $x^{(i)}$ и всех предыдущих *истинных* токенов ответа $y_1^{(i)}, ..., y_{j-1}^{(i)}$ (этот режим называется "teacher forcing").
*   Суммирование ведется по всем примерам $i$ в датасете $D_{SFT}$ и по всем токенам $j$ в каждом эталонном ответе $y^{(i)}$.
*   Часто используется усреднение по длине последовательности $L_i$ (как показано в формуле) или по общему числу токенов в батче для нормализации потерь.

**Практическая реализация.** На практике это часто реализуется путем конкатенации промпта и ответа ($c^{(i)} = x^{(i)} \oplus y^{(i)}$) и применения стандартной функции потерь для предсказания следующего токена ко всей последовательности $c^{(i)}$. Однако, при вычислении градиентов и обновлении весов учитываются только потери, соответствующие токенам из $y^{(i)}$. Это достигается с помощью механизма маскирования (loss masking), который обнуляет (игнорирует) потери для токенов, принадлежащих входному промпту $x^{(i)}$. Таким образом, модель обучается предсказывать и генерировать только желаемый ответ, продолжая последовательность, заданную промптом.

Оптимизация параметров $\phi$ выполняется с использованием стандартных методов стохастического градиентного спуска (например, AdamW), аналогично этапу предварительного обучения, но обычно с меньшей скоростью обучения (learning rate) и на значительно меньшем (на порядки) объеме данных по сравнению с pre-training. Цель SFT — не столько обучить модель новым знаниям (хотя это тоже может происходить в некоторой степени), сколько "научить" ее использовать уже имеющиеся знания для генерации ответов в определенном формате и стиле, соответствующем примерам из $D_{SFT}$.

Стоит отметить, что SFT немного отличается от обобщённого fine-tuning. Обычно fine-tuning модели глубокого обучения выполняется для того, чтобы научить модель решению конкретной задачи, но это делает модель более специализированной и менее обобщённой — модель становится "нишевым специалистом". Модель с большей вероятностью будет точнее решать задачу, на которую выполнялся fine-tuning, по сравнению с обобщённой моделью, но может потерять способность решать другие задачи. SFT же — это фундаментальный компонент выравнивания языковых моделей, в том числе и обобщённых базовых моделей. Так как мы выполняем fine-tuning модели, чтобы она подражала правильному стилю или поведению, а не чтобы она решала конкретную задачу, она не теряет своей способности решать обобщённые задачи.



</details>

<details> 
    <summary><em><strong>Этап 4. Reinforcement Learning from Human Feedback (RLHF)</strong></em></summary>

### 4. Reinforcement Learning from Human Feedback (RLHF)

Это наиболее сложный этап, направленный на то, чтобы согласовать поведение модели с человеческими предпочтениями, делая ее ответы более полезными, честными и безвредными.

*   **Цель:** Тонкая настройка модели с использованием сигналов обратной связи от людей, чтобы она генерировала ответы, которые люди оценивают как высококачественные.
*   **Процесс состоит из двух основных стадий:**

    **Стадия 4.1: Обучение модели вознаграждения (Reward Model - RM)**
    *   **Сбор данных:**
        1.  Берется набор промптов.
        2.  SFT-модель генерирует несколько (часто два) вариантов ответа на каждый промпт.
        3.  Люди-асессоры сравнивают эти ответы и выбирают лучший (или ранжируют их).
        4.  Собирается датасет предпочтений: `(промпт, выбранный_ответ, отвергнутый_ответ)`.
    *   **Архитектура RM:** Обычно используется та же архитектура LLM, что и основная модель (или только ее энкодер), но с добавлением "головы" (линейного слоя), которая предсказывает скалярное значение — "оценку" (вознаграждение) качества ответа на данный промпт.
    *   **Функция потерь (на основе модели Брэдли-Терри):** Обучаем RM так, чтобы она присваивала более высокую оценку выбранному ответу ($y_w$) по сравнению с отвергнутым ($y_l$).

        *   **Математическая формализация:**
            $L_{RM}(\phi) = - \mathbb{E}_{(x, y_w, y_l) \sim D} [\log(\sigma(r_\phi(x, y_w) - r_\phi(x, y_l)))]$
            Где:
            *   $D$ — датасет человеческих предпочтений.
            *   $x$ — промпт.
            *   $y_w$ — выбранный (предпочтительный) ответ.
            *   $y_l$ — отвергнутый ответ.
            *   $r_\phi(x, y)$ — скалярная оценка, выдаваемая моделью RM с параметрами $\phi$.
            *   $\sigma$ — сигмоидная функция.

        *   **Пояснение:** Функция потерь штрафует RM, если она присваивает отвергнутому ответу $y_l$ оценку, близкую или большую, чем выбранному ответу $y_w$. Минимизация потерь заставляет RM научиться предсказывать, какой ответ люди сочтут лучшим.

    **Стадия 4.2: Оптимизация политики с помощью Reinforcement Learning (RL)**
    *   **Цель:** Настроить SFT-модель (теперь она называется "политикой" $\pi^{RL}$), чтобы она генерировала ответы, максимизирующие оценку от обученной RM, при этом не слишком сильно отклоняясь от исходной SFT-модели.
    *   **Процесс (с использованием PPO - Proximal Policy Optimization):**
        1.  Берется промпт $x$ из датасета промптов.
        2.  Текущая политика $\pi_{\theta}^{RL}$ (инициализированная весами SFT-модели) генерирует ответ $y$.
        3.  Модель вознаграждения $r_\phi(x, y)$ оценивает сгенерированный ответ $y$.
        4.  Алгоритм PPO обновляет параметры $\theta$ политики $\pi_{\theta}^{RL}$, чтобы максимизировать ожидаемое вознаграждение, используя специальную целевую функцию.
    *   **Целевая функция (упрощенная для PPO):**
        $J(\theta) = \mathbb{E}_{x \sim D_{prompt}, y \sim \pi_{\theta}^{RL}(y|x)} [r_\phi(x, y) - \beta \text{KL}(\pi_{\theta}^{RL}(y|x) || \pi^{SFT}(y|x))]$
        Где:
        *   $\pi_{\theta}^{RL}$ — оптимизируемая политика (LLM).
        *   $\pi^{SFT}$ — исходная SFT-модель (ее веса заморожены).
        *   $r_\phi(x, y)$ — вознаграждение от RM.
        *   $\text{KL}(\pi_{\theta}^{RL} || \pi^{SFT})$ — KL-дивергенция между распределениями вероятностей токенов, выдаваемыми текущей политикой и исходной SFT-моделью. Это штраф за отклонение от SFT-модели.
        *   $\beta$ — коэффициент, контролирующий силу KL-штрафа.

    *   **Пояснение:**
        *   Первый член $r_\phi(x, y)$ побуждает модель генерировать ответы, которые нравятся RM (и, следовательно, людям).
        *   Второй член (KL-штраф) не дает модели слишком сильно измениться по сравнению с SFT-моделью. Это важно по двум причинам: 1) Предотвращает "оптимизационный коллапс", когда модель находит способ получить высокое вознаграждение от RM, генерируя бессмысленные или нежелательные ответы (эксплуатируя недостатки RM). 2) Помогает сохранить общие языковые способности модели, приобретенные на этапах pre-training и SFT.
        *   PPO использует более сложную суррогатную функцию потерь с "клиппингом" отношения вероятностей для обеспечения стабильности обновлений, но общая цель остается той же.

*   **Альтернативы RLHF:** В последнее время набирают популярность методы, такие как **Direct Preference Optimization (DPO)**, которые позволяют оптимизировать модель на основе данных о предпочтениях напрямую, без явного обучения отдельной модели вознаграждения и использования сложных RL-алгоритмов, что может упростить и стабилизировать процесс.

**Результат RLHF:** Модель, чьи ответы лучше соответствуют человеческим предпочтениям по полезности, честности и безвредности. Это обычно финальная версия модели, готовая к оценке и развертыванию.

</details> 

<details> 
    <summary><em><strong>Этап 5. Оценка и Развертывание</strong></em></summary>

### 5. Оценка и Развертывание

После обучения модель проходит тщательную оценку перед развертыванием.

*   **Оценка:**
    *   **Академические бенчмарки:** Наборы задач для оценки понимания языка, ответов на вопросы, логических рассуждений (например, MMLU, HellaSwag, ARC, TruthfulQA).
    *   **Оценка людьми:** Самый важный вид оценки для диалоговых моделей. Асессоры оценивают качество ответов по различным критериям (полезность, релевантность, безопасность, тон голоса) или сравнивают ответы разных моделей на одни и те же промпты (A/B тесты, Side-by-Side сравнения).
    *   **Специализированные тесты:** Проверка на наличие предвзятостей, генерацию токсичного контента, способность к кодированию, математике и т.д.
*   **Развертывание (Deployment):**
    *   **Инференс:** Запуск модели для генерации ответов на запросы пользователей. Требует значительных вычислительных ресурсов (GPU).
    *   **Оптимизация для инференса:** Техники вроде квантования (уменьшение точности представления весов модели) и дистилляции (обучение меньшей модели повторять поведение большой) для снижения требований к памяти и ускорения работы.
    *   **Мониторинг:** Постоянное отслеживание производительности модели, сбор обратной связи от пользователей, выявление проблем (например, "галлюцинаций" - генерации неверной информации).
    *   **Итеративное улучшение:** Цикл обучения (особенно SFT и RLHF) может повторяться с новыми данными и обратной связью для постоянного улучшения модели.

</details>

<details> 
    <summary><em><strong>Прогнозирование следующего токена (next-token prediction)</strong></em></summary>

## Прогнозирование следующего токена в обучении LLM

Прогнозирование следующего токена (next-token prediction) — это фундаментальная задача, лежащая в основе предварительного обучения (pre-training) большинства современных больших языковых моделей (LLM) авторегрессионного типа, таких как GPT, Llama, PaLM, Mistral и другие. Эта стратегия является ключевым примером **самообучения (self-supervised learning)**, позволяя моделям изучать сложные закономерности языка, грамматику, семантику и даже факты о мире, используя огромные объемы неразмеченного текста без необходимости ручной разметки. В данном обзоре мы детально рассмотрим механизмы, лежащие в основе next-token prediction, принципы их работы, математическую формализацию и значение этого подхода для создания мощных LLM.

### Механизм Next-Token Prediction

В основе механизма лежит идея авторегрессии: предсказание следующего элемента последовательности на основе всех предыдущих. Когда LLM получает на вход последовательность токенов $t_1, t_2, ..., t_{k-1}$, ее задача — предсказать наиболее вероятный следующий токен $t_k$.

1.  **Обработка контекста с помощью Transformer:** Современные LLM используют архитектуру **Transformer**. Ее ключевой элемент — механизм **Self-Attention**. Он позволяет модели динамически определять, какие из предыдущих токенов ($t_1$ до $t_{k-1}$) наиболее релевантны для предсказания следующего токена $t_k$. Модель вычисляет "оценки внимания" между текущей позицией (где ожидается $t_k$) и всеми предыдущими позициями, преобразуя их в веса, которые определяют, насколько сильно каждый предыдущий токен влияет на предсказание.
    *   **Маскированное Self-Attention (Causal Masking):** В моделях, обучаемых на next-token prediction (декодерах Transformer), используется специальный тип внимания — маскированное. Маска не позволяет механизму внимания "заглядывать вперед" в последовательность. При обработке $i$-й позиции модель может учитывать только токены с $1$-й по $i$-ю (или $i-1$ для предсказания $i$-го токена), но не $i+1$, $i+2$ и т.д. Это критически важно для сохранения авторегрессионного свойства: предсказание зависит только от прошлого, а не от будущего.

2.  **Генерация распределения вероятностей:** После того как входная последовательность $t_1, ..., t_{k-1}$ обработана несколькими слоями Transformer, модель генерирует вектор скрытого состояния $h_{k-1}$, который содержит информацию обо всем предыдущем контексте. Этот вектор $h_{k-1}$ подается на:
    *   **Линейный слой (Output Embedding Layer):** Преобразует вектор скрытого состояния $h_{k-1}$ в вектор **логитов** $z_k$ размерностью $V$, где $V$ — размер словаря модели. Каждый элемент $z_{k,j}$ этого вектора соответствует "оценке" (не нормализованной логарифмической вероятности) того, насколько вероятен $j$-й токен словаря как следующий токен $t_k$.
    *   **Функция Softmax:** Преобразует вектор логитов $z_k$ в вектор **вероятностей** $P_k$. Каждый элемент $P_{k,j}$ этого вектора представляет собой вероятность $P(t_k = \text{token}_j | t_1, ..., t_{k-1})$, то есть вероятность того, что $j$-й токен словаря является следующим.
        $P_{k,j} = \frac{\exp(z_{k,j})}{\sum_{i=1}^{V} \exp(z_{k,i})}$
        Сумма всех элементов вектора $P_k$ равна 1, что делает его корректным распределением вероятностей над словарем.

### Процесс обучения

Обучение модели заключается в настройке ее параметров (весов) таким образом, чтобы она как можно точнее предсказывала следующий токен в реальных текстовых данных из огромного обучающего корпуса.

1.  **Задача:** Для заданной последовательности токенов $T = (t_1, t_2, ..., t_L)$ из обучающего корпуса, модель должна научиться максимизировать вероятность этой последовательности. В авторегрессионной модели вероятность последовательности раскладывается на произведение условных вероятностей (согласно цепному правилу теории вероятностей):
    $P(T; \theta) = P(t_1, ..., t_L; \theta) = \prod_{i=1}^{L} P(t_i | t_1, ..., t_{i-1}; \theta)$
    где $\theta$ обозначает параметры модели. Максимизация этой вероятности (или, что эквивалентно, максимизация логарифма правдоподобия) является целью обучения.

2.  **Функция потерь (Loss Function):** На практике вместо максимизации правдоподобия минимизируют **отрицательный логарифм правдоподобия (Negative Log-Likelihood - NLL)**. Для задачи классификации на множество классов (где классы - это токены словаря), NLL эквивалентен **Cross-Entropy Loss (Перекрестная энтропия)**. Для одного предсказания $i$-го токена она измеряет расхождение между предсказанным моделью распределением вероятностей $P(\cdot | t_1, ..., t_{i-1}; \theta)$ и "истинным" распределением, где вся вероятность (равная 1) сосредоточена на реальном следующем токене $t_i$. Потери обычно усредняются по всем токенам в последовательности и по всем последовательностям в **батче (batch)** данных.

3.  **Оптимизация:** Параметры модели $\theta$ (миллиарды или даже триллионы весов в современных LLM) итеративно обновляются с помощью алгоритмов стохастического градиентного спуска (SGD), чаще всего используются его адаптивные варианты, такие как **Adam** или **AdamW**. На каждом шаге обучения:
    *   Берется батч текстовых последовательностей.
    *   Модель делает предсказания следующих токенов для всех позиций в батче.
    *   Вычисляется средняя Cross-Entropy Loss по батчу.
    *   С помощью **алгоритма обратного распространения ошибки (Backpropagation)** вычисляется градиент функции потерь по всем параметрам модели ($\nabla_\theta L$).
    *   Оптимизатор (Adam/AdamW) использует этот градиент для обновления параметров $\theta$ в направлении, уменьшающем потери: $\theta_{new} = \theta_{old} - \eta \nabla_\theta L$ (где $\eta$ - скорость обучения, а оптимизатор применяет более сложные правила обновления).

### Математическая формализация

**1. Вероятность следующего токена:**
Модель $\pi_\theta$ с параметрами $\theta$ вычисляет условную вероятность следующего токена $t_i$ при заданном контексте $t_{<i} = (t_1, ..., t_{i-1})$:
$P(t_i | t_{<i}; \theta) = \text{Softmax}(z_i)_k \quad \text{где } t_i = \text{token}_k$
Где:
*   $z_i = f(t_{<i}; \theta)$ - вектор логитов размерности $V$ (размер словаря), вычисленный нейросетью (Transformer) на основе контекста $t_{<i}$.
*   $\text{Softmax}(z_i)_k = \frac{\exp(z_{i,k})}{\sum_{j=1}^{V} \exp(z_{i,j})}$ - вероятность $k$-го токена словаря быть следующим.
*   Мы выбираем ту вероятность из вектора Softmax, которая соответствует индексу $k$ *истинного* следующего токена $t_i$ из обучающих данных.

**2. Функция потерь (Cross-Entropy Loss):**
Для одной последовательности $T = (t_1, ..., t_L)$, общие потери (отрицательный логарифм правдоподобия) вычисляются как сумма отрицательных логарифмов вероятностей истинных следующих токенов на каждой позиции:
$L(T; \theta) = - \log P(T; \theta) = - \log \prod_{i=1}^{L} P(t_i | t_{<i}; \theta) = - \sum_{i=1}^{L} \log P(t_i | t_{<i}; \theta)$

*   **Пояснение:**
    *   $L(T; \theta)$: Функция потерь для последовательности $T$ при параметрах модели $\theta$. Наша цель - минимизировать это значение.
    *   $\sum_{i=1}^{L}$: Суммирование по всем позициям предсказания в последовательности (от первого до последнего токена). Часто первое предсказание (для $t_1$) опускается, так как нет контекста, или используется специальный токен начала последовательности.
    *   $P(t_i | t_{<i}; \theta)$: Вероятность, которую модель присвоила *истинному* токену $t_i$, который действительно следовал за контекстом $t_{<i}$ в обучающих данных. Это значение берется из выходного вектора Softmax на шаге $i$.
    *   $\log(\cdot)$: Натуральный логарифм.
    *   Знак минус: Мы минимизируем *отрицательный* логарифм правдоподобия, что эквивалентно максимизации самого правдоподобия $P(T; \theta)$. Если модель присваивает истинному токену $t_i$ высокую вероятность (близкую к 1), то $\log P(\cdot)$ будет близок к 0, и вклад в общие потери будет мал. Если же вероятность низкая (близкая к 0), то $\log P(\cdot)$ будет большим отрицательным числом, а $-\log P(\cdot)$ - большим положительным числом, что увеличит общие потери и "накажет" модель, стимулируя ее скорректировать веса.
    *   **Связь с Cross-Entropy:** Для одного предсказания $i$, если представить истинный следующий токен $t_i$ как one-hot вектор $y_i$ (где $y_{i,k}=1$ для истинного токена $k$, и 0 для остальных), а предсказанное распределение как вектор $p_i = \text{Softmax}(z_i)$, то перекрестная энтропия между $y_i$ и $p_i$ равна $H(y_i, p_i) = - \sum_{j=1}^{V} y_{i,j} \log p_{i,j}$. Поскольку $y_{i,j}$ равен 1 только для истинного токена $k$ и 0 для остальных, эта сумма упрощается до $- \log p_{i,k}$, что в точности совпадает с членом $-\log P(t_i | t_{<i}; \theta)$ в формуле выше.

**3. Потери на батче:**
На практике обучение происходит на батчах данных. Если батч $B$ состоит из $M$ последовательностей $T^{(1)}, ..., T^{(M)}$, то средние потери по батчу вычисляются как:
$L_{batch}(\theta) = \frac{1}{N_{tokens}} \sum_{j=1}^{M} \sum_{i=1}^{L_j} (-\log P(t_i^{(j)} | t_{<i}^{(j)}; \theta))$
Где $N_{tokens} = \sum_{j=1}^{M} L_j$ - общее количество токенов (предсказаний) в батче.

*   **Пояснение:**
    *   $L_{batch}(\theta)$: Средние потери по батчу $B$. Именно градиент этой величины используется для обновления параметров $\theta$ на одном шаге оптимизации.
    *   $\sum_{j=1}^{M} \sum_{i=1}^{L_j}$: Суммирование потерь по всем токенам всех последовательностей в батче.
    *   $\frac{1}{N_{tokens}}$: Нормализация на общее количество токенов в батче для получения среднего значения потерь на один токен. Это делает значение потерь более стабильным и сравнимым между батчами разного размера.
    *   $L_j$: Длина $j$-й последовательности $T^{(j)}$.
    *   $t_i^{(j)}$: $i$-й токен $j$-й последовательности.
    *   $t_{<i}^{(j)}$: Контекст для $i$-го токена $j$-й последовательности.

### Пример работы

Представим, что модель обучается на фразе "Солнце светит ярко". Для простоты будем использовать токенизацию по словам, хотя в реальности используются субсловные токены (например, "светит" может быть `[" свет", "ит"]`). Последовательность токенов: `["Солнце", "светит", "ярко"]`.

1.  **Шаг 1: Предсказание второго токена ("светит")**
    *   **Вход модели:** Токен `["Солнце"]`.
    *   **Обработка:** Модель (Transformer) обрабатывает этот токен.
    *   **Выход (логиты):** Модель генерирует вектор логитов $z_2$.
    *   **Выход (вероятности):** Функция Softmax преобразует логиты в распределение вероятностей над всем словарем. Допустим, для некоторых слов из словаря вероятности такие:
        *   P("светит" | "Солнце") = 0.6
        *   P("зашло" | "Солнце") = 0.2
        *   P("большое" | "Солнце") = 0.1
        *   ... (остальные вероятности для других слов)
    *   **Истинный следующий токен:** "светит".
    *   **Расчет потерь (для этого шага):** $L_1 = -\log P(\text{"светит"} | \text{"Солнце"}) = -\log(0.6) \approx 0.51$.

2.  **Шаг 2: Предсказание третьего токена ("ярко")**
    *   **Вход модели:** Последовательность токенов `["Солнце", "светит"]`.
    *   **Обработка:** Модель обрабатывает оба токена, используя self-attention для учета взаимосвязи между "Солнце" и "светит".
    *   **Выход (логиты):** Модель генерирует вектор логитов $z_3$.
    *   **Выход (вероятности):** Softmax преобразует логиты в новое распределение вероятностей. Допустим:
        *   P("ярко" | "Солнце", "светит") = 0.7
        *   P("сегодня" | "Солнце", "светит") = 0.15
        *   P("но" | "Солнце", "светит") = 0.05
        *   ...
    *   **Истинный следующий токен:** "ярко".
    *   **Расчет потерь (для этого шага):** $L_2 = -\log P(\text{"ярко"} | \text{"Солнце", "светит"}) = -\log(0.7) \approx 0.36$.

**Общие потери и обновление:**
Общие потери для этой последовательности (в данном примере, для двух предсказаний) равны $L = L_1 + L_2 = 0.51 + 0.36 = 0.87$. Алгоритм обратного распространения ошибки вычислит, как изменение каждого параметра модели повлияет на эту суммарную потерю (вычислит градиент $\nabla_\theta L$). Затем оптимизатор обновит параметры $\theta$, чтобы уменьшить $L$. В результате модель станет немного лучше предсказывать "светит" после "Солнце" и "ярко" после "Солнце светит" в будущем.

### Значение и ограничения

**Значение Next-Token Prediction:**

*   **Эффективное Самообучение (Self-Supervised Learning):** Главное преимущество — возможность обучаться на гигантских объемах неразмеченного текста. Задача формулируется самой структурой текста, что устраняет необходимость в дорогостоящей ручной разметке данных.
*   **Изучение Глубоких Языковых Структур:** Чтобы хорошо предсказывать следующий токен, модель вынуждена неявно изучать грамматику, синтаксис, семантику, стилистику и фактические знания, содержащиеся в обучающих данных.
*   **Фундамент для Генеративных Моделей:** Эта стратегия обучения естественным образом приводит к созданию моделей, способных генерировать текст авторегрессионно.
*   **Основа для Авторегрессионных LLM:** Next-token prediction является основной задачей pre-training для всего класса авторегрессионных моделей (Decoder-only), таких как GPT, Llama, PaLM, Mistral и др. (В отличие от моделей типа BERT, которые используют Masked Language Modeling).

**Ограничения:**

*   **Локальная Оптимизация vs Глобальная Когерентность:** Модель оптимизируется на предсказание *локально* наиболее вероятного следующего токена, что не всегда гарантирует глобальную осмысленность или фактическую точность сгенерированного текста ("галлюцинации").
*   **Отсутствие Явной Цели и Согласованности:** Задача next-token prediction не учит модель напрямую быть полезной, честной или безвредной. Она имитирует статистику данных, поэтому требуются дополнительные этапы обучения (SFT, RLHF/DPO).
*   **Чувствительность к Качеству Данных:** Модель отражает свойства обучающих данных, включая ошибки, предвзятости и нежелательный контент.
*   **Вычислительная Сложность:** Pre-training требует огромных вычислительных ресурсов и времени.

### Заключение

В заключение, стратегия прогнозирования следующего токена (next-token prediction) является краеугольным камнем в предварительном обучении (pre-training) современных авторегрессионных больших языковых моделей. Ее элегантность и мощь заключаются в способности эффективно использовать огромные объемы неразмеченного текста для самообучения. Решая, казалось бы, простую задачу предсказания непосредственно следующего элемента последовательности, модель вынуждена неявно усваивать сложнейшие закономерности языка.

Мы рассмотрели механизм работы этого подхода, основанный на архитектуре Transformer и функции Softmax, детально изучили математическую формализацию через призму минимизации Cross-Entropy Loss и проиллюстрировали процесс на простом примере.

Несмотря на свои ограничения, next-token prediction создает невероятно мощную базовую модель с фундаментальным пониманием языка. Это понимание затем направляется и уточняется с помощью последующих этапов обучения (SFT, RLHF/DPO), превращая модель в полезного, безопасного и согласованного с человеческими ожиданиями ИИ-ассистента. Таким образом, next-token prediction остается незаменимым первым шагом на пути к созданию передовых LLM.

</details> 

### Заключение

Полный цикл обучения современных LLM — это сложный, итеративный процесс, объединяющий масштабное самообучение на неструктурированных данных (pre-training) с последующей тонкой настройкой под руководством человека (SFT, RLHF). Каждый этап вносит свой вклад:

*   **Pre-training:** Формирует базовые знания и языковые способности.
*   **SFT:** Учит модель следовать инструкциям и формату диалога.
*   **RLHF (или DPO):** Согласует поведение модели с человеческими предпочтениями по качеству и безопасности.

Этот многоступенчатый подход позволяет создавать мощные и полезные языковые модели, но также требует постоянных исследований для повышения эффективности, безопасности и интерпретируемости этих сложных систем. Развитие методов обучения, таких как DPO, и подходов вроде Constitutional AI (обучение на основе набора правил или "конституции") продолжают формировать будущее этой области.

</details>

### **3.2 Основной механизм: Формирование контуров внимания для recall в период плато**

**Ключевая идея:** Несмотря на то, что внешние метрики модели (потери, точность) не улучшаются в период плато, внутри происходят важные структурные изменения — формируются *контуры recall на основе внимания* (Attention-based Recall Circuits), необходимые для выполнения фактического извлечения информации.  

**Теоретическая база:**  
При выполнении Transformer задач на recall фактов обычно наблюдается следующая картина:  
- **Ранние слои внимания:** агрегируют информацию из нескольких токенов имени, формируя концентрированное представление объекта (обычно на позиции последнего токена имени);
- **Средние слои MLP:** выступают в роли *хранилища "ключ-значение"*, связывая представление имени (как ключ запроса) с соответствующей фактической информацией (значение); 
- **Поздние слои внимания:** используют контекст (например, какой атрибут нужно предсказать) для запроса представления объекта и извлечения сохраненных фактов для итогового предсказания.  

**Гипотеза:** Период плато соответствует процессу формирования этих контуров recall, особенно — развитию способности поздних слоев внимания корректно *маршрутизировать* информацию. Обучение стагнирует, потому что до полного формирования этих контуров ошибки предсказания атрибутов не могут эффективно распространяться обратно (через backpropagation) к соответствующим представлениям имен или ячейкам хранения знаний в MLP.

#### **3.2.1 Экспериментальная проверка: патч внимания**

**Логика эксперимента:**

Если контуры recall действительно формируются в период плато, то "пересадка" паттернов внимания из модели, уже прошедшей плато, в только что начавшую обучение модель должна значительно ускорить обучение и даже устранить плато.  

![Figure_3](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-15/assets/Figure_04.png)
> Контуры внимания, обеспечивающие recall, формируются в период плато потерь. **(Слева)** Мы разработали эксперимент с **"подменой внимания" (attention patching)**: берём снимок (snapshot) референсной модели на определённом этапе её обучения. Используем её паттерны внимания **вместо собственных** в модифицированной модели на всём протяжении её обучения. **(По центру)** Чем более обучена референсная модель, тем полезнее её паттерны внимания для модифицированной модели — и основные изменения происходят именно **во время плато**. **Исключение:** самый ранний этап обучения демонстрирует обратную тенденцию. Это коррелирует с тем, что в этот период: токены имён (по сравнению с остальным текстом, содержащим информацию о типе атрибута). Получают **меньше внимания** при предсказании первого токена значения атрибута *(см. правую панель)*  

**Метод эксперимента:**

| Шаг | Действие | Цель |  
|------|----------|-------|  
| 1 | Обучить **"референсную модель"** и сохранить её состояния (checkpoint) на разных этапах (до/во время/после плато). | Получить паттерны внимания разной степени зрелости. |  
| 2 | Инициализировать новую **"модифицированную модель"**. | Создать тестируемую модель. |  
| 3 | В процессе обучения модифицированной модели **не вычислять** её собственные паттерны внимания, а вместо этого использовать замороженные паттерны из соответствующих слоёв checkpoint референсной модели. | Проверить влияние готовых паттернов внимания на эффективность обучения. |  
| 4 | Наблюдать за кривой обучения (изменение потерь на атрибутах). | Оценить "качество" или "зрелость" паттернов на разных этапах. |  

**Результаты:**  

- **Использование паттернов внимания после плато:** скорость обучения резко возрастает, потери на атрибутах быстро снижаются — плато эффективно пропускается;
- **Использование паттернов внимания во время плато:** чем ближе checkpoint к концу плато, тем заметнее ускорение.
- **Использование паттернов из ранней стадии обучения (этап 1):** результат даже хуже, чем при случайной инициализации. Причина: раннее внимание фокусируется на токенах типа атрибута, а не на имени, что мешает усвоению ассоциаций "объект-факт".

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
<p style="margin: 0; font-weight: bold; color: #2c3e50;">First Checkpoint:</p>
<p style="margin: 8px 0 0 0; color: #2c3e50;">Обучение языковых моделей проходит через трехэтапную схему приобретения знаний: (1) начальное статистическое обучение с быстрым снижением потерь, (2) длительное плато производительности, пропорциональное количеству изучаемых индивидуумов, и (3) проявление знаний, когда модель начинает связывать конкретные индивидуумы с их атрибутами. Эта структура стабильно сохраняется при различных гиперпараметрах и архитектурах на этапе предварительного обучения (pre-training).</p>
</div>

## **4. Как атрибуты распределения данных способствуют приобретению знаний**

**Основной вопрос:** ранее мы обсуждали временную динамику обучения модели, но как внутренние свойства обучающих данных, особенно их распределение, влияют на этот процесс? В реальном мире данные часто несбалансированы (imbalanced), то есть некоторые сущности/факты встречаются гораздо чаще, чем другие. Ускоряет ли такая несбалансированность обучение или, наоборот, препятствует ему?

**Основное открытие:** степень сбалансированности распределения данных значительно влияет на динамику обучения, формируя четкий компромисс (trade-off) между длительностью плато и скоростью приобретения знаний на финальной стадии. Используя это свойство, можно оптимизировать общую эффективность обучения за счет стратегий планирования данных (data scheduling).

### **4.1 Двойное влияние несбалансированности распределения данных: эффект компромисса**

**Анализ и количественная оценка:**

- **Длительность плато (Plateau Length):** рпределяется в основном небольшим числом наиболее часто встречающихся индивидов. Когда модель "усваивает" информацию о высокочастотных индивидах (ЧАСТОТНОЕ РАСПРЕДЕЛЕНИЕ), ключевые механизмы, такие как цепи восстановления памяти, могут быть построены предварительно, что помогает модели быстрее выйти из плато. Таким образом, некоторая степень несбалансированности может сократить продолжительность плато.

- **Скорость приобретения знаний (Knowledge Acquisition Speed):** после плато модель должна изучить информацию обо всех индивидах. На этом этапе узкое место скорости обучения заключается в наименее частых индивидах. Чем более сбалансировано распределение данных, тем больше шансов для редких индивидов быть замеченными, и тем выше общая скорость обучения. Таким образом, сбалансированное распределение способствует ускорению этапа приобретения знаний.

**Экспериментальная проверка:** Введение степенного закона для контроля несбалансированности  
Чтобы систематически исследовать влияние несбалансированности, авторы использовали обратный степенной закон (inverse power law) для управления вероятностью выборки $i$-го индивида в наборе данных:

$$
P(i) = \frac{i^{-\alpha}}{\sum_{j=1}^{N} j^{-\alpha}},
$$

где $\alpha$ — гиперпараметр, контролирующий степень несбалансированности:

- $\alpha = 0$: равномерное распределение (Uniform), все индивиды встречаются с одинаковой вероятностью.
- $\alpha > 1$: распределение Ципфа (Zipf's Law), сильно несбалансированное, где немногие индивиды встречаются крайне часто, а большинство — очень редко.
- Промежуточные значения $\alpha$: различные степени несбалансированности.

**Результаты эксперимента** (фиксированное количество шагов обучения, например, 16k):

**Вывод:** По результатм эксперимента, существует оптимальная степень несбалансированности ($\alpha_{opt}  \approx 0.6 \sim 0.8)$, которая обеспечивает наилучший баланс между ускорением выхода модели из плато и поддержанием эффективности последующего обучения. Это оптимальное значение относительно стабильно для разных общих чисел индивидов $N$.

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
<p style="margin: 0; font-weight: bold; color: #2c3e50;">Second Checkpoint:</p>
<p style="margin: 8px 0 0 0; color: #2c3e50;">Несбалансированность распределения данных создаёт компромисс в обучении языковых моделей: с одной стороны, высокочастотные индивиды ускоряют выход из плато, формируя необходимые механизмы обработки информации, с другой стороны, низкочастотные индивиды замедляют финальную стадию приобретения знаний. Существует оптимальная степень несбалансированности (α_opt), обеспечивающая наилучший баланс между этими факторами и максимальную общую эффективность обучения.</p>
</div>

### **4.2 Стратегия планирования данных: динамическая оптимизация приобретения знаний**

**Новый вопрос:** Поскольку требования к распределению данных различаются для плато и этапа приобретения знаний (плато предпочитает несбалансированность, а этап приобретения знаний — сбалансированность), можем ли мы разработать динамическую стратегию, которая объединяет преимущества обоих подходов?

**Интуиция и решение:** Data Curriculum / Scheduling

1. **На ранних этапах обучения (соответствует плато):** используется несбалансированное распределение данных (или только подмножество высокочастотных индивидов) для быстрого создания ключевых механизмов и сокращения плато.
2. **На поздних этапах обучения (соответствует этапу приобретения знаний):** переход к сбалансированному распределению данных, чтобы гарантировать, что все индивиды будут изучены достаточно полно.

**Конкретная реализация:** Стратегия "разогрева" (Warm-up)

- **Фаза разогрева:** обучение начинается с подмножества индивидов (indiv_warmup) в течение epochs_warmup эпох. Это подмножество создает естественную несбалансированность.
- **Основная фаза обучения:** переход ко всем индивидам с использованием равномерной выборки для продолжения обучения.

**Результаты эксперимента:**

По сравнению с постоянным использованием равномерного распределения или оптимального фиксированного значения $\alpha$, эта динамическая стратегия "разогрева" значительно увеличивает общий объем знаний, получаемых моделью (что приводит к более низкому значению Attribute Loss), когда число индивидов велико ($N$ большое).

**Значение:** это демонстрирует редкий и конкретный пример того, как стратегии "Data Curriculum" обучения данных могут эффективно повысить производительность в сценариях самообучения.

![Figure_4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-15/assets/Figure_05.png)
> Свойства распределения данных могут ускорить приобретение знаний. (слева) Длина плато значительно сокращается, когда некоторые индивиды встречаются чаще, чем другие, что в данном случае достигается увеличением 𝛼. (в центре) Таким образом, полезно обучать модель на более несбалансированных распределениях, особенно когда количество шагов обучения уменьшается или общее количество индивидов увеличивается. (справа) Такая стратегия повышает итоговый объем знаний, содержащихся в сети (линия фиолетового цвета против серой). Динамическая адаптация распределения данных дает еще больший эффект (синяя линия).

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
<p style="margin: 0; font-weight: bold; color: #2c3e50;">Third Checkpoint:</p>
<p style="margin: 8px 0 0 0; color: #2c3e50;">Динамическое планирование данных (Data Curriculum) позволяет оптимизировать процесс обучения языковых моделей путём использования разных распределений на разных этапах: несбалансированное распределение на этапе плато для быстрого формирования ключевых механизмов, и сбалансированное распределение на этапе приобретения знаний для равномерного обучения всем индивидам. Стратегия "разогрева" (Warm-up), начинающая с подмножества частых индивидов и переходящая к полному набору, значительно повышает общий объем усвоенных знаний, особенно при большом количестве индивидов.</p>
</div>

## **5. Галлюцинации как препятствие для интеграции новых знаний после обучения**

**Основной вопрос:**

На практике добавление новой информации в уже предобученную большую языковую модель (LLM), например, через дообучение (fine-tuning), часто оказывается неэффективным. Модель либо с трудом "усваивает" новые данные, либо существенно "забывает" ранее изученное. Почему это происходит?

**Ключевое открытие**

В данной главе показано, что процесс приобретения знаний сопровождается возникновением "галлюцинаций" — феномена, когда модель делает уверенные, но ложные утверждения о незнакомых объектах. Наличие галлюцинаций и хрупкость ассоциативной памяти модели создают значительные трудности при попытке интеграции новых знаний через fine-tuning.

### **5.1 Симбиоз знаний и галлюцинаций**

#### **Наблюдение 1 (Постановка проблемы)**
Как модель реагирует на сущности, которые она никогда не видела (например, held-out individuals из тестового набора)?

**Определение галлюцинации:**  
Галлюцинация — это явление, при котором модель делает **чрезмерно уверенные**, но **ложные** фактические прогнозы о незнакомых объектах.

**Экспериментальные результаты:**
- **Синхронное появление:**  
  Как только модель начинает точно воспроизводить знания о сущностях из обучающего набора (Attribute Accuracy > 0, Attribute Loss < No Knowledge Baseline), её ошибки (Attribute Loss) относительно незнакомых объектов начинают значительно превышать базовый уровень (No Knowledge Baseline). Это указывает на наличие галлюцинаций.
  
- **Разница в уверенности:**  
  Несмотря на галлюцинации, уверенность модели в своих ошибочных прогнозах (измеряемая через вероятности предсказанных токенов или энтропию распределения) обычно ниже, чем уверенность в правильных прогнозах для объектов из обучающего набора. Однако даже эта "меньшая" уверенность остаётся выше разумного порога.

- **Потенциальная связь:**  
  Совместное проявление галлюцинаций и знаний предполагает, что галлюцинации могут быть **неизбежным побочным эффектом** текущей архитектуры моделей и механизма их обучения.

### **5.2 Катастрофическое забывание старых знаний при fine-tuning новых**

#### **Наблюдение 2 (Основная проблема)**
Что происходит, если провести fine-tuning предобученной модели на данных о новых персонажах и их биографиях?

**Экспериментальные наблюдения:**

| Этап | Поведение относительно старых знаний (предобученных персонажей) | Поведение относительно новых знаний (дообученных персонажей) | Ключевые явления |
|------|---------------------------------------------------------------|------------------------------------------------------------|------------------|
| **Начало fine-tuning (первые сотни шагов)** | - Атрибутивные потери резко возрастают<br>- Точность атрибутов резко падает | - Атрибутивные потери медленно снижаются<br>- Точность атрибутов медленно растёт | Быстрое и массовое забывание старых знаний, пока новые ещё не усвоены. |
| **Поздние этапы fine-tuning** | Производительность может частично восстановиться (особенно при использовании Replay). | Производительность продолжает улучшаться. | Новые знания постепенно усваиваются, в то время как старые либо стабилизируются, либо медленно восстанавливаются. |
| **Добавление Replay** | Значительное падение точности и рост потерь всё ещё заметны в начале. | - | Replay помогает частично восстановить старые знания на поздних этапах, но не предотвращает катастрофического забывания в начале. |

#### Исследование причин забывания

**Гипотеза 1: Разрушение паттернов внимания?**  
- **Логика:**  
  Введение новых персонажей может нарушить сложившиеся паттерны внимания, ответственные за вызов ранее усвоенных данных.

- **Результат:**  
  Паттерны внимания остаются стабильными на протяжении всего процесса fine-tuning, что опровергает данную гипотезу.

**Гипотеза 2: Нарушение ассоциативной памяти в полносвязных сетях (FFN)?**  
- **Логика:**   
  Полносвязные слои (FFN/MLP) рассматриваются как хранилище ключ-значение для знаний. Добавление новых "ключей" (имён новых персонажей) и "значений" (их атрибутов) может мешать или перезаписывать ранее сохранённые пары.  

- **Результат:**  
  В упрощённой модели также наблюдались быстрое забывание старых ключей и значений на ранних этапах fine-tuning. Данная гипотеза получила подтверждение.

Таким образом, исследование выявляет, что галлюцинации и катастрофическое забывание являются следствием внутренней организации моделей и особенностей их обучения. Эти явления требуют дальнейшего изучения для улучшения способности моделей эффективно обучаться без потери уже накопленных знаний.

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
<p style="margin: 0; font-weight: bold; color: #2c3e50;">Fourth Checkpoint:</p>
<p style="margin: 8px 0 0 0; color: #2c3e50;">Галлюцинации и катастрофическое забывание возникают из-за внутренней организации языковых моделей, где полносвязные сети (FFN) выступают в роли ассоциативной памяти. При дообучении (fine-tuning) новые знания перезаписывают ранее усвоенные "ключ-значение" пары в FFN, что приводит к потере старых данных. Галлюцинации проявляются как ложные уверенные утверждения о незнакомых объектах, возникающие одновременно с усвоением новых знаний. Эти феномены указывают на необходимость пересмотра архитектурных решений и методов обучения для обеспечения стабильной интеграции информации без катастрофического забывания.</p>
</div>

## **6. Обсуждение**

### **6.1 Новый взгляд на динамику обучения языковых моделей**

#### Выводы и их значение:
- **Распределение данных > Размер модели?**  
  По сравнению с простым увеличением масштаба модели, характеристики распределения обучающих данных могут оказывать большее влияние на динамику обучения (особенно на продолжительность переходных фаз).  

- **Возможные источники "возникающих способностей"?**  
  Так называемое "возникновение" может частично объясняться тем, что с увеличением масштаба модели и данных время обучения также увеличивается. Это позволяет модели преодолеть длительные плато для определенных задач и "внезапно" проявить новые способности.

#### Рекомендации по стратегии обучения:
- **Использование синтетических данных на ранних этапах?**  
  Учитывая, что данные, подаваемые до плато, вносят ограниченный вклад в финальную модель (поскольку соответствующие механизмы еще не сформированы), использование вычислительно менее затратных синтетических данных для "разминки" или формирования механизмов может быть более эффективной стратегией.  

- **Потенциал планировщиков данных:**  
  Разработка адаптивных планировщиков данных (data schedulers), способных динамически корректировать распределение данных (например, снижать разнообразие данных во время плато для ускорения формирования механизмов), представляет собой крайне перспективное направление для повышения скорости и эффективности обучения.

### **6.2 Выводы для динамики обучения универсальных нейронных сетей**

#### Порядок формирования механизмов:
- Исследование наблюдает явление, при котором "цепи внимания и отзыва" формируются раньше, чем "ассоциативная память в прямых слоях". Этот порядок может иметь универсальное значение.  

- **Гипотеза:**  
  Формирование эффективных механизмов маршрутизации/выбора информации (например, внимания) усиливает корреляцию между входными данными и сигналами ошибки, предоставляя более четкие и эффективные сигналы обучения для последующих механизмов хранения контента (например, ассоциативной памяти в MLP).

- **Связь с феноменами Grokking и т.д.:**  
  Этот порядок формирования механизмов может быть связан с явлениями "Grokking" (сначала переобучение, затем обобщение) или процессом, когда модель сначала находит недостаточное, но быстрое решение (например, полагаясь только на локальную статистику), а затем переходит к более обобщающим решениям благодаря формированию более оптимальных механизмов (например, глобальных цепей отзыва) и регуляризации.

- **Ценность методов анализа:**  
  Разделение функций внимания (token-mixing) и других вычислений (например, хранения знаний в FFN) оказалось мощным инструментом для понимания динамики обучения Transformer. Этот подход имеет важное значение для будущих исследований внутренних механизмов нейронных сетей.

### **6.3 Неравномерность данных, эффективность обучения и психология развития**

#### Основные выводы: Ускорение формирования механизмов через неравномерность
- Исследование точно проанализировало, как неравномерность (non-uniformity) в обучающих данных помогает модели быстрее преодолевать плато обучения за счет усиления сигналов и ускорения идентификации ключевых отношений.  

- **Компромисс:**  
  Однако отмечается, что такое ускорение может происходить за счет снижения качества обучения на редких данных и обобщающей способности модели (особенно при отсутствии последующих этапов равномерного обучения).

#### Связь с когнитивной наукой и психологией развития:
- **Неявное учебное пособие (Implicit Curriculum):**  
  Предложенная в статье стратегия динамического планирования данных (сначала неравномерно/просто, затем равномерно/сложно) поразительно напоминает модели обучения младенцев. Из-за ограниченного диапазона активности в раннем возрасте и частого контакта с знакомыми лицами и объектами младенцы естественным образом проходят путь от простых, повторяющихся входных данных к постепенному взаимодействию с более богатой и разнообразной средой. Такое "снизу-вверх" сформированное учебное пособие считается ключевым фактором, способствующим раннему эффективному обучению.  

- **Повторение и обобщение:**  
  Как показывает это исследование, раннее повторение небольшого количества примеров способствует быстрому формированию основных представлений и связей, тогда как последующий контакт с разнообразием является необходимым условием для достижения надежного обобщения.  

- **Потенциальный вклад:**  
  Количественный анализ влияния динамики распределения данных может заложить основу для создания более совершенной статистической теории обучения и развития (statistical theory of development).

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;">
<p style="margin: 0; font-weight: bold; color: #2c3e50;">Fifth Checkpoint:</p>
<p style="margin: 8px 0 0 0; color: #2c3e50;">Динамика обучения языковых моделей зависит от стратегии подачи данных:
1. Неравномерные данные ускоряют формирование механизмов (например, внимания), но требуют последующей "доработки" на разнообразных данных для обобщения.
2. Порядок компонент (внимание → ассоциативная память) объясняет феномены вроде Grokking и подчеркивает роль адаптивного обучения.
3. Синтетические данные и планировщики оптимизируют процесс: дешевые данные на старте + динамическая корректировка распределения.</p>
</div>

## **Вывод**

Это исследование предоставляет всеобъемлющую основу для понимания того, как языковые модели изучают, хранят и извлекают фактические знания. Выявление трехфазного процесса обучения и задействованных нейронных механизмов предлагает ценные сведения как о возможностях, так и об ограничениях современных языковых моделей.

Выводы предлагают несколько направлений для будущих исследований, включая:

1. Разработка более эффективных учебных программ на основе выявленной динамики обучения;
2. Разработка архитектурных модификаций для лучшего отделения приобретения знаний от развития галлюцинаций;
3. Создание подходов тонкой настройки, которые могут включать новые знания с минимальным искажением существующих воспоминаний;
4. Изучение связей между масштабом модели, размером набора данных и продолжительностью плато для более крупных моделей.

Понимание этих фундаментальных принципов обучения имеет решающее значение для разработки более способных, эффективных и правдивых языковых моделей, которые могут служить надежным интерфейсом для человеческих знаний. Это исследование представляет собой значительный шаг на пути к механистическим объяснениям поведения языковых моделей, выходя за рамки оценок типа "черный ящик" и углубляясь в понимание того, как эти все более важные системы учатся и работают.