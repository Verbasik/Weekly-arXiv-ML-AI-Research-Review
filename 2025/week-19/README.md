[![I-CON](https://img.shields.io/badge/I-CON-blue)](https://mhamilton.net/icon)
[![Telegram Channel](https://img.shields.io/badge/Telegram-TheWeeklyBrief-blue)](https://t.me/TheWeeklyBrief)

# I‑CON — универсальный фреймворк для обучения представлений

> **Одна формула → весь зоопарк лоссов**
> I‑CON показывает, что SNE, t‑SNE, InfoNCE, SupCon, CLIP, k‑Means, CE и ещё \~20 методов — это частные случаи минимизации одной и той же KL‑дивергенции между «идеальным» распределением соседства *p* и «фактическим» распределением *q*. 

## 🚀 Кратко о достижениях

* 🧩 **«Периодическая таблица» методов** — визуальная карта, где сменой *p* или *q* можно «переходить» от т‑SNE к SimCLR или от CLIP к SupCon. 
* 📈 **+8 pp к SOTA** в unsupervised классификации ImageNet‑1K благодаря новому Debiased InfoNCE Clustering. 
* 🧹 **α‑дебиасинг**: добавление равномерного шума к *p* резко повышает устойчивость контрастивных моделей.
* 🔄 **Перенос идей «через оси»**: приёмы label‑smoothing в классификации работают и в контрастивном обучении, а графовые трики из DR — в кластеризации.

## Почему I‑CON важен?

| Боль                                          | Решение от I‑CON                                |
| --------------------------------------------- | ----------------------------------------------- |
| Несвязные лоссы для DR / CL / Clustering / CE | Одна KL‑формула                                 |
| Трудно проектировать новые методы             | Достаточно «комбинировать» распределения        |
| Переуверенность в контрастивных моделях       | α‑дебиасинг сглаживает *p*                      |
| Нет метрик «в самом лоссе»                    | KL‑дивергенция даёт естественную шкалу качества |

## Ключевые идеи

1. \**Выберите «идеальное» $p(j|i)$:
   Gaussian, k‑NN, one‑hot, cross‑modal пары — определяет «что считать соседством».
2. \**Определите «модельное» $q(j|i)$:
   Gaussian / t‑распределение в эмбеддингах, равномерно по кластерам и т.д.
3. **Минимизируйте  $D_{KL}\!\bigl(p(\cdot\!\mid i)\,\Vert\,q(\cdot\!\mid i)\bigr)$.**
   Получите SNE, InfoNCE, CE… или новый гибрид.
4. **Добавьте α‑дебиасинг:**
   $\tilde p=(1-\alpha)p+\alpha/N$ — аналог label‑smoothing для любых *p*.

## 🌟 Поддержите проект

- Понравилось? Поставьте звезду и присоединяйтесь к обсуждению!

---

<p align="center">Исследуйте вместе с нами 🚀</p>
