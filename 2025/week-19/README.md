[![I-CON](https://img.shields.io/badge/I-CON-blue  )](https://mhamilton.net/icon  )
[![Telegram Channel](https://img.shields.io/badge/Telegram-TheWeeklyBrief-blue  )](https://t.me/TheWeeklyBrief  )

# Iâ€‘CON â€” A Unifying Framework for Representation Learning

> **One formula â†’ the entire zoo of losses**  
> Iâ€‘CON reveals that SNE, tâ€‘SNE, InfoNCE, SupCon, CLIP, kâ€‘Means, CE, and ~20 other methods are all special cases of minimizing the same KL divergence between a "perfect" neighborhood distribution *p* and the "actual" distribution *q*.

## ğŸš€ Key Achievements

* ğŸ§© **"Periodic Table" of Methods** â€” A visual map where switching *p* or *q* lets you "transition" from tâ€‘SNE to SimCLR or from CLIP to SupCon.  
* ğŸ“ˆ **+8 pp over SOTA** in unsupervised classification on ImageNetâ€‘1K via the new Debiased InfoNCE Clustering.  
* ğŸ§¹ **Î±â€‘Debiasing**: Adding uniform noise to *p* dramatically improves the robustness of contrastive models.  
* ğŸ”„ **Cross-Axis Idea Transfer**: Techniques like label-smoothing from classification work in contrastive learning, and graph-based tricks from DR apply to clustering.

## Why Iâ€‘CON Matters?

| Problem                                       | Iâ€‘CON Solution                                  |
| --------------------------------------------- | ----------------------------------------------- |
| Disconnected losses for DR / CL / Clustering / CE | Single KL formulation                           |
| Hard to design new methods                    | Simply "combine" distributions                  |
| Overconfidence in contrastive models          | Î±â€‘debiasing smooths *p*                         |
| No intrinsic metrics within the loss          | KL divergence provides a natural quality scale  |

## Core Ideas

1. \**Choose the "ideal"Â $p(j|i)$:  
   Gaussian, kâ€‘NN, oneâ€‘hot, crossâ€‘modal pairs â€” defines "what constitutes neighborhood".  
2. \**Define the "learned"Â $q(j|i)$:  
   Gaussian / t-distribution in embeddings, uniform over clusters, etc.  
3. **Minimize  $D_{KL}\!\bigl(p(\cdot\!\mid i)\,\Vert\,q(\cdot\!\mid i)\bigr)$.**  
   Obtain SNE, InfoNCE, CEâ€¦ or a novel hybrid.  
4. **Add Î±â€‘debiasing:**  
   $\tilde p=(1-\alpha)p+\alpha/N$ â€” analog of label-smoothing for any *p*.

---

<div align="center">

**Explore with us ğŸš€**

â­ Star this repository if you found it helpful

</div>