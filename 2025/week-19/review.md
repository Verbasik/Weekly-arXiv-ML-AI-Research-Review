# I-CON: Объединяющий фреймворк для обучения представлений (технический обзор)  

## 1. Архитектура и принципы работы фреймворка I-CON  

**I-CON (Information Contrastive Learning)** – это единый информационно-теоретический подход к обучению представлений, способный описать множество разных методов через одну общую формулировку. В основе I-CON лежит **функция потерь в виде среднего дивергенса Кульбака-Лейблера (KL)** между двумя условными распределениями – *«супервизорным»* (задающим соседство объектов на основе данных или внешних сигналов) и *«выученным»* (задаваемым моделью представлений). Формально, для каждого образца $i$ из набора данных $X$ определяется два распределения вероятностей $p_θ(j|i)$ и $q_φ(j|i)$, и I-CON минимизирует суммарный KL-дивергенс:  

$$
L = \sum_i D_{KL}[p_θ(·|i) \| q_φ(·|i)] = \sum_i \sum_j p_θ(j|i) \log \left[ \frac{p_θ(j|i)}{q_φ(j|i)} \right].  
$$  

Здесь $p_θ$ называют **супервизорным распределением**, определяющим "соседей" для каждого образца $i$ (например, близкие точки в исходном пространстве данных, аугментации того же объекта или элементы того же класса). $q_φ$ – это **выученное распределение**, основанное на представлениях модели (например, близость в embedding-пространстве или распределение кластерной принадлежности). В большинстве практических случаев $p$ считается фиксированным, заданным исходными данными или внешней информацией (метки, аугментации и пр.), а $q_φ$ – обучается посредством параметров модели $φ$. Таким образом, обучение сводится к тому, чтобы подстроить представления модели так, чтобы распределение соседей в пространстве признаков $q$ приближалось к заданному распределению $p$ (минимизируя $D_{KL}(p \| q)$).  

**Универсальность формулировки.** Несмотря на простоту, такая постановка оказалась удивительно всеобъемлющей – различные выборы распределений $p$ и $q$ соответствуют самым разным популярным методам обучения представлений. Авторы показали, что **более 23 методов** в областях **контрастивного обучения, понижения размерности, кластеризации, спектрального обучения графов и даже классического обучения с учителем** являются частными случаями I-CON. Например, метод стохастического соседнего вложения **SNE** можно представить как частный случай I-CON, где $p(j|i)$ – это распределение Гаусса вокруг точки $i$ в исходном высокомерном пространстве, а $q(j|i)$ – Гауссово распределение вокруг точки $i$ в низкоразмерном пространстве (embedding); минимизация KL-дивергенции между ними приводит к сохранению близости соседей при переходе к новому пространству. Если вместо непрерывного embedding-представления использовать распределение принадлежности кластеру, получаем алгоритм **k-средних**: $p(j|i)$ задаётся Гауссианом в исходном пространстве (соседями считаются близкие точки), а $q(j|i)$ отражает вероятность принадлежности $i$ и $j$ одному кластеру – минимизация KL в этом случае подгоняет кластерные распределения под локальную структуру данных. Замена Гауссовского соседства на структуру графа (соседство по рёбрам графа) даёт **спектральную кластеризацию**, а использование пар аугментаций одного образа в качестве "близких" соседей приводит к **контрастивным методам типа InfoNCE (SimCLR, CLIP)**. Даже стандартная **кросс-энтропия в обучении с учителем** вписывается в эту схему: $p(j|i)$ – это распределение-набор меток (например, индикатор правильного класса для образца $i$), а $q(j|i)$ – распределение выходных предсказаний модели; их KL-дивергенция равна функции потерь кросс-энтропии.  

Авторы представили удобную **«периодическую таблицу» методов** (рис. 1 статьи), организовав их по двум осям: строки соответствуют типу выученного представления (например, непрерывное embedding-пространство, распределение по кластерам, распределение по классам и т.п.), а столбцы – типу супервизорного сигнала (метрика близости в исходном пространстве: Гауссовское или Студентовское распределение по соседям, графовые соседства, аугментации, парность "кросс-модальных" соответствий, принадлежность к классу и др.). В каждой ячейке такой таблицы располагается известный метод, который получается выбором соответствующей пары распределений $p$ и $q$. Эта классификация выявляет "пробелы" – сочетания, не представленные известными методами – указывая на возможность **разработки новых алгоритмов** на основе недостающих комбинаций. Например, комбинируя **контрастивное обучение и кластеризацию**, авторы предложили новый метод *contrastive clustering* (контрастивная кластеризация), сочетающий соседство на основе аугментаций с распределением по кластерам. Общая математическая основа I-CON позволяет также перенимать техники из одной области в другую: так, идеи дебалансировки (debiasing) из контрастивного обучения оказались применимы для улучшения кластеризации без меток (см. ниже эксперименты).

---

<details> 
    <summary><em><strong>Формализация дивергенции Кульбака-Лейблера</strong></em></summary>

### **Дивергенция Кульбака-Лейблера (Kullback-Leibler divergence, KL-дивергенция)**

**Дивергенция Кульбака-Лейблера (Kullback-Leibler divergence, KL-дивергенция)**, также известная как **относительная энтропия**, является мерой того, насколько одно распределение вероятностей отличается от второго, эталонного распределения вероятностей. Она количественно определяет потерю информации при аппроксимации одного распределения другим.

**Математическая формализация:**

1.  **Для дискретных распределений:**
    Пусть $P(x)$ и $Q(x)$ — два дискретных распределения вероятностей, определённых на одном и том же пространстве элементарных событий $\mathcal{X}$. KL-дивергенция от $Q$ к $P$ (обозначается как $D_{KL}(P || Q)$) определяется как:
    $$
    D_{KL}(P || Q) = \sum_{x \in \mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right)
    $$
    *   Суммирование ведётся по всем возможным значениям $x$ из $\mathcal{X}$.
    *   Логарифм обычно берётся по основанию $e$ (натуральный логарифм, результат в "натах") или по основанию 2 (результат в "битах").
    *   Важное условие: требуется абсолютная непрерывность $P$ относительно $Q$. Это означает, что если $Q(x) = 0$ для некоторого $x$, то и $P(x)$ также должно быть равно 0. Если это условие не выполняется, дивергенция не определена (или считается бесконечной). На практике часто используют сглаживание или добавляют малую константу к $Q(x)$, чтобы избежать деления на ноль.

2.  **Для непрерывных распределений:**
    Пусть $p(x)$ и $q(x)$ — плотности двух непрерывных распределений вероятностей, определённых на одном и том же пространстве $\mathcal{X}$. KL-дивергенция от $q$ к $p$ (обозначается как $D_{KL}(P || Q)$) определяется как:
    $$
    D_{KL}(P || Q) = \int_{\mathcal{X}} p(x) \log \left( \frac{p(x)}{q(x)} \right) dx
    $$
    *   Интеграл берётся по всему пространству $\mathcal{X}$.
    *   Аналогично дискретному случаю, требуется, чтобы носитель распределения $P$ был подмножеством носителя распределения $Q$ (т.е., если $q(x) = 0$, то и $p(x) = 0$).

![Пример KL-дивергенции для задачи next token prediction](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-19/assets/Example_01.png)

**Смысл и интерпретация:**

*   **Информационно-теоретическая (Избыточность кодирования):** $D_{KL}(P || Q)$ представляет собой среднее количество *дополнительных* бит (или нат), необходимых для кодирования выборок из истинного распределения $P$, если мы вынуждены использовать код, оптимальный для аппроксимирующего распределения $Q$, по сравнению с использованием кода, оптимального для самого $P$. Чем ближе $Q$ к $P$, тем меньше эта "дополнительная" длина кода, и тем лучше $Q$ моделирует $P$. Если $D_{KL}(P || Q) = 0$, значит $Q$ является таким же эффективным кодом для данных из $P$, как и сам $P$ (т.е., $P=Q$).

*   **Связь с энтропией:** KL-дивергенцию можно выразить через энтропию Шеннона $H(P)$ и перекрестную энтропию $H(P, Q)$. Перекрестная энтропия $H(P, Q) = -\sum_x P(x) \log Q(x)$ (или интеграл для непрерывного случая) измеряет среднюю длину сообщения в битах (или натах) при кодировании событий из $P$ с использованием оптимального кода для $Q$. Тогда:
    $$D_{KL}(P || Q) = H(P, Q) - H(P) = \left( -\sum_x P(x) \log Q(x) \right) - \left( -\sum_x P(x) \log P(x) \right)$$
    Таким образом, $D_{KL}(P || Q)$ — это разница между средней длиной кода при использовании *неправильной* модели $Q$ для данных из $P$ (перекрестная энтропия) и минимально возможной средней длиной кода при использовании *правильной* модели $P$ (энтропия $P$). Это мера **неэффективности** или **избыточности** кодирования, вызванной использованием модели $Q$ вместо $P$.

*   **Статистическая (Несоответствие моделей):** KL-дивергенция измеряет степень несоответствия или "удивления" (англ. surprise) при наблюдении данных, сгенерированных по распределению $P$, если мы ожидали, что они подчиняются распределению $Q$. Это фундаментальная мера различия между двумя статистическими моделями. Хотя её часто называют "KL-расстоянием", она не является метрикой в строгом математическом смысле, так как:
    1.  **Несимметрична:** в общем случае $D_{KL}(P || Q) \neq D_{KL}(Q || P)$. Выбор того, какое распределение считать "истинным" ($P$), а какое "моделью" ($Q$), критически важен.
    2.  **Не удовлетворяет неравенству треугольника.**

*   **Асимметрия и её последствия при оптимизации:** асимметрия $D_{KL}$ имеет важные практические следствия, особенно когда мы минимизируем дивергенцию для подгонки модели $Q$ к данным $P$:
    *   **Минимизация $D_{KL}(P || Q)$ ("Прямая" KL, Forward KL):** эта постановка задачи сильно штрафует ситуации, когда модель $Q$ присваивает низкую вероятность ($Q(x) \to 0$) тем событиям $x$, которые на самом деле вероятны согласно $P$ ($P(x) > 0$). Чтобы избежать бесконечной дивергенции, $Q$ будет стараться присвоить ненулевую вероятность всем областям, где $P$ имеет заметную вероятность. Это приводит к тому, что $Q$ стремится "покрыть" все моды распределения $P$ (англ. *mode-covering behavior*), возможно, становясь слишком "размазанным".
    *   **Минимизация $D_{KL}(Q || P)$ ("Обратная" KL, Reverse KL):** эта постановка (часто используемая в вариационном выводе, например, в VAE) сильно штрафует ситуации, когда аппроксимация $Q$ присваивает высокую вероятность ($Q(x) > 0$) тем событиям $x$, которые маловероятны согласно $P$ ($P(x) \to 0$). Это заставляет $Q$ концентрироваться в областях высокой вероятности $P$, точно воспроизводя одну или несколько мод $P$, но потенциально игнорируя другие моды (англ. *mode-seeking behavior*). $Q$ предпочитает быть "уверенным" там, где $P$ "уверено".

*   **Вариационный вывод и приближение распределений:** в байесовском машинном обучении и теории информации KL-дивергенция является основой вариационного вывода (Variational Inference, VI). Цель VI — найти наилучшее приближение $Q$ (из некоторого параметризованного, обычно простого семейства распределений, например, гауссиан) к истинному, но обычно сложному или невычислимому, распределению $P$ (например, апостериорному распределению параметров модели). Минимизация $D_{KL}(Q || P)$ (обратная KL) по параметрам $Q$ позволяет найти такое $Q$, которое наиболее близко к $P$ в смысле KL-дивергенции в рамках выбранного семейства.

**Ключевые свойства:**

*   **Неотрицательность:** $D_{KL}(P || Q) \ge 0$. Равенство нулю достигается тогда и только тогда, когда $P = Q$ (почти всюду).
*   **Асимметрия:** в общем случае $D_{KL}(P || Q) \neq D_{KL}(Q || P)$. Это важное отличие от стандартных метрик расстояния (например, евклидова). Выбор того, какое распределение является "истинным" ($P$), а какое "аппроксимирующим" ($Q$), имеет значение.

**Применение в машинном обучении:**

*   **Функция потерь:** KL-дивергенция часто используется как компонент функции потерь для задач, где нужно приблизить одно распределение другим. Например, в обучении с подкреплением для ограничения изменения политики или в генеративных моделях.
*   **Вариационные автоэнкодеры (VAE):** в VAE KL-дивергенция является ключевым регуляризатором. Она минимизируется между апостериорным распределением латентных переменных $q(z|x)$, выученным энкодером, и априорным распределением $p(z)$ (часто выбираемым как стандартное нормальное распределение $\mathcal{N}(0, I)$). Это заставляет латентное пространство иметь структуру, близкую к априорной, что способствует генерации новых данных.
*   **Обучение представлений (как в I-CON):** в контексте фреймворка I-CON, KL-дивергенция используется для сравнения распределения вероятностей полученных представлений (например, $p(z|x)$) с некоторым целевым или контрольным распределением. Минимизация этой дивергенции позволяет формировать представления, которые соответствуют определённым желаемым статистическим свойствам или несут информацию о конкретных аспектах данных.

</details>