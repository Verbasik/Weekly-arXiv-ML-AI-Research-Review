# I-CON: Унифицированная платформа для обучения представлений  

## Содержание
0. [TL;DR](#tldr)
1. [Введение](#введение)  
2. [Теоретическая основа](#теоретическая-основа)  
3. [Унификация разрозненных методов](#унификация-разрозненных-методов)  
4. [Распределения представлений](#распределения-представлений)  
5. [Стратегия устранения смещения](#стратегия-устранения-смещения)  
6. [Экспериментальные результаты](#экспериментальные-результаты)  
7. [Приложения и последствия](#приложения-и-последствия)  
8. [Заключение](#заключение)

## **0. TL;DR**

<details> 
    <summary><em><strong>Too long; Didn't read</strong></em></summary>

## **Too long; Didn't read**

1. **Введение и Проблема Фрагментации в Обучении Представлений**

Развитие машинного обучения в последние годы привело к появлению множества разнообразных методов обучения представлений. Каждый из этих методов часто имеет свои уникальные архитектуры, функции потерь и стратегии обучения. Эта фрагментация создает значительные трудности для исследователей:

- **Сложность понимания взаимосвязей**: Трудно увидеть, как связаны между собой, казалось бы, разные подходы (например, методы снижения размерности и контрастное обучение).
- **Выбор оптимального метода**: Определить, какой из множества существующих методов лучше всего подходит для конкретной задачи, становится нетривиальной задачей.
- **Разрозненные формулировки**: Методы используют разные математические языки (например, "максимизация взаимной информации vs. минимизация MSE"), что затрудняет их сравнение и анализ.

Как отмечается в источнике:  
*"Эта фрагментация затрудняет для исследователей понимание взаимосвязей между различными методами и определение того, какой подход лучше всего подходит для данной задачи."*

---

2. **Концепция Обучения Представлений (Representation Learning)**

**Обучение представлений** — это ключевое направление, которое направлено на автоматическое извлечение полезных признаков из "сырых" данных. Вместо ручного проектирования признаков, алгоритмы самостоятельно учатся кодировать информацию в компактные векторные формы (эмбеддинги), сохраняющие семантические закономерности.

**Ключевые аспекты**:
- **Автоматическое извлечение**: Преобразование данных (изображений, текста, звука) в векторные эмбеддинги.
- **Типы методов**: Включают *supervised* (с учителем, используют метки), *self-supervised* (без учителя, учатся на структуре данных, например, контрастивные методы) и методы на основе информационной теории (оптимизация взаимной информации).
- **Применение**: Уменьшение размерности, transfer learning, интерпретируемость моделей.

Проблема фрагментации выражается в существовании "зоопарка" методов, таких как Triplet Loss, NT-Xent (в SimCLR), VAE, которые используют разные математические основы для достижения схожих целей.

---

3. **Фреймворк I-CON: Унификация через Теорию Информации**

Статья *"I-CON: Унифицированная платформа для обучения представлений"* предлагает решение проблемы фрагментации, вводя всеобъемлющую информационно-теоретическую платформу, которая *"объединяет более 23 различных методов обучения представлений под единой математической формулировкой."*

**Основная идея I-CON** заключается в рассмотрении обучения представлений как задачи минимизации средней дивергенции Кульбака-Лейблера (KL) между двумя условными распределениями вероятностей:

- **Эталонное распределение** $(p(j|i))$: Отражает желаемые взаимосвязи между точками данных (например, близость, принадлежность к классу, связь в графе).
- **Обучаемое распределение** $(q(j|i))$: Моделирует эти взаимосвязи в пространстве представлений, которое модель научилась строить.

Целевая функция формулируется следующим образом:  
$$ \mathcal{L}_{I-CON} = \mathbb{E}_i \left[ D_{KL}(p(j|i) \| q(j|i)) \right] $$  

Эта формулировка создает "универсальный язык", позволяющий описать разные виды "соседства" (близость по пикселям, принадлежность к классу и т.д.) и четкую цель для обучения: *"сделать так, чтобы ее собственное видение соседства ($q$) стало как можно больше похоже на идеальное видение ($p$)"*.

*Рисунок 1* иллюстрирует этот процесс: данные проходят через функцию отображения, создавая представления, которые затем сравниваются с контрольными распределениями с помощью KL-дивергенции.

---

4. **Унификация Существующих Методов в Рамках I-CON**

Авторы демонстрируют, что I-CON может воспроизвести широкий спектр существующих алгоритмов обучения представлений, *"выбирая определенные параметризации для контрольных и обученных распределений"*.

*Рисунок 3* визуализирует эту унификацию, классифицируя методы по категориям (уменьшение размерности, кластерное обучение, самообучение, обучение с учителем).

**Примеры унификации**:
- **t-SNE**: Получается, когда $p(j|i)$ и $q(j|i)$ являются распределениями Стьюдента.
- **SimCLR**: Получается, когда $p(j|i)$ равномерно по парам аугментации, а $q(j|i)$ — гауссово на единичной сфере.
- **K-means**: Получается, когда $p(j|i)$ — гауссово, а $q(j|i)$ — равномерно по членам кластера.
- **PCA**: Получается, когда $p(j|i)$ — тождественное распределение, а $q(j|i)$ — гауссово с $σ→∞$.

Эта способность объединять методы *"выявляет неожиданные связи между, казалось бы, разрозненными методами."*

---

5. **Роль и Выбор Распределений Представлений**

Выбор конкретных вероятностных распределений для $p(j|i)$ и $q(j|i)$ критически важен и определяет свойства получаемых эмбеддингов. В статье исследуются несколько ключевых распределений:

- **Гауссовское распределение**: Используется в SNE, создает вложения на основе евклидова расстояния.  
  $$ p(j|i) \propto \exp\left(-\frac{|x_i - x_j|^2}{2\sigma^2}\right) $$
- **Распределение Стьюдента**: Используется в t-SNE, сохраняет как локальную, так и глобальную структуру благодаря более тяжелым хвостам.  
  $$ p(j|i) \propto \left(1 + \frac{|x_i - x_j|^2}{\gamma^2}\right)^{-1} $$
- **Равномерное распределение по k ближайшим соседям**: Учитывает только локальную структуру, фокусируясь на k ближайших точках.  
  $$ p(j|i) = \begin{cases} 
    1, & \text{если } x_j \in k \text{ ближайших соседей } x_i \\
    0, & \text{в противном случае}
  \end{cases} $$

*Рисунок 4* визуализирует эти распределения, демонстрируя их разные характеристики. Использование распределений позволяет точно измерить расхождение между идеальным и реальным соседством с помощью KL-дивергенции и обучать модель для минимизации этого расхождения.

---

6. **Стратегия Устранения Смещения (Bias Correction)**

Одним из ключевых нововведений I-CON является принципиальный подход к устранению внутренних смещений в методах обучения представлений. Эта стратегия заключается в модификации целевого (эталонного) распределения $p(j|i)$. Вместо того чтобы модель $q(j|i)$ соответствовала исходному $p(j|i)$, она стремится соответствовать его измененной версии $\hat{p}(j|i)$.

Предлагается модификация контролирующего распределения с помощью равномерного компонента, управляемого параметром $\alpha$:  
$$ \hat{p}(j|i) = (1 - \alpha)p(j|i) + \frac{\alpha}{N} $$  

Эта стратегия имеет два важных следствия:
1. *"Она способствует более разнообразному вниманию к различным примерам".*
2. *"Она улучшает калибровку оценок уверенности в изученных представлениях".*

*Рисунок 5* показывает, как увеличение $\alpha$ улучшает кластеризацию, делая ее "более четкой и лучше разделенной". *Рисунок 6* демонстрирует, что устранение смещения улучшает как точность, так и калибровку на реальных данных.

---

7. **Экспериментальные Результаты**

Авторы оценивают эффективность алгоритмов, полученных с помощью I-CON, на стандартных наборах данных классификации изображений (ImageNet-1K, CIFAR-100, STL-10). Используя предобученный Vision Transformer DiNO, они показывают значительные улучшения:

- На **ImageNet-1K** их подход кластеризации InfoNCE с устранением смещения достигает *"8% улучшения по сравнению с предыдущими передовыми методами для неконтролируемой классификации"*.
- Визуализации на **CIFAR** и **STL-10** (*Рисунки 7 и 8*) демонстрируют, как параметры устранения смещения (например, $\tau^+$) влияют на структуру вложений, улучшая связность кластеров.
- Тестирование на моделях разного размера (*Рисунок 9*) показывает, что устранение смещения последовательно улучшает точность валидации.

---

8. **Приложения и Последствия**

I-CON предлагает как теоретические, так и практические преимущества:

- **Перенос идей**: Фреймворк облегчает обмен успешными техниками между различными областями обучения представлений.
- **Разработка алгоритмов**: Позволяет систематически создавать новые методы, варьируя $p(j|i)$ и $q(j|i)$.
- **Повышенная производительность**: Стратегия устранения смещения приводит к более надежным представлениям и улучшению производительности в downstream задачах.
- **Простота реализации**: Унифицированная формулировка позволяет *"более лаконично и последовательно реализовывать различные методы."*

---

9. **Заключение**

I-CON является *"значительным прогрессом в нашем понимании обучения представлений"*. Предоставляя *"единую математическую структуру"*, которая объединяет разнообразные методы (кластеризация, снижение размерности, контрастное обучение, контролируемая классификация), фреймворк проясняет основные принципы, лежащие в их основе.

Фреймворк не только объединяет существующие методы, но и стимулирует разработку новых, более эффективных алгоритмов, что подтверждается улучшенными результатами в экспериментах, особенно за счет подхода к устранению смещения.

Как заключают авторы:  
*"Поскольку обучение представлений продолжает развиваться, I-CON предоставляет исследователям мощный инструмент для понимания существующих методов, разработки новых алгоритмов и повышения производительности в широком спектре задач машинного обучения."*  

Способность фреймворка объединять традиционно разделенные области предполагает, что *"дальнейшее перекрестное опыление идей может привести к еще более эффективным методам обучения представлений в будущем."*

</details> 

---

## **1. Введение**

В последние годы в исследованиях машинного обучения наблюдается распространение методов обучения представлений, каждый из которых имеет уникальную архитектуру, функции потерь и стратегии обучения. Эта фрагментация затрудняет для исследователей понимание взаимосвязей между различными методами и определение того, какой подход лучше всего подходит для данной задачи.  

В статье **"I-CON: Унифицированная платформа для обучения представлений"** предлагается всеобъемлющая информационно-теоретическая платформа, которая вносит ясность в этот сложный ландшафт, объединяя более 23 различных методов обучения представлений под единой математической формулировкой.  

> **Фреймворк I-CON**, показывающий взаимосвязь между входными данными, контрольными сигналами, изученными представлениями и распределениями вероятностей.  

---

### **Что такое методы обучения представлений?**

**Обучение представлений** (representation learning) — это направление в машинном обучении, целью которого является автоматическое выявление и выделение полезных признаков (features) из исходных данных. В отличие от традиционных методов, где признаки проектируются вручную (feature engineering), обучение представлений позволяет алгоритмам самостоятельно находить оптимальные способы кодирования информации.

#### Ключевые аспекты:
1. **Автоматическое извлечение признаков**  
   - Методы обучения представлений преобразуют "сырые" данные (изображения, текст, звук) в компактные векторные формы (эмбеддинги), сохраняющие семантически значимые закономерности.
   - Пример: свёрточные нейросети (CNN) выделяют иерархию признаков в изображениях — от границ объектов до их семантических частей.

2. **Типы методов**  
   - **С учителем** (supervised): используют размеченные данные (например, классификационные метки) для обучения.  
     *Пример:* Fine-tuning предобученных моделей (ResNet, BERT).  
   - **Без учителя** (self-supervised): учатся на внутренней структуре данных без явных меток.  
     *Пример:* Контрастивные методы (SimCLR), маскирование в NLP (BERT).  
   - **На основе информационной теории**: оптимизируют взаимную информацию между представлениями (например, VAE, InfoGAN).  

3. **Применение**  
   - Уменьшение размерности (t-SNE, PCA).  
   - Перенос обучения (transfer learning).  
   - Интерпретируемость моделей (визуализация эмбеддингов).  

#### Проблема фрагментации
Разнообразие подходов — от автоэнкодеров до contrastive learning — создаёт "зоопарк" методов, которые часто:  
- Используют разные математические формулировки (например, максимизация взаимной информации vs. минимизация MSE).  
- Требуют специфичных архитектур (например, наличие memory bank в MoCo).  
- Затрудняют сравнение эффективности на новых задачах.  

**Пример:** Для одной и той же задачи (классификация изображений) можно применить:  
- Triplet Loss (на основе расстояний между эмбеддингами).  
- Нормализованную температурно-масштабированную кросс-энтропию (NT-Xent, как в SimCLR).  
- Вариационный автоэнкодер (VAE) с регуляризацией KL-дивергенцией.

---

![Рисунок 1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-19/assets/Image_01.png)

**Рисунок 1:** Фреймворк I-CON иллюстрирует, как обучение представлений может быть сформулировано как согласование условных распределений вероятностей. Фреймворк показывает, как данные проходят через функцию отображения для создания представлений, которые затем сравниваются с контрольными распределениями с использованием дивергенции Куллбака-Лейблера.

## **2. Теоретические основы**

Метод I-CON (Информационно-теоретическая конвергенция) рассматривает обучение представлений как задачу минимизации средней дивергенции Кульбака-Лейблера (KL) между двумя условными распределениями вероятностей:

1. **Эталонное распределение** $( p(j|i) )$, которое отражает взаимосвязи между точками данных.  
2. **Обучаемое распределение** $( q(j|i) )$, которое моделирует эти взаимосвязи в пространстве представлений.  

Целевая функция метода записывается следующим образом:  

$$
\mathcal{L}_{I-CON} = \mathbb{E}_i \left[ D_{KL}(p(j|i) \| q(j|i)) \right]
$$

Эта компактная формулировка позволяет анализировать и интерпретировать различные методы обучения. Эталонное распределение $( p(j|i) )$ задаёт желаемые взаимосвязи между точками данных (например, на основе близости в пространстве, принадлежности к одному классу или пар аугментаций), а обучаемое распределение $( q(j|i) )$ показывает, как модель воспроизводит эти взаимосвязи.

<details> 
    <summary><em><strong>Формализация дивергенции Кульбака-Лейблера</strong></em></summary>

### **Дивергенция Кульбака-Лейблера (Kullback-Leibler divergence, KL-дивергенция)**

**Дивергенция Кульбака-Лейблера (Kullback-Leibler divergence, KL-дивергенция)**, также известная как **относительная энтропия**, является мерой того, насколько одно распределение вероятностей отличается от второго, эталонного распределения вероятностей. Она количественно определяет потерю информации при аппроксимации одного распределения другим.

**Математическая формализация:**

1.  **Для дискретных распределений:**
    Пусть $P(x)$ и $Q(x)$ — два дискретных распределения вероятностей, определённых на одном и том же пространстве элементарных событий $\mathcal{X}$. KL-дивергенция от $Q$ к $P$ (обозначается как $D_{KL}(P || Q)$) определяется как:
    $$
    D_{KL}(P || Q) = \sum_{x \in \mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right)
    $$
    *   Суммирование ведётся по всем возможным значениям $x$ из $\mathcal{X}$.
    *   Логарифм обычно берётся по основанию $e$ (натуральный логарифм, результат в "натах") или по основанию 2 (результат в "битах").
    *   Важное условие: требуется абсолютная непрерывность $P$ относительно $Q$. Это означает, что если $Q(x) = 0$ для некоторого $x$, то и $P(x)$ также должно быть равно 0. Если это условие не выполняется, дивергенция не определена (или считается бесконечной). На практике часто используют сглаживание или добавляют малую константу к $Q(x)$, чтобы избежать деления на ноль.

2.  **Для непрерывных распределений:**
    Пусть $p(x)$ и $q(x)$ — плотности двух непрерывных распределений вероятностей, определённых на одном и том же пространстве $\mathcal{X}$. KL-дивергенция от $q$ к $p$ (обозначается как $D_{KL}(P || Q)$) определяется как:
    $$
    D_{KL}(P || Q) = \int_{\mathcal{X}} p(x) \log \left( \frac{p(x)}{q(x)} \right) dx
    $$
    *   Интеграл берётся по всему пространству $\mathcal{X}$.
    *   Аналогично дискретному случаю, требуется, чтобы носитель распределения $P$ был подмножеством носителя распределения $Q$ (т.е., если $q(x) = 0$, то и $p(x) = 0$).

![Пример KL-дивергенции для задачи next token prediction](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-19/assets/Example_01.png)

**Смысл и интерпретация:**

*   **Информационно-теоретическая (Избыточность кодирования):** $D_{KL}(P || Q)$ представляет собой среднее количество *дополнительных* бит (или нат), необходимых для кодирования выборок из истинного распределения $P$, если мы вынуждены использовать код, оптимальный для аппроксимирующего распределения $Q$, по сравнению с использованием кода, оптимального для самого $P$. Чем ближе $Q$ к $P$, тем меньше эта "дополнительная" длина кода, и тем лучше $Q$ моделирует $P$. Если $D_{KL}(P || Q) = 0$, значит $Q$ является таким же эффективным кодом для данных из $P$, как и сам $P$ (т.е., $P=Q$).

*   **Связь с энтропией:** KL-дивергенцию можно выразить через энтропию Шеннона $H(P)$ и перекрестную энтропию $H(P, Q)$. Перекрестная энтропия $H(P, Q) = -\sum_x P(x) \log Q(x)$ (или интеграл для непрерывного случая) измеряет среднюю длину сообщения в битах (или натах) при кодировании событий из $P$ с использованием оптимального кода для $Q$. Тогда:
    $$D_{KL}(P || Q) = H(P, Q) - H(P) = \left( -\sum_x P(x) \log Q(x) \right) - \left( -\sum_x P(x) \log P(x) \right)$$
    Таким образом, $D_{KL}(P || Q)$ — это разница между средней длиной кода при использовании *неправильной* модели $Q$ для данных из $P$ (перекрестная энтропия) и минимально возможной средней длиной кода при использовании *правильной* модели $P$ (энтропия $P$). Это мера **неэффективности** или **избыточности** кодирования, вызванной использованием модели $Q$ вместо $P$.

*   **Статистическая (Несоответствие моделей):** KL-дивергенция измеряет степень несоответствия или "удивления" (англ. surprise) при наблюдении данных, сгенерированных по распределению $P$, если мы ожидали, что они подчиняются распределению $Q$. Это фундаментальная мера различия между двумя статистическими моделями. Хотя её часто называют "KL-расстоянием", она не является метрикой в строгом математическом смысле, так как:
    1.  **Несимметрична:** в общем случае $D_{KL}(P || Q) \neq D_{KL}(Q || P)$. Выбор того, какое распределение считать "истинным" ($P$), а какое "моделью" ($Q$), критически важен.
    2.  **Не удовлетворяет неравенству треугольника.**

*   **Асимметрия и её последствия при оптимизации:** асимметрия $D_{KL}$ имеет важные практические следствия, особенно когда мы минимизируем дивергенцию для подгонки модели $Q$ к данным $P$:
    *   **Минимизация $D_{KL}(P || Q)$ ("Прямая" KL, Forward KL):** эта постановка задачи сильно штрафует ситуации, когда модель $Q$ присваивает низкую вероятность ($Q(x) \to 0$) тем событиям $x$, которые на самом деле вероятны согласно $P$ ($P(x) > 0$). Чтобы избежать бесконечной дивергенции, $Q$ будет стараться присвоить ненулевую вероятность всем областям, где $P$ имеет заметную вероятность. Это приводит к тому, что $Q$ стремится "покрыть" все моды распределения $P$ (англ. *mode-covering behavior*), возможно, становясь слишком "размазанным".
    *   **Минимизация $D_{KL}(Q || P)$ ("Обратная" KL, Reverse KL):** эта постановка (часто используемая в вариационном выводе, например, в VAE) сильно штрафует ситуации, когда аппроксимация $Q$ присваивает высокую вероятность ($Q(x) > 0$) тем событиям $x$, которые маловероятны согласно $P$ ($P(x) \to 0$). Это заставляет $Q$ концентрироваться в областях высокой вероятности $P$, точно воспроизводя одну или несколько мод $P$, но потенциально игнорируя другие моды (англ. *mode-seeking behavior*). $Q$ предпочитает быть "уверенным" там, где $P$ "уверено".

*   **Вариационный вывод и приближение распределений:** в байесовском машинном обучении и теории информации KL-дивергенция является основой вариационного вывода (Variational Inference, VI). Цель VI — найти наилучшее приближение $Q$ (из некоторого параметризованного, обычно простого семейства распределений, например, гауссиан) к истинному, но обычно сложному или невычислимому, распределению $P$ (например, апостериорному распределению параметров модели). Минимизация $D_{KL}(Q || P)$ (обратная KL) по параметрам $Q$ позволяет найти такое $Q$, которое наиболее близко к $P$ в смысле KL-дивергенции в рамках выбранного семейства.

**Ключевые свойства:**

*   **Неотрицательность:** $D_{KL}(P || Q) \ge 0$. Равенство нулю достигается тогда и только тогда, когда $P = Q$ (почти всюду).
*   **Асимметрия:** в общем случае $D_{KL}(P || Q) \neq D_{KL}(Q || P)$. Это важное отличие от стандартных метрик расстояния (например, евклидова). Выбор того, какое распределение является "истинным" ($P$), а какое "аппроксимирующим" ($Q$), имеет значение.

**Применение в машинном обучении:**

*   **Функция потерь:** KL-дивергенция часто используется как компонент функции потерь для задач, где нужно приблизить одно распределение другим. Например, в обучении с подкреплением для ограничения изменения политики или в генеративных моделях.
*   **Вариационные автоэнкодеры (VAE):** в VAE KL-дивергенция является ключевым регуляризатором. Она минимизируется между апостериорным распределением латентных переменных $q(z|x)$, выученным энкодером, и априорным распределением $p(z)$ (часто выбираемым как стандартное нормальное распределение $\mathcal{N}(0, I)$). Это заставляет латентное пространство иметь структуру, близкую к априорной, что способствует генерации новых данных.
*   **Обучение представлений (как в I-CON):** в контексте фреймворка I-CON, KL-дивергенция используется для сравнения распределения вероятностей полученных представлений (например, $p(z|x)$) с некоторым целевым или контрольным распределением. Минимизация этой дивергенции позволяет формировать представления, которые соответствуют определённым желаемым статистическим свойствам или несут информацию о конкретных аспектах данных.

</details>

## **3. Унификация разрозненных методов**

Авторы демонстрируют, что, выбирая определенные параметризации для контрольных и обученных распределений, I-CON может воспроизвести широкий спектр существующих алгоритмов обучения представлений:

![Рисунок 2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-19/assets/Image_02.png)

**Рисунок 2:** Различные типы контрольных сигналов могут использоваться для определения взаимосвязей между точками данных, включая пространственную близость, дискретные взаимосвязи, принадлежность к кластеру и связность графа.

Например:

- **t-SNE:** когда оба распределения являются распределениями Стьюдент;
- **SimCLR:** когда $p(j|i)$ является равномерным по парам аугментации, а $q(j|i)$ является гауссовым на единичной сфере;
- **K-means:** когда $p(j|i)$ является гауссовым, а $q(j|i)$ является равномерным по членам кластера;
- **PCA:** когда $p(j|i)$ является тождественным распределением, а $q(j|i)$ является гауссовым с $σ→∞$.

Эта унификация выявляет неожиданные связи между, казалось бы, разрозненными методами. Например, авторы показывают, что подходы контрастного обучения (например, InfoNCE) и методы снижения размерности (например, t-SNE) фундаментально связаны через их основные целевые функции.

![Рисунок 3](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-19/assets/Image_03.png)

**Рисунок 3:** I-CON объединяет различные алгоритмы обучения представлений, показывая, как они возникают из различных вариантов контролирующих и изученных распределений. Цвета указывают на различные категории: уменьшение размерности (синий), кластерное обучение (оранжевый), одномодальное самообучение (красный), многомодальное самообучение (фиолетовый) и обучение с учителем (зеленый).

## **4. Распределения представлений**

Выбор распределения представлений значительно влияет на результирующие вложения. В статье исследуются несколько ключевых распределений:

1. **Гауссовское распределение**: создает вложения на основе евклидова расстояния, как используется в SNE.

   $$p(j|i) \propto \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)$$

2. **Распределение Стьюдента**: сохраняет как локальную, так и глобальную структуру с более тяжелыми хвостами, как в t-SNE.

   $$p(j|i) \propto \left(1 + \frac{\|x_i - x_j\|^2}{\gamma^2}\right)^{-1}$$

3. **Равномерное распределение по k ближайшим соседям**: учитывает только k ближайших соседей каждой точки.

   $$
   p(j|i) = \begin{cases} 
   1, & \text{если } x_j \in k \text{ ближайших соседей } x_i \\
   0, & \text{в противном случае}
   \end{cases}
   $$

![Рисунок 4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-19/assets/Image_04.png)

**Рисунок 4**: Визуализация различных распределений вероятностей, используемых в I-CON: гауссовское (слева), Стьюдента (в центре) и равномерное по k ближайшим соседям (справа). Каждое распределение создает различные свойства вложения. 

### **Почему вообще речь зашла про распределения в I-CON?**

Представьте, что основная задача обучения представлений — это **создать хорошую "карту" ваших данных**. На этой карте похожие объекты (например, картинки с котиками) должны оказаться рядом, а непохожие (котики и автомобили) — далеко друг от друга.

Фреймворк I-CON предлагает посмотреть на эту задачу так:

1.  **Определим "идеальное соседство":** сначала решим, как *должны* выглядеть окрестности каждого объекта на нашей идеальной карте. Например, мы можем сказать: "Для этой картинки с котиком (`i`) другие картинки с котиками (`j`) должны считаться 'близкими соседями' с высокой вероятностью, а картинки с машинами — с низкой". Это наше **целевое** представление о соседстве.
2.  **Посмотрим на "реальное соседство" в модели:** затем посмотрим, как нейросеть *на самом деле* расположила объекты на своей текущей "карте" (в пространстве эмбеддингов). Насколько близки там котики к котикам? Насколько далеки от машин? Это **фактическое** соседство, созданное моделью.

**Как сравнить "идеальное" и "реальное" соседство?** Вот тут-то и приходят на помощь **вероятностные распределения**:

*   **$p(j|i)$ — это описание "идеального" соседства:** для каждого объекта `i` это распределение говорит, с какой **вероятностью** любой другой объект `j` должен считаться его "важным соседом". Это наша **цель**, основанная на исходных данных или знаниях (например, метках классов, аугментациях).
*   **$q(j|i)$ — это описание "реального" соседства в модели:** для каждого объекта `i` это распределение показывает, с какой **вероятностью** модель *считает* объект `j` его "важным соседом", основываясь на близости их текущих представлений (эмбеддингов). Это то, **что модель выучила**.

**Зачем это нужно?**

1.  **Универсальный язык:** вероятности позволяют описать самые разные виды "соседства" (близость по пикселям, принадлежность к классу, связь в графе) единым математическим языком.
2.  **Четкая цель для обучения:** задача модели теперь проста — **сделать так, чтобы ее собственное видение соседства ($q$) стало как можно больше похоже на идеальное видение ($p$)**.
3.  **Измеримое расхождение:** мы можем использовать KL-дивергенцию ($D_{KL}(p || q)$), чтобы точно измерить, насколько $q$ отличается от $p$. Минимизируя эту дивергенцию, мы **обучаем модель** создавать представления, которые отражают желаемую структуру соседства.

### **Пример: интерпретация I-Con SNE, SimCLR и K-средних**

![Figure 03](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-19/assets/Figure_03.png)

**Фигура 3:** Примеры методов как частных случаев I-Con через различные варианты выбора p и q с соответствующими конфигурациями кода

**1. SNE (стохастическое вложение соседей, задача снижения размерности)**  
Целью SNE является сохранение локальной структуры данных в высокоразмерном пространстве.  
- **Наблюдаемое распределение** $p(j|i)$ в исходном пространстве задается гауссовым распределением, центрированным вокруг точки $x_i$, с последующей нормализацией.  
- **Моделируемое распределение** $q_{\phi}(j|i)$ в пространстве низкой размерности определяется аналогичным распределением (чаще — распределением Стьюдента, как в t-SNE), центрированным вокруг вектора $\phi_i$.  
Минимизация KL-дивергенции между $p(j|i)$ и $q_{\phi}(j|i)$ обеспечивает сохранение локальных расстояний при проекции.

**2. SimCLR (контрастное обучение)**  
SimCLR использует аугментации данных для определения позитивных пар.  
- **Наблюдаемое распределение** $p(j|i)$ принимает значение 1, если $x_j$ является аугментированной версией $x_i$, и 0 в противном случае.  
- **Моделируемое распределение** $q_{\phi}(j|i)$ строится на основе косинусной близости векторов $\phi_i$ и $\phi_j$ с применением температурного масштабирования (функция потерь InfoNCE).  
Оптимизация KL-дивергенции способствует сближению аугментированных версий одного изображения и удалению негативных примеров.

**3. K-Means (кластеризация)**  
K-Means относит точки к ближайшим кластерным центрам.  
- **Наблюдаемое распределение** $p(j|i)$ может быть задано гауссовым распределением, зависящим от расстояний в исходном пространстве.  
- **Моделируемое распределение** $q_{\phi}(j|i)$ отражает принадлежность $x_j$ к тому же кластеру, что и $x_i$.  
Минимизация KL-дивергенции согласует кластерные назначения с исходной структурой соседства. I-Con обобщает как жесткое (hard assignment), так и вероятностное (soft assignment) отнесение точек к кластерам.

**Заключение**  
Приведенные примеры иллюстрируют способность подхода I-Con унифицировать разнородные задачи за счет:  
1. Гибкого определения наблюдаемого ($p$) и моделируемого ($q$) распределений.  
2. Использования единого принципа минимизации KL-дивергенции.  
Это подтверждает, что I-Con обеспечивает общую теоретическую основу для анализа методов машинного обучения, формализующую их интуитивные цели через призму информационной теории.  


## **5. Стратегия устранения смещения**

Одним из ключевых нововведений в I-CON является принципиальный подход к устранению смещения, который решает проблему внутренних смещений в методах обучения представлений. Стратегия устранения смещения в I-CON заключается в модификации целевого (эталонного) распределения $p(j|i)$. Вместо того чтобы модель $q(j|i)$ пыталась идеально соответствовать исходному $p(j|i)$, она будет стремиться соответствовать его слегка измененной версии, $\hat{p}(j|i)$. Авторы предлагают модифицировать контролирующее распределение с помощью равномерного компонента, контролируемого параметром α:

$$\hat{p}(j|i) = (1 - \alpha)p(j|i) + \frac{\alpha}{N}$$

Эта стратегия устранения смещения имеет два важных следствия:

- Она способствует более разнообразному вниманию к различным примерам;
- Она улучшает калибровку оценок уверенности в изученных представлениях.

Влияние этого устранения смещения можно визуализировать в пространствах вложения:

![Рисунок 5](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-19/assets/Image_05.png)

**Рисунок 5:** Визуализация изученных вложений с различными коэффициентами устранения смещения (α). По мере увеличения α от 0 до 0.6 кластеры становятся более четкими и лучше разделенными.

В статье демонстрируется, что этот подход к устранению смещения приводит к значительным улучшениям в различных наборах данных:

![Рисунок 6](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-19/assets/Image_06.png)

**Рисунок 6:** Слева: Точность проверки с различными размерами пакетов и коэффициентами устранения смещения. Справа: Кривые калибровки, показывающие, как устранение смещения улучшает взаимосвязь между вероятностями присваивания и фактической точностью.

## **6. Экспериментальные результаты**

Авторы оценивают алгоритмы, полученные с помощью I-CON, на стандартных задачах классификации изображений, включая ImageNet-1K, CIFAR-100 и STL-10. Используя предварительно обученный Vision Transformer DiNO в качестве экстрактора признаков, они демонстрируют, что их подход кластеризации InfoNCE с устранением смещения достигает 8% улучшения по сравнению с предыдущими передовыми методами для неконтролируемой классификации на ImageNet-1K.

Визуализация вложений на наборе данных CIFAR показывает, как различные параметры устранения смещения влияют на представление:

![Рисунок 7](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-19/assets/Image_07.png)

**Рисунок 7:** Визуализация вложений набора данных CIFAR с различной степенью устранения смещения. Параметр τ⁺ контролирует информативность сигналов контроля.

Аналогично, на STL-10 фреймворк показывает улучшенную кластеризацию с устранением смещения:

![Рисунок 8](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-19/assets/Image_08.png)

**Рисунок 8:** Встраивания набора данных STL-10 с различными параметрами устранения смещения. На правом изображении (τ⁺ = 0.1) показаны более связные кластеры по сравнению с левым (τ⁺ = 0).

В статье также исследуется влияние различных коэффициентов устранения смещения на модели разных размеров:

![Рисунок 9](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-19/assets/Image_09.png)

**Рисунок 9:** Точность валидации с различными параметрами устранения смещения для трех размеров модели DiNO (маленький, базовый, большой). Устранение смещения обеих дистрибуций последовательно превосходит устранение смещения только целевой дистрибуции.

## **7. Приложения и последствия**

Помимо теоретического вклада, I-CON предлагает практические преимущества для обучения представлений:

1. **Перенос идей:** фреймворк облегчает перекрестное опыление успешных техник между различными областями обучения представлений.

2. **Разработка алгоритмов:** исследователи могут систематически изучать новые методы обучения представлений, изменяя контролирующие и изученные дистрибуции в рамках фреймворка I-CON.

3. **Повышенная производительность:** стратегия устранения смещения, полученная из I-CON, приводит к более надежным и устойчивым представлениям, улучшая производительность в последующих задачах.

4. **Простота реализации:** Унифицированная формулировка позволяет более лаконично и последовательно реализовывать различные методы:

```python
# Пример реализации SNE во фреймворке I-CON
SNE_model = ICon(
    target_dist = Gaussian(sigma = 2),
    learned_dist = Gaussian(sigma = 1),
    mapper = Embedding(num_embeddings=N, dim=m)
)

# Пример реализации SimCLR во фреймворке I-CON
SimCLR_model = ICon(
    target_dist = Augmentation(num_views = 2),
    learned_dist = Gaussian(sigma=0.7, metric='cos'),
    mapper = ResNet50(embedding_dim=d)
)
```

## **8. Заключение**

I-CON представляет собой значительный прогресс в нашем понимании обучения представлений. Предоставляя единую математическую структуру, которая охватывает различные методы от кластеризации и снижения размерности до контрастного обучения и контролируемой классификации, она помогает прояснить основные принципы, связывающие эти подходы.

Фреймворк не только объединяет существующие методы, но и позволяет разрабатывать новые алгоритмы с улучшенной производительностью. Подход к устранению смещения, полученный из принципов I-CON, демонстрирует существенные успехи в задачах неконтролируемой классификации, подчеркивая практическую ценность этого теоретического объединения.

Поскольку обучение представлений продолжает развиваться, I-CON предоставляет исследователям мощный инструмент для понимания существующих методов, разработки новых алгоритмов и повышения производительности в широком спектре задач машинного обучения. Способность фреймворка объединять традиционно разделенные области предполагает, что дальнейшее перекрестное опыление идей может привести к еще более эффективным методам обучения представлений в будущем.