# Inference-Time Scaling for Generalist Reward Modeling — Technical Review

## Table of Contents

1. [Introduction and Motivation](#introduction-and-motivation)

    1.1. [RLHF and the Role of the Reward Model](#rlhf-and-the-role-of-the-reward-model)

    1.2. [Limitations of Existing Approaches](#limitations-of-existing-approaches)

    1.3. [Opportunities for Inference-Time Scaling](#opportunities-for-inference-time-scaling)

    1.4. [Proposed Approach](#proposed-approach)

2. [Methodology](#methodology)

   2.1. [Architecture and Training of GRM](#architecture-and-training-of-grm)

   2.1.1. [GRM Architecture](#grm-architecture)

   2.1.2. [Training GRM (Self-Principled Critique Tuning, SPCT)](#training-grm-self-principled-critique-tuning-spct)

   2.2. [Inference-Time Scaling (ITS) Technique](#inference-time-scaling-its)

   2.2.1. [Naive Voting](#naive-voting)

   \- Step-by-step example of Naive Voting

   2.2.2. [Meta Reward Model (Meta RM) — Improved Voting](#meta-reward-model-meta-rm--improved-voting)

   2.2.3. [Intuition of ITS and Managing the Trade-off](#intuition-of-its-and-managing-the-trade-off)

3. [Experiments](#experiments)

   3.1. [Tasks, Models, and Metrics](#tasks-models-and-metrics)

   \- Benchmarks

   \- Evaluation Procedure

   \- Models for Comparison

   3.2. [Tested Hypotheses](#tested-hypotheses)

   3.3. [Results and Analysis](#results-and-analysis)

   \- Overall Quality on Benchmarks

   \- Effect of SPCT

   \- Inference-Time Scaling vs Task-specific Models (H2, H3)

   \- Quality Growth with Increasing $k$

   \- Failure Analysis and Examples

4. [Comparison with Previous Approaches](#comparison-with-previous-approaches)

   4.1. [Task-Conditioned Reward Models](#task-conditioned-reward-models)

   4.2. [Multi-task RLHF](#multi-task-rlhf)

   4.3. [Pairwise and Semi-scalar RM](#pairwise-and-semi-scalar-rm)

   4.4. [LLM-as-a-Judge (e.g., GPT-4)](#llm-as-a-judge-e-g-gpt-4)

5. [Conclusions and Future Work](#conclusions-and-future-work)

   5.1. [Key Findings](#key-findings)

   5.2. [Limitations](#limitations)

   5.3. [Future Directions](#future-directions)

   \- Integrating GRM into RL Loops

   \- Coordinated Scaling of Policy and RM

   \- Automated Evaluators for Research

   \- Improving Efficiency and Reducing Bias

---

### **TWRB_FM 📻**

<audio controls>
  <source src="https://github.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/raw/refs/heads/develop/2025/week-22/TWRB_FM.wav" type="audio/mpeg">
  Your browser does not support the audio element.
</audio>

---

<details> 
    <summary><em><strong>Key Terms</strong></em></summary>

* **Reward Modeling (RM)**    
  A post-training method for large language models aimed at obtaining precise reward signals; defined by reward generation paradigms and evaluation templates.

* **Large Language Models (LLM)**     
  A class of models for which RM and reinforcement learning methods are widely applied during post-training.

* **Generative Reward Model (GRM)**    
  A paradigm where the model generates textual critiques instead of scalar scores; provides flexibility and scalability during inference.

* **Pointwise Evaluation Template**      
  Evaluating each response independently; enables GRM to generate rewards for any number of responses in a uniform format.

* **Pairwise Evaluation Template**     
  Selecting the single best response from a set of candidates; used, for example, in "LLM-as-a-Judge" methods.

* **Scalar Paradigm (Scalar RM)**       
  Generating a single numerical score per query-response pair; limited in scalability due to homogeneous signal nature (examples: DeepSeek-BTRM, DeepSeek-PairRM).

* **Semi-Scalar Paradigm (Semi-Scalar)**     
  Simultaneously generating textual critique and a scalar reward value to enrich the signal (example: Cloud-Gemma-2-27B).

* **Critique**        
  Textual feedback generated by models under semi-scalar or generative paradigms; in SPCT, precise critique judgments are trained.

* **Principles**     
  A set of criteria guiding reward generation; enable defining evaluation boundaries and improving RM accuracy.

* **SPCT (Self-taught with Principles and Critiques Tuning)**       
  A method for pointwise GRM that combines rejection fine-tuning with online RL based on rules. Trains the model to generate adaptive principles and critiques for efficient inference-time scaling.

* **Rejection Fine-Tuning (RFT)**       
  The "cold start" phase of SPCT, where the model adapts to generating principles and critiques in the correct format by rejecting overly soft or incorrect rewards.

* **Rule-Based Online RL**        
  The SPCT phase that reinforces the GRM’s ability to immediately optimize principle and critique generation via reinforcement learning.

* **Inference-Time Scaling**        
  Improving reward quality through repeated sampling and intensified computation during query execution.

* **Parallel Sampling**      
  DeepSeek-GRM’s approach to simultaneously generate multiple sets of principles and critiques followed by voting.

* **Voting**      
  Aggregating scalar scores (summing) or critique judgments to expand the reward space; can be supported by a meta-model.

* **Meta Reward Model (Meta RM)**   
  An additional scalar RM trained to manage the voting process by evaluating the correctness of the primary GRM’s principles and critiques.

</details> 

## 1. Introduction and Motivation

![Various Reward Generation Paradigms](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Figure_2.png)

> Figure 2: Various reward generation paradigms

### 1.1. RLHF and the Role of the Reward Model

Reinforcement Learning from Human Feedback (RLHF) is a popular approach for post-training large language models (LLMs) using human feedback. The key component of RLHF is the reward model (RM), which evaluates the quality of model-generated responses. However, traditional reward models are typically trained on individual tasks or domains and struggle to generalize to new scenarios. In multi-task RLHF settings, a problem arises: for each new task, one must either train a separate RM or use a single model for everything, potentially leading to reduced accuracy due to conflicting evaluation criteria. There is a need for a universal reward model (Generalist Reward Model, GRM) capable of flexibly handling diverse query types and quality criteria while maintaining high evaluation accuracy across multiple domains.

### 1.2. Limitations of Existing Approaches

Problems with applying classical RMs in multi-task settings stem from limitations in their training paradigms. Various approaches to reward generation exist: scalar (returning a single number), semi-scalar (combining textual evaluation and a number), and generative (fully textual justification/evaluation) models. Evaluation schemes also differ: pointwise (independently evaluating each response) and pairwise (comparing responses in pairs). These differences affect RM flexibility and inference-time scalability. For instance, pairwise models only assess relative preference between two responses and cannot directly handle single responses or lists of multiple candidates. Scalar RMs return a single number and cannot generate diverse quality signals for the same response, making it difficult to improve evaluations through multiple sampling during inference. In RLHF contexts across many tasks, such limitations lead to narrow specialization of the reward model: it is either trained for a specific data format or cannot effectively leverage additional computational budget during inference to enhance evaluation accuracy.

### 1.3. Opportunities for Inference-Time Scaling

Recent work indicates that as LLM scale increases, more computation can be deployed during inference to improve reasoning and evaluation without further model training. This has led researchers to hypothesize that appropriately trained RMs could efficiently scale evaluation quality by increasing compute during inference. In simpler terms, we want a single universal reward model to improve its evaluation when given more time/compute for complex queries. Such inference-time scalability is especially critical for generalist RMs: a model trained on diverse tasks could potentially match or surpass specialized RMs by leveraging additional resources for deeper evaluation of each response.

### 1.4. Proposed Approach

The authors of this paper propose precisely such a system. They develop Generative Reward Modeling—a generative approach to response evaluation—and introduce the technique of Inference-Time Scaling (ITS) to improve evaluation quality without altering the model architecture. Specifically, their pointwise generative reward model (GRM) can uniformly process one or multiple responses to a query, representing the evaluation as text (so-called principles and critique), from which a final score is extracted. Such a model easily adapts to various input types (satisfying the requirement for flexibility) and allows application of sampling and aggregation methods during inference (solving the scalability problem). In other words, GRM is a universal evaluation model trained simultaneously on multiple types of queries and responses, capable of improving its evaluations as the number of samples (evaluation variants) increases during inference. We will now examine the methodology of the proposed approach, experiments, comparisons with alternatives, and draw conclusions about the significance of this work.

## 2. Methodology

### 2.1. Architecture and Training of GRM

#### GRM Architecture

The paper adopts a generative architecture for the reward model based on a large language model (LLM). Unlike standard RMs, which typically output only a numerical score, the *generative reward model* generates an extended textual evaluation. Specifically, for each query and set of responses, the model first formulates a set of evaluation criteria (*principles*), then provides a detailed analysis (*critique*) of each response based on these principles, from which a numerical reward is derived.

Formally, this process can be represented as: the model generates a set of principles $\{p_i\}$ based on the input $(x, \{y_i\}_{i=1}^n)$, then generates a conclusion $C$ (critique) and computes a final reward $R$, taking into account the formulated principles. Mathematically, this is described as:

$$\{p_i\}_{i=1}^m \sim p_{\theta}(x, \{y_i\}_{i=1}^n), \qquad R = C \sim r_{\theta}\big(x, \{y_i\}_{i=1}^n, \{p_i\}_{i=1}^m\big),\tag{3}$$

where:
- $p_{\theta}$ – principle generation function
- $r_{\theta}$ – critique and reward generation function, both implemented by a single model with parameters $\theta$ (i.e., GRM generates both principles and rewards).

This two-stage generation allows for *adaptive* selection of evaluation criteria tailored to each specific query and response, enhancing the quality and granularity of the resulting reward signals. Intuitively, principles serve as the model’s **internal instruction**: they define which aspects to evaluate, while critique applies these principles to the specific response. This enables GRM to produce more meaningful and transparent evaluations, especially on complex and heterogeneous tasks.

#### Training GRM (Self-Principled Critique Tuning)

Let us first recall how the GRPO algorithm works 👇

![GRPO](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-07_%26_08/assets/Figure_19.jpeg)

To train the model to generate such principles and critiques, the authors introduce a specialized training method—**Self-Principled Critique Tuning (SPCT)**. SPCT consists of two stages:
1. **Rejection Fine-Tuning**: Training the model to generate structured evaluations with principles and critique.
2. **Rule-Based RL Fine-Tuning**: Fine-tuning using reinforcement learning based on rules.

![SPCT](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Figure_3.png)

> Figure 3: Illustration of SPCT, including rejection fine-tuning, rule-based RL, and corresponding scalable behaviors during inference. Inference-time scaling is achieved via naive voting or meta-reward-model-guided voting with principles generated at scale, leading to more detailed reward outcomes in an expanded value space.

In the first stage (**rejection fine-tuning**), the model is trained to generate evaluations in the required format (first principles, then critique with a score) on diverse data. To construct the training set, the authors combine various task types: those with a single correct answer, those with paired responses (preferred vs. not), and those with multiple responses. All these variants are converted into a *unified pointwise-GRM format*: the model receives a query and one or multiple responses and must output text with principles and evaluations for each response.

To improve data quality, a *rejection strategy* is applied: model-generated trajectories (principles + critique + scores) are compared against known ground truth (e.g., human ratings or identification of the best answer). If the model makes an error (predicts the wrong best answer or incorrect score), the example is rejected; trivial cases where the model always guesses correctly are also rejected (to prevent overfitting to easy examples). Formally, the criterion for correct predicted reward scores can be expressed as: predicted pointwise scores $S_i^*$ are deemed correct if (a) for multi-response tasks, the model assigns the highest score to the answer that is ground-truth best (i.e., $S_{i_{\text{best}}}^* > S_j^*$ for all others $j$), and (b) for single-response tasks, the predicted score equals the true rating. All incorrect or overly simple examples are filtered out, and the remaining ones are used for fine-tuning, enabling GRM to learn basic skills: generating meaningful principles and evaluating responses in the correct format.

The second stage—**Rule-Based RL**—aims to further improve accuracy and *teach the model to correctly rank responses*, especially in complex cases. The authors apply a modification of an RL algorithm (they reference GRPO, a variant of Proximal Policy Optimization) with a simple reward signal: the model receives a positive reward (+1) for correctly ordered scores and a negative reward (–1) for incorrect ones. In simpler terms, if the GRM generates critique and scores where the best answer is correctly identified (matches human preference), this generation is rewarded; otherwise, it is penalized. Additionally, a strong KL-penalty is introduced to prevent the model from deviating too far from its pre-trained knowledge and to preserve coherent output format.

Unlike some prior work, no hand-crafted templates or complex scoring functions are used—only the rule of correct ranking and output format. Thus, **SPCT fine-tuning** instills in the model *self-principled critique*: the GRM generates principles itself, applies them, and through RL corrections, learns to do so with maximum precision.

### 2.2. Inference-Time Scaling (ITS) Technique

The key feature of this approach is **inference-time scaling** (*Inference-Time Scaling*, ITS), enabling improvement of GRM evaluation quality without changing its parameters, simply by deploying more computation during inference. The idea is to apply *parallel sampling* and subsequent *aggregation of results* to obtain a more reliable reward signal. Since our RM is generative, each model run may differ slightly—especially if randomness is introduced (e.g., partial nondeterminism during principle and critique generation). By generating multiple evaluation variants for the same response, we can aggregate them to smooth out errors from individual runs.

#### Naive Voting

The basic way to aggregate $k$ GRM samples is to average or sum the resulting scores. Suppose for some response $i$, the model produces a numerical score $S_{i,j}$ (e.g., a score from 1 to 10) during each run $j = 1..k$. Then the final *aggregate* rating for response $S_i^*$ can be obtained by summing all partial scores:

$$S_i^{*} = \sum_{j=1}^{k} S_{i,j}.\ \tag{6}$$

The authors define the voting procedure precisely this way—as summing reward scores from all samples (equivalent to averaging, up to a $1/k$ multiplier). Each model run $j$ includes generation of its own set of principles $\{p_{i,j}\}$ and, based on them, critique with scores $S_{i,j}$ for all considered responses.

Ultimately, voting accumulates information from multiple *different principles and evaluation perspectives*, leading to a more accurate and nuanced final decision. Since each individual score $S_{i,j}$ typically lies within a limited range (e.g., 1–10), summing several independent scores effectively **expands the reward value space**, enabling finer discrimination between responses.

For example, if in one run both responses receive the maximum score of 10, the model cannot distinguish them; but over 10–20 runs, differences may accumulate (e.g., one response frequently receives 10, while the other sometimes receives 9), resulting in a total of 100 vs. 95—a noticeable difference. The authors emphasize that increasing the number of samples $k$ proportionally expands the score scale and increases the diversity of generated principles, thereby **improving the quality and granularity of final reward signals**.

Intuitively, one can think of *each principle as reflecting a separate "perspective" on evaluation*; thus, a greater number of random principles better covers all aspects of response quality. A crucial implementation detail: to avoid systematic biases, responses are randomly shuffled before each sampling, so the model does not become biased toward response position in the list.

![Naive Voting](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Figure_3.1.png)

#### Step-by-step Example of Naive Voting

In the naive voting procedure, the evaluation model (GRM) repeatedly—$k$ times—"re-evaluates" the same set of *question ↔ answer(s)*. Each run $j$ is performed with stochastic sampling (typically $T\!=\!0{,}5$), so the model regenerates:

1. **A set of principles** $\{p_{\,j}\}$ — independent criteria for judging answers;
2. **Critique + scores** $S_{i,j}$ for each answer $y_i$.

In the voting formula, this is evident in the indices $j$ and $\{p_{i,j}^{\,m_j}\}$ — each sample has its own principles and, consequently, its own scores, which are then summed:

$$
S_i^{*} = \sum_{j=1}^{k} S_{i,j} \quad \text{where} \quad
\bigl\{S_{i,j}\bigr\} = f_{\text{point}}(C_j,\{y_i\}) \sim r_\theta\!\bigl(x,\{y_i\},\{p_{i,j}\}\bigr),
$$

> **Query:** "Explain quantum superposition in simple terms."
> **Answer 1:** "Superposition is..." (an accessible popular explanation).
> **Answer 2:** "Superposition is described by a linear combination of vectors..." (a formal-mathematical description).

**Multiple Evaluation Runs (k = 4)**

| Sample j | Score $S_{1,j}$ for Answer 1 | Score $S_{2,j}$ for Answer 2 |
| ------- | -------------------------- | -------------------------- |
| 1       | 8                          | 6                          |
| 2       | 9                          | 7                          |
| 3       | 7                          | 7                          |
| 4       | 8                          | 6                          |

*(numbers are illustrative, reflecting the discrete 1–10 range described in the paper)*

**Aggregated Voting**

$$
S_{1}^{*}=8+9+7+8=32,\qquad
S_{2}^{*}=6+7+7+6=26.
$$

**Final Result**

Since $S_{1}^{*}>S_{2}^{*}$, the RM recognizes **Answer 1** as better.
If $k$ is increased, the range $[0,10k]$ expands, and the difference between answers becomes even more pronounced (e.g., at $k = 16$, the score might be 128 vs. 104).

**Key Features of the Approach**

1. **Simplicity** — no weighted scheme: each partial score counts as one vote.
2. **Linear Scalability** — performance scales proportionally with $k$ until compute limits.
3. **Noise Reduction** — summation smooths out random fluctuations from individual samples.
4. **Scale Expansion** — summation "compresses" the discrete scale, improving resolution between close answers.

Thus, **naive voting** in the context of GRM is a simple summation of discrete reward scores, enabling efficient use of additional inference compute budget for more accurate and robust ranking of responses.

### Meta Reward Model (Meta RM) — Improved Voting

![Meta Reward Model](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Figure_3.2.png)

Simple averaging/voting across multiple samples already yields gains, but not all generated critiques are equally high-quality. Some runs may be unsuccessful—e.g., the model selects inappropriate principles or is misled by randomness, yielding an incorrect conclusion.

To account for this, the authors introduce an additional model—the *meta reward model*. Meta RM is a separate, smaller **scalar** reward model trained to evaluate the correctness or quality of the GRM-generated critique. In simpler terms, meta RM takes as input the text: the model-generated principles and critique—and outputs a score indicating how well this critique aligns with a *correct* analysis of the response.

Meta RM is trained using binary cross-entropy: on a set of cases, the "correctness" of each critique is known (1 if GRM correctly identified the best answer, 0 if it erred), and meta RM learns to classify these cases. Training data comes from both RL-tuning trajectories (where correctness of each GRM run is known) and additional generated examples.

During inference, meta RM is applied as follows: for each of the $k$ GRM samples, the meta-model predicts an auxiliary "meta-reward" $q_j$, reflecting confidence in that evaluation trajectory. The final decision is then not a simple average of all $S_{i,j}$, but a **weighted voting**—for example, only the top-$k_{\text{meta}}$ samples with the highest $q_j$ are averaged. In experiments, the authors used $k_{\text{meta}} = k/2$, i.e., discarding the half of most questionable samples.

This approach filters out noise and significantly improves final accuracy. They report that *meta-guided voting* achieves higher performance than naive voting, especially at large $k$, as it effectively filters out random GRM errors.

<details> 
    <summary><em><strong>Formalizing the Concept of the Meta Reward Model (Meta RM)</strong></em></summary>

---

The Meta Reward Model is a scalar model designed to evaluate the quality of critiques generated by the GRM (Generative Reward Model). The primary task of the Meta RM is to determine how correct a given critique is and to use this information in weighted voting to select the final response.

The model is trained using binary cross-entropy: the target label is 1 if the critique is deemed correct (i.e., the GRM correctly identified the best response) and 0 if incorrect. During inference, the model predicts a `meta-reward` value $q_j \in [0, 1]$ for each critique, reflecting the confidence in its correctness.

### Problem Formalization

Let the training dataset be denoted as:

$$
D = \{(x_i, c_i, y_i)\}_{i=1}^N
$$

where:

* $x_i$ — the input query or task,
* $c_i$ — the critique generated by the GRM based on principles,
* $y_i \in \{0, 1\}$ — the binary label indicating the correctness of the critique.

The structure of critique $c_i$ can be represented as a triplet:

$$
c_i = (P_i, A_i, R_i)
$$

where:

* $P_i$ — the set of evaluation principles used by the GRM,
* $A_i$ — the textual critique analysis,
* $R_i$ — the ranking or score assigned by the GRM.

The target variable is defined as:

$$
y_i =
\begin{cases}
1, & \text{if the GRM correctly identified the best response} \\
0, & \text{otherwise}
\end{cases}
$$

### Model and Loss Function

The Meta RM model is a differentiable function:

$$
f_\theta: (P, A, R) \rightarrow [0, 1]
$$

where $\theta$ are the model parameters, and the output $f_\theta(c_i)$ is interpreted as the probability of critique correctness.

The loss function (binary cross-entropy) is formulated as:

$$
\mathcal{L}_{BCE}(\theta) = -\frac{1}{N} \sum_{i=1}^N \left[y_i \log(f_\theta(c_i)) + (1 - y_i)\log(1 - f_\theta(c_i))\right]
$$

This function penalizes the model:

* when it assigns low probability to genuinely correct critiques ($y_i = 1$),
* or high probability to incorrect ones ($y_i = 0$).

### Role in Inference and Weighted Voting

During inference, the Meta RM is applied as follows:

1. **Critique Generation:** For each input $x$, a set $\{c_j\}_{j=1}^k$ of possible critiques is generated.
2. **Critique Scoring:** Each critique is passed through the model:

   $$
   q_j = f_\theta(c_j)
   $$
3. **Selection:** The top-$k_{\text{meta}}$ critiques are selected in descending order of $q_j$:

   $$
   \mathcal{T} = \text{top}_{k_{\text{meta}}}(\{(c_j, q_j)\})
   $$

   Typically, $k_{\text{meta}} = k / 2$.
4. **Weighted Voting:** The final score is aggregated as:

   $$
   S_{\text{final}} = \frac{1}{|\mathcal{T}|} \sum_{j \in \mathcal{T}} S_{i,j} \cdot q_j
   $$

   where $S_{i,j}$ is the corresponding candidate's score from the GRM.

### Filtering Effect

The Meta RM performs a crucial filtering function under multi-sample sampling:

* High $q_j$ values correspond to critiques that statistically more often lead to correct conclusions.
* Low values suppress GRM errors.
* Weighted averaging during voting minimizes the impact of noise and random failures.

</details> 

### Intuition of ITS and Managing the Trade-off

The ITS technique allows explicit control over the trade-off between computation time and evaluation quality/specialization. The **Generalist RM** is trained to cover a broad spectrum of tasks, but with limited time (e.g., $k=1$, i.e., one run), its output may be more *superficial*—the model delivers a general evaluation based on the most obvious principles. However, by increasing $k$, we compel the model to "think longer" about the response: by exploring different principles, the GRM considers finer and rarer aspects of quality.

In the limit, with a large number of samples, the universal model can detect nuances as effectively as a specialized model trained specifically on that narrow task. Thus, *ITS provides an adaptation mechanism*: for simple cases, a small $k$ can be used (fast and sufficiently accurate), while for complex or critical tasks, $k$ can be increased, effectively allowing the model to become more *specialized* for that specific query.

The paper notes that a properly trained GRM demonstrates increasing accuracy as compute at inference increases, whereas for some other models, increasing $k$ yields a smaller effect. This indicates that SPCT skills (principle generation, precise critique) have made the model particularly adept at leveraging additional time.

As a result, developers gain a hybrid "one-size-fits-all" solution that, when needed, can nearly match narrowly trained solutions simply by working harder during evaluation. By controlling the parameter $k$, one can balance **generalization ability** (small $k$, faster, one model for all) and **task-specific precision** (large $k$, slower, but more accurate)—without switching models or additional training.

## 3. Experiments

### 3.1 Tasks, Models, and Metrics
The authors conducted experiments on reward model benchmarks across diverse domains. The following task sets and datasets were used:

#### Benchmarks
- **Reward Bench** (Lambert et al., 2024):  
  Tasks selecting the best answer from multiple candidates. Metric: Accuracy.

- **PPE** (Frick et al., 2025):  
  Includes two metrics: 
  - *PPE Preference* (query satisfaction)
  - *PPE Correctness* (actual correctness).  
  Both metrics measured as selection accuracy.

- **RMB** (Zhou et al., 2025):  
  Generalized benchmark for reward models. Metric: Accuracy.
- **RealMistake** (Kamoi et al., 2024):  
  Binary classification task (correct/incorrect answer). Metric: ROC-AUC.

#### Evaluation Procedure
- For multi-response tasks: Accuracy of selecting the best answer.
- For binary classification: ROC-AUC.
- In case of tied scores: Random shuffling of answers followed by selection of the highest score.

#### Models for Comparison
1. **Public models and LLM judges**:
   - Specialized RMs:  
     `InternLM2-20B-Reward`, `Nemotron-4-340B-Reward`, `ArmoRM-8B`
   - LLM judges:  
     `GPT-4 (GPT-4o)`, `Claude-3.5`, `LLaMA-3.1-70B-Instruct`
   - LLM-as-a-Judge (Zheng et al., 2023)

2. **Base models on Gemma-2-27B architecture**:
   - `DeepSeek-BTRM-27B` (pairwise comparisons with Bradley-Terry aggregation)
   - `CLoud-Gemma-2-27B` (semi-scalar approach)
   - `DeepSeek-PairRM-27B` (pairwise-trained RM)

3. **Variants of the proposed model**:
   - `DeepSeek-GRM-27B-RFT` (Rejection Fine-Tuning without RL)
   - `DeepSeek-GRM-27B` (final version with SPCT)
   - Inference modes:  
     `Voting@k`, `MetaRM@k` (k ∈ {1,2,4,8,16,32})

### 3.2 Tested Hypotheses
1. **(H1) Quality Improvement via SPCT**  
   Compare `DeepSeek-GRM-27B-RFT` and `DeepSeek-GRM-27B` to assess the contribution of RL training.

2. **(H2) Competition with Large Models via ITS**  
   Test whether `DeepSeek-GRM-27B` with Inference-Time Scaling (ITS) can surpass 70B+/GPT-4 models.

3. **(H3) Compute Efficiency vs. Model Size**  
   Analyze scaling laws: quality improvement via increasing $k$ (fixed 27B parameters) vs. transitioning to larger models.

4. **(H4) Universality of the ITS Approach**  
   Test limitations of inference-time scaling for baseline models:
   - Scalar models: limited sample diversity
   - Pairwise models: difficulties aggregating beyond pairwise comparisons

### 3.3 Results and Analysis

**Overall Quality on Benchmarks.**  
Final aggregate results are presented in the paper’s tables (see example in Table 2). Below is a brief summary of key observations:

![Table 2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Table_2.png)

> Table 2: Overall results of various methods and models on RM tests. Underlined numbers denote best performance, bold numbers denote best performance among base methods and our methods, and italics indicate scalar or semi-scalar RMs. For meta RM with guided voting (MetaRM).

*   **Without ITS (i.e., $k=1$):**  
    **DeepSeek-GRM-27B (Ours)** already shows competitive quality relative to top baseline models. For instance, on the overall score (average across all tasks), it achieves ~69.9, comparable to GPT-4o (~71.3) and surpassing other open models of similar or larger size. It outperforms alternatives on the same architecture: DeepSeek-GRM-27B (69.9) vs. DeepSeek-PairRM-27B (~69.0) vs. CLoud-Gemma-2-27B (~68.7) vs. BTRM-27B (~68.6) on overall score. This confirms hypothesis H1: the SPCT method successfully enhanced the quality of a universal RM compared to other approaches, even without additional compute.

*   **Effect of SPCT:**  
    Comparing *GRM-27B-RFT* (without RL) and the full *GRM-27B* demonstrates a significant gain. For example, on the overall metric, the model after RL (69.9) outperforms before RL-tuning (~68.8 overall). SPCT particularly improves metrics on complex, unverifiable tasks: on one domain (e.g., PPE Preference), the score rose from ~64.1 to ~64.7, with improvements visible in other columns. This means online RL with principles taught the model to more confidently identify better responses, relying less on training data statistics. Interestingly, on easily formalized checks (PPE Correctness—likely factual correctness), the difference is minimal, as the model could rely on explicit rules even without RL.

*   **Inference-Time Scaling vs Task-specific Models (H2, H3):**  
    With scaling $k = 32$, the model achieved new records. In **Voting@32** mode (simple ensemble of 32 samples), DeepSeek-GRM-27B raised its overall metric from 69.9 to ~71.0. With **Meta RM (@32)**, it rose even higher, to **72.8**. For comparison, the largest competitor, *Nemotron-4 (340B)*, achieved ~70.5, and GPT-4o ~71.3. Thus, the **27-billion parameter model with ITS surpassed a 340B parameter model** and even outperformed (on overall score) GPT-4o. This is a remarkable result confirming hypothesis H2: correctly utilizing inference compute can compensate for hundreds of billions of parameters. Per-task results are as follows:

    Thus, ITS was especially beneficial on tasks requiring fine judgments (preferences, subjective evaluations)—where the gain was maximal. Verifiable facts (fact-checking) were already handled well, but even there, modest improvements occurred.

*   **Quality Growth with Increasing $k$:**  
    The authors provide a separate analysis of *inference-time scalability* (Tables 3-4 and Figure 1 in the paper).
    
    The DeepSeek-GRM quality curve rises steeply from $k=1$ to $k=8$, then slows but continues increasing up to $k=32$.
    
    ![Figure 1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Figure_1.png)
    
     Interestingly, *LLM-as-a-Judge* also shows some gain from voting: clearly, if multiple independent large LMs are asked to evaluate answers and a majority decision is taken, accuracy improves (authors note, that *“LLM-as-a-judge also shows a significant performance increase, indicating reliability of majority vote”*). 
     
      ![Table 3-4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Table_3-4.png)
     
     However, other methods, such as *Cloud-Gemma (semi-scalar)* or *PairRM*, achieved only limited improvement with increasing $k$. This aligns with our earlier arguments: if a model cannot generate diverse scores or requires pairwise comparisons, repeated runs offer little benefit. In contrast, GRM, thanks to random principles, achieves nearly linear gains up to 32 samples. Moreover, the authors note that **the efficiency of increasing $k$ for GRM is higher than simply increasing model size**. In experiments, DeepSeek-GRM-27B with $k=32$ outperformed DeepSeek-GRM-67B with $k=1$ (a model twice as large). This is a critical finding: adding inference computation (cheaper than training and storing a model 2–10 times larger) can be a more cost-effective path to improving RMs than increasing parameters, provided proper training like SPCT.

*   **Failure Analysis and Examples:**  
    The paper also presents examples where the method still fails. It is noted that DeepSeek-GRM sometimes struggles with subtle cases where no answer is clearly superior (or all are poor)—leading to random selection. Or if the principle generated by the model itself is incorrect or biased, all evaluations become systematically flawed. Meta RM partially mitigates these cases by filtering out absurd critiques but cannot eliminate all errors. Nevertheless, overall, **DeepSeek-GRM demonstrated lower domain bias** than compared scalar models and sufficient robustness due to diverse principles.

## 4. Comparison with Previous Approaches

The proposed approach (ITS + GRM) combines ideas from other methods but differs in implementation and properties:

1. **Task-Conditioned Reward Models**  
   * **Core idea:** Use explicit task/metric labels as input to the RM for generalization  
   * **Drawbacks:** Limited to predefined tasks, inability to generate new criteria, scaling complexity due to labeling requirements  
   * **ITS/GRM distinction:** GRM **dynamically generates** relevant evaluation principles from the query without fixed labels, ensuring flexibility. ITS samples principles multiple times, automatically covering different "tasks"

2. **Multi-task RLHF**  
   * **Core idea:** Train one LM on multiple types of feedback or use multiple RMs for different aspects  
   * **Drawbacks:** Goal conflicts, signal averaging, loss of precision when merging into one RM, management complexity  
   * **ITS/GRM distinction:** One GRM acts as a **universal "judge"** generating justifications for scores. ITS allows different samples to focus on distinct aspects of a complex task, and voting aggregates the evaluations

3. **Pairwise and Semi-scalar RM**  
   * **Core idea:** Pairwise RM compares two answers; semi-scalar combines textual feedback with a scalar score  
   * **Drawbacks:** Pairwise—poor scalability beyond two answers, no absolute scores. Semi-scalar—training complexity, limited expressiveness  
   * **ITS/GRM distinction:** Generative pointwise GRM evaluates all answers uniformly, provides graded scores, and is easily averaged. Full-text generation ensures flexibility

4. **LLM-as-a-Judge (e.g., GPT-4)**  
   * **Core idea:** Use a powerful pre-trained LLM (without training) as an evaluator via instruction  
   * **Drawbacks:** High computational cost, **lack of transparency** in criteria, inability to fine-tune, divergence from human metrics  
   * **ITS/GRM distinction:** ITS+SPCT proposes **training a specialized GRM**. Explicit principle generation provides **transparency**. Multiple runs are more cost-effective than using one LLM  

## 5. Conclusions and Future Work

**Key Findings.** This work demonstrates that *generalist reward models* (GRM) can be successfully implemented and surpass traditional narrow-specialized RMs if trained to generate detailed evaluations (principles + critique) and endowed with the ability to scale via compute. The presented method, **Self-Principled Critique Tuning (SPCT)**, significantly enhances reward model quality across diverse domains, and crucially, enables it to effectively utilize additional inference compute. The model **DeepSeek-GRM-27B**, trained via SPCT, showed higher accuracy on multi-domain benchmarks than several strong public RMs of similar (and even larger) size. Its capability for *inference-time scaling* was empirically confirmed: adding parallel sampling and voting substantially improves results, especially with meta RM filtering noisy samples. In effect, the authors rewrite the scaling rule: previously, to improve quality, one had to increase model size (from 27B to 70B and beyond); now, one can fix the model size but make it "think harder"—achieving equivalent or superior results. This establishes a new direction for RLHF and LLM evaluation systems, pointing toward more **cost-efficient and universal reward systems**.

**Limitations.** Despite its success, the approach has several limitations. First, *efficiency*: running dozens of samples at inference increases time and computational cost. For practical applications, a balance of $k$ must be found or the model optimized (e.g., via compression or specialized accelerators), otherwise evaluation may become a bottleneck. Second, *some tasks remain challenging*: authors note that DeepSeek-GRM still errs in isolated cases, particularly when data falls outside training domain boundaries or contains novel error types. If the model has not encountered a certain type of content, its generated principles may be inadequate. This partially manifested on specific tasks where specialized RMs outperformed GRM. Third, *principles and critiques, being automatically generated*, may carry *biases or errors* inherited from training data. The authors candidly admit that although DeepSeek-GRM showed reduced bias, complete elimination was not achieved. Moreover, principle generation is a double-edged sword: the model may generate a plausible but irrelevant rule and then incorrectly judge an answer based on it. Methods for controlling or validating these internal reasoning steps are needed.

**Future Directions.** The paper opens several avenues for future work:

* **Integrating GRM into RL Loops:** One obvious next step is to use DeepSeek-GRM directly within RLHF as a universal "critic" for policy training. Currently tested offline on fixed responses, it can be incorporated into on-policy learning of new models (e.g., training a Chat model directly using signals from GRM). This would transform the SPCT-trained RM into a *multifunctional reward module* for any scenario.
* **Coordinated Scaling of Policy and RM:** An intriguing idea is *inference-time co-scaling* with the answer-generating model. If both the policy (generating answers) and the critic (our RM) can exchange multiple samples or iteratively improve each other, this could lead to even higher-quality iterative inference. For example, the policy generates several candidates, the RM evaluates them multiple times, the policy refines the response, etc. Such reasoning cycles, where the model deliberates over its answer with a critic, could enhance the final output.
* **Automated Evaluators for Research:** Universal RMs with ITS can serve as powerful offline quality evaluators for comparing various LLMs. The authors note such a model can become a **robust assessment tool** for evaluating large models without human involvement. If DeepSeek-GRM sufficiently mimics human preferences, it can be used for rapid testing of new models, detecting subtle errors (RealMistake), etc. Open release of the model (authors plan to publish it publicly) will aid independent research.
* **Improving Efficiency and Reducing Bias:** A separate branch focuses on improving the GRM itself: reducing size without quality loss (e.g., knowledge distillation of SPCT models into smaller architectures), developing methods to detect and remove systematic biases in generated principles. Meta RM can be strengthened or replaced with more advanced quality-control models to more reliably filter poor samples. Adapting the approach to multimodal data (e.g., evaluating responses containing images, code, etc.) is also promising.

In conclusion, **Inference-Time Scaling for Generalist Reward Models** represents a significant advancement in RLHF. It demonstrates how, by intelligently leveraging LLM capabilities (generating textual reasoning) and additional inference compute, evaluation quality can be substantially improved without increasing model size. This work lays the foundation for more flexible and powerful feedback systems capable of keeping pace with the growing capabilities of large language models themselves.