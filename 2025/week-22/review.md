# Inference-Time Scaling for Generalist Reward Modeling — технический обзор

<details> 
    <summary><em><strong>Ключевые термины</strong></em></summary>

* **Моделирование наград (Reward Modeling, RM)**    
  Метод пост-обучения больших языковых моделей, цель которого — получение точных сигналов вознаграждения; определяется парадигмами генерации наград и шаблонами оценки.

* **Большие языковые модели (LLM)**     
  Класс моделей, для которых в процессе пост-обучения широко применяют RM и методы обучения с подкреплением (RL).

* **Генеративное моделирование наград (Generative RM, GRM)**    
  Парадигма, при которой модель генерирует текстовую критику вместо скалярных оценок. Обеспечивает гибкость и масштабируемость при инференсе.

* **Поточечный шаблон оценки (Pointwise)**      
  Оценка каждого ответа отдельно; позволяет GRM генерировать награды для произвольного числа ответов в одном формате.

* **Попарный шаблон оценки (Pairwise)**     
  Выбор одного наилучшего ответа из набора кандидатов; применяется, например, в методе «LLM-as-a-Judge».

* **Скалярная парадигма (Scalar RM)**       
  Генерация единой числовой оценки за запрос–ответ. Ограничена в масштабировании из-за однообразия сигналов (примеры: DeepSeek-BTRM, DeepSeek-PairRM).

* **Полускалярная парадигма (Semi-Scalar)**     
  Одновременная генерация текстовой критики и скалярного значения награды для обогащения сигнала (пример: Cloud-Gemma-2-27B).

* **Критика (Critique)**        
  Текстовая обратная связь, создаваемая моделями в рамках полускалярной или генеративной парадигм; в SPCT обучаются точные критические суждения.

* **Принципы (Principles)**     
  Набор критериев, направляющих генерацию наград; позволяют задавать рамки оценки и повышать точность RM.

* **SPCT (Self-taught with Principles and Critiques Tuning)**       
  Метод для поточечного GRM, объединяющий тонкую настройку с отклонением и онлайновое RL на основе правил. Обучает модель генерировать адаптивные принципы и критику для эффективного масштабирования при инференсе.

* **Тонкая настройка с отклонением (Rejection Fine-Tuning, RFT)**       
  Фаза «холодного старта» SPCT, на которой модель адаптируется генерировать принципы и критику в корректном формате, отбрасывая слишком мягкие или неверные награды.

* **Онлайновое RL на основе правил**        
  Фаза SPCT, укрепляющая способность GRM сразу оптимизировать генерацию принципов и критики методом подкрепления.

* **Масштабирование при инференсе (Inference-Time Scaling)**        
  Увеличение качества наград за счёт многократной выборки и более интенсивных вычислений во время выполнения запроса.

* **Параллельная выборка (Parallel Sampling)**      
  Подход DeepSeek-GRM для одновременного создания множества наборов принципов и критики с последующим голосованием.

* **Голосование (Voting)**      
  Агрегация скалярных оценок (суммирование) или критикующих суждений для расширения пространства наград; может поддерживаться мета-моделью.

* **Мета-RM (Meta Reward Model)**   
  Дополнительный скалярный RM, обученный управлять процессом голосования, оценивая корректность принципов и критики основной GRM.

</details> 

---

## 1. Введение и мотивация

### 1.1. RLHF и роль модели вознаграждения

Reinforcement Learning from Human Feedback (RLHF) – популярный подход для пост-тренировки больших языковых моделей (LLM) с использованием обратной связи человека. Ключевым компонентом RLHF является модель вознаграждения (reward model, RM), которая оценивает качество сгенерированных ответов модели. Однако традиционно модели вознаграждения обучаются на отдельных задачах или доменах и сталкиваются с трудностями при обобщении на новые сценарии. В многозадачных сценариях RLHF возникает проблема: для каждой новой задачи приходится либо обучать отдельную RM, либо использовать одну модель на всё, что может приводить к снижению точности из-за конфликтующих критериев оценки. Необходима универсальная модель вознаграждения (Generalist Reward Model, GRM), способная гибко работать с разными типами запросов и критериями качества, сохраняя высокую точность оценок во множестве доменов.

### 1.2. Ограничения существующих подходов

Проблемы применения классических RM в мультизадачных настройках связаны с ограничениями парадигм их обучения. Существуют различные подходы к генерации вознаграждений: скалярные (возвращают одно число), полу-скалярные (комбинация текстовой оценки и числа) и генеративные (полностью текстовое обоснование/оценка) модели. Также различают схемы оценки: pointwise (независимо оценивает каждый ответ) и pairwise (попарно сравнивает ответы). Эти различия влияют на гибкость RM и возможность масштабирования при выводе. Например, pairwise-модели оценивают только относительное предпочтение двух ответов и не умеют напрямую работать с одиночным ответом или списком из нескольких вариантов. Скалярные RM возвращают одно число и не могут генерировать разнообразные сигналы качества для одного и того же ответа, из-за чего трудно улучшить оценки путем множественного семплирования при выводе. В условиях RLHF на многих задачах такие ограничения приводят к узкой специализации модели вознаграждения: она либо обучена под конкретный формат данных, либо не может эффективно использовать дополнительный вычислительный бюджет на этапе вывода для повышения точности оценки.

### 1.3. Возможности масштабируемости при выводе

Недавние работы указывают, что с ростом масштабов LLM можно задействовать больше вычислений на этапе вывода (inference) для улучшения рассуждений и оценки без дополнительного обучения модели. Это навело исследователей на мысль, что правильные методы обучения RM могут позволить эффективно масштабировать качество оценки за счет увеличения compute на этапе вывода. Проще говоря, хотим, чтобы одна универсальная reward-модель могла улучшать свою оценку, если ей дать больше времени/вычислений при ответе на сложный запрос. Такая масштабируемость на этапе вывода особенно важна для обобщающей RM: модель, обученная на разнообразных задачах, потенциально может догнать или превзойти специализированные RM, если задействовать дополнительные ресурсы для более глубокой оценки каждого ответа.

### 1.4. Предлагаемый подход

Авторы статьи предлагают именно такую систему. Они разрабатывают Generative Reward Modeling – генеративный подход к оценке ответов, и вводят технику Inference-Time Scaling (ITS) – масштабирование на этапе вывода – для улучшения качества оценок без изменения архитектуры модели. В частности, их pointwise generative reward model (GRM) способна единообразно обрабатывать один или несколько ответов на запрос, представляя оценку в виде текста (так называемых принципов и критики), из которого извлекается итоговый балл. Такая модель легко адаптируется к разным типам входных данных (решает требование гибкости) и допускает применение методов семплирования и агрегации ответов при выводе (решает задачу масштабируемости). Иначе говоря, GRM – это универсальная оценочная модель, обученная сразу на множестве типов запросов и ответов, способная улучшать свои оценки по мере увеличения числа сэмплов (вариантов оценки) при инференсе. Далее мы подробно рассмотрим методологию предлагаемого подхода, эксперименты, сравнение с альтернативами и сделаем выводы о значимости работы.

# 2. Методология

## 2.1. Архитектура и обучение GRM

### Архитектура GRM

В работе выбрана генеративная архитектура reward-модели, основанная на большой языковой модели (LLM). В отличие от стандартных RM, которые обычно выдают только оценочное число, *generative reward model* генерирует развернутую оценку в текстовом формате. Конкретно, модель для каждого запроса и набора ответов сначала формулирует некоторый набор критериев оценки (*principles*), а затем выдаёт подробный разбор каждого ответа (*critique*) на основе этих критериев, по итогам которого определяется числовая оценка (reward).

Формально этот процесс можно представить так: модель порождает множество принципов $\{p_i\}$ на основе входных данных $(x, \{y_i\}_{i=1}^n)$, после чего генерирует заключение $C$ (критику) и вычисляет итоговую оценку $R$, учитывая сформулированные принципы. В математической форме это описывается как:

$$\{p_i\}_{i=1}^m \sim p_{\theta}(x, \{y_i\}_{i=1}^n), \qquad R = C \sim r_{\theta}\big(x, \{y_i\}_{i=1}^n, \{p_i\}_{i=1}^m\big),\tag{3}$$

где:
- $p_{\theta}$ – функция генерации принципов
- $r_{\theta}$ – функция генерации критики и reward, обе реализованы одной моделью с параметрами $\theta$ (то есть GRM сама генерирует и принципы, и оценку). 

Такая двухэтапная генерация позволяет *адаптивно* подбирать критерии оценки под каждый конкретный запрос и ответ, что повышает качество и тонкость (granularity) итоговых reward-сигналов. Интуитивно, принципы играют роль **внутренней инструкции** для модели: они определяют, по каким аспектам оценивать ответ, а критика – это уже применение этих принципов к конкретному ответу. Благодаря этому GRM может выдавать более осмысленные и прозрачные оценки, особенно на сложных и разнородных задачах.

### Обучение GRM (Self-Principled Critique Tuning)

Давайте сначало вспомним, как устроен алгоритм GRPO 👇

![GRPO](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-07_%26_08/assets/Figure_19.jpeg)

Чтобы научить модель генерировать такие принципы и критику, авторы вводят специальный метод обучения – **Self-Principled Critique Tuning (SPCT)**. SPCT состоит из двух стадий: 
1. **Rejective Fine-Tuning**: обучение модели генерации структурированных оценок с принципами и критикой.  
2. **Rule-Based RL Fine-Tuning**: тонкая настройка с использованием обучения с подкреплением на основе правил.

![SPCT](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Figure_3.png)

>Рисунок 3: Иллюстрация SPCT, включая реджективную тонкую настройку, RL на основе правил и соответствующие масштабируемые поведения во время вывода. Масштабирование во время вывода достигается за счёт наивного голосования или голосования с управлением от мета-модели оценки (RM) с принципами, генерируемыми в масштабе, что приводит к более детализированным вознаграждениям результатов в расширенном пространстве значений.

На первой стадии (**rejective fine-tuning**) модель обучается генерировать оценки в требуемом формате (сначала принципы, затем критика с выводом балла) на разнообразных данных. Для этого авторы формируют обучающую выборку, объединяя различные типы задач: с одним правильным ответом, с парой ответов (предпочтительный vs. нет) и с несколькими ответами. Все эти варианты приводятся к *единому формату pointwise-GRM*: модель принимает запрос и один или несколько ответов и должна выдать текст с принципами и оценками для каждого ответа.

Чтобы улучшить качество данных, применяется стратегия *отбраковки*: сгенерированные моделью траектории (принципы + критика + оценки) сравниваются с известной “истиной” (например, с человеческими оценками или отметкой лучшего ответа). Если модель ошиблась (предсказала не тот лучший ответ или неверный балл) – такой пример отклоняется; также отклоняются слишком тривиальные случаи, где модель *всегда* угадывает правильно (чтобы не перенатренироваться на легких примерах). Формально критерий правильности предсказанных reward-оценок можно выразить так: предсказанные моделью pointwise-оценки $S_i^*$ считаются корректными, если (а) для задачи с несколькими ответами модель присвоила наивысший балл именно тому ответу, который по ground truth является наилучшим (т.е. $S_{i_{\text{best}}}^* > S_j^*$ для всех прочих $j$), и (б) для задачи с одним ответом предсказанный балл равен истинному рейтингу. Все некорректные или слишком простые примеры отфильтровываются, а оставшиеся используются для дообучения модели, благодаря чему GRM учится базовым навыкам: выдавать осмысленные принципы и оценивать ответы в правильном формате.

Вторая стадия – **Rule-Based RL** – призвана дальше улучшить точность и *научить модель правильно расставлять относительные оценки*, особенно в сложных случаях. Авторы применяют модификацию алгоритма RL (они ссылаются на GRPO, вариант Proximal Policy Optimization) с простым сигналом вознаграждения: модель получает положительный ревард (+1) за правильно упорядоченные оценки и отрицательный (–1) за ошибочные. Проще говоря, если GRM сгенерировала критику и баллы, где лучший ответ идентифицирован верно (совпадает с человеческим предпочтением), то эта генерация вознаграждается, иначе – штрафуется. Дополнительно вводится сильная штрафующая функция за отклонение от исходного распределения (KL-penalty), чтобы модель не уходила далеко от предтренированных знаний и сохранила адекватный формат вывода.

В отличие от некоторых предыдущих работ, здесь не применяются вручную написанные шаблоны или сложные оценочные функции – только правило правильного ранжирования и формат ответа. Таким образом, **SPCT-файнтюнинг** формирует у модели *само-принципиальную критику*: GRM сама генерирует принципы, сама же их применяет и посредством RL-корректировок обучается делать это максимально точно.

## 2.2. Техника *Inference-Time Scaling* (ITS)

Ключевая особенность подхода – это **масштабирование на этапе вывода** (*Inference-Time Scaling*, ITS), позволяющее повышать качество оценок GRM без изменения её параметров, просто задействуя больше вычислений в момент инференса. Идея состоит в применении *параллельного семплирования* и последующего *агрегирования результатов* для получения более надежного reward-сигнала. Поскольку наша RM является генеративной, каждый прогон модели может немного отличаться – особенно если добавить случайность (например, неполный детерминизм при генерации принципов и критики). За счет этого можно получить несколько вариантов оценки одного и того же ответа и затем объединить их, сглаживая ошибки отдельного прогона.

### Naive voting (голосование)

Базовый способ агрегировать $k$ сэмплов от GRM – это усреднить или суммировать полученные оценки. Предположим, для некоторого ответа $i$ модель при каждом запуске $j = 1..k$ выдает числовую оценку $S_{i,j}$ (например, балл от 1 до 10). Тогда окончательный *совокупный* рейтинг ответа $S_i^*$ может быть получен суммированием всех частичных оценок:

$$S_i^{*} = \sum_{j=1}^{k} S_{i,j}.\ \tag{6}$$

Авторы определяют процедуру голосования именно таким образом – как суммирование reward-оценок из всех семплов (эквивалентно усреднению, с точностью до множителя $1/k$). Каждый запуск модели $j$ включает генерацию собственного набора принципов $\{p_{i,j}\}$ и на их основе – критики с оценками $S_{i,j}$ для всех рассматриваемых ответов.

В итоге голосование аккумулирует информацию от множества *разных принципов и перспектив оценки*, что приводит к более точному и тонкому итоговому решению. Поскольку каждая отдельная оценка $S_{i,j}$ обычно лежит в некотором ограниченном диапазоне (например, 1–10), суммирование нескольких независимых оценок фактически **расширяет пространство возможных значений reward** и позволяет различать ответы более тонко.

Например, если в одном прогоне оба ответа получили максимальный балл 10, модель не различит их; но при 10–20 прогонов могут накопиться различия (скажем, один ответ чаще получает 10, а другой – иногда 9), что даст суммарно 100 vs 95 – заметное различие. Авторы подчёркивают, что увеличение количества сэмплов $k$ пропорционально расширяет шкалу оценок и увеличивает разнообразие сгенерированных принципов, благодаря чему **качество и детализация финальных reward-сигналов улучшаются**.

Интуитивно, можно считать, что *каждый принцип отражает отдельную «точку зрения» при оценке*, поэтому большее число случайных принципов лучше покрывает все аспекты качества ответа. Важная деталь реализации: чтобы избежать систематических смещений, ответы перед каждой выборкой перемешиваются случайно (shuffling), так модель не привязывается к позиции ответа в списке.

![Naive voting (голосование)](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Figure_3.1.png)

#### Пошаговый пример работы Naive Voting

В процедуре naive voting модель-оценщик (GRM) многократно ― $k$ раз ― «пере-оценивает» один и тот же набор *вопрос ↔ ответ(ы)*. Каждый прогон $j$ выполняется со стохастическим семплированием (обычно $T\!=\!0{,}5$), поэтому модель заново генерирует:

1. **Набор принципов** $\{p_{\,j}\}$ — самостоятельные критерии, по которым следует судить ответы;
2. **Критику + баллы** $S_{i,j}$ для каждого ответа $y_i$.

В формуле голосования это видно по индексам $j$ и $\{p_{i,j}^{\,m_j}\}$ — каждый семпл имеет свои принципы и, как следствие, собственные оценки, которые затем суммируются:

$$
S_i^{*} = \sum_{j=1}^{k} S_{i,j} \quad \text{при} \quad
\bigl\{S_{i,j}\bigr\} = f_{\text{point}}(C_j,\{y_i\}) \sim r_\theta\!\bigl(x,\{y_i\},\{p_{i,j}\}\bigr),
$$

> **Запрос:** «Объясните, что такое квантовая суперпозиция простыми словами».
> **Ответ 1:** «Суперпозиция — это …» (доступное популярное объяснение).
> **Ответ 2:** «Суперпозиция описывается линейным сочетанием векторов …» (формально-математическое описание).

**Многократная генерация оценок (k = 4)**

| Семпл j | Балл $S_{1,j}$ для Ответ 1 | Балл $S_{2,j}$ для Ответ 2 |
| ------- | -------------------------- | -------------------------- |
| 1       | 8                          | 6                          |
| 2       | 9                          | 7                          |
| 3       | 7                          | 7                          |
| 4       | 8                          | 6                          |

*(цифры условные, но отражают дискретный диапазон 1–10, который описан в статье)*

**Агрегация голосов**

$$
S_{1}^{*}=8+9+7+8=32,\qquad
S_{2}^{*}=6+7+7+6=26.
$$

**Итог**

Так как $S_{1}^{*}>S_{2}^{*}$, RM признаёт **Ответ 1** лучшим.
Если увеличить k, диапазон $[0,10k]$ расширится, и разница между ответами станет ещё более заметной (например, при k = 16 счёт мог быть 128 vs 104).

**Ключевые особенности подхода**

1. **Простота** — никакой взвешенной схемы: каждая частичная оценка равна одному голосу.
2. **Линейная масштабируемость** — производительность растёт пропорционально k до ограничений compute.
3. **Борьба с шумом** — суммирование сглаживает случайные колебания отдельных семплов.
4. **Расширение шкалы** — суммирование «уплотняет» дискретную шкалу, улучшая разрешение между близкими ответами.

Таким образом, **naive voting** в контексте GRM — это элементарное суммирование дискретных reward-оценок, позволяющее эффективно использовать дополнительный вычислительный бюджет инференса для более точной и устойчивой ранжирующей оценки ответов.

### Meta Reward Model (meta RM) – улучшенное голосование

![Meta Reward Model](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Figure_3.2.png)

Простое усреднение/голосование по множеству сэмплов уже даёт выигрыш, но не все сгенерированные критики одинаково качественны. Некоторые прогоны могут быть неудачными – например, модель выбрала неподходящие принципы или сбилась из-за случайности, дав неправильный вывод.

Чтобы учесть это, авторы вводят дополнительную модель – так называемую *meta reward model*. Meta RM – это отдельная небольшая **скалярная** модель вознаграждения, обученная оценивать правильность или качество сгенерированной GRM-критики. Проще говоря, meta RM принимает на вход текст: сгенерированные моделью принципы и критику, – и выдаёт оценку того, насколько эта критика соответствует *правильному* разбору ответа.

Для обучения meta RM используется бинарная кросс-энтропия: на множестве случаев известна “верность” данной критики (1 – если GRM правильно определила лучший ответ, 0 – если ошиблась), и meta RM учится классифицировать эти случаи. Источником данных служат как траектории из RL-тюнинга (где известна правильность каждого прогона GRM), так и дополнительные сгенерированные примеры.

При инференсе meta RM применяется так: для каждого из $k$ сэмплов GRM meta-модель предсказывает вспомогательный “meta-reward” $q_j$, отражающий доверие к данной траектории оценки. Далее финальное решение получается не простым средним всех $S_{i,j}$, а **взвешенным голосованием** – например, усредняются только топ-$k_{\text{meta}}$ сэмплов с наивысшим $q_j$. В экспериментах авторы брали $k_{\text{meta}} = k/2$, т.е. отбрасывали половину наиболее сомнительных выборок.

Такой подход отфильтровывает шум и заметно улучшает итоговую точность. Они сообщают, что *meta-guided voting* даёт более высокий результат, чем наивное голосование, особенно при большом $k$, поскольку эффективно отсеивает случайные ошибки GRM.

<details> 
    <summary><em><strong>Формализуем концепцию Meta Reward Model (meta RM)</strong></em></summary>

---

Meta Reward Model — это скалярная модель, предназначенная для оценки качества критик, сгенерированных GRM (Generative Reviewer Model). Основная задача Meta RM — определить, насколько корректна данная критика, и использовать эту информацию при взвешенном голосовании для итогового выбора ответа.

Модель обучается по бинарной кросс-энтропии: целевая метка равна 1, если критика признана корректной (т.е. GRM правильно определил лучший ответ), и 0 — если ошибочной. Во время инференса модель предсказывает значение `meta-reward` $q_j \in [0, 1]$ для каждой критики, отражающее степень уверенности в её корректности.

### Формализация задачи

Обозначим обучающую выборку как:

$$
D = \{(x_i, c_i, y_i)\}_{i=1}^N
$$

где:

* $x_i$ — входной запрос или задача,
* $c_i$ — критика, сгенерированная GRM, на основе принципов,
* $y_i \in \{0, 1\}$ — бинарная метка корректности критики.

Структура критики $c_i$ может быть представлена в виде тройки:

$$
c_i = (P_i, A_i, R_i)
$$

где:

* $P_i$ — набор принципов оценки, использованных GRM,
* $A_i$ — текст критического анализа,
* $R_i$ — ранжирование или оценка, выданная GRM.

Целевая переменная определяется следующим образом:

$$
y_i =
\begin{cases}
1, & \text{если GRM корректно определил лучший ответ} \\
0, & \text{в противном случае}
\end{cases}
$$

### Модель и функция потерь

Модель Meta RM представляет собой дифференцируемую функцию:

$$
f_\theta: (P, A, R) \rightarrow [0, 1]
$$

где $\theta$ — параметры модели, а выход $f_\theta(c_i)$ интерпретируется как вероятность корректности критики.

Функция потерь (бинарная кросс-энтропия) формулируется как:

$$
\mathcal{L}_{BCE}(\theta) = -\frac{1}{N} \sum_{i=1}^N \left[y_i \log(f_\theta(c_i)) + (1 - y_i)\log(1 - f_\theta(c_i))\right]
$$

Эта функция штрафует модель:

* если она даёт низкую вероятность на действительно корректные критики ($y_i = 1$),
* или высокую вероятность на некорректные ($y_i = 0$).

### Роль в инференсе и взвешенном голосовании

Во время инференса Meta RM используется следующим образом:

1. **Генерация критик:** для каждого входа $x$ генерируется набор $\{c_j\}_{j=1}^k$ возможных критик.
2. **Оценка критик:** каждая критика пропускается через модель:

   $$
   q_j = f_\theta(c_j)
   $$
3. **Отбор:** выбираются топ-$k_{\text{meta}}$ критик по убыванию $q_j$:

   $$
   \mathcal{T} = \text{top}_{k_{\text{meta}}}(\{(c_j, q_j)\})
   $$

   Обычно $k_{\text{meta}} = k / 2$.
4. **Взвешенное голосование:** итоговый скор агрегируется как:

   $$
   S_{\text{final}} = \frac{1}{|\mathcal{T}|} \sum_{j \in \mathcal{T}} S_{i,j} \cdot q_j
   $$

   где $S_{i,j}$ — оценка соответствующего кандидата от GRM.

### Эффект фильтрации

Meta RM выполняет важную функцию фильтрации в условиях многократного сэмплирования:

* Высокие значения $q_j$ получают критики, которые статистически чаще ведут к корректным заключениям.
* Низкие значения подавляют ошибки GRM.
* Взвешенное усреднение на этапе голосования минимизирует влияние шума и случайных сбоев.

</details> 