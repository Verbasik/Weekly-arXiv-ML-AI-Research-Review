# Inference-Time Scaling for Generalist Reward Modeling

## Содержание

1. [Введение](#введение)  
2. [Понимание парадигм моделирования вознаграждения](#понимание-парадигм-моделирования-вознаграждения)  
3. [Самостоятельная точная настройка критики](#самостоятельная-точная-настройка-критики)  
4. [Стратегии масштабирования во время инференса](#стратегии-масштабирования-во-время-инференса)  
5. [Экспериментальные результаты](#экспериментальные-результаты)  
6. [Конвейер обучения модели](#конвейер-обучения-модели)  
7. [Преимущества по сравнению с существующими подходами](#преимущества-по-сравнению-с-существующими-подходами)  
8. [Ограничения и режимы отказа](#ограничения-и-режимы-отказа)  
9. [Выводы и будущие направления](#выводы-и-будущие-направления)

<details> 
    <summary><em><strong>Too long; Didn't read</strong></em></summary>

## **Inference-Time Scaling for Generalist Reward Modeling (GRMs)**

### **Введение**

Этот документ представляет собой обзор исследования, посвящённого улучшению моделей вознаграждения (Reward Models, RM) для крупных языковых моделей (Large Language Models, LLMs), с особым акцентом на масштабирование производительности моделей вознаграждения в процессе инференса (inference-time scaling). Цель исследования — разработать эффективные методы обучения, позволяющие GRM (Generative Reward Models) адаптироваться к увеличению вычислительных ресурсов и демонстрировать высокую точность в широком спектре задач.

### **Основные проблемы и вызовы**

1. **Получение точных сигналов вознаграждения**  
   Reinforcement Learning (RL) играет важную роль в пост-обучении LLMs. Однако получение точных и значимых сигналов вознаграждения в различных предметных областях остаётся одной из ключевых проблем. Особенно это сложно в ситуациях, где нет явных правил или однозначно верифицируемых ответов.

2. **Гибкость модели вознаграждения**  
   Обобщённые модели вознаграждения (Generalist RMs) должны:
   - Поддерживать разнообразные типы входных данных.
   - Генерировать точные сигналы вознаграждения в различных доменах.

3. **Масштабируемость во время инференса**  
   Эффективная модель вознаграждения должна уметь:
   - Улучшать качество вознаграждений при увеличении вычислительных ресурсов.
   - Обучаться поведению, которое позволяет эффективно использовать дополнительные мощности, обеспечивая баланс между производительностью и затратами.

### **Решение: Generative Pointwise Reward Modeling (GRM)**

#### **Что такое GRM?**
Pointwise GRM — это подход, позволяющий унифицировать оценку как одного, так и нескольких ответов в рамках единого текстового представления. Это делает GRM гибкой и универсальной моделью вознаграждения.

#### **Формулировка GRM**
Для множества ответов {yi}n i=1 модель GRM генерирует соответствующие сигналы вознаграждения {Si}n i=1 следующим образом:

$$
\{S_i\}_{i=1}^n = f_{\text{point}}(R, \{y_i\}_{i=1}^n) = f_{\text{extract}}(C),
$$

где:

$$
R = C \sim r_{\theta}(x, \{y_i\}_{i=1}^n), \quad S_i \in \mathbb{R}.
$$

#### **Преимущества GRM**
- Единый формат для оценки одного, пары или множества ответов.
- Возможность масштабирования во время инференса через семплирование.
- Высокая гибкость и выразительность по сравнению со скалярными и полускалярными моделями.

### **Self-Principled Critique Tuning (SPCT)**

#### **Описание метода**
SPCT — новый метод обучения, направленный на улучшение масштабируемости GRM во время инференса. Метод использует правило-ориентированный онлайн RL, чтобы научить GRM адаптивно формулировать принципы и критики на основе входных данных.

#### **Как работает SPCT?**
- Использует rejective fine-tuning для начального этапа обучения.
- Затем применяет RL, основанный на правилах, для оптимизации генерации принципов и критики.
- Принципы направляют модель к более точной и детализированной оценке вознаграждений.

#### **Цель SPCT**
Создать GRM, способную динамически адаптироваться к запросам и отвечать более качественно и точно при увеличении доступных вычислительных ресурсов.

### **Масштабирование во время инференса**

#### **Техника семплирования и голосования**
- GRM может генерировать несколько наборов вознаграждений для одного и того же запроса.
- Окончательное вознаграждение определяется путём голосования среди результатов.
- Такой подход расширяет пространство возможных оценок и позволяет достигать более тонкой градации.

#### **Пример: DeepSeek-GRM**
- DeepSeek-GRM-27B обучалась с использованием SPCT на основе Gemma-2-27B.
- С увеличением объёма семплирования модель демонстрирует рост точности и детализации вознаграждений за счёт большего разнообразия принципов.

### **Meta RM: поддержка масштабирования**

#### **Зачем нужен Meta RM?**
Для улучшения качества голосования используется дополнительная модель — meta RM, которая оценивает корректность сгенерированных принципов и критик.

#### **Функции Meta RM**
- Фильтрация низкокачественных результатов.
- Повышение общей точности и надежности вознаграждений.
- Дополнительный уровень контроля в сложных задачах.

### **Сравнение с другими подходами**

| Тип RM              | Формат вознаграждения     | Гибкость | Масштабируемость | Выразительность |
|---------------------|----------------------------|-----------|--------------------|------------------|
| **Scalar RM**       | Число                      | Низкая    | Низкая             | Низкая           |
| **Semi-scalar RM**  | Текст + число              | Средняя   | Средняя            | Средняя          |
| **Generative RM**   | Только текст               | Высокая   | Высокая            | Высокая          |

### **Экспериментальные результаты**

- **DeepSeek-GRM-27B** показала значительное улучшение по сравнению с существующими методами:
  - Превосходит скалярные и полускалярные RM без выраженных смещений.
  - Конкурентоспособна с сильными публичными моделями (например, Nemotron-4-340B-Reward, GPT-4o).
- При масштабировании во время инференса DeepSeek-GRM-27B дополнительно улучшает свои показатели.
- Использование meta RM усиливает эффект масштабирования.
- Абляционные эксперименты подтверждают важность:
  - Генерации принципов.
  - Rejective sampling.
  - Обучающих данных общего назначения.

### **Будущие направления исследований**

1. Интеграция GRM в конвейеры онлайн RL.
2. Исследование совместного масштабирования GRM и моделей политики.
3. Использование GRM в качестве надёжных оффлайн-оценщиков для фундаментальных моделей.

### **Ключевые выводы**

- RL играет важную роль в пост-обучении LLM, но требует точных и гибких сигналов вознаграждения.
- Pointwise GRM предлагает универсальное и масштабируемое решение для моделирования вознаграждений.
- SPCT значительно улучшает качество и масштабируемость GRM благодаря обучению с генерацией принципов и критик.
- Параллельное семплирование и голосование позволяют эффективно использовать вычислительные ресурсы.
- Meta RM помогает повысить точность и надёжность оценок.
- DeepSeek-GRM-27B, обученная с помощью SPCT, превосходит другие подходы по производительности и устойчивости к смещениям.
- В отличие от скалярных и полускалярных RM, GRM обеспечивают более равномерное качество в различных задачах.

</details>    

## **1. Введение**

Большие языковые модели (LLM) произвели революцию в приложениях искусственного интеллекта, значительно улучшив свои способности благодаря обучению с подкреплением на основе обратной связи от человека (RLHF).  

Одним из ключевых компонентов RLHF является **моделирование вознаграждения** — процесс создания моделей, способных оценивать качество ответов, генерируемых LLM.  

Несмотря на значительные достижения в этой области, большинство существующих подходов сталкиваются с проблемой **масштабируемости во время инференса**, что ограничивает их способность повышать точность оценок при увеличении доступных вычислительных ресурсов.

![Рисунок 1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_01.jpeg)

В этой статье представлен новый подход к моделированию вознаграждения, который специально решает проблему масштабируемости во время инференса. Авторы представляют Self-Principled Critique Tuning (SPCT), метод обучения, который позволяет Generative Reward Models (GRM) генерировать обоснованные оценки, которые можно масштабировать во время инференса, используя дополнительные вычисления. Как показано на рисунке выше, их подход (DeepSeek-GRM-27B) демонстрирует значительное улучшение производительности при сборе большего количества выборок вознаграждения, превосходя существующие модели.

## **2. Понимание парадигм моделирования вознаграждения**

Подходы к моделированию вознаграждения можно разделить на три основные парадигмы в зависимости от способа генерации оценок:

### 1. Модели скалярного вознаграждения
Эти модели напрямую выдают **числовую оценку** (скалярное значение), характеризующую качество сгенерированного ответа.

### 2. Полускалярные модели вознаграждения
Данные модели генерируют как **текстовую критику**, так и **числовую оценку**, сочетая интерпретируемость и количественную точность.

### 3. Генеративные модели вознаграждения (GRM)
Эти модели фокусируются исключительно на генерации **текстовой критики**, предоставляя подробную обратную связь без явного числового представления вознаграждения.

![Рисунок 2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_02.jpeg)

Эти парадигмы можно комбинировать с двумя шаблонами оценки:

- **Точечная оценка:** оценка отдельных ответов независимо;

- **Парная оценка:** непосредственное сравнение двух ответов.

Авторы подчеркивают, что точечные GRM предлагают уникальные преимущества - они обеспечивают интерпретируемую обратную связь, сохраняют гибкость в обработке различных типов входных данных и, что немаловажно, обеспечивают масштабирование во время инференса за счет выборки нескольких вознаграждений. Эта масштабируемость позволяет моделям генерировать более точные вознаграждения при наличии дополнительных вычислений, не требуя переобучения или использования более крупных моделей.

## **3. Self-Principled Critique Tuning**

Основным новшеством этой статьи является "самостоятельная точная настройка критики (SPCT)", которая обучает GRM генерировать оценки, соответствующие структурированному формату принципов и критики. SPCT состоит из двух ключевых этапов:

1. **Отрицательная точная настройка (RFT):** этот начальный этап знакомит модель со структурированным форматом оценки. Модель генерирует принципы и критику, которые затем фильтруются для создания набора данных для обучения, отклоняя некачественные результаты.

2. **Онлайн-обучение с подкреплением на основе правил (RL):** этот этап дополнительно совершенствует способность модели генерировать качественные оценки, применяя сигналы вознаграждения на основе предопределенных правил.

![Рисунок 3](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_03.jpeg)

Ключевое понимание SPCT заключается в том, что генерация принципов рассматривается как неотъемлемая часть процесса генерации вознаграждения, а не как этап предварительной обработки. Это позволяет модели адаптивно генерировать принципы на основе конкретного запроса и оцениваемых ответов, что делает модель вознаграждения более гибкой и универсальной.

Во время инференса GRM можно масштабировать:

1. Сэмплированием нескольких независимых оценок;
2. Агрегированием этих оценок (посредством голосования или более сложных методов);
3. Использованием мета-модели вознаграждения для дальнейшего улучшения качества агрегирования.

## **4. Стратегии масштабирования во время инференса**

В статье исследуются несколько стратегий масштабирования моделей вознаграждения во время инференса:

1. **Параллельное сэмплирование:** генерация нескольких независимых оценок для одной и той же пары запрос-ответ.
2. **Голосование:** простой метод агрегирования, который учитывает большинство голосов по нескольким оценкам.
3. **Мета-модель вознаграждения (Meta RM):** более сложный подход, который использует классификатор для определения того, какие оценки являются более надежными.

![Рисунок 4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_04.jpeg)

Авторы обнаружили, что масштабирование во время инференса может фактически превзойти масштабирование во время обучения (увеличение размера модели). Как показано на рисунке выше, модель с 27B параметрами с масштабированием во время инференса может достичь лучшей производительности, чем гораздо большая модель, оптимизированная с помощью обучения с подкреплением.

Этот вывод ставит под сомнение общепринятое мнение о том, что увеличение размера модели является наиболее эффективным способом повышения производительности. Вместо этого, лучшее использование вычислительных ресурсов во время инференса может дать превосходные результаты при сохранении эффективности модели.

## **5. Экспериментальные результаты**

Авторы провели обширные эксперименты для оценки своего подхода:

1. **Производительность на бенчмарках:** модели DeepSeek-GRM были протестированы на нескольких бенчмарках, включая AlpacaEval, MT-Bench и проприетарный бенчмарк моделирования вознаграждения (RMB). Результаты показали, что DeepSeek-GRM с масштабированием во время инференса превосходит существующие современные модели вознаграждения.

![Рисунок 5](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_05.jpeg)

2. **Эффективность масштабирования:** модель с 27B параметрами показала стабильное улучшение производительности с увеличением количества образцов (от 1 до 32), особенно при использовании Meta RM для агрегирования.

3. **Возможность обобщения:** модели продемонстрировали высокую производительность в различных задачах, включая оценку чатов, оценку безопасности и задачи рассуждения.

4. **Эффективность обучения:** обучение SPCT требовало значительно меньше вычислений, чем полные подходы обучения с подкреплением, при сохранении сопоставимой производительности.

![Рисунок 6](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_06.jpeg)

Эксперименты подтверждают, что масштабирование во время инференса предлагает практичный и эффективный подход к улучшению качества модели вознаграждения без вычислительной нагрузки, связанной с обучением более крупных моделей или использованием дорогостоящего обучения с подкреплением.

## **6. Конвейер обучения модели**

Авторы подробно описывают комплексный конвейер обучения для разработки своих универсальных моделей вознаграждения:

1. **Base Model Preparation**: начинается с предварительно обученной языковой модели (DeepSeek-V3).
2. **Rejective Fine-Tuning**: обучение модели генерации структурированных оценок с принципами и критикой.
3. **Rule-Based RL**: дальнейшее улучшение модели посредством онлайн-обучения с вознаграждениями на основе правил.
4. **Meta RM Training**: разработка классификатора для улучшения агрегирования нескольких оценок.

![Рисунок 7](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_07.jpeg)

Этот конвейер позволяет разрабатывать модели вознаграждения, которые могут эффективно использовать дополнительные вычисления во время вывода. Благодаря интеграции как контролируемого обучения (RFT), так и обучения с подкреплением, модели учатся генерировать высококачественные оценки, которые можно масштабировать с помощью выборки.

## **7. Преимущества перед существующими подходами**

Подход DeepSeek-GRM предлагает несколько ключевых преимуществ по сравнению с традиционными методами моделирования вознаграждения:

1. **Интерпретируемость:** в отличие от скалярных моделей типа "черный ящик", GRM предоставляют подробные объяснения своих оценок через принципы и критические замечания.

2. **Гибкость:** подход точечной оценки может обрабатывать различные форматы входных данных (одиночные ответы, пары или множественные ответы).

3. **Масштабируемость:** модель может использовать дополнительные вычислительные ресурсы во время вывода для повышения производительности без переобучения.

4. **Общность:** адаптивно генерируя принципы, модель может оценивать ответы в различных областях и задачах.

5. **Эффективность:** масштабирование во время вывода предлагает более экономичный подход к повышению производительности по сравнению с обучением более крупных моделей.

Экспериментальные результаты показывают, что эти преимущества приводят к практической пользе: модели DeepSeek-GRM превосходят существующие модели вознаграждения при использовании масштабирования во время вывода.

## **8. Ограничения и режимы отказа**

Несмотря на свои сильные стороны, авторы выявили несколько ограничений и потенциальных режимов отказа своего подхода:

1. **Некорректные критические замечания:** GRM может иногда генерировать некорректные или непоследовательные критические замечания, особенно для сложных задач рассуждения.

![Рисунок 8](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_08.jpeg)

2. **Несбалансированное взвешивание:** модель может присваивать несоответствующие веса различным принципам, что приводит к искаженным оценкам.

3. **Неправильные принципы:** для некоторых запросов модель может генерировать принципы, которые не являются релевантными или уместными.

4. **Противоречия с истиной:** в проверяемых задачах оценки модели могут противоречить фактическим знаниям.

Эти ограничения указывают на направления для будущего улучшения, такие как повышение способности модели проверять фактическую информацию и улучшение согласованности ее оценок.

## **9. Заключение и будущие направления**

В статье представлен значительный прогресс в моделировании вознаграждения для больших языковых моделей путем внедрения настройки с критикой на основе самопринципов и демонстрации эффективности масштабирования во время вывода. Этот подход обеспечивает более эффективное использование вычислительных ресурсов и предоставляет способ улучшить модели вознаграждения без необходимости использования все более крупных моделей или дорогостоящего обучения с подкреплением.

Будущие направления исследований, определенные авторами, включают:

1. **Интеграция с онлайн-RL:** использование GRM непосредственно в конвейерах обучения с подкреплением.
2. **Совместное масштабирование с моделями политик:** изучение того, как масштабирование во время вывода как моделей политики, так и моделей вознаграждения может привести к синергетическим улучшениям.
3. **Устойчивость и смягчение предвзятости:** устранение выявленных режимов отказа для создания более надежных моделей вознаграждения.
4. **Оффлайн-оценка:** использование GRM в качестве надежных оценщиков для базовых моделей.

Эта работа представляет собой сдвиг парадигмы в мышлении о моделировании вознаграждения, подчеркивая, что то, как вычислительные ресурсы используются во время вывода, может быть так же важно, как размер модели или методы обучения. Это понимание имеет широкие последствия для разработки более эффективных и результативных больших языковых моделей в будущем.