# Inference-Time Scaling for Generalist Reward Modeling

## Table of Contents

1. [Introduction](#introduction)  
2. [Understanding Reward Modeling Paradigms](#understanding-reward-modeling-paradigms)  
3. [Self-Principled Critique Tuning](#self-principled-critique-tuning)  
4. [Inference-Time Scaling Strategies](#inference-time-scaling-strategies)  
5. [Experimental Results](#experimental-results)  
6. [Model Training Pipeline](#model-training-pipeline)  
7. [Advantages Over Existing Approaches](#advantages-over-existing-approaches)  
8. [Limitations and Failure Modes](#limitations-and-failure-modes)  
9. [Conclusions and Future Directions](#conclusions-and-future-directions)

## **1. Introduction**

Large language models (LLMs) have revolutionized artificial intelligence applications, significantly enhancing their capabilities through Reinforcement Learning from Human Feedback (RLHF).  

A key component of RLHF is **reward modeling**—the process of creating models capable of evaluating the quality of responses generated by LLMs.  

Despite significant advances in this field, most existing approaches face a problem of **inference-time scalability**, limiting their ability to improve evaluation accuracy when additional computational resources are available.

![Figure 1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_01.jpeg)

This paper presents a novel approach to reward modeling specifically designed to address inference-time scalability. The authors introduce Self-Principled Critique Tuning (SPCT), a training method that enables Generative Reward Models (GRMs) to generate reasoned evaluations that can be scaled during inference using additional computation. As shown in the figure above, their approach (DeepSeek-GRM-27B) demonstrates significant performance gains when collecting more reward samples, surpassing existing models.

## **2. Understanding Reward Modeling Paradigms**

Approaches to reward modeling can be divided into three primary paradigms based on how evaluations are generated:

### 1. Scalar Reward Models
These models directly output a **numerical score** (scalar value) characterizing the quality of a generated response.

### 2. Semi-Scalar Reward Models
These models generate both a **textual critique** and a **numerical score**, combining interpretability with quantitative precision.

### 3. Generative Reward Models (GRM)
These models focus exclusively on generating **textual critique**, providing detailed feedback without an explicit numerical reward representation.

![Figure 2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_02.jpeg)

These paradigms can be combined with two evaluation templates:

- **Pointwise evaluation:** evaluating individual responses independently;

- **Pairwise evaluation:** directly comparing two responses.

The authors emphasize that pointwise GRMs offer unique advantages—they provide interpretable feedback, maintain flexibility in handling diverse input formats, and, crucially, enable inference-time scaling through sampling multiple rewards. This scalability allows models to generate more accurate rewards when additional computation is available, without requiring retraining or larger models.

## **3. Self-Principled Critique Tuning**

The central innovation of this paper is **Self-Principled Critique Tuning (SPCT)**, which trains GRMs to generate evaluations conforming to a structured format of principles and critique. SPCT consists of two key stages:

1. **Rejection Fine-Tuning (RFT):** This initial stage introduces the model to the structured evaluation format. The model generates principles and critique, which are then filtered to create a training dataset by rejecting low-quality outputs.

2. **Rule-Based Online Reinforcement Learning (RL):** This stage further refines the model’s ability to generate high-quality evaluations by applying reward signals based on predefined rules.

![Figure 3](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_03.jpeg)

The key insight of SPCT is that principle generation is treated as an integral part of the reward generation process, not a preprocessing step. This enables the model to adaptively generate principles based on the specific query and evaluated responses, making the reward model more flexible and generalizable.

During inference, GRMs can be scaled by:

1. Sampling multiple independent evaluations;
2. Aggregating these evaluations (via voting or more sophisticated methods);
3. Utilizing a Meta Reward Model (Meta RM) to further improve aggregation quality.

## **4. Inference-Time Scaling Strategies**

The paper investigates several strategies for scaling reward models during inference:

1. **Parallel Sampling:** Generating multiple independent evaluations for the same query-response pair.
2. **Voting:** A simple aggregation method that considers the majority vote across multiple evaluations.
3. **Meta Reward Model (Meta RM):** A more sophisticated approach that uses a classifier to determine which evaluations are more reliable.

![Figure 4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_04.jpeg)

The authors found that inference-time scaling can actually outperform training-time scaling (increasing model size). As shown in the figure above, a 27B-parameter model with inference-time scaling achieves higher performance than a much larger model optimized through reinforcement learning.

This finding challenges the prevailing assumption that increasing model size is the most effective way to improve performance. Instead, better utilization of computational resources during inference can yield superior results while preserving model efficiency.

## **5. Experimental Results**

The authors conducted extensive experiments to evaluate their approach:

1. **Performance on Benchmarks:** DeepSeek-GRM models were tested on multiple benchmarks, including AlpacaEval, MT-Bench, and a proprietary reward modeling benchmark (RMB). Results show that DeepSeek-GRM with inference-time scaling outperforms existing state-of-the-art reward models.

![Figure 5](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_05.jpeg)

2. **Scaling Efficiency:** The 27B-parameter model demonstrated consistent performance gains with increasing sample count (from 1 to 32), especially when using Meta RM for aggregation.

3. **Generalization Capability:** Models showed high performance across diverse tasks, including chat evaluation, safety assessment, and reasoning tasks.

4. **Training Efficiency:** SPCT training required significantly less computation than full reinforcement learning approaches, while maintaining comparable performance.

![Figure 6](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_06.jpeg)

Experiments confirm that inference-time scaling offers a practical and efficient approach to improving reward model quality without the computational burden of training larger models or expensive reinforcement learning.

## **6. Model Training Pipeline**

The authors detail a comprehensive training pipeline for developing their generalist reward models:

1. **Base Model Preparation**: Begins with a pretrained language model (DeepSeek-V3).
2. **Rejection Fine-Tuning**: Training the model to generate structured evaluations with principles and critique.
3. **Rule-Based RL**: Further refining the model through online reinforcement learning with rule-based rewards.
4. **Meta RM Training**: Developing a classifier to improve aggregation of multiple evaluations.

![Figure 7](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_07.jpeg)

This pipeline enables the development of reward models that can effectively leverage additional computation during inference. By integrating both supervised learning (RFT) and reinforcement learning, models learn to generate high-quality evaluations that can be scaled through sampling.

## **7. Advantages Over Existing Approaches**

The DeepSeek-GRM approach offers several key advantages over traditional reward modeling methods:

1. **Interpretability:** Unlike "black-box" scalar models, GRMs provide detailed explanations for their evaluations through principles and critique.

2. **Flexibility:** The pointwise evaluation approach can handle various input formats (single responses, pairs, or multiple responses).

3. **Scalability:** The model can utilize additional computational resources during inference to improve performance without retraining.

4. **Generality:** By adaptively generating principles, the model can evaluate responses across diverse domains and tasks.

5. **Efficiency:** Inference-time scaling provides a more cost-effective approach to improving performance compared to training larger models.

Experimental results demonstrate that these advantages translate into practical benefits: DeepSeek-GRM models outperform existing reward models when inference-time scaling is applied.

## **8. Limitations and Failure Modes**

Despite its strengths, the authors identified several limitations and potential failure modes of their approach:

1. **Incorrect Critiques:** GRMs may occasionally generate incorrect or inconsistent critiques, especially for complex reasoning tasks.

![Figure 8](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_08.jpeg)

2. **Imbalanced Weighting:** The model may assign inappropriate weights to different principles, leading to distorted evaluations.

3. **Irrelevant Principles:** For some queries, the model may generate principles that are irrelevant or inappropriate.

4. **Contradictions with Ground Truth:** In verifiable evaluation tasks, model assessments may contradict factual knowledge.

These limitations highlight directions for future improvement, such as enhancing the model’s ability to verify factual information and improving the consistency of its evaluations.

## **9. Conclusions and Future Directions**

This paper presents significant progress in reward modeling for large language models by introducing self-principled critique tuning and demonstrating the effectiveness of inference-time scaling. This approach enables more efficient use of computational resources and provides a pathway to improve reward models without relying on ever-larger models or expensive reinforcement learning.

Future research directions identified by the authors include:

1. **Integration with Online RL:** Using GRMs directly within reinforcement learning training pipelines.
2. **Coordinated Scaling with Policy Models:** Investigating how inference-time scaling of both policy and reward models can lead to synergistic improvements.
3. **Robustness and Bias Mitigation:** Addressing identified failure modes to create more reliable reward models.
4. **Offline Evaluation:** Utilizing GRMs as reliable evaluators for base models.

This work represents a paradigm shift in thinking about reward modeling, emphasizing that *how* computational resources are used during inference may be as important as model size or training methods. This understanding has broad implications for developing more efficient and effective large language models in the future.