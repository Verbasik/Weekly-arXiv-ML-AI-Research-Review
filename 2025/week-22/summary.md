# Inference-Time Scaling for Generalist Reward Modeling

## Содержание

1. [Введение](#введение)  
2. [Понимание парадигм моделирования вознаграждения](#понимание-парадигм-моделирования-вознаграждения)  
3. [Самостоятельная точная настройка критики](#самостоятельная-точная-настройка-критики)  
4. [Стратегии масштабирования во время инференса](#стратегии-масштабирования-во-время-инференса)  
5. [Экспериментальные результаты](#экспериментальные-результаты)  
6. [Конвейер обучения модели](#конвейер-обучения-модели)  
7. [Преимущества по сравнению с существующими подходами](#преимущества-по-сравнению-с-существующими-подходами)  
8. [Ограничения и режимы отказа](#ограничения-и-режимы-отказа)  
9. [Выводы и будущие направления](#выводы-и-будущие-направления)

## **1. Введение**

Большие языковые модели (LLM) произвели революцию в приложениях искусственного интеллекта, значительно улучшив свои способности благодаря обучению с подкреплением на основе обратной связи от человека (RLHF).  

Одним из ключевых компонентов RLHF является **моделирование вознаграждения** — процесс создания моделей, способных оценивать качество ответов, генерируемых LLM.  

Несмотря на значительные достижения в этой области, большинство существующих подходов сталкиваются с проблемой **масштабируемости во время инференса**, что ограничивает их способность повышать точность оценок при увеличении доступных вычислительных ресурсов.

![Рисунок 1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_01.jpeg)

В этой статье представлен новый подход к моделированию вознаграждения, который специально решает проблему масштабируемости во время инференса. Авторы представляют Self-Principled Critique Tuning (SPCT), метод обучения, который позволяет Generative Reward Models (GRM) генерировать обоснованные оценки, которые можно масштабировать во время инференса, используя дополнительные вычисления. Как показано на рисунке выше, их подход (DeepSeek-GRM-27B) демонстрирует значительное улучшение производительности при сборе большего количества выборок вознаграждения, превосходя существующие модели.

## **2. Понимание парадигм моделирования вознаграждения**

Подходы к моделированию вознаграждения можно разделить на три основные парадигмы в зависимости от способа генерации оценок:

### 1. Модели скалярного вознаграждения
Эти модели напрямую выдают **числовую оценку** (скалярное значение), характеризующую качество сгенерированного ответа.

### 2. Полускалярные модели вознаграждения
Данные модели генерируют как **текстовую критику**, так и **числовую оценку**, сочетая интерпретируемость и количественную точность.

### 3. Генеративные модели вознаграждения (GRM)
Эти модели фокусируются исключительно на генерации **текстовой критики**, предоставляя подробную обратную связь без явного числового представления вознаграждения.

![Рисунок 2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_02.jpeg)

Эти парадигмы можно комбинировать с двумя шаблонами оценки:

- **Точечная оценка:** оценка отдельных ответов независимо;

- **Парная оценка:** непосредственное сравнение двух ответов.

Авторы подчеркивают, что точечные GRM предлагают уникальные преимущества - они обеспечивают интерпретируемую обратную связь, сохраняют гибкость в обработке различных типов входных данных и, что немаловажно, обеспечивают масштабирование во время инференса за счет выборки нескольких вознаграждений. Эта масштабируемость позволяет моделям генерировать более точные вознаграждения при наличии дополнительных вычислений, не требуя переобучения или использования более крупных моделей.

## **3. Self-Principled Critique Tuning**

Основным новшеством этой статьи является "самостоятельная точная настройка критики (SPCT)", которая обучает GRM генерировать оценки, соответствующие структурированному формату принципов и критики. SPCT состоит из двух ключевых этапов:

1. **Отрицательная точная настройка (RFT):** этот начальный этап знакомит модель со структурированным форматом оценки. Модель генерирует принципы и критику, которые затем фильтруются для создания набора данных для обучения, отклоняя некачественные результаты.

2. **Онлайн-обучение с подкреплением на основе правил (RL):** этот этап дополнительно совершенствует способность модели генерировать качественные оценки, применяя сигналы вознаграждения на основе предопределенных правил.

![Рисунок 3](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_03.jpeg)

Ключевое понимание SPCT заключается в том, что генерация принципов рассматривается как неотъемлемая часть процесса генерации вознаграждения, а не как этап предварительной обработки. Это позволяет модели адаптивно генерировать принципы на основе конкретного запроса и оцениваемых ответов, что делает модель вознаграждения более гибкой и универсальной.

Во время инференса GRM можно масштабировать:

1. Сэмплированием нескольких независимых оценок;
2. Агрегированием этих оценок (посредством голосования или более сложных методов);
3. Использованием мета-модели вознаграждения для дальнейшего улучшения качества агрегирования.

## **4. Стратегии масштабирования во время инференса**

В статье исследуются несколько стратегий масштабирования моделей вознаграждения во время инференса:

1. **Параллельное сэмплирование:** генерация нескольких независимых оценок для одной и той же пары запрос-ответ.
2. **Голосование:** простой метод агрегирования, который учитывает большинство голосов по нескольким оценкам.
3. **Мета-модель вознаграждения (Meta RM):** более сложный подход, который использует классификатор для определения того, какие оценки являются более надежными.

![Рисунок 4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_04.jpeg)

Авторы обнаружили, что масштабирование во время инференса может фактически превзойти масштабирование во время обучения (увеличение размера модели). Как показано на рисунке выше, модель с 27B параметрами с масштабированием во время инференса может достичь лучшей производительности, чем гораздо большая модель, оптимизированная с помощью обучения с подкреплением.

Этот вывод ставит под сомнение общепринятое мнение о том, что увеличение размера модели является наиболее эффективным способом повышения производительности. Вместо этого, лучшее использование вычислительных ресурсов во время инференса может дать превосходные результаты при сохранении эффективности модели.

## **5. Экспериментальные результаты**

Авторы провели обширные эксперименты для оценки своего подхода:

1. **Производительность на бенчмарках:** модели DeepSeek-GRM были протестированы на нескольких бенчмарках, включая AlpacaEval, MT-Bench и проприетарный бенчмарк моделирования вознаграждения (RMB). Результаты показали, что DeepSeek-GRM с масштабированием во время инференса превосходит существующие современные модели вознаграждения.

![Рисунок 5](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_05.jpeg)

2. **Эффективность масштабирования:** модель с 27B параметрами показала стабильное улучшение производительности с увеличением количества образцов (от 1 до 32), особенно при использовании Meta RM для агрегирования.

3. **Возможность обобщения:** модели продемонстрировали высокую производительность в различных задачах, включая оценку чатов, оценку безопасности и задачи рассуждения.

4. **Эффективность обучения:** обучение SPCT требовало значительно меньше вычислений, чем полные подходы обучения с подкреплением, при сохранении сопоставимой производительности.

![Рисунок 6](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_06.jpeg)

Эксперименты подтверждают, что масштабирование во время инференса предлагает практичный и эффективный подход к улучшению качества модели вознаграждения без вычислительной нагрузки, связанной с обучением более крупных моделей или использованием дорогостоящего обучения с подкреплением.

## **6. Конвейер обучения модели**

Авторы подробно описывают комплексный конвейер обучения для разработки своих универсальных моделей вознаграждения:

1. **Base Model Preparation**: начинается с предварительно обученной языковой модели (DeepSeek-V3).
2. **Rejective Fine-Tuning**: обучение модели генерации структурированных оценок с принципами и критикой.
3. **Rule-Based RL**: дальнейшее улучшение модели посредством онлайн-обучения с вознаграждениями на основе правил.
4. **Meta RM Training**: разработка классификатора для улучшения агрегирования нескольких оценок.

![Рисунок 7](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_07.jpeg)

Этот конвейер позволяет разрабатывать модели вознаграждения, которые могут эффективно использовать дополнительные вычисления во время вывода. Благодаря интеграции как контролируемого обучения (RFT), так и обучения с подкреплением, модели учатся генерировать высококачественные оценки, которые можно масштабировать с помощью выборки.

## **7. Преимущества перед существующими подходами**

Подход DeepSeek-GRM предлагает несколько ключевых преимуществ по сравнению с традиционными методами моделирования вознаграждения:

1. **Интерпретируемость:** в отличие от скалярных моделей типа "черный ящик", GRM предоставляют подробные объяснения своих оценок через принципы и критические замечания.

2. **Гибкость:** подход точечной оценки может обрабатывать различные форматы входных данных (одиночные ответы, пары или множественные ответы).

3. **Масштабируемость:** модель может использовать дополнительные вычислительные ресурсы во время вывода для повышения производительности без переобучения.

4. **Общность:** адаптивно генерируя принципы, модель может оценивать ответы в различных областях и задачах.

5. **Эффективность:** масштабирование во время вывода предлагает более экономичный подход к повышению производительности по сравнению с обучением более крупных моделей.

Экспериментальные результаты показывают, что эти преимущества приводят к практической пользе: модели DeepSeek-GRM превосходят существующие модели вознаграждения при использовании масштабирования во время вывода.

## **8. Ограничения и режимы отказа**

Несмотря на свои сильные стороны, авторы выявили несколько ограничений и потенциальных режимов отказа своего подхода:

1. **Некорректные критические замечания:** GRM может иногда генерировать некорректные или непоследовательные критические замечания, особенно для сложных задач рассуждения.

![Рисунок 8](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-22/assets/Image_08.jpeg)

2. **Несбалансированное взвешивание:** модель может присваивать несоответствующие веса различным принципам, что приводит к искаженным оценкам.

3. **Неправильные принципы:** для некоторых запросов модель может генерировать принципы, которые не являются релевантными или уместными.

4. **Противоречия с истиной:** в проверяемых задачах оценки модели могут противоречить фактическим знаниям.

Эти ограничения указывают на направления для будущего улучшения, такие как повышение способности модели проверять фактическую информацию и улучшение согласованности ее оценок.

## **9. Заключение и будущие направления**

В статье представлен значительный прогресс в моделировании вознаграждения для больших языковых моделей путем внедрения настройки с критикой на основе самопринципов и демонстрации эффективности масштабирования во время вывода. Этот подход обеспечивает более эффективное использование вычислительных ресурсов и предоставляет способ улучшить модели вознаграждения без необходимости использования все более крупных моделей или дорогостоящего обучения с подкреплением.

Будущие направления исследований, определенные авторами, включают:

1. **Интеграция с онлайн-RL:** использование GRM непосредственно в конвейерах обучения с подкреплением.
2. **Совместное масштабирование с моделями политик:** изучение того, как масштабирование во время вывода как моделей политики, так и моделей вознаграждения может привести к синергетическим улучшениям.
3. **Устойчивость и смягчение предвзятости:** устранение выявленных режимов отказа для создания более надежных моделей вознаграждения.
4. **Оффлайн-оценка:** использование GRM в качестве надежных оценщиков для базовых моделей.

Эта работа представляет собой сдвиг парадигмы в мышлении о моделировании вознаграждения, подчеркивая, что то, как вычислительные ресурсы используются во время вывода, может быть так же важно, как размер модели или методы обучения. Это понимание имеет широкие последствия для разработки более эффективных и результативных больших языковых моделей в будущем.