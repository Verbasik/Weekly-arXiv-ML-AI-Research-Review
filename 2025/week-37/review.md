# Математические основы DeepConf: улучшение рассуждений через оценку уверенности

## Концептуальная основа DeepConf

Deep Think with Confidence (DeepConf) представляет собой простой, но эффективный метод, **устраняющий необходимость в повторной генерации полных цепочек рассуждений** через элегантное использование внутренних сигналов уверенности модели. Метод существенно повышает как эффективность рассуждений, так и вычислительную производительность больших языковых моделей в тестовом режиме.

Принципиальное отличие DeepConf от классических подходов к параллельному мышлению (parallel thinking) заключается в его способности **динамически фильтровать низкокачественные трассы рассуждений** как во время (онлайн), так и после (офлайн) генерации, без необходимости в дополнительном обучении модели или настройке гиперпараметров.

## Метрики уверенности в качестве сигнала качества рассуждений

![Рисунок 5: DeepConf с параллельным мышлением отклоняет трассировки рассуждений с низкой уверенностью во время генерации, чтобы достичь более высокой производительности рассуждений, используя при этом значительно меньше сгенерированных токенов.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-37/assets/Image-05.png)

![Рисунок 6: Измерение уверенности и уверенное мышление в офлайн-режиме.](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-37/assets/Image-06.png)

Основу DeepConf составляет использование различных метрик уверенности, извлекаемых из распределения следующих токенов модели. Формально, можно выделить следующие ключевые метрики:

### 1. Энтропия токена

$$H_i = -\sum_{j} P_i(j) \log P_i(j)$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$H_i$** — энтропия распределения вероятностей токенов на позиции $i$.
- **$P_i(j)$** — вероятность $j$-го токена из словаря в позиции $i$.
</details>

---

Энтропия распределения $P$ — это мера его **неопределённости**. Чем ниже энтропия, тем более "сконцентрировано" распределение вероятностей и тем увереннее модель в своём предсказании. Высокая энтропия означает, что вероятностная масса "размазана" по многим токенам, что указывает на неуверенность модели.

Например, рассмотрим предсказание следующего токена в контексте:
- **Контекст:** `"Теорема Пифагора гласит, что в прямоугольном треугольнике сумма квадратов ___"`
- **Возможные токены:** `"катетов"` (0.9), `"гипотенузы"` (0.05), `"сторон"` (0.05)

Энтропия будет низкой ($H_i \approx 0.47$), указывая на высокую уверенность модели. Если же распределение близко к равномерному, например, `"катетов"` (0.4), `"гипотенузы"` (0.3), `"сторон"` (0.3), энтропия будет высокой ($H_i \approx 1.57$), что говорит о низкой уверенности модели.

### 2. Уверенность токена

$$C_i = -\frac{1}{k}\sum_{j=1}^{k} \log P_i(j)$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$C_i$** — уверенность модели при генерации токена на позиции $i$.
- **$k$** — количество рассматриваемых топ-токенов.
- **$P_i(j)$** — вероятность $j$-го топ-токена из словаря.
</details>

---

Уверенность токена — это отрицательное среднее логарифмических вероятностей топ-$k$ токенов. Этот показатель количественно определяет, насколько модель уверена в своём предсказании. Высокое значение $C_i$ соответствует пиковым распределениям и большей уверенности модели, в то время как низкое значение указывает на неопределённость в предсказании токена.

Важное отличие от энтропии заключается в том, что уверенность токена учитывает только топ-$k$ наиболее вероятных токенов, игнорируя "хвост" распределения. Это делает метрику более устойчивой к шуму в маловероятных токенах и лучше отражает "решительность" модели в выборе между наиболее вероятными альтернативами.

### 3. Групповая уверенность

$$C_{G_i} = \frac{1}{|G_i|} \sum_{t \in G_i} C_t$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$C_{G_i}$** — уверенность группы токенов $G_i$.
- **$G_i$** — группа токенов, состоящая из $n$ предыдущих токенов с перекрывающимися соседними окнами.
- **$|G_i|$** — количество токенов в группе $G_i$.
- **$C_t$** — уверенность токена $t$.
</details>

---

Групповая уверенность предоставляет более локализованный и сглаженный сигнал путём усреднения уверенности токенов по перекрывающимся промежуткам рассуждения. Этот подход позволяет выявлять проблемные участки в цепочке рассуждений, где модель становится менее уверенной.

Например, если модель начинает сомневаться и генерировать фразы типа "подождите, давайте проверим", "нет, я ошибся", групповая уверенность в этом сегменте резко падает. Это более надежный индикатор проблем в рассуждении, чем средняя уверенность по всей трассе, которая может быть размыта высокоуверенными сегментами в других частях.

### 4. Уверенность нижних 10% групп

$$C_{\text{bottom-10}}(t) = \frac{1}{|G_b|} \sum_{G_j \in G_b} C_{G_j}$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$C_{\text{bottom-10}}(t)$** — метрика, учитывающая только наименее уверенные группы токенов.
- **$G_b$** — множество групп с наименьшими 10% значений уверенности в трассе.
- **$|G_b|$** — количество групп в $G_b$.
</details>

---

Уверенность нижних 10% групп фокусируется на наиболее проблемных сегментах рассуждения. Исследователи обнаружили, что качество рассуждения часто определяется его самыми слабыми звеньями — небольшими участками, где модель теряет уверенность или допускает ошибки.

Эта метрика вычисляет среднюю уверенность только для 10% наименее уверенных групп в трассе. Таким образом, даже если большая часть рассуждения выглядит уверенной, но есть критический момент, где модель начинает "плутать" или колебаться, эта метрика это зафиксирует.

### 5. Уверенность наименее уверенной группы

$$C_{\text{least}}(t) = \min_{G_j \in G} C_{G_j}$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$C_{\text{least}}(t)$** — уверенность наименее уверенной группы токенов в трассе.
- **$G$** — множество всех групп токенов в трассе рассуждения.
</details>

---

Уверенность наименее уверенной группы — это экстремальный случай метрики "нижних 10%", учитывающий только абсолютный минимум уверенности во всей трассе рассуждения. Эта метрика исходит из предположения, что качество рассуждения может быть не лучше качества его самого слабого звена.

Использование минимума вместо среднего делает эту метрику особенно чувствительной к локальным падениям уверенности, что позволяет эффективно идентифицировать трассы с критическими ошибками рассуждения. Именно эта метрика оказалась наиболее эффективной для раннего останова в онлайн-режиме DeepConf.

### 6. Уверенность хвоста

$$C_{\text{tail}}(t) = \frac{1}{|T_{\text{tail}}|} \sum_{t \in T_{\text{tail}}} C_t$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$C_{\text{tail}}(t)$** — средняя уверенность последних токенов трассы.
- **$T_{\text{tail}}$** — фиксированное количество токенов в конце последовательности (например, 2048).
</details>

---

Уверенность хвоста оценивает надежность рассуждения, фокусируясь на его заключительной части. Эта метрика мотивирована наблюдением, что качество рассуждения часто ухудшается к концу длинных цепочек мысли, а заключительные шаги критически важны для правильных выводов.

В математических рассуждениях окончательный ответ и заключительные шаги особенно важны: трассы, которые начинаются сильно, но заканчиваются слабо, могут привести к неверным результатам, несмотря на многообещающие промежуточные рассуждения.

## Алгоритм DeepConf: офлайн и онлайн режимы

DeepConf работает в двух основных режимах: офлайн и онлайн. В офлайн режиме все трассы рассуждений уже сгенерированы, а в онлайн режиме DeepConf может динамически прерывать генерацию низкокачественных трасс.

### Офлайн режим (Offline Thinking with Confidence)

1. **Голосование с учетом уверенности (Confidence-Weighted Majority Voting):**

Вместо равного учета всех трасс, каждый итоговый ответ взвешивается уверенностью соответствующей трассы:

$$V(a) = \sum_{t \in T} C_t \cdot I(\text{answer}(t) = a)$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$V(a)$** — взвешенное количество голосов за ответ $a$.
- **$T$** — множество всех сгенерированных трасс.
- **$C_t$** — уверенность трассы $t$, вычисленная с помощью одной из метрик уверенности.
- **$I(\text{answer}(t) = a)$** — индикаторная функция, равная 1, если ответ трассы $t$ совпадает с $a$, и 0 в противном случае.
</details>

---

В отличие от стандартного мажоритарного голосования, где каждая трасса имеет равный вес, взвешенное голосование учитывает уверенность модели в каждой трассе. Это позволяет отдавать предпочтение ответам, которые поддерживаются более уверенными рассуждениями, даже если таких трасс меньше.

Например, если 6 трасс с низкой уверенностью (по 0.4) дают ответ "103", а 4 трассы с высокой уверенностью (по 0.9) дают ответ "109", то взвешенное голосование выберет "109" с общим весом 3.6 против 2.4 для "103", в то время как простое мажоритарное голосование выбрало бы "103".

2. **Фильтрация по уверенности (Confidence Filtering):**

Отбираются только трассы с наибольшей уверенностью (например, топ-10% или топ-90%):

$$\hat{a} = \arg\max_a V(a)$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$\hat{a}$** — итоговый выбранный ответ с наибольшим весом.
</details>

---

Фильтрация по уверенности позволяет сосредоточиться только на наиболее надежных трассах, исключая потенциально ошибочные или запутанные рассуждения. В DeepConf предусмотрены две основные стратегии фильтрации:

- **DeepConf-high (top-90%)**: консервативный подход, который отбрасывает только 10% наименее уверенных трасс. Это обеспечивает хорошую сбалансированность между точностью и разнообразием рассуждений.

- **DeepConf-low (top-10%)**: агрессивный подход, который сохраняет только 10% наиболее уверенных трасс. Это может давать наибольший прирост точности в большинстве случаев, но иногда может приводить к потере точности из-за чрезмерной концентрации на ограниченном наборе рассуждений.

### Онлайн режим (Online Thinking with Confidence)

В онлайн режиме DeepConf использует уверенность наименее уверенной группы для динамического прерывания генерации:

1. **Разминка (Offline Warmup):**
   - Генерируется $N_{init}$ (например, 16) полных трасс рассуждений.
   - Устанавливается порог останова $s$ на основе перцентиля уверенности:
   
   $$s = \text{Percentile}_{100-\eta}(\{C_t : t \in T_{warmup}\})$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$s$** — пороговое значение для останова генерации.
- **$\eta$** — процент трасс, который мы хотим сохранить (например, 10% или 90%).
- **$T_{warmup}$** — множество всех трасс разминки.
- **$C_t$** — уверенность трассы $t$.
</details>

---

Фаза разминки необходима для определения порога останова на основе распределения уверенности на конкретной задаче. Для каждого нового запроса DeepConf сначала генерирует небольшое количество полных трасс рассуждений, чтобы "понять", какой уровень уверенности является типичным для данной задачи.

Для DeepConf-low порог устанавливается как 90-й перцентиль (отсекаются трассы ниже этого уровня), а для DeepConf-high — как 10-й перцентиль (сохраняются почти все трассы, кроме самых неуверенных).

2. **Адаптивная выборка (Adaptive Sampling):**
   - При генерации новой трассы, если групповая уверенность $C_{G_i}$ падает ниже порога $s$, генерация останавливается.
   - DeepConf динамически регулирует количество генерируемых трасс на основе сложности задачи, оцениваемой коэффициентом консенсуса:
   
   $$\beta = \frac{V(\hat{a})}{\sum_a V(a)}$$

<details> 
    <summary><em><strong>пояснение переменных</strong></em></summary>

где:
- **$\beta$** — коэффициент консенсуса.
- **$V(\hat{a})$** — взвешенное количество голосов за наиболее популярный ответ.
- **$\sum_a V(a)$** — общее количество взвешенных голосов.
</details>

---

В процессе онлайн-генерации DeepConf отслеживает групповую уверенность модели в скользящем окне (обычно 2048 токенов). Если уверенность падает ниже установленного порога, это свидетельствует о том, что модель начала "сомневаться" в своем рассуждении, и генерация этой трассы преждевременно прерывается.

Это существенно экономит вычислительные ресурсы, так как нет необходимости генерировать полностью трассы, которые, скорее всего, будут отфильтрованы на этапе взвешенного голосования.

3. **Остановка генерации:**
   - Если $\beta \geq \tau$ (где $\tau$ — порог консенсуса, например, 0.95), генерация новых трасс прекращается.
   - В противном случае, генерация продолжается до достижения фиксированного бюджета $B$.

Адаптивный останов генерации на основе консенсуса позволяет DeepConf завершать работу раньше, если модель достигла высокого согласия относительно итогового ответа. Это особенно полезно для простых задач, где нет необходимости генерировать все $B$ трасс.

## Конкретный пример работы DeepConf

Рассмотрим, как DeepConf работает на примере решения математической задачи:

### Сценарий для примера

* **Задача:** "Найдите количество целочисленных решений (x, y) с 1≤ x, y ≤ 100, где x² + y² = z² для некоторого положительного целого z."
* **Размер группы:** $G = 512$ трасс рассуждений.
* **Метрика уверенности:** Групповая уверенность с окном 2048 токенов.
* **Модель:** Большая языковая модель (например, GPT-OSS-120B).

<details> 
    <summary><em><strong>пример работы DeepConf</strong></em></summary>

### Пример: Как работает DeepConf "под капотом"

#### **Шаг 1: Генерация начальных трасс и вычисление порога (Offline Warmup)**

DeepConf генерирует $N_{init} = 16$ полных трасс рассуждений. Для каждой трассы вычисляется групповая уверенность с окном 2048 токенов. Предположим, значения уверенности колеблются в диапазоне от 11 до 18.

Для DeepConf-low (топ-10%):
- Вычисляется 90-й перцентиль: $s_{low} = 16.5$

Для DeepConf-high (топ-90%):
- Вычисляется 10-й перцентиль: $s_{high} = 12.8$

#### **Шаг 2: Онлайн генерация с ранним останавливанием**

Начинается генерация новых трасс. Рассмотрим три примера:

**Трасса 1:**
```
Рассмотрим задачу пошагово. Шаг 1: Уравнение x² + y² = z² представляет пифагоровы тройки.
Все примитивные тройки можно сгенерировать с помощью формулы x = m² - n², y = 2mn, z = m² + n²...
```

Пока уверенность этой трассы остается высокой (например, $C_{G_i} \approx 17.3 > s_{low}$), генерация продолжается до конца.

**Трасса 2:**
```
Рассмотрим задачу пошагово. Шаг 1: Уравнение x² + y² = z² представляет пифагоровы тройки.
Все примитивные... Подождите, мне нужно проверить результаты... Я должен перепроверить шаг 1... 
```

На этом моменте групповая уверенность падает ($C_{G_i} \approx 11.5 < s_{low}$), и DeepConf-low останавливает генерацию этой трассы.

**Трасса 3:**
```
Рассмотрим задачу пошагово. Шаг 1: Мы ищем решения уравнения x² + y² = z²...
```

Уверенность этой трассы ($C_{G_i} \approx 13.5$) выше порога DeepConf-high ($s_{high} = 12.8$), но ниже порога DeepConf-low ($s_{low} = 16.5$). Поэтому DeepConf-high продолжит генерацию, а DeepConf-low остановит.

#### **Шаг 3: Взвешенное голосование с фильтрацией**

После генерации достаточного количества трасс (или достижения высокого консенсуса) DeepConf выполняет взвешенное голосование с фильтрацией:

1. Отбираются трассы на основе выбранной стратегии (топ-10% или топ-90% по уверенности).
2. Для каждого уникального ответа (например, "109") вычисляется взвешенная сумма голосов.
3. Выбирается ответ с наибольшим весом.

Например, если ответ "109" получил наибольший вес $V(109) = 17$, то он выбирается как итоговый ответ.

#### **Шаг 4: Сравнение с базовыми методами**

В отличие от обычного мажоритарного голосования, DeepConf достигает:

1. Лучшей точности: 99.9% vs 97.0% (cons@512) на AIME 2025 с моделью GPT-OSS-120B.
2. Меньшего количества токенов: сокращение на 84.7% для DeepConf-low и на 56.0% для DeepConf-high.

Это происходит благодаря способности DeepConf эффективно отфильтровывать низкокачественные трассы рассуждений и концентрироваться на наиболее уверенных.

</details>

---

## Основные преимущества метода DeepConf

1. **Вычислительная эффективность**: DeepConf сокращает количество генерируемых токенов на 43-85% (DeepConf-low) и 18-59% (DeepConf-high) при сохранении или улучшении точности по сравнению с мажоритарным голосованием.

2. **Улучшенная точность**: На сложных задачах, таких как AIME 2025, DeepConf@512 достигает до 99.9% точности по сравнению с 97.0% для обычного мажоритарного голосования и 91.8% для одиночного прохода.

3. **Простота интеграции**: DeepConf не требует дополнительного обучения модели или настройки гиперпараметров и может быть интегрирован в существующие фреймворки обслуживания.

4. **Гибкость**: Метод предлагает два режима работы (офлайн и онлайн) и различные стратегии фильтрации (DeepConf-low и DeepConf-high), позволяющие настраивать компромисс между точностью и эффективностью.

## Практическая реализация

DeepConf можно реализовать с минимальными изменениями в стандартных библиотеках для обслуживания больших языковых моделей, таких как vLLM. Необходимые модификации включают:

1. Расширение обработчика логарифмов вероятностей для вычисления и поддержки скользящего окна уверенности.
2. Добавление проверки условия раннего останова на основе уверенности.
3. Реализацию фильтрации и взвешенного голосования для итогового ответа.

Эти изменения могут быть интегрированы в API обслуживания моделей, обеспечивая эффективное применение DeepConf в производственных системах.