# Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention

When Musk released Grok 3, while Sam Altman was still hesitating whether to open-source, Liang Wenfeng, as a co-author, worked with the DeepSeek research team to create a shocking and sensational research paper. DeepSeek officially unveiled its latest scientific breakthrough — **Native Sparse Attention (NSA)**! This technology is of great significance. It is likely to significantly enhance the next generation of large language models' ability to process long texts, while fully accounting for operational efficiency. There is no doubt that this is another milestone in the field of large language models (LLMs)!

## 1. Research Background and General Findings

The problem addressed by this paper is the high computational cost of modeling long contexts. The standard attention mechanism has high computational complexity when processing long sequences, becoming a performance bottleneck. Challenges in researching this problem include:

- How to improve efficiency while preserving model capabilities.
- How to achieve end-to-end training to reduce pre-computations without sacrificing model performance.

Related research efforts on this problem include:

- KV cache pruning methods.
- KV cache block selection methods.
- Sampling, clustering, or hashing-based selection methods, among others.

However, these methods often fail to achieve theoretical acceleration effects in real-world deployments and primarily focus on the inference stage, lacking effective support during training.

### General Conclusion

The NSA architecture proposed in this paper enables accelerated training and inference while preserving full attention through the integration of hierarchical token compression and block-wise token selection into a trainable architecture. NSA matches the baseline full attention in overall metrics, surpasses long-context modeling capabilities, and improves reasoning performance, while significantly reducing computational latency and achieving substantial speedup.

### Key NSA Innovations

1. **Natively Designed Sparse Attention**:
   - Enables end-to-end optimization of the sparse pattern during pre-training.
   - The sparse attention module can be synchronously adapted with other model components, enhancing overall performance.

2. **Hierarchical Sparse Attention Mechanism**:
   - Skillfully balances local and global information processing.
   - Provides a more efficient and comprehensive solution for long-text modeling.

![Table_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-09/assets/Figure_1.png  )

> **Figure 1 | Performance and efficiency comparison between Full Attention and our NSA.**  
> **Left:** Despite being a sparse model, NSA on average outperforms the full attention baseline across general benchmarks, long-context tasks, and reasoning evaluations.  
> **Right:** For processing 64k-length sequences, NSA delivers significant computational speedup compared to full attention across all stages: decoding, forward pass, and backpropagation.

# 2. Research Methods

This paper proposes NSA (Native Sparse Attention) — a hardware-aligned and trainable sparse attention mechanism designed to address the high computational cost of modeling long contexts. To leverage the potential of sparse attention, NSA replaces the original key-value pairs $(k_t, v_t)$ with a more compact and informative representation $(\tilde{k}, \tilde{v})$ conditioned on each query $(q_t)$. Engineers also aimed to optimize arithmetic intensity.

**Arithmetic intensity** — the ratio of computational operations to memory accesses — becomes a critical factor in hardware-level optimization. Efficient GPU utilization requires maximizing arithmetic intensity to ensure computations are bound by GPU throughput rather than memory bandwidth. In the context of causal self-attention, especially during autoregressive decoding, memory bandwidth becomes particularly critical due to the need to load the entire key and value space.

![Table_2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-09/assets/Figure_2.png  )

> **Figure 2 | Overall NSA architecture overview.**  
> **Left:** The framework processes input sequences through three parallel attention branches. For a given query, previous keys and values are transformed into compressed attention for coarse-grained patterns, selected attention for important tokens, and sliding attention for local context.  
> **Right:** Visualization of the distinct attention patterns generated by each branch. Green areas indicate regions where attention coefficients must be computed; white areas are regions that can be skipped.

## 2.1 Algorithm Overview

To effectively exploit sparse attention, this paper proposes replacing the original key-value pairs in Equation (1) with a more compact and information-rich representation. 

This representation is constructed for each query $(q_t)$ as follows:
- It is based on the current query $(q_t)$ and contextual memory.
- It is dynamically built depending on context.

Thus, the proposed approach enables optimization of computations and enhances the efficiency of the attention mechanism.

NSA employs three mapping strategies $(C = \{ \text{cmp}, \text{slc}, \text{win} \})$, corresponding to compression, selection, and sliding window, respectively. The output attention in NSA is formed as a combination of results from these strategies:

$$
o'_t = \sum_{c \in C} g^c_t \cdot Attn(q_t, \tilde{k}^c, \tilde{v}^c)
$$

where each strategy $(c \in C)$ generates its own $(\tilde{k}^c, \tilde{v}^c)$. The value $(g^c_t \in [0,1])$ is a "gate" score for each strategy, computed via an MLP with sigmoid activation based on input features. It controls the contribution of each strategy and the total number of reassigned keys and values:

$$
N_t = \sum_{c \in C} g^c_t \cdot size[\tilde{k}^c]
$$

Thus, NSA maintains high sparsity by ensuring $N_t \ll t$, reducing computational cost and improving efficiency in processing long sequences. In particular, the optimized attention output is formally defined by the equations above.

## 2.2 Dynamic Multi-Level Sparse Strategy:

NSA employs a dynamic hierarchical sparse strategy that combines coarse token compression and precise token selection with a sliding window to preserve global contextual awareness and local accuracy. The NSA architecture uses hierarchical token modeling and processes the input sequence through three parallel attention branches, representing the following three mechanisms.

1. **Compressed Attention**: Handles coarse-grained patterns and gathers global information by compressing token blocks.
2. **Selected Attention**: Processes important token blocks and selectively retains detailed information.
3. **Sliding Window Attention**: Processes local contextual information.

### (1) Coarse-Grained Compression (Global Attention):

Coarse compression reduces the number of tokens to be processed by aggregating consecutive token blocks into block-level representations. Specifically, the model compresses token blocks of a fixed length (e.g., 32 tokens) into a single representation, thereby capturing higher-level semantic information. This compression method significantly reduces computational volume, especially for long sequences, and effectively lowers the computational complexity of the attention mechanism. This step can intuitively be understood as summarizing text.

Below is the formula for coarse-grained compression:

$$
K_{t}^{\text{cmp}} \;=\; f_{k}^{\text{cmp}}\bigl(k_{t}\bigr) \;=\; \Bigl\{\,\varphi\bigl(k_{id + 1:id+l}\bigr)\;\Bigm|\;1 \leqslant i \leqslant \Bigl\lfloor\frac{t - l}{d}\Bigr\rfloor \Bigr\}
$$

where:

- **$k_t$** – the original sequence of key vectors $[k_1, k_2, \dots, k_t]$ up to time $t$.
- **$f_k^{\text{cmp}}$** – the compression function applied to the original keys.
- **$\varphi$** – a trainable multilayer perceptron (MLP) that transforms a block of consecutive keys into a single compressed vector. Positional encoding may be used to preserve token order within the block.
- **$k_{id+1:id+l}$** – a block of $l$ consecutive keys starting at index $id+1$ and ending at index $id+l$; parameter $l$ defines block length.
- **$d$** – the stride between blocks. If $d < l$, blocks overlap, helping preserve information continuity.
- **$\lfloor\frac{t-l}{d}\rfloor$** – the number of complete blocks that can be formed from a sequence of length $t$.

$$
\tilde{K}_{t}^{\mathrm{cmp}} \;\in\; \mathbb{R}^{\,d_k\times\left\lfloor \frac{t-l}{d} \right\rfloor}
$$

represents a tensor of compressed keys. We typically adopt $d < l$ to reduce information fragmentation. A similar formula applies to compressed values. The compressed representation captures coarser, higher-level semantic information and reduces attention computational load.

#### **How $\varphi$ works** – the trainable multilayer perceptron (MLP):

The function **$\varphi$** is implemented as a trainable MLP that transforms a block of **$l$** consecutive keys into a single compressed vector of dimension **$d_{\text{model}}$**. Here is a step-by-step explanation:

#### 1. **Input Data**
- Key block:  
  
  $$
  k_{id+1:id+l} \in \mathbb{R}^{l \times d_{\text{model}}}
  $$

  where:
  - **$l$** — block size (e.g., 32 tokens),
  - **$d_{\text{model}}$** — embedding dimension per token.

#### 2. **Input Vector Preparation**
1. **Token Concatenation**:  
   All tokens in the block are concatenated into a single vector:  
   
   $$
   x_{\text{concat}} = \text{concat}(k_{id+1}, k_{id+2}, \dots, k_{id+l}) \in \mathbb{R}^{l \cdot d_{\text{model}}}
   $$

   For example, with **$l=32$** and **$d_{\text{model}}=512$**, the dimension of **$x_{\text{concat}}$** becomes **$32 \times 512 = 16384$**.

2. **Addition of Positional Encoding**:  
   Positional encoding is added to each token in the block to preserve order information:  
   
   $$
   x_{\text{pos}} = x_{\text{concat}} + \text{PositionalEncoding}(id+1, id+2, \dots, id+l)
   $$

#### 3. **MLP Architecture**
The MLP consists of the following layers:  

$$
\varphi(x) = \text{LayerNorm}(W_2 \cdot \text{GELU}(W_1 \cdot x_{\text{pos}} + b_1) + b_2)
$$

where:

- **$W_1 \in \mathbb{R}^{(l \cdot d_{\text{model}}) \times h}$** — weight matrix of the first layer,
- **$W_2 \in \mathbb{R}^{h \times d_{\text{model}}}$** — weight matrix of the second layer,
- **$h$** — hidden layer size (typically **$h = d_{\text{model}}$**),
- **GELU** — activation function,
- **LayerNorm** — normalization layer.

#### 4. **How It Works**
1. **Semantic Preservation**:  
   The MLP learns to extract the most important features from the token block. For text, it may emphasize key words or phrases.

2. **Dynamic Adaptation**:  
   Weights **$W_1$** and **$W_2$** are trained during model training, allowing $\varphi$ to flexibly adapt to different data types.

3. **Positional Information**:  
   Adding positional encoding helps the model preserve token order within the block.

#### 5. **Compression Example**
- **Original block**: 32 tokens → 32 × 512 = 16,384 parameters.
- **After compression**: 1 × 512 parameters.  
Compression ratio: **32×** in token count.

#### 6. **Advantages of This Approach**
- **Computational Efficiency**: Reduces dimensionality of keys/values for attention.
- **Adaptability**: The MLP learns better aggregation than fixed methods (averaging, max-pooling).
- **Context Retention**: Positional encoding and nonlinearities preserve block semantics.

#### Summary
Function **$\varphi$** dynamically "summarizes" a token block into a compact vector while preserving key information. This allows NSA to process long sequences with low computational cost.

#### **How the Formula Works:**

1. **Sequence Segmentation**: The original key sequence $k_t$ is segmented into blocks of length $l$ with stride $d$. For example, if $l=32$ and $d=16$, each new block starts 16 tokens after the previous one, with partial overlap.
2. **Application of Compression Function**: For each block $k_{id+1:id+l}$, the function $\varphi$ (MLP) transforms this token set into a single compressed vector. This process "compresses" information from a group of tokens into a compact representation while preserving critical semantic connections.
3. **Formation of Compressed Key Set**: The result is a set of compressed keys $K_t^{\text{cmp}}$ consisting of $\lfloor\frac{t-l}{d}\rfloor$ elements — far fewer than the original $t$ tokens. These compressed keys replace the original ones in the attention mechanism, significantly reducing computational complexity.

**Intuitive Understanding:**

Imagine you need to quickly grasp the content of a long text, such as a novel or news summary. Instead of reading every page, you divide the text into chapters and skim the summary of each. Here:
- **Original text** – the key sequence $k_t$,
- **Chapter** – a block $k_{id+1:id+l}$,
- **Chapter summary** – the compressed vector produced by $\varphi$.

This approach efficiently processes large volumes of information while preserving core meaning and dramatically saving computational resources.

**Advantages of This Method:**

- **Computational Efficiency**: Reducing token count lowers attention complexity, especially critical for long sequences.
- **Scalability**: The model can handle significantly larger contexts.
- **Adaptability**: The trainable function $\varphi$ dynamically extracts the most relevant information in each block, making the method more flexible than simple averaging or pooling.

> **Note**: An example of this method is demonstrated in the Jupyter notebook 'CompressedAttention.ipynb'.

Based on experimental results, the following conclusions can be drawn:

#### **Key Findings from Testing**

1. **Efficiency Depends on Sequence Length**:
   - For sequences <1000 tokens, standard attention is more efficient.
   - For sequences >10K tokens, compressed attention provides substantial advantages.
   - Potential speedup increases with sequence length.

2. **Optimal Compression Parameters**:
   - For long sequences, larger block sizes (256 instead of 32) are more effective.
   - Optimal stride/block_size ratio is close to 0.5, balancing compression and information retention.

3. **Trade-off Between Efficiency and Accuracy**:
   - Compressed attention sacrifices token-level attention granularity for long-context processing.
   - In the full NSA architecture, this trade-off is compensated by combining it with other attention mechanisms.

4. **Hardware Considerations**:
   - Compressed attention not only accelerates computation but also significantly reduces memory requirements.
   - Potentially enables processing of 100K+ token contexts on standard hardware.

<details> 
    <summary><em><strong>Manual Calculation Example: Step-by-Step</strong></em></summary>

## 1. Example Input Data

Consider a sequence of 16 key vectors, each of dimension 4. For clarity, we organize them into 4 semantic groups:

**Group 1 (tokens describing weather):**
- $k_1 = [0.2, 0.3, 0.8, 0.1]$
- $k_2 = [0.3, 0.2, 0.7, 0.2]$
- $k_3 = [0.1, 0.4, 0.9, 0.1]$
- $k_4 = [0.2, 0.3, 0.8, 0.2]$

**Group 2 (tokens describing location):**
- $k_5 = [0.7, 0.8, 0.2, 0.5]$
- $k_6 = [0.8, 0.9, 0.1, 0.6]$
- $k_7 = [0.6, 0.7, 0.3, 0.4]$
- $k_8 = [0.7, 0.8, 0.2, 0.5]$

**Group 3 (tokens describing time):**
- $k_9 = [0.4, 0.1, 0.3, 0.9]$
- $k_{10} = [0.5, 0.2, 0.2, 0.8]$
- $k_{11} = [0.3, 0.1, 0.4, 0.9]$
- $k_{12} = [0.4, 0.2, 0.3, 0.8]$

**Group 4 (tokens describing action):**
- $k_{13} = [0.9, 0.5, 0.6, 0.3]$
- $k_{14} = [0.8, 0.6, 0.7, 0.2]$
- $k_{15} = [0.9, 0.4, 0.5, 0.4]$
- $k_{16} = [0.8, 0.5, 0.6, 0.3]$

## 2. Algorithm Parameter Configuration

Define parameters for coarse-grained compression:

- **Block length $(l) = 4$**: Aggregate every 4 consecutive vectors
- **Stride $(d) = 2$**: Each new block starts 2 positions after the start of the previous block
- **Compression function $\varphi$**: For simplicity, use the arithmetic mean of vectors within each block

According to the formula, the number of compressed vectors will be:
$$\Bigl\lfloor\frac{t - l}{d}\Bigr\rfloor = \Bigl\lfloor\frac{16 - 4}{2}\Bigr\rfloor = \lfloor 6 \rfloor = 6$$

## 3. Application of Coarse-Grained Compression Formula

The coarse-grained compression formula:
$$K_{t}^{\text{cmp}} = f_{k}^{\text{cmp}}(k_{t}) = \Bigl\{\varphi(k_{id + 1:id+l})\Bigm|1 \leqslant i \leqslant \Bigl\lfloor\frac{t - l}{d}\Bigr\rfloor \Bigr\}$$

Compute all 6 compressed vectors by applying function $\varphi$ to each block:

### Block 1 (i=1):
Token indices: $id+1 = 1×2+1 = 3$ to $id+l = 1×2+4 = 6$  
Tokens in block: $k_3, k_4, k_5, k_6$

$$\varphi(k_{3:6}) = \frac{k_3 + k_4 + k_5 + k_6}{4}$$

$$= \frac{[0.1, 0.4, 0.9, 0.1] + [0.2, 0.3, 0.8, 0.2] + [0.7, 0.8, 0.2, 0.5] + [0.8, 0.9, 0.1, 0.6]}{4}$$

$$= \frac{[1.8, 2.4, 2.0, 1.4]}{4} = [0.45, 0.6, 0.5, 0.35]$$

### Block 2 (i=2):
Token indices: $id+1 = 2×2+1 = 5$ to $id+l = 2×2+4 = 8$  
Tokens in block: $k_5, k_6, k_7, k_8$

$$\varphi(k_{5:8}) = \frac{k_5 + k_6 + k_7 + k_8}{4}$$

$$= \frac{[0.7, 0.8, 0.2, 0.5] + [0.8, 0.9, 0.1, 0.6] + [0.6, 0.7, 0.3, 0.4] + [0.7, 0.8, 0.2, 0.5]}{4}$$

$$= \frac{[2.8, 3.2, 0.8, 2.0]}{4} = [0.7, 0.8, 0.2, 0.5]$$

### Block 3 (i=3):
Token indices: $id+1 = 3×2+1 = 7$ to $id+l = 3×2+4 = 10$  
Tokens in block: $k_7, k_8, k_9, k_{10}$

$$\varphi(k_{7:10}) = \frac{k_7 + k_8 + k_9 + k_{10}}{4}$$

$$= \frac{[0.6, 0.7, 0.3, 0.4] + [0.7, 0.8, 0.2, 0.5] + [0.4, 0.1, 0.3, 0.9] + [0.5, 0.2, 0.2, 0.8]}{4}$$

$$= \frac{[2.2, 1.8, 1.0, 2.6]}{4} = [0.55, 0.45, 0.25, 0.65]$$

### Block 4 (i=4):
Token indices: $id+1 = 4×2+1 = 9$ to $id+l = 4×2+4 = 12$  
Tokens in block: $k_9, k_{10}, k_{11}, k_{12}$

$$\varphi(k_{9:12}) = \frac{k_9 + k_{10} + k_{11} + k_{12}}{4}$$

$$= \frac{[0.4, 0.1, 0.3, 0.9] + [0.5, 0.2, 0.2, 0.8] + [0.3, 0.1, 0.4, 0.9] + [0.4, 0.2, 0.3, 0.8]}{4}$$

$$= \frac{[1.6, 0.6, 1.2, 3.4]}{4} = [0.4, 0.15, 0.3, 0.85]$$

### Block 5 (i=5):
Token indices: $id+1 = 5×2+1 = 11$ to $id+l = 5×2+4 = 14$  
Tokens in block: $k_{11}, k_{12}, k_{13}, k_{14}$

$$\varphi(k_{11:14}) = \frac{k_{11} + k_{12} + k_{13} + k_{14}}{4}$$

$$= \frac{[0.3, 0.1, 0.4, 0.9] + [0.4, 0.2, 0.3, 0.8] + [0.9, 0.5, 0.6, 0.3] + [0.8, 0.6, 0.7, 0.2]}{4}$$

$$= \frac{[2.4, 1.4, 2.0, 2.2]}{4} = [0.6, 0.35, 0.5, 0.55]$$

### Block 6 (i=6):
Token indices: $id+1 = 6×2+1 = 13$ to $id+l = 6×2+4 = 16$  
Tokens in block: $k_{13}, k_{14}, k_{15}, k_{16}$

$$\varphi(k_{13:16}) = \frac{k_{13} + k_{14} + k_{15} + k_{16}}{4}$$

$$= \frac{[0.9, 0.5, 0.6, 0.3] + [0.8, 0.6, 0.7, 0.2] + [0.9, 0.4, 0.5, 0.4] + [0.8, 0.5, 0.6, 0.3]}{4}$$

$$= \frac{[3.4, 2.0, 2.4, 1.2]}{4} = [0.85, 0.5, 0.6, 0.3]$$

## 4. Compression Result

The final compressed representation (denoted as $c_i$):

- $c_1 = [0.45, 0.6, 0.5, 0.35]$
- $c_2 = [0.7, 0.8, 0.2, 0.5]$
- $c_3 = [0.55, 0.45, 0.25, 0.65]$
- $c_4 = [0.4, 0.15, 0.3, 0.85]$
- $c_5 = [0.6, 0.35, 0.5, 0.55]$
- $c_6 = [0.85, 0.5, 0.6, 0.3]$

## 5. Mathematical Justification and Intuitive Understanding

### Key Features of the Algorithm:

1. **Overlapping Block Partitioning**:
   Notice how blocks are formed with overlap:
   - Block 1: $[k_3, k_4, k_5, k_6]$
   - Block 2: $[k_5, k_6, k_7, k_8]$
   
   This overlap (shared tokens $k_5$ and $k_6$) ensures continuity of information between blocks.

2. **Continuous Representation at Semantic Group Boundaries**:
   - $c_1$ contains information from weather (Group 1) and location (Group 2)
   - $c_3$ combines location (Group 2) and time (Group 3)
   - $c_5$ links time (Group 3) and action (Group 4)

3. **Semantic Information Preservation**:
   For blocks aligned with semantic groups:
   - $c_2 = [0.7, 0.8, 0.2, 0.5]$ exactly represents the "location" group
   - $c_4 = [0.4, 0.15, 0.3, 0.85]$ exactly represents the "time" group
   - $c_6 = [0.85, 0.5, 0.6, 0.3]$ exactly represents the "action" group

### Schematic Representation of the Process:

If we represent semantic groups with letters:
```
[A][A][A][A][B][B][B][B][C][C][C][C][D][D][D][D]
 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
```

Then the block formation process looks like:
```
    [--Block 1--]                     → c₁ (A+B)
        [--Block 2--]                 → c₂ (B)
            [--Block 3--]             → c₃ (B+C)
                [--Block 4--]         → c₄ (C)
                    [--Block 5--]     → c₅ (C+D)
                        [--Block 6--] → c₆ (D)
```

## 6. Compression Efficiency

In our example:
- Original size: 16 vectors × 4 values = 64 numerical values
- Compressed size: 6 vectors × 4 values = 24 numerical values
- Compression ratio: 64/24 = 2.67 (62.5% reduction)

In real systems with longer sequences (e.g., 10,000 tokens) and block sizes of 256 with stride 128, compression ratios can reach 100x or more, drastically reducing the computational complexity of the attention mechanism.

## 7. Practical Implementation of the Compression Function

In practical systems, instead of simple arithmetic averaging, a multilayer perceptron (MLP) is used:

$$\varphi(k_{id+1:id+l}) = \text{MLP}([k_{id+1} \oplus \text{pe}_1; k_{id+2} \oplus \text{pe}_2; \ldots; k_{id+l} \oplus \text{pe}_l])$$

where:
- $\oplus$ denotes concatenation
- $\text{pe}_j$ is positional encoding for the j-th position in the block
- $\text{MLP}$ is a neural network trained to extract the most important information within the block
</details> 

### Conclusion

The experiment convincingly demonstrates that compressed attention is an effective solution to the quadratic complexity problem of standard attention for long sequences. The actual speedup achieved was (4.69×), and it will increase further with longer sequence lengths.

### (2) Selected Attention (Local Attention):

Selected Attention — the second branch in the NSA architecture — focuses on selecting the most relevant token blocks for the current query. Unlike coarse-grained compression, which processes global context through information aggregation, selected attention preserves fine-grained detail at the individual token level, but only for the most important segments of the sequence. This selection mechanism ensures the model retains critical local information during long-sequence processing and avoids losing important details due to compression. The formula is as follows:

#### 1. Compute Importance Scores for Compressed Blocks:

$$p_{t}^{slc} = \text{Softmax} \left( q_{t}^{T} \tilde{K}_{t}^{cmp} \right)$$

where:
- $q_t$ — the current query vector at position $t$
- $\tilde{K}_{t}^{cmp}$ — the matrix of compressed keys obtained during coarse-grained compression
- $p_{t}^{slc}$ — a probability vector (importance scores), where each element $p_{t}^{slc}[i]$ represents the relative importance of the $i$-th compressed block for the current query

Here, we compute the dot product of the query $q_t$ with each compressed key, then apply the Softmax function to normalize these scores into a probability distribution. This identifies how relevant each compressed block is to the current query.

#### 2. Select the Most Important Blocks:

$$I_{t} = \left\{ i \mid \text{rank} \left( p_{t}^{slc}[i] \right) \leq n \right\}$$

where:
- $I_{t}$ — set of indices of selected blocks
- $\text{rank}(p_{t}^{slc}[i])$ — the rank of the $i$-th element in the probability vector, where rank 1 corresponds to the highest probability
- $n$ — a hyperparameter defining the number of blocks to select

This formula means we select the $n$ blocks with the highest importance scores. The parameter $n$ is critical in determining the trade-off between computational efficiency and modeling quality.

### (3) Sliding Window:

The sliding window mechanism is used to process local contextual information. It retains the most recent tokens (e.g., the last 512 tokens), ensuring the model can rapidly adapt to changes in local patterns when processing the current token. This mechanism prevents "short-circuiting" of the model due to compression or selection, guaranteeing that both local and long-range dependencies can be captured. The formula is as follows:

$$\tilde{K}_{t}^{win} = k_{t-w:t}, \tilde{V}_{t}^{win} = v_{t-w:t}$$

Let’s break down each component:

- **$\tilde{K}_{t}^{win}$** — modified key matrix for the sliding window mechanism at position $t$
- **$\tilde{V}_{t}^{win}$** — modified value matrix for the sliding window mechanism at position $t$
- **$k_{t-w:t}$** — subsequence of key vectors from position $t-w$ to current position $t$
- **$v_{t-w:t}$** — corresponding subsequence of value vectors
- **$w$** — window size (hyperparameter, e.g., 512 tokens)

#### How the Sliding Window Works

The sliding window mechanism is extremely simple yet highly effective. Instead of using the entire history of key and value vectors from the start of the sequence (which would lead to quadratic complexity), the model considers only a fixed number of the most recent tokens:

1. For each current query $q_t$, a local attention window of width $w$ is formed.
2. Only the $w$ most recent tokens are included in this window.
3. Tokens outside the window are ignored during attention computation.

Graphically, this can be represented as:
```
Token history: [t₁, t₂, ..., t_{t-w-1}, t_{t-w}, t_{t-w+1}, ..., t_{t-1}, t_t]
                                          └─────────────────────────────┘
                                                Sliding Window
```

#### Computational Advantages

The sliding window provides constant computational complexity O(w) per token, in contrast to standard attention, whose complexity grows linearly with sequence length O(t). This yields significant advantages:

1. **Constant Memory Consumption**: A fixed number of key-value pairs are stored for each position.
2. **Stable Computation Time**: Token processing time is independent of prior context length.
3. **GPU Optimization**: Fixed window size enables efficient utilization of GPU memory and compute units.

#### Role of the Sliding Window in NSA Architecture

In the overall NSA architecture, the sliding window performs three critical functions:

1. **Local Contextualization**: Provides access to immediate context, crucial for semantic understanding of the current token.

2. **Prevention of "Short-Circuiting"**: Even if the other two NSA components (compressed and selected attention) miss important local information, the sliding window ensures recent tokens are accounted for.

3. **Improved Arithmetic Intensity**: Limiting window size improves the ratio of computations to memory accesses, which is vital for optimizing performance on modern GPUs.

#### Practical Example

Consider text generation with a long context. When generating the word at position 10,000:

- **Sliding window (w=512)**: Considers tokens from positions 9488–10000
- **Compressed attention**: Provides overall summary of all prior context
- **Selected attention**: Identifies key tokens across the entire context relevant to the current query

The sliding window ensures that immediate local context is not lost due to compression or selective sampling, which might otherwise overlook critical local dependencies.

All three mechanisms complement each other, enabling both global context understanding and detailed perception of local information.

### Summary

#### Step 1: Compression (Phrase Generalization)

NSA abandons the traditional method of storing every individual word in text and instead takes initiative to compress phrases into summarized "blocks." This operation can be intuitively visualized as summarizing a chapter in a book: when we summarize a chapter, we do not memorize every word verbatim but extract several key points to capture the main ideas. Similarly, NSA transforms phrases in text into smaller, compact representations. This dramatically reduces data volume, improves processing efficiency, and retains essential information.

#### Step 2: Selection (Choosing Important Words)

After text compression, NSA filters the most relevant words for further processing. This resembles reading an article: we highlight the most important sentences. NSA does not attempt to preserve every detail but prioritizes the most significant words based on defined rules and algorithms. This allows focus on critical information and avoids wasting computational resources on irrelevant content, thereby enhancing the model’s ability to process meaningful data.

#### Step 3: Sliding Window (Preserving Local Context)

Although NSA performs phrase compression and selects key vocabulary, it must still track neighboring words to ensure relationships between words are not lost. This is similar to reading a complex sentence: we not only focus on main words but also pay attention to content before and after them to obtain full contextual understanding. NSA captures nearby important information by moving a small window over the text. This mechanism helps the model better understand semantic relationships between words, enabling more accurate judgments and analysis when processing long texts. In summary, NSA enables efficient long-text processing through the combined operation of compression, selection, and sliding windows, providing a superior solution for large language models handling long-text tasks.

## 2.3 Perception Training Design:

NSA implements end-to-end training by reducing pre-computation overhead without compromising model quality, enabling efficient use of sparse attention modes throughout the model’s lifecycle.

## 2.4 Hardware-Aligned System:

To achieve FlashAttention-level acceleration during training and pre-filling, the paper implements a hardware-optimized sparse attention kernel in Triton. Block-wise sparse attention is optimized for Tensor Core utilization and memory access patterns, ensuring balanced arithmetic intensity. Specifically, the following optimizations were applied:

- **Blocked Memory Access**: Increases Tensor Core utilization through coalesced loading, reducing redundant key-value (KV) transfers.
- **Cyclic Scheduling**: Intelligent loop ordering within the kernel to eliminate redundant KV transfers.
- **Grouped Data Loading**: For each inner loop, all queries within a group and their shared sparse KV block indices are loaded together.
- **Coalesced KV Fetching**: Within the inner loop, continuous KV blocks are sequentially loaded into SRAM to minimize memory fetches.
- **Grid-Level Outer Loop Scheduling**: Since inner loop lengths are nearly uniform across query blocks, query/output loops are scheduled in Triton’s grid scheduler to simplify and optimize the kernel.

![Table_3](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-09/assets/Figure_3.png  )

> **Figure 3 | NSA kernel design.**  
> The kernel loads queries via GQA group (outer loop), retrieves corresponding sparse key-value blocks (inner loop), and performs attention computation in SRAM.  
> Green blocks represent data in SRAM; blue blocks represent data in HBM.

# 3. Training Settings

## 3.1 Pre-training Configuration

Following state-of-the-art practices in modern large language models (LLMs), their experiments use an architecture combining Grouped Query Attention (GQA) and Mixture of Experts (MoE). The model contains 27 billion parameters, of which 3 billion are active. The model consists of 30 layers with a hidden dimension of 2560. For GQA, the number of groups is set to 4, with a total of 64 attention heads. For each head, the hidden dimensions for queries, keys, and values are configured as $(d_q = d_k = 192)$ and $(d_v = 128)$. For MoE, the DeepSeekMoE structure is used with 72 routing experts and 2 shared experts, with the number of selected experts $(K)$ set to 6. To ensure training stability, the first MoE layer is replaced with an MLP in SwiGLU form.

## 3.2 NSA Architecture Parameters

The architecture achieves an efficient balance between computational cost and model performance. The following parameters are set for NSA:
- Compressed block size $(l = 32)$,
- Sliding window stride $(d = 16)$,
- Selected block size $(l' = 64)$,
- Number of selected blocks $(n = 16)$ (including 1 fixed initial block and 2 local blocks),
- Sliding window size $(w = 512)$.

Both full-attention and sparse-attention models are pre-trained on 270 billion tokens with 8k context length, then fine-tuned on 32k-length texts using YaRN for long-context adaptation. Both models are trained to full convergence to ensure fair comparison.

# 4. Results and Analysis

## 4.1 General Evaluation:

On benchmarks such as MMLU, MMLU-PRO, CMMLU, BBH, GSM8K, MATH, DROP, MBPP, and HumanEval, NSA outperforms the full-attention baseline on most metrics, despite its higher sparsity.

![Table_4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-09/assets/Figure_4.png  )

## 4.2 Long-Context Evaluation:

The authors evaluated the Native Sparse Attention (NSA) method on the LongBench benchmark and compared its performance against state-of-the-art (SOTA) sparse attention methods and the full-attention baseline. To ensure consistent sparsity across methods, researchers set the number of activated tokens per query in all sparse baselines to 2560, exactly matching the average number of tokens activated by NSA when processing 32k-length sequences.

Per StreamLLM rules, the 2560-token budget includes the first 128 tokens and 512 local tokens. Given that some subsets in LongBench exhibit uniformly low scores across all evaluated models and thus provide limited discriminative value for comparing method performance, the authors excluded these subsets from evaluation. Results are shown in the table below. NSA demonstrates exceptionally strong performance, leading with a mean score of 0.469, significantly surpassing all baselines. In particular, its mean score is 0.032 higher than full attention and 0.046 higher than Exact-Top.

![Table_5](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-09/assets/Figure_5.png  )

![Table_6](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-09/assets/Figure_6.png  )

## 4.3 Chain-of-Thought Evaluation:

After supervised fine-tuning, NSA-R outperforms the full-attention baseline (Full Attention-R) on AIME instruction-based reasoning benchmarks at both 8k and 16k context lengths.

![Table_7](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/main/2025/week-09/assets/Figure_7.png  )
> **Table 3** | AIME instruction-based evaluation after supervised training. Our NSA-R demonstrates superior performance over Full Attention-R at both 8k and 16k sequence lengths.  

> **Figure 6** | Comparison of the Triton-based NSA kernel against the Triton-based FlashAttention-2 kernel. Our implementation significantly reduces latency across all context lengths, with improvements becoming more pronounced as input length increases.

# 5. Conclusion

## 5.1 Innovations Achieved in This Work

**Significant Acceleration:**  
Through algorithmic design optimization and modern hardware implementation, NSA achieves substantial acceleration compared to the full-attention model during decoding, forward pass, and backpropagation, particularly for 64k-length sequences.

**End-to-End Training Support:**  
NSA supports end-to-end training, reducing pre-computation overhead without sacrificing model performance, enabling efficient deployment of sparse attention throughout the model’s lifecycle.

**Dynamic Hierarchical Sparsity Strategy:**  
NSA combines coarse token compression and precise token selection to preserve both global contextual awareness and local accuracy.

**Hardware-Aware System Optimization:**  
Balanced arithmetic intensity is achieved by optimizing block-wise sparse attention for Tensor Core utilization and memory access patterns, maximizing real-world efficiency.

**Learning-Aware Algorithm Design:**  
Stable end-to-end training is enabled by efficient algorithms and backward operators that support effective deployment and end-to-end learning.

**Comprehensive Experimental Evaluation:**  
Extensive experiments across diverse text corpora demonstrate that NSA delivers strong performance on general benchmarks, long-context tasks, and chain-of-thought reasoning evaluations, surpassing the full-attention baseline.