# Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention

Когда Маск выпустил Grok 3, а Сэм Альтман все еще колебался, стоит ли открывать исходный код, Лян Вэньфэн, как соавтор, работал с исследовательской группой DeepSeek над созданием шокирующей и сенсационной исследовательской статьи. DeepSeek официально представила свой последний научный прорыв — **Native Sparse Attention (NSA)**! Эта технология имеет большое значение. Она, скорее всего, значительно повысит способность следующего поколения больших языковых моделей обрабатывать длинные тексты, полностью учитывая при этом операционную эффективность. Нет сомнений, что это еще одна веха в области больших языковых моделей (LLM)!

## 1. Предпосылки исследования и общие выводы

Проблема, на решение которой направлена данная статья, заключается в высоких вычислительных затратах на моделирование длинных контекстов. Стандартный механизм внимания имеет высокую вычислительную сложность при обработке длинных последовательностей, что становится узким местом для производительности модели. Трудности исследования этой проблемы включают:

- Как повысить эффективность, сохранив при этом возможности модели.
- Как добиться сквозного обучения, чтобы сократить предварительные вычисления, не жертвуя при этом производительностью модели.

Сопутствующие исследовательские работы по этой проблеме включают:

- Метод исключения кэша KV.
- Метод выбора блочного кэша KV.
- Метод выборки, кластеризации или выбора хэша и т. д.

Однако эти методы часто не позволяют достичь теоретического эффекта ускорения в реальном развертывании и в основном сосредоточены на этапе вывода, не имея эффективной поддержки во время обучения.

### Общий вывод

Архитектура NSA, предложенная в данной статье, обеспечивает ускоренное обучение и вывод, сохраняя при этом полную концентрацию внимания, за счет интеграции иерархического сжатия токенов и поблочного выбора токенов в обучаемую архитектуру. NSA соответствует базовому уровню полного внимания по общим показателям, превосходит возможности моделирования при оценке длительного контекста и улучшает возможности рассуждения, при этом значительно сокращая вычислительную задержку и достигая существенного ускорения.

### Ключевые инновации NSA

1. **Собственная конструкция разреженного внимания**:
   - Позволяет проводить сквозную оптимизацию разреженного шаблона на этапе предварительного обучения.
   - Модуль разреженного внимания может быть синхронно адаптирован с другими компонентами модели, улучшая общую производительность.

2. **Иерархический механизм разреженного внимания**:
   - Умело достигает баланса между локальной обработкой информации и глобальной обработкой информации.
   - Предоставляет более эффективное и комплексное решение для модели обработки длинных текстов.

![Table_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-09/assets/Figure_1.jpg)

> **Рисунок 1 | Сравнение производительности и эффективности между полным вниманием (Full Attention) и нашим NSA.**  
> **Слева:** Несмотря на то, что NSA является разреженной (sparse) моделью, она в среднем превосходит базовый вариант с полным вниманием по общим бенчмаркам, задачам с длинным контекстом и оценке умозаключений.  
> **Справа:** Для обработки последовательностей длиной 64k NSA обеспечивает значительный выигрыш в скорости вычислений по сравнению с полным вниманием на всех этапах: декодировании, прямом проходе и обратном распространении ошибки.

## 2. Методы исследования

В данной статье предлагается NSA (Native Sparse Attention) — аппаратно-ориентированный и обучаемый механизм разреженного внимания, позволяющий решить проблему высоких вычислительных затрат при моделировании длительного контекста.  Чтобы задействовать потенциал внимания в разреженном режиме, NSA заменяет исходные пары «ключ-значение» $(k_t, v_t)$ на более компактный и информативный набор представлений $(\tilde{k}, \tilde{v})$, зависящих от каждого запроса $(q_t)$. Так же инженеры стремились оптимизировать арифметическую интенсивность. 

**Арифметическая интенсивность** — отношение числа вычислительных операций к числу обращений к памяти — становится важным фактором, определяющим оптимизацию на уровне аппаратуры. Для эффективного использования GPU необходимо учитывать этот параметр, стремясь к высокой арифметической интенсивности, чтобы вычисления были ограничены производительностью GPU, а не пропускной способностью памяти. В контексте причинного самовнимания, особенно при авторегрессионном декодировании, проблема пропускной способности памяти становится особенно актуальной из-за необходимости загружать все пространство ключей и значений.

![Table_2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-09/assets/Figure_2.jpg)

> **Рисунок 2 | Общий обзор архитектуры NSA.**  
> **Слева:** Фреймворк обрабатывает входные последовательности через три параллельные ветви внимания. Для заданного запроса предыдущие ключи и значения преобразуются в сжатое внимание (compressed attention) для крупнозернистых паттернов, выборочное внимание (selected attention) для важных токенов и скользящее внимание (sliding attention) для локального контекста.  
> **Справа:** Визуализация различных паттернов внимания, которые формируются каждой ветвью. Зелёные области показывают зоны, где необходимо вычислять коэффициенты внимания, а белые области — это зоны, которые можно пропускать.

### 2.1 Общая схема алгоритма

Для эффективного использования механизма внимания в разреженном режиме в статье предлагается заменить исходные пары ключ-значение в уравнении (1) на более компактное и информационно насыщенное представление. 

Это представление формируется для каждого запроса $( q_t )$ следующим образом:
- Оно основывается на текущем запросе $( q_t )$ и контекстной памяти.
- Динамически строится в зависимости от контекста. 

Таким образом, предлагаемый подход позволяет оптимизировать вычисления и улучшить эффективность работы механизма внимания.

В NSA используются три стратегии отображения $( C = \{ \text{cmp}, \text{slc}, \text{win} \} )$, которые представляют собой сжатие, выбор и скользящее окно соответственно.  Выход внимания в NSA формируется как комбинация результатов этих стратегий:

$$
o'_t = \sum_{c \in C} g^c_t \cdot Attn(q_t, \tilde{k}^c, \tilde{v}^c).
$$

где каждая стратегия $(c \in C)$ генерирует свои $(\tilde{k}^c, \tilde{v}^c)$.  Величина $(g^c_t \in [0,1])$ — это «gate»-оценка для каждой стратегии, вычисляемая на основе входных признаков с помощью MLP и сигмоидной активации. Она управляет вкладом каждой стратегии и общим количеством переназначенных ключей и значений:

$$
N_t = \sum_{c \in C} g^c_t \cdot size[\tilde{k}^c].
$$

Таким образом, NSA поддерживает высокую степень разреженности, гарантируя, что $N_t \ll t$, что позволяет снизить вычислительные затраты и повысить эффективность обработки длинных последовательностей. В частности, оптимизированный выход внимания формально определяется представленными выше уравнениями.

### 2.2 Динамическая многоуровневая разреженная стратегия:

NSA использует динамическую иерархическую разреженную стратегию, которая сочетает в себе грубое сжатие токенов и точный выбор токенов со скользящим окном для сохранения глобальной осведомленности о контексте и локальной точности. Архитектура NSA использует иерархическое моделирование токенов и обрабатывает входную последовательность через три параллельные ветви внимания, которые представляют собой следующие три вещи.

1. Сжатое внимание: имеет дело с крупнозернистыми шаблонами и собирает глобальную информацию путем сжатия блоков токенов.

2. Выбранное внимание: обработка важных блоков токенов и выборочное сохранение подробной информации.

3. Скользящее окно Внимание: Обработка локальной контекстной информации.

#### (1) Крупнозернистое сжатие:

Грубое сжатие сокращает количество токенов, которые необходимо обработать, путем объединения последовательных блоков токенов в представления на уровне блоков. В частности, модель сжимает блоки токенов определенной длины (например, 32 токена) в единое представление, тем самым фиксируя семантическую информацию более высокого уровня. Этот метод сжатия значительно сокращает объем вычислений, особенно при обработке длинных последовательностей, и может эффективно снизить вычислительную сложность механизма внимания. Фактически этот шаг можно понимать как резюмирование текста.

Ниже приведена формула расчета для крупнозернистого сжатия:

$$
K_{t}^{\text{cmp}} \;=\; f_{k}^{\text{cmp}}\bigl(k_{t}\bigr) \;=\; \Bigl\{\,\varphi\bigl(k_{id + 1:id+l}\bigr)\;\Bigm|\;1 \leqslant i \leqslant \Bigl\lfloor\frac{t - l}{d}\Bigr\rfloor \Bigr\}
$$

где:

- **$k_t$** – исходная последовательность векторов-ключей $[k_1, k_2, \dots, k_t]$ до момента $t$.
- **$f_k^{\text{cmp}}$** – функция сжатия, применяемая к исходным ключам.
- **$\varphi$** – обучаемый многослойный персептрон (MLP), который преобразует блок последовательных ключей в один сжатый вектор. При этом может использоваться позиционное кодирование, чтобы учитывать порядок токенов внутри блока.
- **$k_{id+1:id+l}$** – блок из $l$ последовательных ключей, начинающийся с индекса $id+1$ и заканчивающийся индексом $id+l$; параметр $l$ определяет длину блока.
- **$d$** – шаг скольжения между блоками. Если $d < l$, блоки будут перекрываться, что помогает сохранить непрерывность информации.
- **$\lfloor\frac{t-l}{d}\rfloor$** – число полных блоков, которые можно сформировать из последовательности длины $t$.

$$
\tilde{K}_{t}^{\mathrm{cmp}} \;\in\; \mathbb{R}^{\,d_k\times\left\lfloor \frac{t-l}{d} \right\rfloor}
$$

представляет собой тензор, состоящий из сжатых ключей. Обычно мы принимаем d < l, чтобы уменьшить фрагментацию информации. Для сжатых значений
есть похожая формула. Сжатое представление фиксирует более грубую семантическую информацию более высокого уровня и снижает вычислительную нагрузку на внимание.

**Как работает формула:**

1. **Разбиение последовательности:** Исходная последовательность ключей $k_t$ разбивается на блоки длины $l$ с шагом $d$. Например, если $l=32$ и $d=16$, каждый новый блок начинается на 16 токенов дальше предыдущего, а сами блоки могут частично перекрываться.
2. **Применение функции сжатия:** Для каждого блока $k_{id+1:id+l}$ функция $\varphi$ (MLP) преобразует этот набор токенов в один сжатый вектор. Этот процесс позволяет "сжать" информацию из группы токенов в компактное представление, сохраняя при этом важные семантические связи.
3. **Формирование множества сжатых ключей:** Результатом является множество сжатых ключей $K_t^{\text{cmp}}$, состоящее из $\lfloor\frac{t-l}{d}\rfloor$ элементов, что существенно меньше исходного числа токенов $t$. Эти сжатые ключи далее используются в механизме внимания вместо исходных, что значительно снижает вычислительную сложность.

**Интуитивное понимание:**

Представьте, что вам нужно быстро ознакомиться с содержанием длинного текста, например, романа или новостной сводки. Вместо того чтобы читать каждую страницу, вы разбиваете текст на главы и просматриваете краткие резюме каждой из них. Здесь:
- **Исходный текст** – это последовательность ключей $k_t$,
- **Глава** – это блок $k_{id+1:id+l}$,
- **Резюме главы** – это сжатый вектор, полученный посредством $\varphi$.

Такой подход позволяет эффективно обрабатывать большие объёмы информации, сохраняя при этом основную смысловую нагрузку, и значительно экономит вычислительные ресурсы.

**Преимущества данного метода:**

- **Вычислительная эффективность:** Сокращение числа токенов снижает сложность механизма внимания, что особенно важно для длинных последовательностей.
- **Масштабируемость:** Модель способна обрабатывать значительно большие контексты.
- **Адаптивность:** Обучаемая функция $\varphi$ позволяет динамически выделять наиболее важную информацию в каждом блоке, что делает метод более гибким по сравнению с простыми операциями усреднения или пуллинга.

> **Примечание:** В юпитер тетрадке 'CompressedAttention.ipynb' представлен пример использования данного метода.

На основе проведенных экспериментов можно сделать следующие выводы:

## Ключевые выводы при тестировании

1. **Эффективность зависит от длины последовательности:**
   - Для последовательностей <1000 токенов стандартное внимание эффективнее
   - Для последовательностей >10K токенов сжатое внимание дает существенное преимущество
   - Потенциальное ускорение растет с увеличением длины последовательности

2. **Оптимальные параметры сжатия:**
   - Для длинных последовательностей эффективнее использовать большие размеры блоков (256 вместо 32)
   - Оптимальное соотношение размера блока и шага (stride/block_size) близко к 0.5, что обеспечивает хороший баланс между сжатием и сохранением информации

3. **Компромисс между эффективностью и точностью:**
   - Сжатое внимание жертвует гранулярностью внимания на уровне отдельных токенов ради обработки длинных контекстов
   - Для полной архитектуры NSA этот компромисс компенсируется комбинированием с другими механизмами внимания

4. **Аппаратные соображения:**
   - Сжатое внимание не только ускоряет вычисления, но и значительно сокращает требования к памяти
   - Потенциально позволяет обрабатывать контексты длиной 100K+ токенов на стандартном оборудовании

<details> 
    <summary><em><strong>Пример расчета руками: Step-by-Step</strong></em></summary>

## 1. Исходные данные для примера

Рассмотрим последовательность из 16 векторов-ключей, каждый размерности 4. Для наглядности организуем их в 4 семантические группы:

**Группа 1 (токены, описывающие погоду):**
- $k_1 = [0.2, 0.3, 0.8, 0.1]$
- $k_2 = [0.3, 0.2, 0.7, 0.2]$
- $k_3 = [0.1, 0.4, 0.9, 0.1]$
- $k_4 = [0.2, 0.3, 0.8, 0.2]$

**Группа 2 (токены, описывающие место):**
- $k_5 = [0.7, 0.8, 0.2, 0.5]$
- $k_6 = [0.8, 0.9, 0.1, 0.6]$
- $k_7 = [0.6, 0.7, 0.3, 0.4]$
- $k_8 = [0.7, 0.8, 0.2, 0.5]$

**Группа 3 (токены, описывающие время):**
- $k_9 = [0.4, 0.1, 0.3, 0.9]$
- $k_{10} = [0.5, 0.2, 0.2, 0.8]$
- $k_{11} = [0.3, 0.1, 0.4, 0.9]$
- $k_{12} = [0.4, 0.2, 0.3, 0.8]$

**Группа 4 (токены, описывающие действие):**
- $k_{13} = [0.9, 0.5, 0.6, 0.3]$
- $k_{14} = [0.8, 0.6, 0.7, 0.2]$
- $k_{15} = [0.9, 0.4, 0.5, 0.4]$
- $k_{16} = [0.8, 0.5, 0.6, 0.3]$

## 2. Настройка параметров алгоритма

Определим параметры крупнозернистого сжатия:

- **Длина блока $(l) = 4$**: объединяем по 4 последовательных вектора
- **Шаг скольжения $(d) = 2$**: каждый новый блок начинается через 2 позиции от начала предыдущего
- **Функция сжатия $\varphi$**: для простоты примера используем среднее арифметическое векторов в блоке

Согласно формуле, количество сжатых векторов будет:
$$\Bigl\lfloor\frac{t - l}{d}\Bigr\rfloor = \Bigl\lfloor\frac{16 - 4}{2}\Bigr\rfloor = \lfloor 6 \rfloor = 6$$

## 3. Применение формулы крупнозернистого сжатия

Формула крупнозернистого сжатия:
$$K_{t}^{\text{cmp}} = f_{k}^{\text{cmp}}(k_{t}) = \Bigl\{\varphi(k_{id + 1:id+l})\Bigm|1 \leqslant i \leqslant \Bigl\lfloor\frac{t - l}{d}\Bigr\rfloor \Bigr\}$$

Рассчитаем все 6 сжатых векторов, применяя функцию $\varphi$ к каждому блоку:

### Блок 1 (i=1):
Индексы токенов: $id+1 = 1×2+1 = 3$ до $id+l = 1×2+4 = 6$
Токены в блоке: $k_3, k_4, k_5, k_6$

$$\varphi(k_{3:6}) = \frac{k_3 + k_4 + k_5 + k_6}{4}$$

$$= \frac{[0.1, 0.4, 0.9, 0.1] + [0.2, 0.3, 0.8, 0.2] + [0.7, 0.8, 0.2, 0.5] + [0.8, 0.9, 0.1, 0.6]}{4}$$

$$= \frac{[1.8, 2.4, 2.0, 1.4]}{4} = [0.45, 0.6, 0.5, 0.35]$$

### Блок 2 (i=2):
Индексы токенов: $id+1 = 2×2+1 = 5$ до $id+l = 2×2+4 = 8$
Токены в блоке: $k_5, k_6, k_7, k_8$

$$\varphi(k_{5:8}) = \frac{k_5 + k_6 + k_7 + k_8}{4}$$

$$= \frac{[0.7, 0.8, 0.2, 0.5] + [0.8, 0.9, 0.1, 0.6] + [0.6, 0.7, 0.3, 0.4] + [0.7, 0.8, 0.2, 0.5]}{4}$$

$$= \frac{[2.8, 3.2, 0.8, 2.0]}{4} = [0.7, 0.8, 0.2, 0.5]$$

### Блок 3 (i=3):
Индексы токенов: $id+1 = 3×2+1 = 7$ до $id+l = 3×2+4 = 10$
Токены в блоке: $k_7, k_8, k_9, k_{10}$

$$\varphi(k_{7:10}) = \frac{k_7 + k_8 + k_9 + k_{10}}{4}$$

$$= \frac{[0.6, 0.7, 0.3, 0.4] + [0.7, 0.8, 0.2, 0.5] + [0.4, 0.1, 0.3, 0.9] + [0.5, 0.2, 0.2, 0.8]}{4}$$

$$= \frac{[2.2, 1.8, 1.0, 2.6]}{4} = [0.55, 0.45, 0.25, 0.65]$$

### Блок 4 (i=4):
Индексы токенов: $id+1 = 4×2+1 = 9$ до $id+l = 4×2+4 = 12$
Токены в блоке: $k_9, k_{10}, k_{11}, k_{12}$

$$\varphi(k_{9:12}) = \frac{k_9 + k_{10} + k_{11} + k_{12}}{4}$$

$$= \frac{[0.4, 0.1, 0.3, 0.9] + [0.5, 0.2, 0.2, 0.8] + [0.3, 0.1, 0.4, 0.9] + [0.4, 0.2, 0.3, 0.8]}{4}$$

$$= \frac{[1.6, 0.6, 1.2, 3.4]}{4} = [0.4, 0.15, 0.3, 0.85]$$

### Блок 5 (i=5):
Индексы токенов: $id+1 = 5×2+1 = 11$ до $id+l = 5×2+4 = 14$
Токены в блоке: $k_{11}, k_{12}, k_{13}, k_{14}$

$$\varphi(k_{11:14}) = \frac{k_{11} + k_{12} + k_{13} + k_{14}}{4}$$

$$= \frac{[0.3, 0.1, 0.4, 0.9] + [0.4, 0.2, 0.3, 0.8] + [0.9, 0.5, 0.6, 0.3] + [0.8, 0.6, 0.7, 0.2]}{4}$$

$$= \frac{[2.4, 1.4, 2.0, 2.2]}{4} = [0.6, 0.35, 0.5, 0.55]$$

### Блок 6 (i=6):
Индексы токенов: $id+1 = 6×2+1 = 13$ до $id+l = 6×2+4 = 16$
Токены в блоке: $k_{13}, k_{14}, k_{15}, k_{16}$

$$\varphi(k_{13:16}) = \frac{k_{13} + k_{14} + k_{15} + k_{16}}{4}$$

$$= \frac{[0.9, 0.5, 0.6, 0.3] + [0.8, 0.6, 0.7, 0.2] + [0.9, 0.4, 0.5, 0.4] + [0.8, 0.5, 0.6, 0.3]}{4}$$

$$= \frac{[3.4, 2.0, 2.4, 1.2]}{4} = [0.85, 0.5, 0.6, 0.3]$$

## 4. Результат сжатия

Итоговое сжатое представление (обозначим как $c_i$):

- $c_1 = [0.45, 0.6, 0.5, 0.35]$
- $c_2 = [0.7, 0.8, 0.2, 0.5]$
- $c_3 = [0.55, 0.45, 0.25, 0.65]$
- $c_4 = [0.4, 0.15, 0.3, 0.85]$
- $c_5 = [0.6, 0.35, 0.5, 0.55]$
- $c_6 = [0.85, 0.5, 0.6, 0.3]$

## 5. Математическое обоснование и интуитивное понимание

### Ключевые особенности алгоритма:

1. **Разбиение на перекрывающиеся блоки**:
   Обратите внимание, как формируются блоки с перекрытием:
   - Блок 1: $[k_3, k_4, k_5, k_6]$
   - Блок 2: $[k_5, k_6, k_7, k_8]$
   
   Такое перекрытие (общие токены $k_5$ и $k_6$) обеспечивает непрерывность информации между блоками.

2. **Непрерывное представление на стыке семантических групп**:
   - $c_1$ содержит информацию о погоде (группа 1) и месте (группа 2)
   - $c_3$ объединяет информацию о месте (группа 2) и времени (группа 3)
   - $c_5$ связывает информацию о времени (группа 3) и действии (группа 4)

3. **Сохранение семантической информации**:
   Для блоков, совпадающих с семантическими группами:
   - $c_2 = [0.7, 0.8, 0.2, 0.5]$ точно представляет группу "место"
   - $c_4 = [0.4, 0.15, 0.3, 0.85]$ точно представляет группу "время"
   - $c_6 = [0.85, 0.5, 0.6, 0.3]$ точно представляет группу "действие"

### Схематическое представление процесса:

Если представить семантические группы буквами:
```
[A][A][A][A][B][B][B][B][C][C][C][C][D][D][D][D]
 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
```

То процесс формирования блоков выглядит так:
```
    [--Блок 1--]                    → c₁ (A+B)
        [--Блок 2--]                → c₂ (B)
            [--Блок 3--]            → c₃ (B+C)
                [--Блок 4--]        → c₄ (C)
                    [--Блок 5--]    → c₅ (C+D)
                        [--Блок 6--] → c₆ (D)
```

## 6. Эффективность сжатия

В нашем примере:
- Исходный размер: 16 векторов × 4 значения = 64 числовых значения
- Сжатый размер: 6 векторов × 4 значения = 24 числовых значения
- Коэффициент сжатия: 64/24 = 2.67 (сокращение на 62.5%)

В реальных системах с большими последовательностями (например, 10,000 токенов) и блоками размером 256 с шагом 128, коэффициент сжатия может достигать 100 и более, что драматически снижает вычислительную сложность механизма внимания.

## 7. Реальная реализация функции сжатия

В практических системах вместо простого среднего арифметического используется многослойный персептрон (MLP):

$$\varphi(k_{id+1:id+l}) = \text{MLP}([k_{id+1} \oplus \text{pe}_1; k_{id+2} \oplus \text{pe}_2; \ldots; k_{id+l} \oplus \text{pe}_l])$$

где:
- $\oplus$ - операция конкатенации
- $\text{pe}_j$ - позиционное кодирование для j-й позиции в блоке
- $\text{MLP}$ - нейронная сеть, обучаемая выделять наиболее важную информацию в блоке

Позиционное кодирование помогает учитывать порядок токенов внутри блока и может быть реализовано через синусоидальные функции:

$$\text{pe}_{i,2j} = \sin(i/10000^{2j/d_k})$$
$$\text{pe}_{i,2j+1} = \cos(i/10000^{2j/d_k})$$

Этот метод крупнозернистого сжатия позволяет значительно расширить контекстное окно моделей трансформерного типа, сохраняя при этом вычислительную эффективность.
</details> 

## Вывод

Эксперимент убедительно демонстрирует, что механизм сжатого внимания является эффективным решением проблемы квадратичной сложности стандартного внимания для длинных последовательностей. Фактическое ускорение составило (4.69×), оно будет увеличиваться с ростом длины последовательности.

#### (2) Выбранное внимание:

Детальный выбор дополнительно выбирает наиболее релевантные блоки токенов для расчета внимания на основе сжатия. Вычисляя оценку важности для каждого фрагмента, модель сохраняет наиболее важные фрагменты токенов и игнорирует те, которые не важны для текущего запроса. Этот механизм выбора гарантирует, что модель сможет сохранить ключевую локальную информацию при обработке длинных последовательностей и избежать потери важных деталей из-за сжатия. Формула выглядит следующим образом: