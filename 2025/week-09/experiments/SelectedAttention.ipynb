{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5yu93BhXw65Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from typing import List, Tuple, Optional, Union\n",
        "\n",
        "class SelectedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "      –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤—ã–±–æ—Ä–æ—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (Selected Attention) –∏–∑ –º–µ—Ç–æ–¥–∞ NSA.\n",
        "\n",
        "    –í—ã–±–æ—Ä–æ—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤:\n",
        "    1. –°–∂–∏–º–∞–µ—Ç –±–ª–æ–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∫–∞–∫ –≤ CompressedAttention\n",
        "    2. –í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ (p_t^slc)\n",
        "    3. –í—ã–±–∏—Ä–∞–µ—Ç —Ç–æ–ø-n –±–ª–æ–∫–æ–≤ —Å –Ω–∞–∏–≤—ã—Å—à–∏–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏ (I_t)\n",
        "    4. –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤\n",
        "    5. –í—ã—á–∏—Å–ª—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —ç—Ç–∏—Ö –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö\n",
        "\n",
        "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
        "        hidden_size (int): –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
        "        block_size (int): –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ –¥–ª—è —Å–∂–∞—Ç–∏—è (–ø–∞—Ä–∞–º–µ—Ç—Ä l –≤ —Å—Ç–∞—Ç—å–µ)\n",
        "        stride (int): –®–∞–≥ –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏ (–ø–∞—Ä–∞–º–µ—Ç—Ä d –≤ —Å—Ç–∞—Ç—å–µ)\n",
        "        num_heads (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "        num_selected_blocks (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤ –¥–ª—è –≤—ã–±–æ—Ä–∞ (–ø–∞—Ä–∞–º–µ—Ç—Ä n –≤ —Å—Ç–∞—Ç—å–µ)\n",
        "        dropout (float): –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥—Ä–æ–ø–∞—É—Ç–∞\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int,\n",
        "        block_size: int = 32,\n",
        "        stride: int = 16,\n",
        "        num_heads: int = 4,\n",
        "        num_selected_blocks: int = 4,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super(SelectedAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.block_size = block_size\n",
        "        self.stride = stride\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_size // num_heads\n",
        "        self.num_selected_blocks = num_selected_blocks\n",
        "\n",
        "        # –ü—Ä–æ–µ–∫—Ü–∏–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        # –ü—Ä–æ–µ–∫—Ü–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–∞\n",
        "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        # –§—É–Ω–∫—Ü–∏—è —Å–∂–∞—Ç–∏—è œÜ (MLP –¥–ª—è —Å–∂–∞—Ç–∏—è –±–ª–æ–∫–æ–≤)\n",
        "        self.block_compressor = nn.Sequential(\n",
        "            nn.Linear(block_size * self.head_dim, 2 * self.head_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(2 * self.head_dim, self.head_dim)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False\n",
        "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, List, List]]:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "          –í—ã–ø–æ–ª–Ω—è–µ—Ç –≤—ã–±–æ—Ä–æ—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞–¥ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é.\n",
        "\n",
        "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
        "            hidden_states: —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
        "            attention_mask: –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "            output_attentions: —Ñ–ª–∞–≥ –¥–ª—è –≤—ã–≤–æ–¥–∞ –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "\n",
        "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "            output: —Ç–µ–Ω–∑–æ—Ä –≤—ã—Ö–æ–¥–∞ —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
        "            attention_weights (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ): –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "            selection_info (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ): –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–∞—Ö\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "        device = hidden_states.device\n",
        "\n",
        "        # –®–∞–≥ 1: –ü—Ä–æ–µ–∫—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "        q = self.q_proj(hidden_states)  # (batch_size, seq_len, hidden_size)\n",
        "        k = self.k_proj(hidden_states)  # (batch_size, seq_len, hidden_size)\n",
        "        v = self.v_proj(hidden_states)  # (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, head_dim)\n",
        "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, head_dim)\n",
        "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, head_dim)\n",
        "\n",
        "        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –≤—Å–µ—Ö –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "        context_layers = []\n",
        "        attention_weights = []\n",
        "        selection_info = []\n",
        "\n",
        "        for h in range(self.num_heads):\n",
        "            # –®–∞–≥ 2: –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –±–ª–æ–∫–∏ –∏ —Å–∂–∞—Ç–∏–µ\n",
        "            blocks_k, block_indices = self._get_blocks(k[:, h])  # –ü–æ–ª—É—á–∞–µ–º –±–ª–æ–∫–∏ –∫–ª—é—á–µ–π\n",
        "            blocks_v, _ = self._get_blocks(v[:, h])              # –ü–æ–ª—É—á–∞–µ–º –±–ª–æ–∫–∏ –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "\n",
        "            # –°–∂–∏–º–∞–µ–º –±–ª–æ–∫–∏ –∫–ª—é—á–µ–π —Å –ø–æ–º–æ—â—å—é MLP\n",
        "            compressed_k = self._compress_blocks(blocks_k)       # (batch_size, num_blocks, head_dim)\n",
        "\n",
        "            # –®–∞–≥ 3: –í—ã–±–æ—Ä –≤–∞–∂–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å –∑–∞–ø—Ä–æ—Å–æ–º\n",
        "            # –§–æ—Ä–º—É–ª–∞: p_t^slc = Softmax(q_t^T * K_t^cmp)\n",
        "            scores = torch.matmul(q[:, h], compressed_k.transpose(-1, -2)) * self.scale        # (batch_size, seq_len, num_blocks)\n",
        "            block_importance = F.softmax(scores, dim=-1)                                       # (batch_size, seq_len, num_blocks)\n",
        "\n",
        "            # –®–∞–≥ 4: –í—ã–±–æ—Ä –±–ª–æ–∫–æ–≤ —Å –Ω–∞–∏–≤—ã—Å—à–∏–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏\n",
        "            # –§–æ—Ä–º—É–ª–∞: I_t = {i | rank(p_t^slc[i]) <= n}\n",
        "            num_blocks = len(block_indices)\n",
        "            num_to_select = min(self.num_selected_blocks, num_blocks)\n",
        "\n",
        "            # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-k –±–ª–æ–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n",
        "            _, selected_block_indices = torch.topk(block_importance, k=num_to_select, dim=-1)  # (batch_size, seq_len, num_to_select)\n",
        "\n",
        "            # –®–∞–≥ 5: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤\n",
        "            head_context = self._compute_attention_with_selected_blocks(\n",
        "                q[:, h],                # (batch_size, seq_len, head_dim)\n",
        "                k[:, h],                # (batch_size, seq_len, head_dim)\n",
        "                v[:, h],                # (batch_size, seq_len, head_dim)\n",
        "                block_indices,          # –°–ø–∏—Å–æ–∫ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –∏–Ω–¥–µ–∫—Å–æ–≤\n",
        "                selected_block_indices  # (batch_size, seq_len, num_to_select)\n",
        "            )\n",
        "\n",
        "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "            context_layers.append(head_context)\n",
        "\n",
        "            if output_attentions:\n",
        "                attention_weights.append(block_importance)\n",
        "                selection_info.append((selected_block_indices, block_indices))\n",
        "\n",
        "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "        context_layer = torch.stack(context_layers, dim=1)                                    # (batch_size, num_heads, seq_len, head_dim)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()                        # (batch_size, seq_len, num_heads, head_dim)\n",
        "        context_layer = context_layer.view(batch_size, seq_len, self.hidden_size)             # (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è\n",
        "        output = self.out_proj(context_layer)\n",
        "\n",
        "        if output_attentions:\n",
        "            return output, attention_weights, selection_info\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def _get_blocks(self, x: torch.Tensor) -> Tuple[List[torch.Tensor], List[Tuple[int, int]]]:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "          –†–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –±–ª–æ–∫–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –∏ —à–∞–≥–æ–º.\n",
        "\n",
        "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
        "            x: —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, head_dim)\n",
        "\n",
        "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "            blocks: —Å–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤\n",
        "            block_indices: —Å–ø–∏—Å–æ–∫ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, head_dim = x.shape\n",
        "        blocks = []\n",
        "        block_indices = []\n",
        "\n",
        "        for i in range(0, seq_len - self.block_size + 1, self.stride):\n",
        "            block = x[:, i:i+self.block_size, :]  # (batch_size, block_size, head_dim)\n",
        "            blocks.append(block)\n",
        "            block_indices.append((i, i+self.block_size))\n",
        "\n",
        "        return blocks, block_indices\n",
        "\n",
        "    def _compress_blocks(self, blocks: List[torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "          –°–∂–∏–º–∞–µ—Ç –±–ª–æ–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –µ–¥–∏–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é MLP.\n",
        "\n",
        "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
        "            blocks: —Å–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ —Ñ–æ—Ä–º—ã (batch_size, block_size, head_dim)\n",
        "\n",
        "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "            compressed_blocks: —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, num_blocks, head_dim)\n",
        "        \"\"\"\n",
        "        batch_size = blocks[0].shape[0]\n",
        "        num_blocks = len(blocks)\n",
        "\n",
        "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –±–ª–æ–∫–∏ –≤ –æ–¥–∏–Ω —Ç–µ–Ω–∑–æ—Ä\n",
        "        blocks_tensor = torch.cat([block.unsqueeze(1) for block in blocks], dim=1)     # (batch_size, num_blocks, block_size, head_dim)\n",
        "\n",
        "        # –†–µ—à–µ–π–ø –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ MLP\n",
        "        blocks_tensor = blocks_tensor.reshape(batch_size * num_blocks, -1)             # (batch_size * num_blocks, block_size * head_dim)\n",
        "\n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∂–∞—Ç–∏–µ (—Ñ—É–Ω–∫—Ü–∏—è œÜ –∏–∑ —Å—Ç–∞—Ç—å–∏)\n",
        "        compressed = self.block_compressor(blocks_tensor)                              # (batch_size * num_blocks, head_dim)\n",
        "\n",
        "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫ –Ω—É–∂–Ω–æ–π —Ñ–æ—Ä–º–µ\n",
        "        compressed_blocks = compressed.reshape(batch_size, num_blocks, self.head_dim)  # (batch_size, num_blocks, head_dim)\n",
        "\n",
        "        return compressed_blocks\n",
        "\n",
        "    def _compute_attention_with_selected_blocks(\n",
        "        self,\n",
        "        queries: torch.Tensor,                # (batch_size, seq_len, head_dim)\n",
        "        keys: torch.Tensor,                   # (batch_size, seq_len, head_dim)\n",
        "        values: torch.Tensor,                 # (batch_size, seq_len, head_dim)\n",
        "        block_indices: List[Tuple[int, int]],\n",
        "        selected_block_indices: torch.Tensor  # (batch_size, seq_len, num_selected)\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "          –í—ã—á–∏—Å–ª—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –±–ª–æ–∫–∏.\n",
        "\n",
        "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
        "            queries: —Ç–µ–Ω–∑–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤\n",
        "            keys: —Ç–µ–Ω–∑–æ—Ä –∫–ª—é—á–µ–π\n",
        "            values: —Ç–µ–Ω–∑–æ—Ä –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "            block_indices: —Å–ø–∏—Å–æ–∫ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –∏–Ω–¥–µ–∫—Å–æ–≤ –±–ª–æ–∫–æ–≤\n",
        "            selected_block_indices: –∏–Ω–¥–µ–∫—Å—ã –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n",
        "\n",
        "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "            context: –≤—ã—Ö–æ–¥ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –¥–∞–Ω–Ω–æ–π –≥–æ–ª–æ–≤—ã\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, head_dim = queries.shape\n",
        "        num_selected = selected_block_indices.size(-1)\n",
        "        device = queries.device\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "        context = torch.zeros(batch_size, seq_len, head_dim, device=device)\n",
        "\n",
        "        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ –±–∞—Ç—á–µ\n",
        "        for b in range(batch_size):\n",
        "            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n",
        "            for q_idx in range(seq_len):\n",
        "                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n",
        "                block_idx_list = selected_block_indices[b, q_idx].tolist()\n",
        "\n",
        "                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∏–Ω–¥–µ–∫—Å—ã —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤\n",
        "                token_indices = []\n",
        "                for block_idx in block_idx_list:\n",
        "                    if block_idx < len(block_indices):\n",
        "                        start, end = block_indices[block_idx]\n",
        "                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∏–Ω–¥–µ–∫—Å—ã –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "                        if start < seq_len and end <= seq_len:\n",
        "                            token_indices.extend(list(range(start, end)))\n",
        "\n",
        "                # –ï—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å\n",
        "                if not token_indices:\n",
        "                    continue\n",
        "\n",
        "                # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º\n",
        "                token_indices = sorted(set(token_indices))\n",
        "\n",
        "                # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "                q = queries[b, q_idx].unsqueeze(0)        # (1, head_dim)\n",
        "                k_selected = keys[b, token_indices, :]    # (num_tokens, head_dim)\n",
        "                v_selected = values[b, token_indices, :]  # (num_tokens, head_dim)\n",
        "\n",
        "                # –í—ã—á–∏—Å–ª—è–µ–º –≤–Ω–∏–º–∞–Ω–∏–µ\n",
        "                attention_scores = torch.matmul(q, k_selected.transpose(0, 1)) * self.scale  # (1, num_tokens)\n",
        "                attention_weights = F.softmax(attention_scores, dim=-1)                      # (1, num_tokens)\n",
        "                attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "                # –í—ã—á–∏—Å–ª—è–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—É—é —Å—É–º–º—É\n",
        "                context[b, q_idx] = torch.matmul(attention_weights, v_selected).squeeze(0)   # (head_dim)\n",
        "\n",
        "        return context\n",
        "\n",
        "\n",
        "def demonstrate_selected_attention(use_long_sequence=False):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "      –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤—ã–±–æ—Ä–æ—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.\n",
        "\n",
        "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
        "        use_long_sequence: –µ—Å–ª–∏ True, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª–∏–Ω–æ–π 32K —Ç–æ–∫–µ–Ω–æ–≤\n",
        "    \"\"\"\n",
        "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
        "    hidden_size = 64\n",
        "    num_heads = 1            # –î–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–Ω—É –≥–æ–ª–æ–≤—É\n",
        "    batch_size = 1\n",
        "    num_selected_blocks = 4  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–±–∏—Ä–∞–µ–º—ã—Ö –±–ª–æ–∫–æ–≤\n",
        "\n",
        "    if use_long_sequence:\n",
        "        seq_len = 32000\n",
        "        block_size = 256\n",
        "        stride = 128\n",
        "    else:\n",
        "        seq_len = 16000\n",
        "        block_size = 128\n",
        "        stride = 64\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ú–ï–•–ê–ù–ò–ó–ú–ê –í–´–ë–û–†–û–ß–ù–û–ì–û –í–ù–ò–ú–ê–ù–ò–Ø (SELECTED ATTENTION)\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(f\"üìå –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\")\n",
        "    print(f\"  - –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è: {hidden_size}\")\n",
        "    print(f\"  - –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞: {block_size}\")\n",
        "    print(f\"  - –®–∞–≥: {stride}\")\n",
        "    print(f\"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è: {num_heads}\")\n",
        "    print(f\"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–±–∏—Ä–∞–µ–º—ã—Ö –±–ª–æ–∫–æ–≤: {num_selected_blocks}\")\n",
        "    print(f\"  - –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {seq_len}\\n\")\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "    model = SelectedAttention(\n",
        "        hidden_size=hidden_size,\n",
        "        block_size=block_size,\n",
        "        stride=stride,\n",
        "        num_heads=num_heads,\n",
        "        num_selected_blocks=num_selected_blocks\n",
        "    )\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏\n",
        "    print(f\"üìå –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏...\")\n",
        "\n",
        "    # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # –ë–∞–∑–æ–≤—ã–π –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä\n",
        "    x = torch.zeros(batch_size, seq_len, hidden_size)\n",
        "\n",
        "    # –ó–∞–ø–æ–ª–Ω—è–µ–º —à—É–º–æ–º (–ø–æ —á–∞—Å—Ç—è–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)\n",
        "    chunk_size = 1000 if use_long_sequence else seq_len\n",
        "    for i in range(0, seq_len, chunk_size):\n",
        "        end = min(i + chunk_size, seq_len)\n",
        "        x[:, i:end, :] = torch.randn(batch_size, end-i, hidden_size) * 0.1\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º \"–≤–∞–∂–Ω—ã–µ\" —Ç–æ–∫–µ–Ω—ã —á–µ—Ä–µ–∑ —Ä–∞–≤–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–∫–∏\n",
        "    important_interval = 1000 if use_long_sequence else 16\n",
        "    for pos in range(0, seq_len, important_interval):\n",
        "        if pos < seq_len:\n",
        "            x[:, pos, :] = torch.ones(hidden_size)\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä –≤–∞–∂–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ\n",
        "    middle_start = seq_len // 3\n",
        "    cluster_positions = [(middle_start, middle_start + 20)]\n",
        "\n",
        "    for start, end in cluster_positions:\n",
        "        for pos in range(start, min(end, seq_len)):\n",
        "            x[:, pos, :] = torch.ones(hidden_size) * 0.8\n",
        "\n",
        "    # –í—ã—á–∏—Å–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ (—Å—É–º–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π)\n",
        "    token_importance = x.sum(dim=2).squeeze().cpu().numpy()\n",
        "\n",
        "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤\n",
        "    print(f\"üìå –í–∞–∂–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ (—Ñ—Ä–∞–≥–º–µ–Ω—Ç):\")\n",
        "    start_idx = middle_start - 8\n",
        "    end_idx = min(middle_start + 28, seq_len)\n",
        "    for i in range(start_idx, end_idx, 4):\n",
        "        end = min(i + 4, end_idx)\n",
        "        values = [f\"{token_importance[j]:4.1f}\" for j in range(i, end)]\n",
        "        print(f\"  –ü–æ–∑–∏—Ü–∏–∏ {i:3d}-{end-1:3d}: {' '.join(values)}\")\n",
        "    print()\n",
        "\n",
        "    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –º–æ–¥–µ–ª–∏\n",
        "    print(f\"üìå –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ –º–æ–¥–µ–ª–∏...\")\n",
        "    output, attention_weights, selection_info = model(x, output_attentions=True)\n",
        "\n",
        "    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –±–ª–æ–∫–∏\n",
        "    selected_indices, block_indices = selection_info[0]   # –ë–µ—Ä–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã\n",
        "    block_importance = attention_weights[0][0].detach().cpu().numpy()  # –ó–Ω–∞—á–∏–º–æ—Å—Ç—å –±–ª–æ–∫–æ–≤\n",
        "\n",
        "    print(f\"üìå –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã–±–æ—Ä–æ—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è:\")\n",
        "\n",
        "    # –í—ã–±–∏—Ä–∞–µ–º –∑–∞–ø—Ä–æ—Å –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞ –≤–∞–∂–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
        "    query_idx = middle_start + 10\n",
        "\n",
        "    print(f\"\\n  –ê–Ω–∞–ª–∏–∑ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –≤ –ø–æ–∑–∏—Ü–∏–∏ {query_idx} (–≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ –≤–∞–∂–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤):\")\n",
        "\n",
        "    # –ü–æ–ª—É—á–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –±–ª–æ–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n",
        "    selected_blocks = selected_indices[0, query_idx].cpu().numpy()\n",
        "\n",
        "    print(f\"  –í—ã–±—Ä–∞–Ω–Ω—ã–µ –±–ª–æ–∫–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ {query_idx}:\")\n",
        "    for i, block_idx in enumerate(selected_blocks):\n",
        "        start, end = block_indices[block_idx]\n",
        "        importance = block_importance[query_idx, block_idx]\n",
        "        print(f\"    {i+1}. –ë–ª–æ–∫ {block_idx} (–ø–æ–∑–∏—Ü–∏–∏ {start}-{end}): –≤–∞–∂–Ω–æ—Å—Ç—å = {importance:.4f}\")\n",
        "\n",
        "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤\n",
        "    num_blocks = len(block_indices)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 1: –í–∞–∂–Ω–æ—Å—Ç—å –±–ª–æ–∫–æ–≤ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n",
        "    block_importances = block_importance[query_idx]\n",
        "    color_map = ['lightgray'] * num_blocks\n",
        "    for idx in selected_blocks:\n",
        "        color_map[idx] = 'blue'\n",
        "\n",
        "    ax1.bar(range(num_blocks), block_importances, color=color_map)\n",
        "    ax1.set_title(f'–í–∞–∂–Ω–æ—Å—Ç—å –±–ª–æ–∫–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –≤ –ø–æ–∑–∏—Ü–∏–∏ {query_idx}')\n",
        "    ax1.set_xlabel('–ò–Ω–¥–µ–∫—Å –±–ª–æ–∫–∞')\n",
        "    ax1.set_ylabel('–ó–Ω–∞—á–∏–º–æ—Å—Ç—å –±–ª–æ–∫–∞')\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Ä–æ–≥ –≤—ã–±–æ—Ä–∞\n",
        "    sorted_importances = sorted(block_importances, reverse=True)\n",
        "    threshold = sorted_importances[min(num_selected_blocks, len(sorted_importances)-1)]\n",
        "    ax1.axhline(y=threshold, color='red', linestyle='--',\n",
        "               label=f'–ü–æ—Ä–æ–≥ –≤—ã–±–æ—Ä–∞ ({num_selected_blocks} –±–ª–æ–∫–æ–≤)')\n",
        "    ax1.legend()\n",
        "\n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è 2: –†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤\n",
        "    ax2.plot(range(seq_len), token_importance, color='gray', alpha=0.7, label='–í–∞–∂–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤')\n",
        "\n",
        "    # –í—ã–¥–µ–ª—è–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –±–ª–æ–∫–∏\n",
        "    for block_idx in selected_blocks:\n",
        "        start, end = block_indices[block_idx]\n",
        "        ax2.axvspan(start, end, color='blue', alpha=0.3)\n",
        "        ax2.text(start + (end-start)/2, max(token_importance)*0.9,\n",
        "                f'–ë–ª–æ–∫ {block_idx}', ha='center', va='center',\n",
        "                bbox=dict(facecolor='white', alpha=0.7))\n",
        "\n",
        "    # –í—ã–¥–µ–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å\n",
        "    ax2.axvline(x=query_idx, color='red', linestyle='-', label='–¢–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å')\n",
        "\n",
        "    ax2.set_title('–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤')\n",
        "    ax2.set_xlabel('–ü–æ–∑–∏—Ü–∏—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏')\n",
        "    ax2.set_ylabel('–í–∞–∂–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–∞')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏\n",
        "    print(f\"\\nüìå –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏:\")\n",
        "\n",
        "    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: O(seq_len^2)\n",
        "    standard_complexity = seq_len * seq_len\n",
        "\n",
        "    # –°–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: O(seq_len * num_blocks)\n",
        "    compressed_complexity = seq_len * num_blocks\n",
        "\n",
        "    # –í—ã–±–æ—Ä–æ—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: O(seq_len * num_selected_blocks * block_size)\n",
        "    selected_complexity = seq_len * num_selected_blocks * block_size\n",
        "\n",
        "    print(f\"  - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: O(seq_len^2) = {standard_complexity:,}\")\n",
        "    print(f\"  - –°–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: O(seq_len * num_blocks) = {compressed_complexity:,}\")\n",
        "    print(f\"  - –í—ã–±–æ—Ä–æ—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: O(seq_len * num_selected_blocks * block_size) = {selected_complexity:,}\")\n",
        "    print(f\"  - –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è: {standard_complexity / selected_complexity:.2f}x\")\n",
        "    print(f\"  - –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–∂–∞—Ç–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è: {compressed_complexity / selected_complexity:.2f}x\")\n",
        "\n",
        "    # –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n",
        "    print(\"\\nüìå –ó–∞–∫–ª—é—á–µ–Ω–∏–µ:\")\n",
        "    print(\"  - –ú–µ—Ö–∞–Ω–∏–∑–º –≤—ã–±–æ—Ä–æ—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –≤–∞–∂–Ω—ã–µ –±–ª–æ–∫–∏\")\n",
        "    print(f\"  - –ò–∑ {num_blocks} –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –≤—ã–±–∏—Ä–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ {num_selected_blocks} –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö\")\n",
        "    print(\"  - –≠—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤–∞–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\")\n",
        "    print(\"  - –í—ã–±–æ—Ä–æ—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\")\n",
        "    print(f\"  - –ü—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–æ–ª—å–∫–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–µ—Ç\")\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –ó–∞–ø—É—Å–∫–∞–µ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é –Ω–∞ –∫–æ—Ä–æ—Ç–∫–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "fig = demonstrate_selected_attention(use_long_sequence=False)\n",
        "plt.savefig('selected_attention_visualization.png')\n",
        "print(\"\\n–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª 'selected_attention_visualization.png'\")\n",
        "\n",
        "plt.close(fig)\n",
        "\n",
        "run_long_test = input(\"\\n–•–æ—Ç–∏—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç –Ω–∞ –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (32K —Ç–æ–∫–µ–Ω–æ–≤)? (y/n): \")\n",
        "\n",
        "if run_long_test.lower() == 'y':\n",
        "    print(\"\\n–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ –Ω–∞ –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è...\")\n",
        "    try:\n",
        "        long_fig = demonstrate_selected_attention(use_long_sequence=True)\n",
        "        plt.savefig('selected_attention_long.png')\n",
        "        print(\"\\n–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª 'selected_attention_long.png'\")\n",
        "        plt.close(long_fig)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {e}\")\n",
        "        print(\"–í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–∞–∫–æ–π –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\")\n",
        "else:\n",
        "    print(\"\\n–¢–µ—Å—Ç –Ω–∞ –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–ø—É—â–µ–Ω.\")"
      ],
      "metadata": {
        "id": "hZ5RBbDrzljy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce73da0f-5d0d-4f03-de5b-a2b8c1019424"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ú–ï–•–ê–ù–ò–ó–ú–ê –í–´–ë–û–†–û–ß–ù–û–ì–û –í–ù–ò–ú–ê–ù–ò–Ø (SELECTED ATTENTION)\n",
            "================================================================================\n",
            "\n",
            "üìå –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\n",
            "  - –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è: 64\n",
            "  - –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞: 128\n",
            "  - –®–∞–≥: 64\n",
            "  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è: 1\n",
            "  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–±–∏—Ä–∞–µ–º—ã—Ö –±–ª–æ–∫–æ–≤: 4\n",
            "  - –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: 16000\n",
            "\n",
            "üìå –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏...\n",
            "üìå –í–∞–∂–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ (—Ñ—Ä–∞–≥–º–µ–Ω—Ç):\n",
            "  –ü–æ–∑–∏—Ü–∏–∏ 5325-5328: -1.0  0.3 -1.1 64.0\n",
            "  –ü–æ–∑–∏—Ü–∏–∏ 5329-5332: -1.3 -0.3  0.6  0.4\n",
            "  –ü–æ–∑–∏—Ü–∏–∏ 5333-5336: 51.2 51.2 51.2 51.2\n",
            "  –ü–æ–∑–∏—Ü–∏–∏ 5337-5340: 51.2 51.2 51.2 51.2\n",
            "  –ü–æ–∑–∏—Ü–∏–∏ 5341-5344: 51.2 51.2 51.2 51.2\n",
            "  –ü–æ–∑–∏—Ü–∏–∏ 5345-5348: 51.2 51.2 51.2 51.2\n",
            "  –ü–æ–∑–∏—Ü–∏–∏ 5349-5352: 51.2 51.2 51.2 51.2\n",
            "  –ü–æ–∑–∏—Ü–∏–∏ 5353-5356:  0.2 -0.1 -0.7  0.3\n",
            "  –ü–æ–∑–∏—Ü–∏–∏ 5357-5360:  0.9  0.7  1.1 64.0\n",
            "\n",
            "üìå –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ –º–æ–¥–µ–ª–∏...\n",
            "üìå –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã–±–æ—Ä–æ—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è:\n",
            "\n",
            "  –ê–Ω–∞–ª–∏–∑ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –≤ –ø–æ–∑–∏—Ü–∏–∏ 5343 (–≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ –≤–∞–∂–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤):\n",
            "  –í—ã–±—Ä–∞–Ω–Ω—ã–µ –±–ª–æ–∫–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ 5343:\n",
            "    1. –ë–ª–æ–∫ 7 (–ø–æ–∑–∏—Ü–∏–∏ 448-576): –≤–∞–∂–Ω–æ—Å—Ç—å = 0.0041\n",
            "    2. –ë–ª–æ–∫ 95 (–ø–æ–∑–∏—Ü–∏–∏ 6080-6208): –≤–∞–∂–Ω–æ—Å—Ç—å = 0.0041\n",
            "    3. –ë–ª–æ–∫ 128 (–ø–æ–∑–∏—Ü–∏–∏ 8192-8320): –≤–∞–∂–Ω–æ—Å—Ç—å = 0.0041\n",
            "    4. –ë–ª–æ–∫ 122 (–ø–æ–∑–∏—Ü–∏–∏ 7808-7936): –≤–∞–∂–Ω–æ—Å—Ç—å = 0.0041\n",
            "\n",
            "üìå –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏:\n",
            "  - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: O(seq_len^2) = 256,000,000\n",
            "  - –°–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: O(seq_len * num_blocks) = 3,984,000\n",
            "  - –í—ã–±–æ—Ä–æ—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: O(seq_len * num_selected_blocks * block_size) = 8,192,000\n",
            "  - –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è: 31.25x\n",
            "  - –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–∂–∞—Ç–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è: 0.49x\n",
            "\n",
            "üìå –ó–∞–∫–ª—é—á–µ–Ω–∏–µ:\n",
            "  - –ú–µ—Ö–∞–Ω–∏–∑–º –≤—ã–±–æ—Ä–æ—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –≤–∞–∂–Ω—ã–µ –±–ª–æ–∫–∏\n",
            "  - –ò–∑ 249 –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –≤—ã–±–∏—Ä–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ 4 –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö\n",
            "  - –≠—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤–∞–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
            "  - –í—ã–±–æ—Ä–æ—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
            "  - –ü—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–æ–ª—å–∫–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–µ—Ç\n",
            "\n",
            "–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª 'selected_attention_visualization.png'\n",
            "\n",
            "–•–æ—Ç–∏—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç –Ω–∞ –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (32K —Ç–æ–∫–µ–Ω–æ–≤)? (y/n): n\n",
            "\n",
            "–¢–µ—Å—Ç –Ω–∞ –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–ø—É—â–µ–Ω.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5XIzG1-91Byh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}