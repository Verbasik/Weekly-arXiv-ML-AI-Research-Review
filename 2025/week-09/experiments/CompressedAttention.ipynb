{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "class CompressedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "      –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ —Å–∂–∞—Ç–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (Compressed Attention) –∏–∑ –º–µ—Ç–æ–¥–∞ NSA\n",
        "\n",
        "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
        "        hidden_size (int): –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
        "        block_size (int): –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ –¥–ª—è —Å–∂–∞—Ç–∏—è (–ø–∞—Ä–∞–º–µ—Ç—Ä l –≤ —Å—Ç–∞—Ç—å–µ)\n",
        "        stride (int): –®–∞–≥ –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏ (–ø–∞—Ä–∞–º–µ—Ç—Ä d –≤ —Å—Ç–∞—Ç—å–µ)\n",
        "        num_heads (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "        dropout (float): –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥—Ä–æ–ø–∞—É—Ç–∞\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, block_size=32, stride=16, num_heads=4, dropout=0.1):\n",
        "        super(CompressedAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.block_size = block_size\n",
        "        self.stride = stride\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_size // num_heads\n",
        "\n",
        "        # –ü—Ä–æ–µ–∫—Ü–∏–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        # –ü—Ä–æ–µ–∫—Ü–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–∞\n",
        "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        # –§—É–Ω–∫—Ü–∏—è —Å–∂–∞—Ç–∏—è œÜ (MLP –¥–ª—è —Å–∂–∞—Ç–∏—è –±–ª–æ–∫–æ–≤)\n",
        "        self.block_compressor = nn.Sequential(\n",
        "            nn.Linear(block_size * self.head_dim, 2 * self.head_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(2 * self.head_dim, self.head_dim)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "    def _get_blocks(self, x, block_size, stride):\n",
        "        \"\"\"\n",
        "        Description:\n",
        "          –†–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –±–ª–æ–∫–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –∏ —à–∞–≥–æ–º\n",
        "\n",
        "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
        "            x: —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
        "            block_size: —Ä–∞–∑–º–µ—Ä –±–ª–æ–∫–∞\n",
        "            stride: —à–∞–≥ –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏\n",
        "\n",
        "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "            blocks: —Å–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤\n",
        "            block_indices: —Å–ø–∏—Å–æ–∫ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "        blocks = []\n",
        "        block_indices = []\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –±–ª–æ–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º\n",
        "        for i in range(0, seq_len - block_size + 1, stride):\n",
        "            block = x[:, i:i+block_size, :]  # (batch_size, block_size, hidden_size)\n",
        "            blocks.append(block)\n",
        "            block_indices.append((i, i+block_size))\n",
        "\n",
        "        return blocks, block_indices\n",
        "\n",
        "    def compress_blocks(self, blocks, head_dim):\n",
        "        \"\"\"\n",
        "        Description:\n",
        "          –°–∂–∏–º–∞–µ—Ç –±–ª–æ–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –µ–¥–∏–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é MLP\n",
        "\n",
        "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
        "            blocks: —Å–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ —Ñ–æ—Ä–º—ã (batch_size, block_size, head_dim)\n",
        "            head_dim: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "\n",
        "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "            compressed_blocks: —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, num_blocks, head_dim)\n",
        "        \"\"\"\n",
        "        batch_size = blocks[0].shape[0]\n",
        "        num_blocks = len(blocks)\n",
        "\n",
        "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –±–ª–æ–∫–∏ –≤ –æ–¥–∏–Ω —Ç–µ–Ω–∑–æ—Ä\n",
        "        blocks_tensor = torch.cat([block.unsqueeze(1) for block in blocks], dim=1)  # (batch_size, num_blocks, block_size, head_dim)\n",
        "\n",
        "        # –†–µ—à–µ–π–ø –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ MLP\n",
        "        reshaped_blocks = blocks_tensor.reshape(batch_size * num_blocks, -1)        # (batch_size * num_blocks, block_size * head_dim)\n",
        "\n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∂–∞—Ç–∏–µ (—Ñ—É–Ω–∫—Ü–∏—è œÜ –∏–∑ —Å—Ç–∞—Ç—å–∏)\n",
        "        compressed = self.block_compressor(reshaped_blocks)                         # (batch_size * num_blocks, head_dim)\n",
        "\n",
        "        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω—É–∂–Ω–æ–π —Ñ–æ—Ä–º–µ\n",
        "        compressed_blocks = compressed.reshape(batch_size, num_blocks, head_dim)    # (batch_size, num_blocks, head_dim)\n",
        "\n",
        "        return compressed_blocks\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n",
        "        \"\"\"\n",
        "        Description:\n",
        "          –í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞–¥ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é\n",
        "\n",
        "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
        "            hidden_states: —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
        "            attention_mask: –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "            output_attentions: —Ñ–ª–∞–≥ –¥–ª—è –≤—ã–≤–æ–¥–∞ –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "\n",
        "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "            context_layer: —Ç–µ–Ω–∑–æ—Ä –≤—ã—Ö–æ–¥–∞ —Ñ–æ—Ä–º—ã (batch_size, seq_len, hidden_size)\n",
        "            attention_probs (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ): –º–∞—Ç—Ä–∏—Ü–∞ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "\n",
        "        # –®–∞–≥ 1: –ü—Ä–æ–µ–∫—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "        q = self.q_proj(hidden_states)  # (batch_size, seq_len, hidden_size)\n",
        "        k = self.k_proj(hidden_states)  # (batch_size, seq_len, hidden_size)\n",
        "        v = self.v_proj(hidden_states)  # (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, head_dim)\n",
        "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, head_dim)\n",
        "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, head_dim)\n",
        "\n",
        "        # –®–∞–≥ 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –±–ª–æ–∫–æ–≤ –∏ –∏—Ö —Å–∂–∞—Ç–∏–µ –¥–ª—è –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "        all_compressed_k  = []\n",
        "        all_compressed_v  = []\n",
        "        all_block_indices = []\n",
        "\n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ\n",
        "        for h in range(self.num_heads):\n",
        "            head_k = k[:, h]  # (batch_size, seq_len, head_dim)\n",
        "            head_v = v[:, h]  # (batch_size, seq_len, head_dim)\n",
        "\n",
        "            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–ª–æ–∫–∏ –∏ –ø–æ–ª—É—á–∞–µ–º –∏—Ö –∏–Ω–¥–µ–∫—Å—ã\n",
        "            blocks_k, block_indices = self._get_blocks(head_k, self.block_size, self.stride)\n",
        "            blocks_v, _ = self._get_blocks(head_v, self.block_size, self.stride)\n",
        "\n",
        "            # –°–∂–∏–º–∞–µ–º –±–ª–æ–∫–∏ —Å –ø–æ–º–æ—â—å—é MLP\n",
        "            compressed_k = self.compress_blocks(blocks_k, self.head_dim)  # (batch_size, num_blocks, head_dim)\n",
        "            compressed_v = self.compress_blocks(blocks_v, self.head_dim)  # (batch_size, num_blocks, head_dim)\n",
        "\n",
        "            all_compressed_k.append(compressed_k)\n",
        "            all_compressed_v.append(compressed_v)\n",
        "            all_block_indices.append(block_indices)\n",
        "\n",
        "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –≤—Å–µ—Ö –≥–æ–ª–æ–≤\n",
        "        compressed_k = torch.stack(all_compressed_k, dim=1)  # (batch_size, num_heads, num_blocks, head_dim)\n",
        "        compressed_v = torch.stack(all_compressed_v, dim=1)  # (batch_size, num_heads, num_blocks, head_dim)\n",
        "\n",
        "        # –î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –±–µ—Ä–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏–∑ –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã, –æ–Ω–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –¥–ª—è –≤—Å–µ—Ö\n",
        "        block_indices = all_block_indices[0]\n",
        "        num_blocks = len(block_indices)\n",
        "\n",
        "        # –®–∞–≥ 3: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∏ —Å–∂–∞—Ç—ã–º–∏ –∫–ª—é—á–∞–º–∏\n",
        "        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –º—ã –≤—ã—á–∏—Å–ª—è–µ–º –µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ —Å–∂–∞—Ç—ã–º –±–ª–æ–∫–∞–º\n",
        "\n",
        "        # –ú–∞—Ç—Ä–∏—Ü–∞ –≤–Ω–∏–º–∞–Ω–∏—è: (batch_size, num_heads, seq_len, num_blocks)\n",
        "        attention_scores = torch.matmul(q, compressed_k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É –≤–Ω–∏–º–∞–Ω–∏—è (–µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é softmax\n",
        "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # –®–∞–≥ 4: –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ —Å–∂–∞—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "        context_layer = torch.matmul(attention_probs, compressed_v)                # (batch_size, num_heads, seq_len, head_dim)\n",
        "\n",
        "        # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º—ã\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()             # (batch_size, seq_len, num_heads, head_dim)\n",
        "        context_layer = context_layer.view(batch_size, seq_len, self.hidden_size)  # (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è\n",
        "        output = self.out_proj(context_layer)\n",
        "\n",
        "        if output_attentions:\n",
        "            return output, attention_probs, block_indices\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "def demonstrate_compressed_attention(use_long_sequence=False):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "      –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –º–µ—Ö–∞–Ω–∏–∑–º–∞ —Å–∂–∞—Ç–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "      –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –æ–±—ã—á–Ω—ã–º –ø–æ–ª–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º\n",
        "\n",
        "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
        "        use_long_sequence: –µ—Å–ª–∏ True, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª–∏–Ω–æ–π 32K —Ç–æ–∫–µ–Ω–æ–≤\n",
        "    \"\"\"\n",
        "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
        "    hidden_size = 64\n",
        "    num_heads = 4\n",
        "    batch_size = 1\n",
        "\n",
        "    if use_long_sequence:\n",
        "        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (32K)\n",
        "        seq_len = 32000\n",
        "        block_size = 256  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è\n",
        "        stride = 128      # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —à–∞–≥ –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è\n",
        "    else:\n",
        "        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–æ—Ä–æ—Ç–∫–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (128)\n",
        "        seq_len = 128\n",
        "        block_size = 32\n",
        "        stride = 16\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏\n",
        "    compressed_attention = CompressedAttention(\n",
        "        hidden_size=hidden_size,\n",
        "        block_size=block_size,\n",
        "        stride=stride,\n",
        "        num_heads=num_heads\n",
        "    )\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏\n",
        "    # –ú—ã —Å–æ–∑–¥–∞–¥–∏–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –≥–¥–µ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–æ–∫–µ–Ω—ã –±—É–¥—É—Ç \"–≤–∞–∂–Ω—ã–º–∏\"\n",
        "    torch.manual_seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
        "\n",
        "    print(f\"üìå –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª–∏–Ω–æ–π {seq_len} —Ç–æ–∫–µ–Ω–æ–≤...\")\n",
        "\n",
        "    # –ë–∞–∑–æ–≤—ã–π –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä (—Å–æ–∑–¥–∞–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ, –±–µ–∑ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏)\n",
        "    x = torch.zeros(batch_size, seq_len, hidden_size)\n",
        "\n",
        "    # –ó–∞–ø–æ–ª–Ω—è–µ–º —à—É–º–æ–º –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ (–ø–æ —á–∞—Å—Ç—è–º)\n",
        "    chunk_size = 1000 if use_long_sequence else seq_len\n",
        "    for i in range(0, seq_len, chunk_size):\n",
        "        end = min(i + chunk_size, seq_len)\n",
        "        x[:, i:end, :] = torch.randn(batch_size, end-i, hidden_size) * 0.1\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º \"–≤–∞–∂–Ω—ã–µ\" —Ç–æ–∫–µ–Ω—ã —á–µ—Ä–µ–∑ —Ä–∞–≤–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–∫–∏\n",
        "    # –î–ª—è –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª\n",
        "    important_interval = 1000 if use_long_sequence else 10\n",
        "    important_positions = list(range(0, seq_len, important_interval))\n",
        "    for pos in important_positions:\n",
        "        if pos < seq_len:\n",
        "            x[:, pos, :] = torch.ones(hidden_size)  # –í—ã–¥–µ–ª—è–µ–º –≤–∞–∂–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏–µ–º 1\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ \"–≤–∞–∂–Ω—ã—Ö\" —Ç–æ–∫–µ–Ω–æ–≤\n",
        "    if use_long_sequence:\n",
        "        # –°–æ–∑–¥–∞–µ–º 3 –∫–ª–∞—Å—Ç–µ—Ä–∞ –≤ –Ω–∞—á–∞–ª–µ, —Å–µ—Ä–µ–¥–∏–Ω–µ –∏ –∫–æ–Ω—Ü–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "        cluster_positions = [\n",
        "            (1000, 1200),      # –ù–∞—á–∞–ª–æ\n",
        "            (seq_len//2-100, seq_len//2+100),  # –°–µ—Ä–µ–¥–∏–Ω–∞\n",
        "            (seq_len-1200, seq_len-1000)       # –ö–æ–Ω–µ—Ü\n",
        "        ]\n",
        "    else:\n",
        "        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ - –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ\n",
        "        middle_start = seq_len // 3\n",
        "        cluster_positions = [(middle_start, middle_start + 20)]\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –≤–∞–∂–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
        "    for start, end in cluster_positions:\n",
        "        for pos in range(start, end):\n",
        "            if pos < seq_len:\n",
        "                x[:, pos, :] = torch.ones(hidden_size) * 0.8  # –ö–ª–∞—Å—Ç–µ—Ä —Å —á—É—Ç—å –º–µ–Ω—å—à–µ–π –≤–∞–∂–Ω–æ—Å—Ç—å—é\n",
        "\n",
        "    # –í—ã—á–∏—Å–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å —Ç–æ–ª—å–∫–æ –¥–ª—è —á–∞—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "    token_importance = x.sum(dim=2).squeeze().cpu().numpy()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ú–ï–•–ê–ù–ò–ó–ú–ê –°–ñ–ê–¢–û–ì–û –í–ù–ò–ú–ê–ù–ò–Ø (COMPRESSED ATTENTION)\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(f\"üìå –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ CompressedAttention —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\")\n",
        "    print(f\"  - –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è (hidden_size): {hidden_size}\")\n",
        "    print(f\"  - –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ (block_size): {block_size}\")\n",
        "    print(f\"  - –®–∞–≥ (stride): {stride}\")\n",
        "    print(f\"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (num_heads): {num_heads}\\n\")\n",
        "\n",
        "    print(f\"üìå –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "    print(f\"  - –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ (batch_size): {batch_size}\")\n",
        "    print(f\"  - –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (seq_len): {seq_len}\\n\")\n",
        "\n",
        "    print(f\"üìå –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "    print(f\"  - –°–æ–∑–¥–∞–ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏:\")\n",
        "    if use_long_sequence:\n",
        "        print(f\"    1. –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ '–≤–∞–∂–Ω—ã–µ' —Ç–æ–∫–µ–Ω—ã –∫–∞–∂–¥—ã–µ 1000 –ø–æ–∑–∏—Ü–∏–π\")\n",
        "        print(f\"    2. –ö–ª–∞—Å—Ç–µ—Ä—ã '–≤–∞–∂–Ω—ã—Ö' —Ç–æ–∫–µ–Ω–æ–≤ –≤ –Ω–∞—á–∞–ª–µ (1000-1200), —Å–µ—Ä–µ–¥–∏–Ω–µ –∏ –∫–æ–Ω—Ü–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\")\n",
        "    else:\n",
        "        print(f\"    1. –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ '–≤–∞–∂–Ω—ã–µ' —Ç–æ–∫–µ–Ω—ã –∫–∞–∂–¥—ã–µ 10 –ø–æ–∑–∏—Ü–∏–π\")\n",
        "        print(f\"    2. –ö–ª–∞—Å—Ç–µ—Ä '–≤–∞–∂–Ω—ã—Ö' —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ (–ø–æ–∑–∏—Ü–∏–∏ {cluster_positions[0][0]}-{cluster_positions[0][1]})\")\n",
        "    print(f\"    3. –°–ª—É—á–∞–π–Ω—ã–π —à—É–º –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\\n\")\n",
        "\n",
        "    print(f\"üìå –í–∞–∂–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ (—Å—É–º–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ —Å–∫—Ä—ã—Ç–æ–º—É –∏–∑–º–µ—Ä–µ–Ω–∏—é, –ø—Ä–∏–º–µ—Ä—ã):\")\n",
        "\n",
        "    # –î–ª—è –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ–±—Ä–∞–∑—Ü—ã\n",
        "    if use_long_sequence:\n",
        "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞—á–∞–ª–æ, —Å–µ—Ä–µ–¥–∏–Ω—É –∏ –∫–æ–Ω–µ—Ü –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "        sample_ranges = [\n",
        "            (0, 32),                          # –ù–∞—á–∞–ª–æ\n",
        "            (seq_len//2-16, seq_len//2+16),   # –°–µ—Ä–µ–¥–∏–Ω–∞\n",
        "            (seq_len-32, seq_len)             # –ö–æ–Ω–µ—Ü\n",
        "        ]\n",
        "\n",
        "        for start, end in sample_ranges:\n",
        "            print(f\"  –ü–æ–∑–∏—Ü–∏–∏ {start:5d}-{end-1:5d} (–ø—Ä–∏–º–µ—Ä):\")\n",
        "            for i in range(start, end, 16):\n",
        "                end_i = min(i + 16, end)\n",
        "                values = [f\"{token_importance[j]:4.1f}\" for j in range(i, end_i)]\n",
        "                print(f\"    {i:5d}-{end_i-1:5d}: {' '.join(values)}\")\n",
        "    else:\n",
        "        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã\n",
        "        for i in range(0, seq_len, 16):\n",
        "            end = min(i + 16, seq_len)\n",
        "            values = [f\"{token_importance[j]:4.1f}\" for j in range(i, end)]\n",
        "            print(f\"  –ü–æ–∑–∏—Ü–∏–∏ {i:3d}-{end-1:3d}: {' '.join(values)}\")\n",
        "    print()\n",
        "\n",
        "    # –ü—Ä–æ–µ–∫—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "    q = compressed_attention.q_proj(x)\n",
        "    k = compressed_attention.k_proj(x)\n",
        "    v = compressed_attention.v_proj(x)\n",
        "\n",
        "    print(f\"üìå –®–∞–≥ 1: –ü—Ä–æ–µ–∫—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π\")\n",
        "    print(f\"  - –§–æ—Ä–º–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ (q): {q.shape}\")\n",
        "    print(f\"  - –§–æ—Ä–º–∞ –∫–ª—é—á–µ–π (k): {k.shape}\")\n",
        "    print(f\"  - –§–æ—Ä–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π (v): {v.shape}\\n\")\n",
        "\n",
        "    print(f\"üìå –®–∞–≥ 2: –°–∂–∞—Ç–∏–µ –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π (—Å–∞–º–∞—è –≤–∞–∂–Ω–∞—è —á–∞—Å—Ç—å)\")\n",
        "    print(f\"  - –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ (block_size): {block_size}\")\n",
        "    print(f\"  - –®–∞–≥ (stride): {stride}\")\n",
        "\n",
        "    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ k –Ω–∞ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "    k_heads = k.view(batch_size, seq_len, num_heads, hidden_size // num_heads).permute(0, 2, 1, 3)\n",
        "    v_heads = v.view(batch_size, seq_len, num_heads, hidden_size // num_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "    # –ü–æ–ª—É—á–∞–µ–º –±–ª–æ–∫–∏ –¥–ª—è –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã\n",
        "    head_k = k_heads[:, 0]  # (batch_size, seq_len, head_dim)\n",
        "    blocks_k, block_indices = compressed_attention._get_blocks(head_k, block_size, stride)\n",
        "\n",
        "    num_blocks = len(blocks_k)\n",
        "    print(f\"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤ –ø–æ—Å–ª–µ —Ä–∞–∑–±–∏–µ–Ω–∏—è: {num_blocks}\")\n",
        "    print(f\"  - –ò–Ω–¥–µ–∫—Å—ã –±–ª–æ–∫–æ–≤: {block_indices}\\n\")\n",
        "\n",
        "    # –°–∂–∏–º–∞–µ–º –±–ª–æ–∫–∏\n",
        "    head_dim = hidden_size // num_heads\n",
        "    compressed_k = compressed_attention.compress_blocks(blocks_k, head_dim)\n",
        "\n",
        "    print(f\"  - –§–æ—Ä–º–∞ —Å–∂–∞—Ç—ã—Ö –∫–ª—é—á–µ–π: {compressed_k.shape}\")\n",
        "    print(f\"  - –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {seq_len} / {compressed_k.shape[1]} = {seq_len / compressed_k.shape[1]:.1f}x\\n\")\n",
        "\n",
        "    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ\n",
        "    print(f\"üìå –®–∞–≥ 3: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ —Å–∂–∞—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö\")\n",
        "    output, attention_probs, block_indices = compressed_attention(x, output_attentions=True)\n",
        "\n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã\n",
        "    print(f\"  - –§–æ—Ä–º–∞ –≤—ã—Ö–æ–¥–∞: {output.shape}\")\n",
        "    print(f\"  - –§–æ—Ä–º–∞ –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è: {attention_probs.shape}\\n\")\n",
        "\n",
        "    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å\n",
        "    print(f\"üìå –®–∞–≥ 4: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏\")\n",
        "\n",
        "    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: O(seq_len^2)\n",
        "    full_attention_complexity = seq_len * seq_len\n",
        "\n",
        "    # –°–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: O(seq_len * num_blocks)\n",
        "    compressed_attention_complexity = seq_len * num_blocks\n",
        "\n",
        "    print(f\"  - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (Full Attention): O(seq_len^2) = {full_attention_complexity}\")\n",
        "    print(f\"  - –°–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (Compressed Attention): O(seq_len * num_blocks) = {compressed_attention_complexity}\")\n",
        "    print(f\"  - –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: {full_attention_complexity / compressed_attention_complexity:.1f}x\\n\")\n",
        "\n",
        "    # –ò–∑–º–µ—Ä—è–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n",
        "    print(f\"üìå –®–∞–≥ 5: –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\")\n",
        "\n",
        "    # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "    def standard_attention(q, k, v, scale):\n",
        "        attention_scores = torch.matmul(q, k.transpose(-1, -2)) * scale\n",
        "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "        context_layer = torch.matmul(attention_probs, v)\n",
        "        return context_layer, attention_probs\n",
        "\n",
        "    # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã, –∫–ª—é—á–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–π –≥–æ–ª–æ–≤—ã\n",
        "    q_head = q.view(batch_size, seq_len, num_heads, head_dim)[:, :, 0, :]  # (batch_size, seq_len, head_dim)\n",
        "    k_head = k.view(batch_size, seq_len, num_heads, head_dim)[:, :, 0, :]\n",
        "    v_head = v.view(batch_size, seq_len, num_heads, head_dim)[:, :, 0, :]\n",
        "\n",
        "    # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "    if use_long_sequence:\n",
        "        print(\"  - –î–ª—è –ø–æ–ª–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 32K —Ç–æ–∫–µ–Ω–æ–≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∑–∞—Ç—Ä–∞—Ç–Ω–æ\")\n",
        "        print(\"  - –û—Ü–µ–Ω–∏–≤–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö (1000 —Ç–æ–∫–µ–Ω–æ–≤) –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ–º\")\n",
        "\n",
        "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤—Ä–µ–º–µ–Ω–∏\n",
        "        sample_size = 1000\n",
        "        q_sample = q_head[:, :sample_size, :]\n",
        "        k_sample = k_head[:, :sample_size, :]\n",
        "        v_sample = v_head[:, :sample_size, :]\n",
        "\n",
        "        # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –Ω–∞ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ\n",
        "        start_time = time.time()\n",
        "        for _ in range(10):  # –ú–µ–Ω—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "            _, _ = standard_attention(q_sample, k_sample, v_sample, compressed_attention.scale)\n",
        "        sample_std_time = (time.time() - start_time) / 10\n",
        "\n",
        "        # –≠–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ–º –≤—Ä–µ–º—è –¥–ª—è –ø–æ–ª–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å)\n",
        "        scaling_factor = (seq_len / sample_size) ** 2\n",
        "        std_time = sample_std_time * scaling_factor\n",
        "        print(f\"  - –ò–∑–º–µ—Ä–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è {sample_size} —Ç–æ–∫–µ–Ω–æ–≤: {sample_std_time:.6f} —Å\")\n",
        "        print(f\"  - –≠–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è {seq_len} —Ç–æ–∫–µ–Ω–æ–≤: {std_time:.6f} —Å\")\n",
        "    else:\n",
        "        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑–º–µ—Ä—è–µ–º –Ω–∞–ø—Ä—è–º—É—é\n",
        "        start_time = time.time()\n",
        "        for _ in range(100):  # –ü–æ–≤—Ç–æ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è\n",
        "            _, _ = standard_attention(q_head, k_head, v_head, compressed_attention.scale)\n",
        "        std_time = (time.time() - start_time) / 100\n",
        "\n",
        "    # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –¥–ª—è —Å–∂–∞—Ç–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "    repeat_count = 10 if use_long_sequence else 100  # –ú–µ–Ω—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "    start_time = time.time()\n",
        "    for _ in range(repeat_count):\n",
        "        _, _ = compressed_attention(x, output_attentions=True)[:2]\n",
        "    compressed_time = (time.time() - start_time) / repeat_count\n",
        "\n",
        "    print(f\"  - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: {std_time:.6f} —Å\")\n",
        "    print(f\"  - –°–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: {compressed_time:.6f} —Å\")\n",
        "    print(f\"  - –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {std_time / compressed_time:.2f}x\\n\")\n",
        "\n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "    print(f\"üìå –®–∞–≥ 6: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è\")\n",
        "\n",
        "    if use_long_sequence:\n",
        "        print(\"  - –î–ª—è 32K —Ç–æ–∫–µ–Ω–æ–≤ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è\")\n",
        "\n",
        "        # –î–ª—è –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã\n",
        "        # –í—ã–±–∏—Ä–∞–µ–º –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã: –Ω–∞—á–∞–ª–æ, —Å–µ—Ä–µ–¥–∏–Ω–∞, –∫–æ–Ω–µ—Ü\n",
        "        sample_ranges = [\n",
        "            (0, 500),                           # –ù–∞—á–∞–ª–æ\n",
        "            (seq_len//2-250, seq_len//2+250),   # –°–µ—Ä–µ–¥–∏–Ω–∞\n",
        "            (seq_len-500, seq_len)              # –ö–æ–Ω–µ—Ü\n",
        "        ]\n",
        "\n",
        "        fig, axes = plt.subplots(3, 1, figsize=(12, 18))\n",
        "\n",
        "        for i, (start, end) in enumerate(sample_ranges):\n",
        "            # –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –≥–æ–ª–æ–≤—É –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä –≤ –±–∞—Ç—á–µ\n",
        "            attention_fragment = attention_probs[0, 0, start:end].cpu().detach().numpy()\n",
        "\n",
        "            # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è –∫–∞–∫ —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É\n",
        "            im = axes[i].imshow(attention_fragment, cmap='viridis', aspect='auto')\n",
        "            fig.colorbar(im, ax=axes[i], label='–í–µ—Å –≤–Ω–∏–º–∞–Ω–∏—è')\n",
        "\n",
        "            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ—Å–∏\n",
        "            axes[i].set_xlabel('–ò–Ω–¥–µ–∫—Å –±–ª–æ–∫–∞')\n",
        "            axes[i].set_ylabel(f'–ò–Ω–¥–µ–∫—Å –∑–∞–ø—Ä–æ—Å–∞ ({start}-{end})')\n",
        "            axes[i].set_title(f'–§—Ä–∞–≥–º–µ–Ω—Ç –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è ({start}-{end})')\n",
        "\n",
        "            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∫–∏ —Ç–∏–∫–æ–≤\n",
        "            axes[i].set_xticks(np.arange(len(block_indices)))\n",
        "            axes[i].set_xticklabels([f\"{s}-{e}\" for s, e in block_indices], rotation=45, fontsize=8)\n",
        "\n",
        "            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–æ–∫–µ–Ω—ã –Ω–∞ y-–æ—Å–∏ –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏\n",
        "            fragment_len = end - start\n",
        "            y_step = max(1, fragment_len // 10)\n",
        "            y_ticks = np.arange(0, fragment_len, y_step)\n",
        "            axes[i].set_yticks(y_ticks)\n",
        "            axes[i].set_yticklabels([str(start + j) for j in y_ticks], fontsize=8)\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
        "        attention_head = attention_probs[0, 0].cpu().detach().numpy()\n",
        "\n",
        "    else:\n",
        "        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å—é –º–∞—Ç—Ä–∏—Ü—É\n",
        "        # –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –≥–æ–ª–æ–≤—É –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä –≤ –±–∞—Ç—á–µ\n",
        "        attention_head = attention_probs[0, 0].cpu().detach().numpy()\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç–∫—É –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤\n",
        "        token_positions = np.arange(seq_len)\n",
        "\n",
        "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –±–ª–æ–∫–æ–≤ –≤ —Å—Ä–µ–¥–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏\n",
        "        block_positions = [(start + end) // 2 for start, end in block_indices]\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "        # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –≤–Ω–∏–º–∞–Ω–∏—è –∫–∞–∫ —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É\n",
        "        im = ax.imshow(attention_head, cmap='viridis', aspect='auto')\n",
        "        fig.colorbar(im, ax=ax, label='–í–µ—Å –≤–Ω–∏–º–∞–Ω–∏—è')\n",
        "\n",
        "        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ—Å–∏\n",
        "        ax.set_xlabel('–ò–Ω–¥–µ–∫—Å –±–ª–æ–∫–∞')\n",
        "        ax.set_ylabel('–ò–Ω–¥–µ–∫—Å –∑–∞–ø—Ä–æ—Å–∞ (—Ç–æ–∫–µ–Ω–∞)')\n",
        "        ax.set_title('–ú–∞—Ç—Ä–∏—Ü–∞ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —Å–∂–∞—Ç–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (Compressed Attention)')\n",
        "\n",
        "        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∫–∏ —Ç–∏–∫–æ–≤\n",
        "        ax.set_xticks(np.arange(len(block_indices)))\n",
        "        ax.set_xticklabels([f\"{start}-{end}\" for start, end in block_indices], rotation=45)\n",
        "\n",
        "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–æ–∫–µ–Ω—ã –Ω–∞ y-–æ—Å–∏ –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏\n",
        "        y_ticks = np.arange(0, seq_len, 16)\n",
        "        ax.set_yticks(y_ticks)\n",
        "        ax.set_yticklabels([str(i) for i in y_ticks])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    print(\"  - –ú–∞—Ç—Ä–∏—Ü–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞\")\n",
        "\n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω—ã—Ö –±–ª–æ–∫–æ–≤\n",
        "    print(\"\\nüìå –®–∞–≥ 7: –ê–Ω–∞–ª–∏–∑ —Å–∂–∞—Ç—ã—Ö –±–ª–æ–∫–æ–≤\")\n",
        "\n",
        "    # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –±–ª–æ–∫–∏ (–ø–æ —Å—É–º–º–µ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è)\n",
        "    if use_long_sequence:\n",
        "        print(\"  - –î–ª—è 32K —Ç–æ–∫–µ–Ω–æ–≤ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\")\n",
        "\n",
        "        # –î–ª—è –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –±–ª–æ–∫–æ–≤ –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ\n",
        "        # –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫–ª—é—á–µ–≤—ã–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π\n",
        "        fragment_samples = [\n",
        "            0,                  # –ù–∞—á–∞–ª–æ\n",
        "            seq_len // 4,       # –ü–µ—Ä–≤–∞—è —á–µ—Ç–≤–µ—Ä—Ç—å\n",
        "            seq_len // 2,       # –°–µ—Ä–µ–¥–∏–Ω–∞\n",
        "            3 * seq_len // 4,   # –¢—Ä–µ—Ç—å—è —á–µ—Ç–≤–µ—Ä—Ç—å\n",
        "            seq_len - 1         # –ö–æ–Ω–µ—Ü\n",
        "        ]\n",
        "\n",
        "        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º\n",
        "        fragment_importances = []\n",
        "        for pos in fragment_samples:\n",
        "            fragment_row = attention_probs[0, 0, pos].cpu().detach().numpy()\n",
        "            fragment_importances.append(fragment_row)\n",
        "\n",
        "        # –£—Å—Ä–µ–¥–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Å–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º\n",
        "        block_importance = np.mean(fragment_importances, axis=0)\n",
        "    else:\n",
        "        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n",
        "        block_importance = attention_head.sum(axis=0)\n",
        "\n",
        "    top_blocks_idx = np.argsort(block_importance)[-3:][::-1]\n",
        "\n",
        "    print(f\"  - –ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –±–ª–æ–∫–∏:\")\n",
        "    for i, idx in enumerate(top_blocks_idx):\n",
        "        start, end = block_indices[idx]\n",
        "        importance = block_importance[idx]\n",
        "        print(f\"    {i+1}. –ë–ª–æ–∫ {idx} (–ø–æ–∑–∏—Ü–∏–∏ {start}-{end}): –≤–∞–∂–Ω–æ—Å—Ç—å = {importance:.3f}\")\n",
        "\n",
        "    print(\"\\nüìå –®–∞–≥ 8: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –æ–±—ã—á–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\")\n",
        "\n",
        "    # –í—ã–±–∏—Ä–∞–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
        "    if use_long_sequence:\n",
        "        # –î–ª—è –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—ã–±–∏—Ä–∞–µ–º –∑–∞–ø—Ä–æ—Å –∏–∑ —Å–µ—Ä–µ–¥–∏–Ω—ã –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
        "        query_idx = seq_len // 2\n",
        "\n",
        "        print(f\"  - –ê–Ω–∞–ª–∏–∑ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –≤ –ø–æ–∑–∏—Ü–∏–∏ {query_idx}:\")\n",
        "        print(f\"    * –í –æ–±—ã—á–Ω–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏ —ç—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–ª –±—ã —Å–≤–æ—ë –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –≤—Å–µ {seq_len} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
        "        print(f\"    * –í —Å–∂–∞—Ç–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏ –≤–Ω–∏–º–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ {len(block_indices)} –±–ª–æ–∫–æ–≤\")\n",
        "\n",
        "        # –î–ª—è –æ–±—ã—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è\n",
        "        print(f\"\\n    (–û–±—ã—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è 32K –Ω–µ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∏–∑-–∑–∞ –≤—ã—Å–æ–∫–æ–π –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏)\")\n",
        "\n",
        "        # –î–ª—è —Å–∂–∞—Ç–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "        compressed_attention_probs = attention_probs[0, 0, query_idx].cpu().detach().numpy()\n",
        "\n",
        "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø –±–ª–æ–∫–æ–≤ –¥–ª—è —Å–∂–∞—Ç–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "        top_k = 5\n",
        "        top_compressed_idx = np.argsort(compressed_attention_probs)[-top_k:][::-1]\n",
        "        print(f\"\\n    –¢–æ–ø-{top_k} –±–ª–æ–∫–æ–≤ –≤ —Å–∂–∞—Ç–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ {query_idx}:\")\n",
        "        for i, idx in enumerate(top_compressed_idx):\n",
        "            start, end = block_indices[idx]\n",
        "            print(f\"      {i+1}. –ë–ª–æ–∫ {idx} (–ø–æ–∑–∏—Ü–∏–∏ {start}-{end}): –≤–µ—Å = {compressed_attention_probs[idx]:.4f}\")\n",
        "    else:\n",
        "        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ - –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ\n",
        "        # –í—ã–±–∏—Ä–∞–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–æ–∫–µ–Ω –≤ –ø–æ–∑–∏—Ü–∏–∏ –≤–∞–∂–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞)\n",
        "        query_idx = cluster_positions[0][0] + 5  # –ò–Ω–¥–µ–∫—Å –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –≤–∞–∂–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
        "\n",
        "        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—ã—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n",
        "        q_token = q_head[:, query_idx:query_idx+1, :]  # (batch_size, 1, head_dim)\n",
        "        attention_scores = torch.matmul(q_token, k_head.transpose(-1, -2)) * compressed_attention.scale  # (batch_size, 1, seq_len)\n",
        "        full_attention_probs = F.softmax(attention_scores, dim=-1).squeeze().cpu().detach().numpy()\n",
        "\n",
        "        # –í—ã—á–∏—Å–ª—è–µ–º —Å–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n",
        "        compressed_attention_probs = attention_head[query_idx]\n",
        "\n",
        "        print(f\"  - –ê–Ω–∞–ª–∏–∑ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –≤ –ø–æ–∑–∏—Ü–∏–∏ {query_idx}:\")\n",
        "        print(f\"    * –í –æ–±—ã—á–Ω–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏ —ç—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–≤–æ—ë –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –≤—Å–µ {seq_len} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
        "        print(f\"    * –í —Å–∂–∞—Ç–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏ –≤–Ω–∏–º–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ {len(block_indices)} –±–ª–æ–∫–æ–≤\")\n",
        "\n",
        "        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "        top_k = 5  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5 –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤/–±–ª–æ–∫–æ–≤\n",
        "\n",
        "        # –î–ª—è –æ–±—ã—á–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "        top_tokens_idx = np.argsort(full_attention_probs)[-top_k:][::-1]\n",
        "        print(f\"\\n    –¢–æ–ø-{top_k} —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–±—ã—á–Ω–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏:\")\n",
        "        for i, idx in enumerate(top_tokens_idx):\n",
        "            print(f\"      {i+1}. –¢–æ–∫–µ–Ω {idx}: –≤–µ—Å = {full_attention_probs[idx]:.4f}\")\n",
        "\n",
        "        # –î–ª—è —Å–∂–∞—Ç–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è\n",
        "        top_compressed_idx = np.argsort(compressed_attention_probs)[-top_k:][::-1]\n",
        "        print(f\"\\n    –¢–æ–ø-{top_k} –±–ª–æ–∫–æ–≤ –≤ —Å–∂–∞—Ç–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏:\")\n",
        "        for i, idx in enumerate(top_compressed_idx):\n",
        "            start, end = block_indices[idx]\n",
        "            print(f\"      {i+1}. –ë–ª–æ–∫ {idx} (–ø–æ–∑–∏—Ü–∏–∏ {start}-{end}): –≤–µ—Å = {compressed_attention_probs[idx]:.4f}\")\n",
        "\n",
        "    print(\"\\nüìå –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\")\n",
        "    if use_long_sequence:\n",
        "        print(\"  - –ú–µ—Ö–∞–Ω–∏–∑–º —Å–∂–∞—Ç–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é (32K —Ç–æ–∫–µ–Ω–æ–≤)\")\n",
        "        print(f\"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤: {len(block_indices)} –≤–º–µ—Å—Ç–æ {seq_len} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
        "        print(f\"  - –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {seq_len / len(block_indices):.1f}x\")\n",
        "        print(f\"  - –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: {(seq_len**2) / (seq_len * len(block_indices)):.1f}x\")\n",
        "        print(f\"  - –î–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å –±—ã –æ–∫–æ–ª–æ {std_time:.4f} —Å–µ–∫—É–Ω–¥ (—ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è)\")\n",
        "        print(f\"  - –î–ª—è —Å–∂–∞—Ç–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å {compressed_time:.4f} —Å–µ–∫—É–Ω–¥\")\n",
        "        print(f\"  - –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ: {std_time / compressed_time:.2f}x\")\n",
        "        print(\"  - –°–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\")\n",
        "        print(\"  - –ü—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ –ø–µ—Ä–µ–≤–µ—à–∏–≤–∞–µ—Ç\")\n",
        "        print(\"    –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã –Ω–∞ —Å–∂–∞—Ç–∏–µ –±–ª–æ–∫–æ–≤\")\n",
        "    else:\n",
        "        print(\"  - –ú–µ—Ö–∞–Ω–∏–∑–º —Å–∂–∞—Ç–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å\")\n",
        "        print(f\"  - –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: {full_attention_complexity / compressed_attention_complexity:.1f}x\")\n",
        "        print(f\"  - –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {std_time / compressed_time:.2f}x\")\n",
        "        print(\"  - –ü—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\")\n",
        "        print(\"  - –°–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\")\n",
        "\n",
        "    return fig\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∑–∞–ø—É—Å–∫–∞–µ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é –Ω–∞ –∫–æ—Ä–æ—Ç–∫–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "    print(\"\\n=== –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ù–ê –ö–û–†–û–¢–ö–û–ô –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–ò (128 —Ç–æ–∫–µ–Ω–æ–≤) ===\")\n",
        "    fig_short = demonstrate_compressed_attention(use_long_sequence=False)\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
        "    plt.savefig('compressed_attention_short.png')\n",
        "    plt.close(fig_short)\n",
        "    print(\"\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ —Ñ–∞–π–ª 'compressed_attention_short.png'\")\n",
        "\n",
        "    # –°–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —Ö–æ—á–µ—Ç –ª–∏ –æ–Ω –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç –Ω–∞ –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "    run_long_test = input(\"\\n–•–æ—Ç–∏—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω–æ–π 32K —Ç–æ–∫–µ–Ω–æ–≤? (y/n): \")\n",
        "\n",
        "    if run_long_test.lower() == 'y':\n",
        "        print(\"\\n=== –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ù–ê –î–õ–ò–ù–ù–û–ô –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–ò (32K —Ç–æ–∫–µ–Ω–æ–≤) ===\")\n",
        "        print(\"–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ: —ç—Ç–æ—Ç —Ç–µ—Å—Ç –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –∏ –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏\")\n",
        "\n",
        "        try:\n",
        "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU –∏ —Å–≤–æ–±–æ–¥–Ω—É—é –ø–∞–º—è—Ç—å\n",
        "            if torch.cuda.is_available():\n",
        "                free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)\n",
        "                print(f\"–î–æ—Å—Ç—É–ø–Ω–∞—è –ø–∞–º—è—Ç—å GPU: {free_memory / 1024**3:.2f} –ì–ë\")\n",
        "\n",
        "                if free_memory < 4 * 1024**3:  # –ú–µ–Ω—å—à–µ 4 –ì–ë —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏\n",
        "                    print(\"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –º–∞–ª–æ —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏ –Ω–∞ GPU, –≤–æ–∑–º–æ–∂–Ω—ã –æ—à–∏–±–∫–∏ OOM\")\n",
        "\n",
        "            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç –Ω–∞ –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "            fig_long = demonstrate_compressed_attention(use_long_sequence=True)\n",
        "\n",
        "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
        "            plt.savefig('compressed_attention_long.png')\n",
        "            plt.close(fig_long)\n",
        "            print(\"\\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ —Ñ–∞–π–ª 'compressed_attention_long.png'\")\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            print(f\"\\n–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}\")\n",
        "            print(\"–í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω–æ–π 32K —Ç–æ–∫–µ–Ω–æ–≤.\")\n",
        "            print(\"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ —Å –±–æ–ª—å—à–∏–º –æ–±—ä–µ–º–æ–º –ø–∞–º—è—Ç–∏ –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç—å –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\")\n",
        "    else:\n",
        "        print(\"\\n–¢–µ—Å—Ç –Ω–∞ –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–ø—É—â–µ–Ω\")"
      ],
      "metadata": {
        "id": "C3DiXx7umcaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15555b27-8f41-4eeb-f3de-3b8c9afb57e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ù–ê –ö–û–†–û–¢–ö–û–ô –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–ò (128 —Ç–æ–∫–µ–Ω–æ–≤) ===\n",
            "üìå –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª–∏–Ω–æ–π 128 —Ç–æ–∫–µ–Ω–æ–≤...\n",
            "\n",
            "================================================================================\n",
            "–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ú–ï–•–ê–ù–ò–ó–ú–ê –°–ñ–ê–¢–û–ì–û –í–ù–ò–ú–ê–ù–ò–Ø (COMPRESSED ATTENTION)\n",
            "================================================================================\n",
            "\n",
            "üìå –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ CompressedAttention —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\n",
            "  - –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è (hidden_size): 64\n",
            "  - –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ (block_size): 32\n",
            "  - –®–∞–≥ (stride): 16\n",
            "  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (num_heads): 4\n",
            "\n",
            "üìå –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\n",
            "  - –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ (batch_size): 1\n",
            "  - –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (seq_len): 128\n",
            "\n",
            "üìå –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö:\n",
            "  - –°–æ–∑–¥–∞–ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏:\n",
            "    1. –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ '–≤–∞–∂–Ω—ã–µ' —Ç–æ–∫–µ–Ω—ã –∫–∞–∂–¥—ã–µ 10 –ø–æ–∑–∏—Ü–∏–π\n",
            "    2. –ö–ª–∞—Å—Ç–µ—Ä '–≤–∞–∂–Ω—ã—Ö' —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ (–ø–æ–∑–∏—Ü–∏–∏ 42-62)\n",
            "    3. –°–ª—É—á–∞–π–Ω—ã–π —à—É–º –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
            "\n",
            "üìå –í–∞–∂–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ (—Å—É–º–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ —Å–∫—Ä—ã—Ç–æ–º—É –∏–∑–º–µ—Ä–µ–Ω–∏—é, –ø—Ä–∏–º–µ—Ä—ã):\n",
            "  –ü–æ–∑–∏—Ü–∏–∏   0- 15: 64.0  0.8 -0.2  0.7 -0.3  0.6 -0.2 -0.4 -0.0  0.3 64.0  1.2 -1.1 -0.9  0.4 -0.3\n",
            "  –ü–æ–∑–∏—Ü–∏–∏  16- 31:  0.4  0.6  1.4 -0.8 64.0  0.2 -0.3 -1.3 -0.5 -0.2  0.1 -0.2 -0.5  0.2 64.0 -0.3\n",
            "  –ü–æ–∑–∏—Ü–∏–∏  32- 47: -1.6  0.3  0.5  1.3  0.4  1.2 -0.0  0.1 64.0  0.8 51.2 51.2 51.2 51.2 51.2 51.2\n",
            "  –ü–æ–∑–∏—Ü–∏–∏  48- 63: 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 -2.1 -0.8\n",
            "  –ü–æ–∑–∏—Ü–∏–∏  64- 79:  0.9  1.5 -1.3 -0.0  0.1  0.5 64.0  0.3  0.6 -0.0 -0.3 -1.1 -0.8  1.2  0.7 -0.6\n",
            "  –ü–æ–∑–∏—Ü–∏–∏  80- 95: 64.0  1.0  0.2  0.9 -0.5 -0.1 -0.4 -1.7  1.0  0.4 64.0  0.9  0.5  2.4  1.1  0.1\n",
            "  –ü–æ–∑–∏—Ü–∏–∏  96-111:  0.2  0.6 -0.7  1.4 64.0  0.1 -0.2 -0.4 -1.0 -1.1 -0.1  0.2  0.2 -0.3 64.0  0.5\n",
            "  –ü–æ–∑–∏—Ü–∏–∏ 112-127: -0.2 -1.1  0.9 -0.0 -0.2 -0.3  0.8 -0.2 64.0  0.2 -0.5 -0.2 -0.4  0.4  0.9 -0.2\n",
            "\n",
            "üìå –®–∞–≥ 1: –ü—Ä–æ–µ–∫—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π\n",
            "  - –§–æ—Ä–º–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ (q): torch.Size([1, 128, 64])\n",
            "  - –§–æ—Ä–º–∞ –∫–ª—é—á–µ–π (k): torch.Size([1, 128, 64])\n",
            "  - –§–æ—Ä–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π (v): torch.Size([1, 128, 64])\n",
            "\n",
            "üìå –®–∞–≥ 2: –°–∂–∞—Ç–∏–µ –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π (—Å–∞–º–∞—è –≤–∞–∂–Ω–∞—è —á–∞—Å—Ç—å)\n",
            "  - –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ (block_size): 32\n",
            "  - –®–∞–≥ (stride): 16\n",
            "  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤ –ø–æ—Å–ª–µ —Ä–∞–∑–±–∏–µ–Ω–∏—è: 7\n",
            "  - –ò–Ω–¥–µ–∫—Å—ã –±–ª–æ–∫–æ–≤: [(0, 32), (16, 48), (32, 64), (48, 80), (64, 96), (80, 112), (96, 128)]\n",
            "\n",
            "  - –§–æ—Ä–º–∞ —Å–∂–∞—Ç—ã—Ö –∫–ª—é—á–µ–π: torch.Size([1, 7, 16])\n",
            "  - –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: 128 / 7 = 18.3x\n",
            "\n",
            "üìå –®–∞–≥ 3: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ —Å–∂–∞—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö\n",
            "  - –§–æ—Ä–º–∞ –≤—ã—Ö–æ–¥–∞: torch.Size([1, 128, 64])\n",
            "  - –§–æ—Ä–º–∞ –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è: torch.Size([1, 4, 128, 7])\n",
            "\n",
            "üìå –®–∞–≥ 4: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏\n",
            "  - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (Full Attention): O(seq_len^2) = 16384\n",
            "  - –°–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (Compressed Attention): O(seq_len * num_blocks) = 896\n",
            "  - –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: 18.3x\n",
            "\n",
            "üìå –®–∞–≥ 5: –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n",
            "  - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: 0.000137 —Å\n",
            "  - –°–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: 0.003267 —Å\n",
            "  - –£—Å–∫–æ—Ä–µ–Ω–∏–µ: 0.04x\n",
            "\n",
            "üìå –®–∞–≥ 6: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è\n",
            "  - –ú–∞—Ç—Ä–∏—Ü–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞\n",
            "\n",
            "üìå –®–∞–≥ 7: –ê–Ω–∞–ª–∏–∑ —Å–∂–∞—Ç—ã—Ö –±–ª–æ–∫–æ–≤\n",
            "  - –ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –±–ª–æ–∫–∏:\n",
            "    1. –ë–ª–æ–∫ 0 (–ø–æ–∑–∏—Ü–∏–∏ 0-32): –≤–∞–∂–Ω–æ—Å—Ç—å = 19.385\n",
            "    2. –ë–ª–æ–∫ 3 (–ø–æ–∑–∏—Ü–∏–∏ 48-80): –≤–∞–∂–Ω–æ—Å—Ç—å = 18.882\n",
            "    3. –ë–ª–æ–∫ 1 (–ø–æ–∑–∏—Ü–∏–∏ 16-48): –≤–∞–∂–Ω–æ—Å—Ç—å = 18.619\n",
            "\n",
            "üìå –®–∞–≥ 8: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –æ–±—ã—á–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n",
            "  - –ê–Ω–∞–ª–∏–∑ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –≤ –ø–æ–∑–∏—Ü–∏–∏ 47:\n",
            "    * –í –æ–±—ã—á–Ω–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏ —ç—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–≤–æ—ë –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –≤—Å–µ 128 —Ç–æ–∫–µ–Ω–æ–≤\n",
            "    * –í —Å–∂–∞—Ç–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏ –≤–Ω–∏–º–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ 7 –±–ª–æ–∫–æ–≤\n",
            "\n",
            "    –¢–æ–ø-5 —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–±—ã—á–Ω–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏:\n",
            "      1. –¢–æ–∫–µ–Ω 0: –≤–µ—Å = 0.0085\n",
            "      2. –¢–æ–∫–µ–Ω 120: –≤–µ—Å = 0.0085\n",
            "      3. –¢–æ–∫–µ–Ω 100: –≤–µ—Å = 0.0085\n",
            "      4. –¢–æ–∫–µ–Ω 110: –≤–µ—Å = 0.0085\n",
            "      5. –¢–æ–∫–µ–Ω 90: –≤–µ—Å = 0.0085\n",
            "\n",
            "    –¢–æ–ø-5 –±–ª–æ–∫–æ–≤ –≤ —Å–∂–∞—Ç–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏:\n",
            "      1. –ë–ª–æ–∫ 4 (–ø–æ–∑–∏—Ü–∏–∏ 64-96): –≤–µ—Å = 0.1618\n",
            "      2. –ë–ª–æ–∫ 3 (–ø–æ–∑–∏—Ü–∏–∏ 48-80): –≤–µ—Å = 0.1600\n",
            "      3. –ë–ª–æ–∫ 0 (–ø–æ–∑–∏—Ü–∏–∏ 0-32): –≤–µ—Å = 0.1599\n",
            "      4. –ë–ª–æ–∫ 5 (–ø–æ–∑–∏—Ü–∏–∏ 80-112): –≤–µ—Å = 0.1593\n",
            "      5. –ë–ª–æ–∫ 1 (–ø–æ–∑–∏—Ü–∏–∏ 16-48): –≤–µ—Å = 0.1586\n",
            "\n",
            "üìå –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n",
            "  - –ú–µ—Ö–∞–Ω–∏–∑–º —Å–∂–∞—Ç–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å\n",
            "  - –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: 18.3x\n",
            "  - –£—Å–∫–æ—Ä–µ–Ω–∏–µ: 0.04x\n",
            "  - –ü—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
            "  - –°–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\n",
            "\n",
            "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ —Ñ–∞–π–ª 'compressed_attention_short.png'\n",
            "\n",
            "–•–æ—Ç–∏—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω–æ–π 32K —Ç–æ–∫–µ–Ω–æ–≤? (y/n): y\n",
            "\n",
            "=== –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ù–ê –î–õ–ò–ù–ù–û–ô –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–ò (32K —Ç–æ–∫–µ–Ω–æ–≤) ===\n",
            "–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ: —ç—Ç–æ—Ç —Ç–µ—Å—Ç –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –∏ –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏\n",
            "üìå –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª–∏–Ω–æ–π 32000 —Ç–æ–∫–µ–Ω–æ–≤...\n",
            "\n",
            "================================================================================\n",
            "–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ú–ï–•–ê–ù–ò–ó–ú–ê –°–ñ–ê–¢–û–ì–û –í–ù–ò–ú–ê–ù–ò–Ø (COMPRESSED ATTENTION)\n",
            "================================================================================\n",
            "\n",
            "üìå –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ CompressedAttention —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\n",
            "  - –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è (hidden_size): 64\n",
            "  - –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ (block_size): 256\n",
            "  - –®–∞–≥ (stride): 128\n",
            "  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (num_heads): 4\n",
            "\n",
            "üìå –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\n",
            "  - –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ (batch_size): 1\n",
            "  - –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (seq_len): 32000\n",
            "\n",
            "üìå –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö:\n",
            "  - –°–æ–∑–¥–∞–ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏:\n",
            "    1. –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ '–≤–∞–∂–Ω—ã–µ' —Ç–æ–∫–µ–Ω—ã –∫–∞–∂–¥—ã–µ 1000 –ø–æ–∑–∏—Ü–∏–π\n",
            "    2. –ö–ª–∞—Å—Ç–µ—Ä—ã '–≤–∞–∂–Ω—ã—Ö' —Ç–æ–∫–µ–Ω–æ–≤ –≤ –Ω–∞—á–∞–ª–µ (1000-1200), —Å–µ—Ä–µ–¥–∏–Ω–µ –∏ –∫–æ–Ω—Ü–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
            "    3. –°–ª—É—á–∞–π–Ω—ã–π —à—É–º –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
            "\n",
            "üìå –í–∞–∂–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ (—Å—É–º–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ —Å–∫—Ä—ã—Ç–æ–º—É –∏–∑–º–µ—Ä–µ–Ω–∏—é, –ø—Ä–∏–º–µ—Ä—ã):\n",
            "  –ü–æ–∑–∏—Ü–∏–∏     0-   31 (–ø—Ä–∏–º–µ—Ä):\n",
            "        0-   15: 64.0  0.8 -0.2  0.7 -0.3  0.6 -0.2 -0.4 -0.0  0.3 -0.1  1.2 -1.1 -0.9  0.4 -0.3\n",
            "       16-   31:  0.4  0.6  1.4 -0.8 -0.4  0.2 -0.3 -1.3 -0.5 -0.2  0.1 -0.2 -0.5  0.2  0.6 -0.3\n",
            "  –ü–æ–∑–∏—Ü–∏–∏ 15984-16015 (–ø—Ä–∏–º–µ—Ä):\n",
            "    15984-15999: 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2\n",
            "    16000-16015: 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2 51.2\n",
            "  –ü–æ–∑–∏—Ü–∏–∏ 31968-31999 (–ø—Ä–∏–º–µ—Ä):\n",
            "    31968-31983: -0.1  0.0 -0.1  0.7  1.2  0.7  0.8  0.9 -1.2 -0.2  0.8 -0.5  0.5 -0.6 -0.8 -0.4\n",
            "    31984-31999: -1.0  0.3  0.2 -0.4  0.4  1.1 -0.5 -1.1  1.0 -0.1  0.3 -0.2  1.3 -0.2  0.9  1.3\n",
            "\n",
            "üìå –®–∞–≥ 1: –ü—Ä–æ–µ–∫—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π\n",
            "  - –§–æ—Ä–º–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ (q): torch.Size([1, 32000, 64])\n",
            "  - –§–æ—Ä–º–∞ –∫–ª—é—á–µ–π (k): torch.Size([1, 32000, 64])\n",
            "  - –§–æ—Ä–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π (v): torch.Size([1, 32000, 64])\n",
            "\n",
            "üìå –®–∞–≥ 2: –°–∂–∞—Ç–∏–µ –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π (—Å–∞–º–∞—è –≤–∞–∂–Ω–∞—è —á–∞—Å—Ç—å)\n",
            "  - –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ (block_size): 256\n",
            "  - –®–∞–≥ (stride): 128\n",
            "  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤ –ø–æ—Å–ª–µ —Ä–∞–∑–±–∏–µ–Ω–∏—è: 249\n",
            "  - –ò–Ω–¥–µ–∫—Å—ã –±–ª–æ–∫–æ–≤: [(0, 256), (128, 384), (256, 512), (384, 640), (512, 768), (640, 896), (768, 1024), (896, 1152), (1024, 1280), (1152, 1408), (1280, 1536), (1408, 1664), (1536, 1792), (1664, 1920), (1792, 2048), (1920, 2176), (2048, 2304), (2176, 2432), (2304, 2560), (2432, 2688), (2560, 2816), (2688, 2944), (2816, 3072), (2944, 3200), (3072, 3328), (3200, 3456), (3328, 3584), (3456, 3712), (3584, 3840), (3712, 3968), (3840, 4096), (3968, 4224), (4096, 4352), (4224, 4480), (4352, 4608), (4480, 4736), (4608, 4864), (4736, 4992), (4864, 5120), (4992, 5248), (5120, 5376), (5248, 5504), (5376, 5632), (5504, 5760), (5632, 5888), (5760, 6016), (5888, 6144), (6016, 6272), (6144, 6400), (6272, 6528), (6400, 6656), (6528, 6784), (6656, 6912), (6784, 7040), (6912, 7168), (7040, 7296), (7168, 7424), (7296, 7552), (7424, 7680), (7552, 7808), (7680, 7936), (7808, 8064), (7936, 8192), (8064, 8320), (8192, 8448), (8320, 8576), (8448, 8704), (8576, 8832), (8704, 8960), (8832, 9088), (8960, 9216), (9088, 9344), (9216, 9472), (9344, 9600), (9472, 9728), (9600, 9856), (9728, 9984), (9856, 10112), (9984, 10240), (10112, 10368), (10240, 10496), (10368, 10624), (10496, 10752), (10624, 10880), (10752, 11008), (10880, 11136), (11008, 11264), (11136, 11392), (11264, 11520), (11392, 11648), (11520, 11776), (11648, 11904), (11776, 12032), (11904, 12160), (12032, 12288), (12160, 12416), (12288, 12544), (12416, 12672), (12544, 12800), (12672, 12928), (12800, 13056), (12928, 13184), (13056, 13312), (13184, 13440), (13312, 13568), (13440, 13696), (13568, 13824), (13696, 13952), (13824, 14080), (13952, 14208), (14080, 14336), (14208, 14464), (14336, 14592), (14464, 14720), (14592, 14848), (14720, 14976), (14848, 15104), (14976, 15232), (15104, 15360), (15232, 15488), (15360, 15616), (15488, 15744), (15616, 15872), (15744, 16000), (15872, 16128), (16000, 16256), (16128, 16384), (16256, 16512), (16384, 16640), (16512, 16768), (16640, 16896), (16768, 17024), (16896, 17152), (17024, 17280), (17152, 17408), (17280, 17536), (17408, 17664), (17536, 17792), (17664, 17920), (17792, 18048), (17920, 18176), (18048, 18304), (18176, 18432), (18304, 18560), (18432, 18688), (18560, 18816), (18688, 18944), (18816, 19072), (18944, 19200), (19072, 19328), (19200, 19456), (19328, 19584), (19456, 19712), (19584, 19840), (19712, 19968), (19840, 20096), (19968, 20224), (20096, 20352), (20224, 20480), (20352, 20608), (20480, 20736), (20608, 20864), (20736, 20992), (20864, 21120), (20992, 21248), (21120, 21376), (21248, 21504), (21376, 21632), (21504, 21760), (21632, 21888), (21760, 22016), (21888, 22144), (22016, 22272), (22144, 22400), (22272, 22528), (22400, 22656), (22528, 22784), (22656, 22912), (22784, 23040), (22912, 23168), (23040, 23296), (23168, 23424), (23296, 23552), (23424, 23680), (23552, 23808), (23680, 23936), (23808, 24064), (23936, 24192), (24064, 24320), (24192, 24448), (24320, 24576), (24448, 24704), (24576, 24832), (24704, 24960), (24832, 25088), (24960, 25216), (25088, 25344), (25216, 25472), (25344, 25600), (25472, 25728), (25600, 25856), (25728, 25984), (25856, 26112), (25984, 26240), (26112, 26368), (26240, 26496), (26368, 26624), (26496, 26752), (26624, 26880), (26752, 27008), (26880, 27136), (27008, 27264), (27136, 27392), (27264, 27520), (27392, 27648), (27520, 27776), (27648, 27904), (27776, 28032), (27904, 28160), (28032, 28288), (28160, 28416), (28288, 28544), (28416, 28672), (28544, 28800), (28672, 28928), (28800, 29056), (28928, 29184), (29056, 29312), (29184, 29440), (29312, 29568), (29440, 29696), (29568, 29824), (29696, 29952), (29824, 30080), (29952, 30208), (30080, 30336), (30208, 30464), (30336, 30592), (30464, 30720), (30592, 30848), (30720, 30976), (30848, 31104), (30976, 31232), (31104, 31360), (31232, 31488), (31360, 31616), (31488, 31744), (31616, 31872), (31744, 32000)]\n",
            "\n",
            "  - –§–æ—Ä–º–∞ —Å–∂–∞—Ç—ã—Ö –∫–ª—é—á–µ–π: torch.Size([1, 249, 16])\n",
            "  - –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: 32000 / 249 = 128.5x\n",
            "\n",
            "üìå –®–∞–≥ 3: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ —Å–∂–∞—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö\n",
            "  - –§–æ—Ä–º–∞ –≤—ã—Ö–æ–¥–∞: torch.Size([1, 32000, 64])\n",
            "  - –§–æ—Ä–º–∞ –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è: torch.Size([1, 4, 32000, 249])\n",
            "\n",
            "üìå –®–∞–≥ 4: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏\n",
            "  - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (Full Attention): O(seq_len^2) = 1024000000\n",
            "  - –°–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (Compressed Attention): O(seq_len * num_blocks) = 7968000\n",
            "  - –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: 128.5x\n",
            "\n",
            "üìå –®–∞–≥ 5: –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n",
            "  - –î–ª—è –ø–æ–ª–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 32K —Ç–æ–∫–µ–Ω–æ–≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∑–∞—Ç—Ä–∞—Ç–Ω–æ\n",
            "  - –û—Ü–µ–Ω–∏–≤–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö (1000 —Ç–æ–∫–µ–Ω–æ–≤) –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ–º\n",
            "  - –ò–∑–º–µ—Ä–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è 1000 —Ç–æ–∫–µ–Ω–æ–≤: 0.004142 —Å\n",
            "  - –≠–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è 32000 —Ç–æ–∫–µ–Ω–æ–≤: 4.241724 —Å\n",
            "  - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: 4.241724 —Å\n",
            "  - –°–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: 0.904735 —Å\n",
            "  - –£—Å–∫–æ—Ä–µ–Ω–∏–µ: 4.69x\n",
            "\n",
            "üìå –®–∞–≥ 6: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è\n",
            "  - –î–ª—è 32K —Ç–æ–∫–µ–Ω–æ–≤ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è\n",
            "  - –ú–∞—Ç—Ä–∏—Ü–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞\n",
            "\n",
            "üìå –®–∞–≥ 7: –ê–Ω–∞–ª–∏–∑ —Å–∂–∞—Ç—ã—Ö –±–ª–æ–∫–æ–≤\n",
            "  - –î–ª—è 32K —Ç–æ–∫–µ–Ω–æ–≤ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
            "  - –ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –±–ª–æ–∫–∏:\n",
            "    1. –ë–ª–æ–∫ 8 (–ø–æ–∑–∏—Ü–∏–∏ 1024-1280): –≤–∞–∂–Ω–æ—Å—Ç—å = 0.005\n",
            "    2. –ë–ª–æ–∫ 124 (–ø–æ–∑–∏—Ü–∏–∏ 15872-16128): –≤–∞–∂–Ω–æ—Å—Ç—å = 0.005\n",
            "    3. –ë–ª–æ–∫ 164 (–ø–æ–∑–∏—Ü–∏–∏ 20992-21248): –≤–∞–∂–Ω–æ—Å—Ç—å = 0.005\n",
            "\n",
            "üìå –®–∞–≥ 8: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –æ–±—ã—á–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n",
            "  - –ê–Ω–∞–ª–∏–∑ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –≤ –ø–æ–∑–∏—Ü–∏–∏ 16000:\n",
            "    * –í –æ–±—ã—á–Ω–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏ —ç—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–ª –±—ã —Å–≤–æ—ë –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –≤—Å–µ 32000 —Ç–æ–∫–µ–Ω–æ–≤\n",
            "    * –í —Å–∂–∞—Ç–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏ –≤–Ω–∏–º–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ 249 –±–ª–æ–∫–æ–≤\n",
            "\n",
            "    (–û–±—ã—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è 32K –Ω–µ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∏–∑-–∑–∞ –≤—ã—Å–æ–∫–æ–π –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏)\n",
            "\n",
            "    –¢–æ–ø-5 –±–ª–æ–∫–æ–≤ –≤ —Å–∂–∞—Ç–æ–º –≤–Ω–∏–º–∞–Ω–∏–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ 16000:\n",
            "      1. –ë–ª–æ–∫ 8 (–ø–æ–∑–∏—Ü–∏–∏ 1024-1280): –≤–µ—Å = 0.0046\n",
            "      2. –ë–ª–æ–∫ 124 (–ø–æ–∑–∏—Ü–∏–∏ 15872-16128): –≤–µ—Å = 0.0046\n",
            "      3. –ë–ª–æ–∫ 109 (–ø–æ–∑–∏—Ü–∏–∏ 13952-14208): –≤–µ—Å = 0.0046\n",
            "      4. –ë–ª–æ–∫ 164 (–ø–æ–∑–∏—Ü–∏–∏ 20992-21248): –≤–µ—Å = 0.0045\n",
            "      5. –ë–ª–æ–∫ 62 (–ø–æ–∑–∏—Ü–∏–∏ 7936-8192): –≤–µ—Å = 0.0045\n",
            "\n",
            "üìå –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n",
            "  - –ú–µ—Ö–∞–Ω–∏–∑–º —Å–∂–∞—Ç–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é (32K —Ç–æ–∫–µ–Ω–æ–≤)\n",
            "  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤: 249 –≤–º–µ—Å—Ç–æ 32000 —Ç–æ–∫–µ–Ω–æ–≤\n",
            "  - –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: 128.5x\n",
            "  - –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: 128.5x\n",
            "  - –î–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å –±—ã –æ–∫–æ–ª–æ 4.2417 —Å–µ–∫—É–Ω–¥ (—ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è)\n",
            "  - –î–ª—è —Å–∂–∞—Ç–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å 0.9047 —Å–µ–∫—É–Ω–¥\n",
            "  - –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ: 4.69x\n",
            "  - –°–∂–∞—Ç–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\n",
            "  - –ü—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ –ø–µ—Ä–µ–≤–µ—à–∏–≤–∞–µ—Ç\n",
            "    –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã –Ω–∞ —Å–∂–∞—Ç–∏–µ –±–ª–æ–∫–æ–≤\n",
            "\n",
            "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ —Ñ–∞–π–ª 'compressed_attention_long.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DvzcHN5rQ8rr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}