{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìò –î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è LSTM –æ—Ç –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–π –¥–æ –ø–æ—Å–ª–æ–≤–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–æ–¥–∫–ª—é—á–∏–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T08:02:25.407080Z",
     "iopub.status.busy": "2024-05-27T08:02:25.406496Z",
     "iopub.status.idle": "2024-05-27T08:02:25.439306Z",
     "shell.execute_reply": "2024-05-27T08:02:25.438367Z",
     "shell.execute_reply.started": "2024-05-27T08:02:25.407026Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π LSTM-–º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\n",
    "\n",
    "–î–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ, –æ–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≥–ª—É–±–æ–∫–æ–π –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π \n",
    "LSTM-—Å–µ—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å —á–µ—Ç—ã—Ä—å–º—è LSTM-—Å–ª–æ—è–º–∏,\n",
    "–∫–∞–∂–¥—ã–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —è–≤–ª—è–µ—Ç—Å—è –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—ã–≤–∞—Ç—å\n",
    "–∫–∞–∫ –ø—Ä–µ–¥—ã–¥—É—â–∏–π, —Ç–∞–∫ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–µ–∫—Å—Ç–∞.\n",
    "\"\"\"\n",
    "\n",
    "# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –∏ –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö\n",
    "import sqlite3\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Any, Optional, Union, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# –£—Ç–∏–ª–∏—Ç—ã\n",
    "from tqdm import tqdm\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "seaborn.set(palette='summer')\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T08:02:25.441146Z",
     "iopub.status.busy": "2024-05-27T08:02:25.440594Z",
     "iopub.status.idle": "2024-05-27T08:02:27.674657Z",
     "shell.execute_reply": "2024-05-27T08:02:27.673847Z",
     "shell.execute_reply.started": "2024-05-27T08:02:25.441115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('/kaggle/input/wikibooks-dataset/wikibooks.sqlite')\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM ru LIMIT 3300\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T08:02:27.676884Z",
     "iopub.status.busy": "2024-05-27T08:02:27.676595Z",
     "iopub.status.idle": "2024-05-27T08:02:37.924493Z",
     "shell.execute_reply": "2024-05-27T08:02:37.923576Z",
     "shell.execute_reply.started": "2024-05-27T08:02:27.676860Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3300/3300 [00:10<00:00, 322.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π 120873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "sentences = []\n",
    "\n",
    "for sentence in tqdm(df['body_text']):\n",
    "    sentences.extend(\n",
    "        [x.lower() for x in sent_tokenize(sentence, language='russian') if len(x) < 256]\n",
    "    )\n",
    "    \n",
    "print(\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\", len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T08:03:53.281940Z",
     "iopub.status.busy": "2024-05-27T08:03:53.281254Z",
     "iopub.status.idle": "2024-05-27T08:03:53.297773Z",
     "shell.execute_reply": "2024-05-27T08:03:53.296704Z",
     "shell.execute_reply.started": "2024-05-27T08:03:53.281909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fit_epoch(\n",
    "    model: nn.Module, \n",
    "    train_loader: DataLoader, \n",
    "    criterion: nn.Module, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    sheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–Ω—É —ç–ø–æ—Ö—É –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "        train_loader: –ó–∞–≥—Ä—É–∑—á–∏–∫ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "        criterion: –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å\n",
    "        optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
    "        sheduler: –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "\n",
    "    Returns:\n",
    "    ---------------\n",
    "        Tuple[float, float]: –ü–µ—Ä–ø–ª–µ–∫—Å–∏—è –∏ –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_data = 0\n",
    "    losses = []\n",
    "    perplexity = []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä—è–º–æ–µ –∏ –æ–±—Ä–∞—Ç–Ω–æ–µ\n",
    "        logits = model(batch['input_ids']).flatten(start_dim=0, end_dim=1)\n",
    "        loss = criterion(\n",
    "            logits, batch['target_ids'].flatten()\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫\n",
    "        perplexity.append(torch.exp(loss).item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    # –†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –º–µ—Ç—Ä–∏–∫\n",
    "    perplexity_avg = sum(perplexity) / len(perplexity)\n",
    "    losses_avg = sum(losses) / len(losses)    \n",
    "    \n",
    "    return perplexity_avg, losses_avg\n",
    "\n",
    "\n",
    "def eval_epoch(\n",
    "    model: nn.Module, \n",
    "    val_loader: DataLoader, \n",
    "    criterion: nn.Module\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏\n",
    "        val_loader: –ó–∞–≥—Ä—É–∑—á–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "        criterion: –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å\n",
    "\n",
    "    Returns:\n",
    "    ---------------\n",
    "        Tuple[float, float]: –ü–µ—Ä–ø–ª–µ–∫—Å–∏—è –∏ –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    perplexity = []\n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            logits = model(batch['input_ids']).flatten(start_dim=0, end_dim=1)\n",
    "            loss = criterion(\n",
    "                logits,\n",
    "                batch['target_ids'].flatten()\n",
    "            )\n",
    "            perplexity.append(torch.exp(loss).item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    # –†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –º–µ—Ç—Ä–∏–∫\n",
    "    perplexity_avg = sum(perplexity) / len(perplexity)\n",
    "    losses_avg = sum(losses) / len(losses)\n",
    "    \n",
    "    return perplexity_avg, losses_avg\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_dataloader: DataLoader, \n",
    "    eval_dataloader: DataLoader, \n",
    "    model: nn.Module, \n",
    "    epochs: int, \n",
    "    ignore_index: int = word2ind['<pad>'],\n",
    "    optimizer: Optional[torch.optim.Optimizer] = None, \n",
    "    criterion: Optional[nn.Module] = None, \n",
    "    sheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None\n",
    ") -> Tuple[nn.Module, List[Tuple[float, float, float, float]]]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –∑–∞–¥–∞–Ω–Ω–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —ç–ø–æ—Ö.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        train_dataloader: –ó–∞–≥—Ä—É–∑—á–∏–∫ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "        eval_dataloader: –ó–∞–≥—Ä—É–∑—á–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "        model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è\n",
    "        ignore_index: –ò–Ω–¥–µ–∫—Å —Ç–æ–∫–µ–Ω–∞, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –≤ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
    "        optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é Adam)\n",
    "        criterion: –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é CrossEntropyLoss)\n",
    "        sheduler: –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "\n",
    "    Returns:\n",
    "    ---------------\n",
    "        Tuple[nn.Module, List[Tuple[float, float, float, float]]]: \n",
    "            –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏ –∏—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è\n",
    "    \"\"\"\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã\n",
    "    if optimizer is None:\n",
    "      optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    if criterion is None:\n",
    "      criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_perplexity = 10e10\n",
    "\n",
    "    # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "    val_loss {v_loss:0.4f} train_perplexirty {t_acc:0.4f} val_perplexirty {v_acc:0.4f}\"\n",
    "\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        for epoch in range(epochs):\n",
    "            # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ\n",
    "            train_perplexirty, train_loss = fit_epoch(\n",
    "                model, train_dataloader, criterion, optimizer\n",
    "            )\n",
    "            \n",
    "            # –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "            val_perplexirty, val_loss = eval_epoch(\n",
    "                model, eval_dataloader, criterion\n",
    "            )\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫\n",
    "            history.append((train_loss, train_perplexirty, val_loss, val_perplexirty))\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "            if val_perplexirty < best_perplexity:\n",
    "                best_perplexity = val_perplexirty\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞ –∏ –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(\n",
    "                ep=epoch+1, \n",
    "                t_loss=train_loss,\n",
    "                v_loss=val_loss, \n",
    "                t_acc=train_perplexirty, \n",
    "                v_acc=val_perplexirty\n",
    "            ))\n",
    "\n",
    "    print('Best val perplexirty: {:4f}'.format(best_perplexity))\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–∏—Ö –≤–µ—Å–æ–≤\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –§—É–Ω–∫—Ü–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏/–∑–∞–≥—Ä—É–∑–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞/–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T08:03:54.154377Z",
     "iopub.status.busy": "2024-05-27T08:03:54.153996Z",
     "iopub.status.idle": "2024-05-27T08:03:54.168526Z",
     "shell.execute_reply": "2024-05-27T08:03:54.167505Z",
     "shell.execute_reply.started": "2024-05-27T08:03:54.154347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class WordDataset:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–æ—Å–ª–æ–≤–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        sentences: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> dataset = WordDataset(sentences)\n",
    "        >>> sample = dataset[0]\n",
    "        >>> len(sample)\n",
    "        # –î–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences: List[str]) -> None:\n",
    "        self.data = sentences\n",
    "        self.unk_id = word2ind['<unk>']\n",
    "        self.bos_id = word2ind['<bos>']\n",
    "        self.eos_id = word2ind['<eos>']\n",
    "        self.pad_id = word2ind['<pad>']\n",
    "\n",
    "    def __getitem__(self, idx: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤ —Å–ª–æ–≤.\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            idx: –ò–Ω–¥–µ–∫—Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            List[int]: –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫–∞–∫ —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "        \"\"\"\n",
    "        tokenized_sentence = [self.bos_id]\n",
    "        tokenized_sentence += [\n",
    "            word2ind.get(word, self.unk_id) \n",
    "            for word in nltk.word_tokenize(self.data[idx])\n",
    "        ]\n",
    "        tokenized_sentence += [self.eos_id]\n",
    "        \n",
    "        return tokenized_sentence\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞.\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            int: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "def collate_fn_with_padding(\n",
    "    input_batch: List[List[int]], \n",
    "    pad_id: int = word2ind['<pad>']\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –±–∞—Ç—á–∞ –¥–∞–Ω–Ω—ã—Ö: –¥–æ–±–∞–≤–ª—è–µ—Ç –ø–∞–¥–¥–∏–Ω–≥ –∏ \n",
    "        —Å–æ–∑–¥–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä—ã –≤–≤–æ–¥–∞ –∏ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        input_batch: –ü–∞–∫–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "        pad_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è (–ø–∞–¥–¥–∏–Ω–≥–∞)\n",
    "\n",
    "    Returns:\n",
    "    ---------------\n",
    "        Dict[str, torch.Tensor]: –°–ª–æ–≤–∞—Ä—å —Å —Ç–µ–Ω–∑–æ—Ä–∞–º–∏ –≤—Ö–æ–¥–Ω—ã—Ö –∏ —Ü–µ–ª–µ–≤—ã—Ö id\n",
    "    \"\"\"\n",
    "    seq_lens = [len(x) for x in input_batch]\n",
    "    max_seq_len = max(seq_lens)\n",
    "\n",
    "    new_batch = []\n",
    "    for sequence in input_batch:\n",
    "        for _ in range(max_seq_len - len(sequence)):\n",
    "            sequence.append(pad_id)\n",
    "        new_batch.append(sequence)\n",
    "\n",
    "    sequences = torch.LongTensor(new_batch).to(device)\n",
    "\n",
    "    new_batch = {\n",
    "        'input_ids': sequences[:, :-1],\n",
    "        'target_ids': sequences[:, 1:]\n",
    "    }\n",
    "\n",
    "    return new_batch\n",
    "\n",
    "\n",
    "def generate_sequence(\n",
    "    model: nn.Module, \n",
    "    dict_2ind: Dict[str, int], \n",
    "    ind2dict: Dict[int, str], \n",
    "    starting_seq: str, \n",
    "    max_seq_len: int = 256\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –Ω–∞—á–∏–Ω–∞—è —Å –∑–∞–¥–∞–Ω–Ω–æ–π.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        model: –û–±—É—á–µ–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å\n",
    "        dict_2ind: –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å—ã\n",
    "        ind2dict: –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ —Ç–æ–∫–µ–Ω—ã\n",
    "        starting_seq: –ù–∞—á–∞–ª—å–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
    "        max_seq_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "\n",
    "    Returns:\n",
    "    ---------------\n",
    "        str: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> result = generate_sequence(model, word2ind, ind2word, \"–∏—Å—Ç–æ—Ä–∏—è\")\n",
    "        >>> print(result)\n",
    "        '–∏—Å—Ç–æ—Ä–∏—è —Ä–æ—Å—Å–∏–∏...'\n",
    "    \"\"\"\n",
    "    device = 'cpu'\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∏–Ω–¥–µ–∫—Å—ã\n",
    "    input_ids = [dict_2ind['<bos>']] + [\n",
    "        dict_2ind.get(char, dict_2ind['<unk>']) for char in starting_seq\n",
    "    ]\n",
    "    input_ids = torch.LongTensor(input_ids).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "            next_token_distribution = model(input_ids.unsqueeze(0))\n",
    "            next_token_logits = next_token_distribution[0, -1, :]\n",
    "            next_token = next_token_logits.argmax()\n",
    "            \n",
    "            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∫ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)])\n",
    "\n",
    "            # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏ –ø–æ—è–≤–ª–µ–Ω–∏–∏ —Ç–æ–∫–µ–Ω–∞ –∫–æ–Ω—Ü–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "            if next_token.item() == dict_2ind['<eos>']:\n",
    "                break\n",
    "\n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç\n",
    "    words = ' '.join([ind2dict[idx.item()] for idx in input_ids])\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T08:12:24.211157Z",
     "iopub.status.busy": "2024-05-27T08:12:24.210610Z",
     "iopub.status.idle": "2024-05-27T08:12:24.222040Z",
     "shell.execute_reply": "2024-05-27T08:12:24.221034Z",
     "shell.execute_reply.started": "2024-05-27T08:12:24.211126Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –ì–ª—É–±–æ–∫–∞—è –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è LSTM-–º–æ–¥–µ–ª—å –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        vocab_size: –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è\n",
    "        hidden_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> model = LanguageModel(vocab_size=1000, hidden_dim=256)\n",
    "        >>> input_tensor = torch.LongTensor([[1, 2, 3, 4]])\n",
    "        >>> output = model(input_tensor)\n",
    "        >>> output.shape\n",
    "        torch.Size([1, 4, 1000])\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size: int, \n",
    "        hidden_dim: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # –°–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        \n",
    "        # –ß–µ—Ç—ã—Ä–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö LSTM-—Å–ª–æ—è\n",
    "        self.lstm_1 = nn.LSTM(\n",
    "            hidden_dim, hidden_dim, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.lstm_2 = nn.LSTM(\n",
    "            hidden_dim*2, hidden_dim, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.lstm_3 = nn.LSTM(\n",
    "            hidden_dim*2, hidden_dim, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.lstm_4 = nn.LSTM(\n",
    "            hidden_dim*2, hidden_dim, batch_first=True, bidirectional=True\n",
    "        )\n",
    "            \n",
    "        # –ü—Ä–æ–µ–∫—Ü–∏–æ–Ω–Ω—ã–µ —Å–ª–æ–∏\n",
    "        self.linear = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.projection = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        # –ù–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n",
    "        self.non_lin = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, input_batch: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            input_batch: –¢–µ–Ω–∑–æ—Ä —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            torch.Tensor: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –ª–æ–≥–∏—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "        \"\"\"\n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "        embeddings = self.embedding(input_batch)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ LSTM-—Å–ª–æ–∏\n",
    "        output, _ = self.lstm_1(embeddings)  # [batch_size, seq_len, hidden_dim*2]\n",
    "        output, _ = self.lstm_2(output)      # [batch_size, seq_len, hidden_dim*2]\n",
    "        output, _ = self.lstm_3(output)      # [batch_size, seq_len, hidden_dim*2]\n",
    "        output, _ = self.lstm_4(output)      # [batch_size, seq_len, hidden_dim*2]\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π\n",
    "        output = self.dropout(\n",
    "            self.linear(self.non_lin(output))\n",
    "        )  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # –ü—Ä–æ–µ–∫—Ü–∏—è –Ω–∞ —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è\n",
    "        projection = self.projection(\n",
    "            self.non_lin(output)\n",
    "        )  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        return projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T08:11:14.033265Z",
     "iopub.status.busy": "2024-05-27T08:11:14.032742Z",
     "iopub.status.idle": "2024-05-27T08:11:43.125402Z",
     "shell.execute_reply": "2024-05-27T08:11:43.124486Z",
     "shell.execute_reply.started": "2024-05-27T08:11:14.033238Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 120873/120873 [00:28<00:00, 4174.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—Å–µ–≥–æ —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ: 40004\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–ª–æ–≤–µ—Å–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "words = Counter()\n",
    "\n",
    "for sentence in tqdm(sentences):\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "            words[word] += 1\n",
    "            \n",
    "vocab = set(['<unk>', '<bos>', '<eos>', '<pad>'])\n",
    "vocab_size = 40000\n",
    "\n",
    "for elem in words.most_common(vocab_size):\n",
    "    vocab.add(elem[0])\n",
    "    \n",
    "print(\"–í—Å–µ–≥–æ —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T08:11:43.128034Z",
     "iopub.status.busy": "2024-05-27T08:11:43.127132Z",
     "iopub.status.idle": "2024-05-27T08:11:43.151741Z",
     "shell.execute_reply": "2024-05-27T08:11:43.150823Z",
     "shell.execute_reply.started": "2024-05-27T08:11:43.127995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "word2ind = {char: i for i, char in enumerate(vocab)}\n",
    "ind2word = {i: char for char, i in word2ind.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T08:11:43.153533Z",
     "iopub.status.busy": "2024-05-27T08:11:43.152952Z",
     "iopub.status.idle": "2024-05-27T08:11:43.197936Z",
     "shell.execute_reply": "2024-05-27T08:11:43.197108Z",
     "shell.execute_reply.started": "2024-05-27T08:11:43.153501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "train_sentences, eval_sentences = train_test_split(\n",
    "    sentences, test_size=0.2\n",
    ")\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö\n",
    "train_dataset = WordDataset(train_sentences)\n",
    "eval_dataset = WordDataset(eval_sentences)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    collate_fn=collate_fn_with_padding, \n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, \n",
    "    collate_fn=collate_fn_with_padding, \n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T08:12:27.844291Z",
     "iopub.status.busy": "2024-05-27T08:12:27.843820Z",
     "iopub.status.idle": "2024-05-27T08:12:28.129201Z",
     "shell.execute_reply": "2024-05-27T08:12:28.128278Z",
     "shell.execute_reply.started": "2024-05-27T08:12:27.844256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageModel(\n",
      "  (embedding): Embedding(40004, 256)\n",
      "  (lstm_1): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "  (lstm_2): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
      "  (lstm_3): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
      "  (lstm_4): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (projection): Linear(in_features=256, out_features=40004, bias=True)\n",
      "  (non_lin): Tanh()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Number of model parameters: 26,436,932\n"
     ]
    }
   ],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "model = LanguageModel(\n",
    "    hidden_dim=256, \n",
    "    vocab_size=len(vocab)\n",
    ").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(model)\n",
    "print(f\"Number of model parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T08:12:30.544725Z",
     "iopub.status.busy": "2024-05-27T08:12:30.543908Z",
     "iopub.status.idle": "2024-05-27T08:40:11.378810Z",
     "shell.execute_reply": "2024-05-27T08:40:11.377791Z",
     "shell.execute_reply.started": "2024-05-27T08:12:30.544696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  10%|‚ñà         | 1/10 [02:46<24:56, 166.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 001 train_loss: 6.2165     val_loss 4.4502 train_perplexirty 950.2023 val_perplexirty 86.3828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  20%|‚ñà‚ñà        | 2/10 [05:32<22:08, 166.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 002 train_loss: 3.6826     val_loss 3.0068 train_perplexirty 43.8529 val_perplexirty 20.3645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  30%|‚ñà‚ñà‚ñà       | 3/10 [08:18<19:21, 165.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 003 train_loss: 2.6966     val_loss 2.2747 train_perplexirty 15.2892 val_perplexirty 9.7852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [11:03<16:35, 165.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 004 train_loss: 1.9999     val_loss 1.7284 train_perplexirty 7.5591 val_perplexirty 5.6607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [13:49<13:48, 165.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 005 train_loss: 1.5587     val_loss 1.4527 train_perplexirty 4.7937 val_perplexirty 4.2946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [16:35<11:03, 165.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 006 train_loss: 1.2813     val_loss 1.2659 train_perplexirty 3.6192 val_perplexirty 3.5613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [19:20<08:17, 165.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 007 train_loss: 1.0773     val_loss 1.1091 train_perplexirty 2.9480 val_perplexirty 3.0433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [22:07<05:32, 166.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 008 train_loss: 0.8751     val_loss 0.9516 train_perplexirty 2.4070 val_perplexirty 2.5987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [24:54<02:46, 166.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 009 train_loss: 0.7142     val_loss 0.8469 train_perplexirty 2.0487 val_perplexirty 2.3398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [27:40<00:00, 166.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 010 train_loss: 0.5945     val_loss 0.7705 train_perplexirty 1.8150 val_perplexirty 2.1671\n",
      "Best val perplexirty: 2.167140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "best_model, losses = train(\n",
    "    train_dataloader, \n",
    "    eval_dataloader, \n",
    "    model, \n",
    "    10, \n",
    "    ignore_index=word2ind[\"<pad>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T08:45:34.688893Z",
     "iopub.status.busy": "2024-05-27T08:45:34.688567Z",
     "iopub.status.idle": "2024-05-27T08:45:34.728929Z",
     "shell.execute_reply": "2024-05-27T08:45:34.728045Z",
     "shell.execute_reply.started": "2024-05-27T08:45:34.688870Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos> –∏–º–ø–µ—Ä–∏—è —á–∏—Å—Ç—ã–π –∞–Ω–≥–ª <eos>'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "generate_sequence(\n",
    "    model, \n",
    "    word2ind, \n",
    "    ind2word,\n",
    "    starting_seq=nltk.word_tokenize('–∏–º–ø–µ—Ä–∏—è')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í—ã–≤–æ–¥—ã\n",
    "\n",
    "**–î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è LSTM**\n",
    "\n",
    "–î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è LSTM –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –ø–æ –º–µ—Ç—Ä–∏–∫–µ –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏. –ë–ª–∞–≥–æ–¥–∞—Ä—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —É—á–∏—Ç—ã–≤–∞—Ç—å –∫–∞–∫ –ø—Ä–µ–¥—ã–¥—É—â–∏–π, —Ç–∞–∫ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã. –ù–æ –¥–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å 0 –∏–ª–∏ –Ω–∞—á–∏–Ω–∞—è —Å –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è LSTM –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–æ–≤–∞, –µ—Å–ª–∏ –æ–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "**–ü–ª—é—Å—ã:**\n",
    "\n",
    "- –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞ —Å—á–µ—Ç —É—á–µ—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –æ–±–µ–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö.\n",
    "\n",
    "**–ú–∏–Ω—É—Å—ã:**\n",
    "\n",
    "- –í—ã—Å–æ–∫–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∏ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å –≤ –±–æ–ª—å—à–æ–º –æ–±—ä–µ–º–µ –ø–∞–º—è—Ç–∏.\n",
    "- –£–≤–µ–ª–∏—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ.\n",
    "- –ù–µ–ø–æ–¥—Ö–æ–¥—è—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å 0 –∏–ª–∏ –∂–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1167113,
     "sourceId": 2730445,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
