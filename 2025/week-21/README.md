[![Made With Love](https://img.shields.io/badge/Made%20With-Love-orange.svg)](https://github.com/chetanraj/awesome-github-badges)
[![Open Source](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)](https://opensource.org/)

# 📘 Generative Models and Transformers — A Jupyter Practical Guide

Welcome to the practical guide on modern architectures in machine learning! This repository is inspired by laboratory work and consists of a series of Jupyter Notebooks with live examples, visualizations, and step-by-step explanations. 📊🧠

## 📌 Contents

### 1. 🔁 Generative Models Based on RNN

**Folder:** [`SimpleRNN`](./SimpleRNN)  
**Notebook:** `SimpleRNN.ipynb`

- Character-level and word-level tokenization
- Training text generation

### 2. 🧠 LSTM: Going Deeper into Memory

**Folder:** [`LSTM`](./LSTM)  
**Notebooks:**  
- `Unidirectional_LSTM.ipynb`
- `Bidirectional_LSTM.ipynb`

- Comparison of unidirectional and bidirectional LSTMs

### 3. 🧬 Transformers and GPT Architecture

**Folder:** [`GPT`](./GPT)  
**Notebooks:**  
- `GPT.ipynb`
- `Fine-tuning_GPT.ipynb`

- Encoder/Decoder architecture
- Self-Attention and its optimizations: RoPE, scaling, masking
- QKV projections, normalization, Feed-Forward blocks
- Practical fine-tuning of GPT: layer freezing, handling sparse gradients

## 🧾 Goals and Benefits

- Understand when to use RNNs/LSTMs versus when to transition to transformers
- Master fine-tuning models for specific tasks
- Gain practical skills useful for both research and production

---

<div align="center">

**Explore with us 🚀**

⭐ Star this repository if you found it helpful

</div>