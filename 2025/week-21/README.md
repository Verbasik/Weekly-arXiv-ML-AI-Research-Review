[![Made With Love](https://img.shields.io/badge/Made%20With-Love-orange.svg)](https://github.com/chetanraj/awesome-github-badges)
[![Open Source](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)](https://opensource.org/)

# ğŸ“˜ Generative Models and Transformers â€” A Jupyter Practical Guide

Welcome to the practical guide on modern architectures in machine learning! This repository is inspired by laboratory work and consists of a series of Jupyter Notebooks with live examples, visualizations, and step-by-step explanations. ğŸ“ŠğŸ§ 

## ğŸ“Œ Contents

### 1. ğŸ” Generative Models Based on RNN

**Folder:** [`SimpleRNN`](./SimpleRNN)  
**Notebook:** `SimpleRNN.ipynb`

- Character-level and word-level tokenization
- Training text generation

### 2. ğŸ§  LSTM: Going Deeper into Memory

**Folder:** [`LSTM`](./LSTM)  
**Notebooks:**  
- `Unidirectional_LSTM.ipynb`
- `Bidirectional_LSTM.ipynb`

- Comparison of unidirectional and bidirectional LSTMs

### 3. ğŸ§¬ Transformers and GPT Architecture

**Folder:** [`GPT`](./GPT)  
**Notebooks:**  
- `GPT.ipynb`
- `Fine-tuning_GPT.ipynb`

- Encoder/Decoder architecture
- Self-Attention and its optimizations: RoPE, scaling, masking
- QKV projections, normalization, Feed-Forward blocks
- Practical fine-tuning of GPT: layer freezing, handling sparse gradients

## ğŸ§¾ Goals and Benefits

- Understand when to use RNNs/LSTMs versus when to transition to transformers
- Master fine-tuning models for specific tasks
- Gain practical skills useful for both research and production

---

<div align="center">

**Explore with us ğŸš€**

â­ Star this repository if you found it helpful

</div>