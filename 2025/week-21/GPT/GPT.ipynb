{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ–≥–Ω–∞–ª–∏ —Ç—Ä–µ–Ω–∏—Ç—å GPT\n",
    "\n",
    "<details>\n",
    "  <summary>–í—Å–ø–æ–º–Ω–∏–º, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä ü§ñ</summary>\n",
    "\n",
    "–í –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –æ–ø–∏—Å–∞–Ω–Ω–æ–π –≤ —Å—Ç–∞—Ç—å–µ ¬´–í–Ω–∏–º–∞–Ω–∏–µ ‚Äî —ç—Ç–æ –≤—Å–µ, —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ¬ª, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∞ –Ω–∞ –¥–≤–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —á–∞—Å—Ç–∏: –∫–æ–¥–µ—Ä –∏ –¥–µ–∫–æ–¥–µ—Ä. –û–±–µ —á–∞—Å—Ç–∏ —Å–æ—Å—Ç–æ—è—Ç –∏–∑ —Å–ª–æ–µ–≤, –∏–º–µ—é—â–∏—Ö –æ–¥–∏–Ω–∞–∫–æ–≤—É—é –æ–±—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –Ω–æ —Å–ª—É–∂–∞—â–∏—Ö —Ä–∞–∑–Ω—ã–º —Ü–µ–ª—è–º.\n",
    "\n",
    "–ù–∞ —Ä–∏—Å—É–Ω–∫–µ –Ω–∏–∂–µ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ Transformer. –û–Ω–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —á–∞—Å—Ç–µ–π: **–∫–æ–¥–µ—Ä–∞ (encoder)** –∏ **–¥–µ–∫–æ–¥–µ—Ä–∞ (decoder)**.\n",
    "\n",
    "![Figure_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-04/assets/Figure_1.png)\n",
    "\n",
    "### –ö–æ–¥–µ—Ä (Encoder)\n",
    "–ö–æ–¥–µ—Ä –æ–±—ã—á–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ª–µ–≤–æ–π —á–∞—Å—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –û–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–µ–≤, –∫–∞–∂–¥—ã–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –≤–∫–ª—é—á–∞–µ—Ç:\n",
    "1. **Multi-Head Attention** ‚Äî –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "2. **Add & Norm** ‚Äî —Å–ª–æ–π, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–±–∞–≤–ª—è–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –≤–Ω–∏–º–∞–Ω–∏—è (residual connection) –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é.\n",
    "3. **Feed Forward** ‚Äî –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –∫–∞–∂–¥–æ–º—É —ç–ª–µ–º–µ–Ω—Ç—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ.\n",
    "4. **Add & Norm** ‚Äî —Å–Ω–æ–≤–∞ –¥–æ–±–∞–≤–ª—è–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç.\n",
    "\n",
    "–≠—Ç–∏ —Å–ª–æ–∏ –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ (–æ–±—ã—á–Ω–æ 6 –∏–ª–∏ –±–æ–ª–µ–µ) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥–ª—É–±–æ–∫–æ–π –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "### –î–µ–∫–æ–¥–µ—Ä (Decoder)\n",
    "–î–µ–∫–æ–¥–µ—Ä –æ–±—ã—á–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø—Ä–∞–≤–æ–π —á–∞—Å—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –û–Ω —Ç–∞–∫–∂–µ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–µ–≤, –Ω–æ –∏–º–µ–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:\n",
    "1. **Masked Multi-Head Attention** ‚Äî –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –º–∞—Å–∫–∏—Ä—É–µ—Ç –±—É–¥—É—â–∏–µ —Ç–æ–∫–µ–Ω—ã, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å \"–ø–æ–¥–≥–ª—è–¥—ã–≤–∞–Ω–∏–µ\" –≤–ø–µ—Ä–µ–¥.\n",
    "2. **Add & Norm** ‚Äî —Å–ª–æ–π, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–±–∞–≤–ª—è–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –≤–Ω–∏–º–∞–Ω–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç.\n",
    "3. **Multi-Head Attention** ‚Äî –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç –≤—ã—Ö–æ–¥ –∫–æ–¥–µ—Ä–∞.\n",
    "4. **Add & Norm** ‚Äî —Å–Ω–æ–≤–∞ –¥–æ–±–∞–≤–ª—è–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç.\n",
    "5. **Feed Forward** ‚Äî –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π —Ç–æ–º—É, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–µ.\n",
    "6. **Add & Norm** ‚Äî –∑–∞–≤–µ—Ä—à–∞—é—â–∏–π —Å–ª–æ–π –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
    "\n",
    "### –í—Ö–æ–¥—ã –∏ –≤—ã—Ö–æ–¥—ã\n",
    "- **Input Embedding** –∏ **Positional Encoding** –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥–∞—é—Ç—Å—è –≤ –∫–æ–¥–µ—Ä.\n",
    "- **Output Embedding** –∏ **Outputs (shifted right)** –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ –≤—ã—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –¥–µ–∫–æ–¥–µ—Ä–æ–º.\n",
    "\n",
    "### **–ö–æ–¥–µ—Ä**:\n",
    "\n",
    "#### –†–æ–ª–∏:\n",
    "–†–æ–ª—å –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –æ—Ç—Ä–∞–∂–∞—é—â–µ–≥–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ª–æ–≤–∞–º–∏ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏). –≠—Ç–∞ —á–∞—Å—Ç—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—è –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∏–∫–∞–∫–æ–≥–æ –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –æ–Ω–∞ –ø—Ä–æ—Å—Ç–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ–∫–æ–¥–µ—Ä.\n",
    "\n",
    "#### –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã:\n",
    "–ö–∞–∂–¥—ã–π —É—Ä–æ–≤–µ–Ω—å –∫–æ–¥–µ—Ä–∞ –∏–º–µ–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º—ã —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è –∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –ø—Ä—è–º–æ–π —Å–≤—è–∑–∏. –ú–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ –∫–æ–¥–µ—Ä–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –≤—Å–µ –ø–æ–∑–∏—Ü–∏–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —É—Ä–æ–≤–Ω—è –∫–æ–¥–µ—Ä–∞ ‚Äî —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –æ–Ω –º–æ–∂–µ—Ç –∏–∑—É—á–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞.\n",
    "\n",
    "#### –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è:\n",
    "–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–µ—Ä–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π —Å–µ—Ä–∏—é –≤–µ–∫—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –≤—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –≠—Ç–∏ –≤–µ–∫—Ç–æ—Ä—ã —á–∞—Å—Ç–æ –Ω–∞–∑—ã–≤–∞—é—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º–∏ –≤–ª–æ–∂–µ–Ω–∏—è–º–∏, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–∏ –∫–æ–¥–∏—Ä—É—é—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞, –Ω–æ –∏ –∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–Ω—É—Ç—Ä–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.\n",
    "\n",
    "#### –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:\n",
    "\n",
    "1. **–°–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ (Self-Attention)**:\n",
    "   - **–í—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä $X$**: –≠—Ç–æ –º–∞—Ç—Ä–∏—Ü–∞, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∞—è –≤—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ª–æ–≤–∞ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏) –≤ –≤–∏–¥–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n",
    "   - **–ö–ª—é—á–∏ $K$**, **–∑–∞–ø—Ä–æ—Å—ã $Q$** –∏ **–∑–Ω–∞—á–µ–Ω–∏—è $V$**: –≠—Ç–∏ –º–∞—Ç—Ä–∏—Ü—ã –ø–æ–ª—É—á–∞—é—Ç—Å—è –ø—É—Ç–µ–º —É–º–Ω–æ–∂–µ–Ω–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ $X$ –Ω–∞ –≤–µ—Å–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã $W_K$, $W_Q$ –∏ $W_V$ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ:\n",
    "\n",
    "     $$\n",
    "     Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
    "     $$\n",
    "\n",
    "   - **–í–Ω–∏–º–∞–Ω–∏–µ $A$**: –í—ã—á–∏—Å–ª—è–µ—Ç—Å—è –ø—É—Ç–µ–º —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∫–ª—é—á–µ–π, –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–π $d_k$:\n",
    "   \n",
    "     $$\n",
    "     A = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "     $$\n",
    "\n",
    "   - **–í—ã—Ö–æ–¥ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è $Z$**: –≠—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É–º–Ω–æ–∂–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏—è:\n",
    "\n",
    "     $$\n",
    "     Z = AV\n",
    "     $$\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def self_attention(X, d_k):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "      –†–µ–∞–ª–∏–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º Self-Attention.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): –í—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä (–º–∞—Ç—Ä–∏—Ü–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤), —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é (n_samples, embedding_dim).\n",
    "        d_k (int): –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–π.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: –í—ã—Ö–æ–¥ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è Z.\n",
    "    \"\"\"\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤—ã—Ö –º–∞—Ç—Ä–∏—Ü —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏\n",
    "    embedding_dim = X.shape[1]\n",
    "    W_Q = np.random.rand(embedding_dim, d_k)\n",
    "    W_K = np.random.rand(embedding_dim, d_k)\n",
    "    W_V = np.random.rand(embedding_dim, d_k)\n",
    "\n",
    "    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "    Q = X @ W_Q\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "\n",
    "    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "    attention_scores = Q @ K.T\n",
    "    scaled_attention_scores = attention_scores / np.sqrt(d_k)\n",
    "\n",
    "    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "    attention_weights = np.exp(scaled_attention_scores) / np.sum(np.exp(scaled_attention_scores), axis=-1, keepdims=True)\n",
    "\n",
    "    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞\n",
    "    Z = attention_weights @ V\n",
    "\n",
    "    return Z\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "# –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —É –Ω–∞—Å –µ—Å—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ 3 —Å–ª–æ–≤, –≥–¥–µ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –≤–µ–∫—Ç–æ—Ä–æ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 4\n",
    "X = np.array([[1.0, 0.5, 0.2, 0.8],\n",
    "              [0.3, 0.9, 0.1, 0.4],\n",
    "              [0.6, 0.2, 0.7, 0.5]])\n",
    "\n",
    "d_k = 2  # –ü—Ä–∏–º–µ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∫–ª—é—á–µ–π\n",
    "\n",
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è\n",
    "output_Z = self_attention(X, d_k)\n",
    "\n",
    "print(\"–í—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä X:\\n\", X)\n",
    "print(\"\\n–í—ã—Ö–æ–¥ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è Z:\\n\", output_Z)\n",
    "```\n",
    "\n",
    "2. **–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –ø—Ä—è–º–æ–π —Å–≤—è–∑–∏ (Feedforward Neural Network)**:\n",
    "   - –í—ã—Ö–æ–¥ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è $Z$ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ –¥–≤–∞ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ—è —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ReLU) –º–µ–∂–¥—É –Ω–∏–º–∏:\n",
    "\n",
    "     $$\n",
    "     \\text{FFN}(Z) = \\max(0, ZW_1 + b_1)W_2 + b_2\n",
    "     $$\n",
    "\n",
    "   - **$W_1$, $W_2$**: –í–µ—Å–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –ø–µ—Ä–≤–æ–≥–æ –∏ –≤—Ç–æ—Ä–æ–≥–æ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è.\n",
    "   - **$b_1$, $b_2$**: –°–º–µ—â–µ–Ω–∏—è –ø–µ—Ä–≤–æ–≥–æ –∏ –≤—Ç–æ—Ä–æ–≥–æ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è.\n",
    "\n",
    "#### **–í–µ—Å–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã $W_K$, $W_Q$ –∏ $W_V$ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤**\n",
    "\n",
    "1. –û–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "\n",
    "  –í –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –≤–∞–∂–Ω–µ–π—à—É—é —Ä–æ–ª—å –∏–≥—Ä–∞–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—ã–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ª–æ–≤ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏). –î–ª—è —ç—Ç–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: **–∑–∞–ø—Ä–æ—Å—ã (queries)**, **–∫–ª—é—á–∏ (keys)** –∏ **–∑–Ω–∞—á–µ–Ω–∏—è (values)**. –≠—Ç–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –≤–µ—Å–æ–≤—ã—Ö –º–∞—Ç—Ä–∏—Ü $W_Q$, $W_K$ –∏ $W_V$ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.\n",
    "\n",
    "2. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤—ã—Ö –º–∞—Ç—Ä–∏—Ü\n",
    "\n",
    "  2.1 –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
    "  \n",
    "  –î–æ–ø—É—Å—Ç–∏–º, —É –Ω–∞—Å –µ—Å—Ç—å –≤—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å $X$ —Ä–∞–∑–º–µ—Ä–æ–º $n \\times d_{model}$, –≥–¥–µ:\n",
    "  - $n$ ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).\n",
    "  - $d_{model}$ ‚Äî —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 512 –∏–ª–∏ 1024).\n",
    "\n",
    "  –≠—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å $X$ –ø–æ—Å—Ç—É–ø–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—É, –≥–¥–µ —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ –ª–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç –µ—ë –≤ –∑–∞–ø—Ä–æ—Å—ã $Q$, –∫–ª—é—á–∏ $K$ –∏ –∑–Ω–∞—á–µ–Ω–∏—è $V$.\n",
    "\n",
    "  2.2 –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.\n",
    "\n",
    "  –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π, –≤—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ —Ç—Ä–∏ —Ä–∞–∑–Ω—ã–µ –≤–µ—Å–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã $W_Q$, $W_K$ –∏ $W_V$:\n",
    "\n",
    "  $$\n",
    "  Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
    "  $$\n",
    "\n",
    "  –≥–¥–µ:\n",
    "  - $W_Q$ ‚Äî –≤–µ—Å–æ–≤–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Ä–∞–∑–º–µ—Ä–∞ $d_{model} \\times d_k$.\n",
    "  - $W_K$ ‚Äî –≤–µ—Å–æ–≤–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –∫–ª—é—á–µ–π —Ä–∞–∑–º–µ—Ä–∞ $d_{model} \\times d_k$.\n",
    "  - $W_V$ ‚Äî –≤–µ—Å–æ–≤–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –∑–Ω–∞—á–µ–Ω–∏–π —Ä–∞–∑–º–µ—Ä–∞ $d_{model} \\times d_v$.\n",
    "\n",
    "  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å $d_k$ –∏ $d_v$ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã–±—Ä–∞–Ω–∞ –ø–æ-—Ä–∞–∑–Ω–æ–º—É, –Ω–æ –æ–±—ã—á–Ω–æ $d_k = d_v = d_{model}$. –≠—Ç–∏ –≤–µ—Å–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –æ–±—É—á–∞—é—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ –∏ –∏—Ö –∑–∞–¥–∞—á–∞ ‚Äî –Ω–∞–π—Ç–∏ —Ç–∞–∫–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ —É—á–∏—Ç—ã–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç.\n",
    "\n",
    "3. –ü–æ—á–µ–º—É –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–∞–∑–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã?\n",
    "\n",
    "  3.1 –ó–∞–ø—Ä–æ—Å—ã ($W_Q$)\n",
    "\n",
    "  –ú–∞—Ç—Ä–∏—Ü–∞ $W_Q$ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –≤ –∑–∞–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥—Ä—É–≥–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –¥–∞–Ω–Ω–æ–≥–æ. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å –ø—ã—Ç–∞–µ—Ç—Å—è \"–≤—ã—è—Å–Ω–∏—Ç—å\", –Ω–∞ –∫–∞–∫–∏–µ –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –µ–º—É —Å–ª–µ–¥—É–µ—Ç –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ.\n",
    "\n",
    "  3.2 –ö–ª—é—á–∏ ($W_K$)\n",
    "\n",
    "  –ú–∞—Ç—Ä–∏—Ü–∞ $W_K$ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –≤ –∫–ª—é—á–∏. –ö–ª—é—á–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å –∑–∞–ø—Ä–æ—Å–∞–º–∏. –ü–æ —Å—É—Ç–∏, –∫–ª—é—á–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–º, \"–Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∞–∂–µ–Ω\" –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ —Å –∑–∞–ø—Ä–æ—Å–æ–º. –ß–µ–º –±–ª–∏–∂–µ –∑–∞–ø—Ä–æ—Å –∫ –∫–ª—é—á—É (–ø–æ –º–µ—Ä–µ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è), —Ç–µ–º –±–æ–ª—å—à–µ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –±—É–¥–µ—Ç —É–¥–µ–ª–µ–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º—É —ç–ª–µ–º–µ–Ω—Ç—É.\n",
    "\n",
    "  3.3 –ó–Ω–∞—á–µ–Ω–∏—è ($W_V$)\n",
    "\n",
    "  –ú–∞—Ç—Ä–∏—Ü–∞ $W_V$ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –≤ –∑–Ω–∞—á–µ–Ω–∏—è. –ó–Ω–∞—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥–∞—é—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è. –ò—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –±—É–¥–µ—Ç –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π —Å—É–º–º–æ–π –∑–Ω–∞—á–µ–Ω–∏–π –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤, –≥–¥–µ –≤–µ—Å–∞ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è —Å—Ç–µ–ø–µ–Ω—å—é –≤–Ω–∏–º–∞–Ω–∏—è (–≤—ã—á–∏—Å–ª–µ–Ω–Ω–æ–π —á–µ—Ä–µ–∑ –∑–∞–ø—Ä–æ—Å—ã –∏ –∫–ª—é—á–∏).\n",
    "\n",
    "4. –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "\n",
    "  4.1 –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "\n",
    "  –ó–∞–ø—Ä–æ—Å—ã $Q$ –∏ –∫–ª—é—á–∏ $K$ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è. –î–ª—è —ç—Ç–æ–≥–æ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∫–ª—é—á–µ–π —Å –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $d_k$ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º softmax:\n",
    "\n",
    "  $$\n",
    "  A = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "  $$\n",
    "\n",
    "  –ì–¥–µ $A$ ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ –≤–Ω–∏–º–∞–Ω–∏—è, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è, –∫–∞–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞–∏–±–æ–ª—å—à–µ–µ –≤–ª–∏—è–Ω–∏–µ –¥—Ä—É–≥ –Ω–∞ –¥—Ä—É–≥–∞.\n",
    "\n",
    "  4.2 –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –∫ –∑–Ω–∞—á–µ–Ω–∏—è–º\n",
    "\n",
    "  –ü–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è, –µ—ë —É–º–Ω–æ–∂–∞—é—Ç –Ω–∞ –º–∞—Ç—Ä–∏—Ü—É –∑–Ω–∞—á–µ–Ω–∏–π $V$, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, —É—á–∏—Ç—ã–≤–∞—é—â–µ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç:\n",
    "\n",
    "  $$\n",
    "  Z = AV\n",
    "  $$\n",
    "\n",
    "5. –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n",
    "\n",
    "  –í–µ—Å–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã $W_Q$, $W_K$ –∏ $W_V$ —Å–ª—É–∂–∞—Ç –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –∑–∞–ø—Ä–æ—Å—ã, –∫–ª—é—á–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è. –≠—Ç–∏ –º–∞—Ç—Ä–∏—Ü—ã –æ–±—É—á–∞—é—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ –∏ –∏–≥—Ä–∞—é—Ç —Ä–µ—à–∞—é—â—É—é —Ä–æ–ª—å –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Ç–æ–≥–æ, –∫–∞–∫ –º–æ–¥–µ–ª—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏.\n",
    "\n",
    "  –¢–∞–∫–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–∞—Ö –ø–µ—Ä–µ–≤–æ–¥–∞, –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –º–Ω–æ–≥–∏—Ö –¥—Ä—É–≥–∏—Ö.\n",
    "\n",
    "## **–û—Ç –≤—Ö–æ–¥–∞ –∫ –≤—ã—Ö–æ–¥—É –≤–Ω—É—Ç—Ä–∏ –∫–æ–¥–µ—Ä–∞**\n",
    "\n",
    "### 1. **–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:**\n",
    "\n",
    "  - **–í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å $X$**: –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $(N, L, D_{model})$, \n",
    "\n",
    "  –≥–¥–µ:\n",
    "\n",
    "  **Batch Size ($N$) - –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞:**\n",
    "\n",
    "  * Batch Size –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Å–∫–æ–ª—å–∫–æ **–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö** –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å—é **–æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ** –∑–∞ –æ–¥–∏–Ω –ø–æ–ª–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è).\n",
    "\n",
    "  **–î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ($L$) - Length of the sequence:**\n",
    "\n",
    "  * –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ **—ç–ª–µ–º–µ–Ω—Ç–æ–≤** (—Ç–æ–∫–µ–Ω–æ–≤) –≤ –∫–∞–∂–¥–æ–π **–æ—Ç–¥–µ–ª—å–Ω–æ–π** –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–Ω—É—Ç—Ä–∏ –ø–∞–∫–µ—Ç–∞.\n",
    "\n",
    "  **–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ($D_{model}$) - Dimension of the embeddings:**\n",
    "\n",
    "  * –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∫–æ–≥–æ-–ª–∏–±–æ –æ–±—ä–µ–∫—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ª–æ–≤–∞, —Ç–æ–∫–µ–Ω–∞).\n",
    "\n",
    "  **–ö–∞–∫ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –º–∞—Ç—Ä–∏—Ü–∞ X:**\n",
    "\n",
    "  1. **–≠–º–±–µ–¥–¥–∏–Ω–≥ —ç–ª–µ–º–µ–Ω—Ç–æ–≤:**  –ö–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ª–æ–≤–æ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏) –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥ - –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $D_{model}$.\n",
    "\n",
    "  2. **–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:** –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –ø–∞–∫–µ—Ç–µ –º—ã –ø–æ–ª—É—á–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $(L, D_{model})$. –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ —ç—Ç–æ–π –º–∞—Ç—Ä–∏—Ü—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥—É –æ–¥–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —É –Ω–∞—Å –µ—Å—Ç—å $L$ —Å—Ç—Ä–æ–∫ (–ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏) –∏ $D_{model}$ —Å—Ç–æ–ª–±—Ü–æ–≤ (–ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞).\n",
    "\n",
    "  3. **–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–∞:**  –ü–æ—Å–∫–æ–ª—å–∫—É —É –Ω–∞—Å –µ—Å—Ç—å $N$ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ –ø–∞–∫–µ—Ç–µ, –º—ã \"—Å–∫–ª–∞–¥—ã–≤–∞–µ–º\" —ç—Ç–∏ –º–∞—Ç—Ä–∏—Ü—ã $(L, D_{model})$ –¥—Ä—É–≥ –Ω–∞ –¥—Ä—É–≥–∞. –≠—Ç–æ —Å–æ–∑–¥–∞–µ—Ç —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä (–∏–ª–∏ –º–∞—Ç—Ä–∏—Ü—É —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 3) $X$ —Å —Ä–∞–∑–º–µ—Ä–∞–º–∏ $(N, L, D_{model})$.\n",
    "\n",
    "### **–í –∏—Ç–æ–≥–µ:**\n",
    "\n",
    "–ú–∞—Ç—Ä–∏—Ü–∞ $X$ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $(N, L, D_{model})$ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ç—Ä–µ—Ö–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä. –ï—Å–ª–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –µ–≥–æ –∫–∞–∫ –Ω–∞–±–æ—Ä –¥–≤—É–º–µ—Ä–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü, —Ç–æ –æ–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ $N$ –º–∞—Ç—Ä–∏—Ü —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $(L, D_{model})$, \"–Ω–∞–ª–æ–∂–µ–Ω–Ω—ã—Ö\" –¥—Ä—É–≥ –Ω–∞ –¥—Ä—É–≥–∞. –ö–∞–∂–¥–∞—è –∏–∑ —ç—Ç–∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –º–∞—Ç—Ä–∏—Ü (—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –ø–∞–∫–µ—Ç–µ) –∏–º–µ–µ—Ç $L$ —Å—Ç—Ä–æ–∫ (–ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏) –∏ $D_{model}$ —Å—Ç–æ–ª–±—Ü–æ–≤ (–ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤).\n",
    "\n",
    "### **–ü—Ä–∏–º–µ—Ä:**\n",
    "\n",
    "```Python\n",
    "import torch\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –±–∞—Ç—á–µ\n",
    "sentences = [\n",
    "    \"–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –Ø —É–≤–ª–µ–∫–∞—é—Å—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º\",  # 7 —Ç–æ–∫–µ–Ω–æ–≤\n",
    "    \"–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\",                                   # 4 —Ç–æ–∫–µ–Ω–∞\n",
    "    \"–ò–ò ‚Äî —ç—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ!\"                                  # 5 —Ç–æ–∫–µ–Ω–æ–≤\n",
    "]\n",
    "\n",
    "# 1. –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "batch_tokens = [\n",
    "    [\"[CLS]\", \"–í—Å–µ–º\", \"–ø—Ä–∏–≤–µ—Ç\", \"!\", \"–Ø\", \"—É–≤–ª–µ–∫–∞—é—Å—å\", \"–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º\", \"–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º\", \"[SEP]\"],  # L=9\n",
    "    [\"[CLS]\", \"–ü—Ä–∏–≤–µ—Ç\", \",\", \"–∫–∞–∫\", \"–¥–µ–ª–∞\", \"?\", \"[SEP]\", \"[PAD]\", \"[PAD]\"],                      # L=9 (—Å –ø–∞–¥–¥–∏–Ω–≥–æ–º)\n",
    "    [\"[CLS]\", \"–ò–ò\", \"‚Äî\", \"—ç—Ç–æ\", \"–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ\", \"!\", \"[SEP]\", \"[PAD]\", \"[PAD]\"]                      # L=9 (—Å –ø–∞–¥–¥–∏–Ω–≥–æ–º)\n",
    "]\n",
    "\n",
    "N = len(sentences)  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ = 3\n",
    "L = 9               # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–ø–æ—Å–ª–µ –ø–∞–¥–¥–∏–Ω–≥–∞)\n",
    "D_model = 512       # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "\n",
    "# 2. –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ–≥–æ –±–∞—Ç—á–∞\n",
    "embeddings = torch.randn(N, L, D_model)\n",
    "\n",
    "# 3. –§–æ—Ä–º–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä X\n",
    "X = embeddings\n",
    "\n",
    "print(\"–§–æ—Ä–º–∞ —Ç–µ–Ω–∑–æ—Ä–∞ X:\", X.shape)  # (3, 9, 512)\n",
    "print(\"\\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "print(f\"‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π (N): {N}\")\n",
    "print(f\"‚Ä¢ –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (L): {L} (—Å —É—á–µ—Ç–æ–º –ø–∞–¥–¥–∏–Ω–≥–∞)\")\n",
    "print(f\"‚Ä¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (D_model): {D_model}\\n\")\n",
    "\n",
    "# ================= –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ =================\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ\n",
    "for batch_idx in range(N):\n",
    "    print(f\"–ë–∞—Ç—á {batch_idx + 1} ({sentences[batch_idx][:20]}...):\")\n",
    "    print(f\"–¢–æ–∫–µ–Ω—ã: {batch_tokens[batch_idx]}\")\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 —ç–ª–µ–º–µ–Ω—Ç–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–∑–∏—Ü–∏–π\n",
    "    print(\"\\n–ü—Ä–∏–º–µ—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:\")\n",
    "    print(f\"‚Ä¢ [CLS] —Ç–æ–∫–µ–Ω: {X[batch_idx, 0, :3].detach().numpy().round(4)}...\")\n",
    "    print(f\"‚Ä¢ 3-–π —Ç–æ–∫–µ–Ω:   {X[batch_idx, 2, :3].detach().numpy().round(4)}...\")\n",
    "    print(f\"‚Ä¢ [SEP] —Ç–æ–∫–µ–Ω: {X[batch_idx, -3, :3].detach().numpy().round(4)}...\")\n",
    "    print(f\"‚Ä¢ [PAD] —Ç–æ–∫–µ–Ω: {X[batch_idx, -1, :3].detach().numpy().round(4)}...\")\n",
    "    print(\"-\" * 60)\n",
    "```\n",
    "\n",
    "```Python\n",
    "# ================= –ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ =================\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ X\n",
    "for batch_idx in range(N):\n",
    "    print(f\"–ë–∞—Ç—á {batch_idx + 1} ({sentences[batch_idx][:20]}...):\")\n",
    "    print(f\"–¢–æ–∫–µ–Ω—ã: {batch_tokens[batch_idx]}\")\n",
    "    \n",
    "    # –í—ã–≤–æ–¥–∏–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "    print(\"\\n–≠–º–±–µ–¥–¥–∏–Ω–≥–∏:\")\n",
    "    for token_idx in range(L):\n",
    "        print(f\"‚Ä¢ –¢–æ–∫–µ–Ω {token_idx}: {X[batch_idx, token_idx].detach().numpy().round(4)}...\")\n",
    "    print(\"-\" * 60)\n",
    "```\n",
    "\n",
    "### 2. **–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (Positional Encodings):**\n",
    "\n",
    "  **–ó–∞—á–µ–º –≤–æ–æ–±—â–µ –Ω—É–∂–Ω—ã –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è?** –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (RNN), —Ç–∞–∫–∏–µ –∫–∞–∫ LSTM –∏–ª–∏ GRU, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω –∑–∞ —Ç–æ–∫–µ–Ω–æ–º, –∏ –ø–æ—Ä—è–¥–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º —É—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ–ª–æ–∂–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞. –û–¥–Ω–∞–∫–æ, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –º–µ—Ö–∞–Ω–∏–∑–º–µ –≤–Ω–∏–º–∞–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ Transformer, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ. –ò–∑-–∑–∞ —ç—Ç–æ–≥–æ –æ–Ω–∏ —Ç–µ—Ä—è—é—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Ä—è–¥–∫–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤. –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤–≤–æ–¥—è—Ç—Å—è –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã \"–ø–æ–¥—Å–∫–∞–∑–∞—Ç—å\" –º–æ–¥–µ–ª–∏, –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "  –ö–æ–≥–¥–∞ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞), –∫–æ–¥–µ—Ä Transformer —Ç–∞–∫–∂–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, —Ñ–æ—Ä–º–∏—Ä—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞.\n",
    "\n",
    "  –¢–∞–∫ –∂–µ –≤–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è ($PE$) –∏–º–µ—é—Ç —Ç—É –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, —á—Ç–æ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ ($D_{model}$). –≠—Ç–æ –∫–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç, –ø–æ—Å–∫–æ–ª—å–∫—É –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–∫–ª–∞–¥—ã–≤–∞—Ç—å –∏—Ö –≤–º–µ—Å—Ç–µ –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ.\n",
    "\n",
    "  ![Figure_2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-04/assets/Figure_2.png)\n",
    "\n",
    "  - –î–ª—è —É—á–µ—Ç–∞ –ø–æ—Ä—è–¥–∫–∞ —Å–ª–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ –≤—Ö–æ–¥–Ω—ã–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è $PE$ —Ç–æ–π –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $D_{model}$.\n",
    "  - $PE$ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –ø–æ —Å–ª–µ–¥—É—é—â–∏–º —Ñ–æ—Ä–º—É–ª–∞–º:\n",
    "    $$\n",
    "    PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/D_{model}}}\\right) \\\\\n",
    "    PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/D_{model}}}\\right)\n",
    "    $$\n",
    "    \n",
    "    –≥–¥–µ:\n",
    "\n",
    "  *   **$pos$ (–ø–æ–∑–∏—Ü–∏—è):**\n",
    "      *   –ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ, —É–∫–∞–∑—ã–≤–∞—é—â–µ–µ –Ω–∞ –ø–æ–∑–∏—Ü–∏—é —Ç–æ–∫–µ–Ω–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "      *   –ù—É–º–µ—Ä–∞—Ü–∏—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 0. –ü–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –∏–º–µ–µ—Ç $pos = 0$, –≤—Ç–æ—Ä–æ–π - $pos = 1$, –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ.\n",
    "      *   –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ \"–°–æ–±–∞–∫–∞ –ª–∞–µ—Ç –≥—Ä–æ–º–∫–æ\", \"–°–æ–±–∞–∫–∞\" –∏–º–µ–µ—Ç $pos = 0$, \"–ª–∞–µ—Ç\" - $pos = 1$, \"–≥—Ä–æ–º–∫–æ\" - $pos = 2$.\n",
    "  *   **$i$ (–∏–Ω–¥–µ–∫—Å –∏–∑–º–µ—Ä–µ–Ω–∏—è):**\n",
    "      *   –≠—Ç–æ –∏–Ω–¥–µ–∫—Å, –∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
    "      *   $i$ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç 0 –¥–æ $D_{model}/2 - 1$.\n",
    "      *   –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è $i$ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –ø–∞—Ä–∞ –∑–Ω–∞—á–µ–Ω–∏–π: –æ–¥–Ω–æ —Å —Å–∏–Ω—É—Å–æ–º, –¥—Ä—É–≥–æ–µ —Å –∫–æ—Å–∏–Ω—É—Å–æ–º.\n",
    "      *   –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ $D_{model} = 512$, —Ç–æ $i$ –±—É–¥–µ—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç 0 –¥–æ 255.\n",
    "  *   **$D_{model}$ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏):**\n",
    "      *   –≠—Ç–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–ª–æ–≤ –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–π.\n",
    "      *   –û–±—ã—á–Ω–æ —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ —Ä–∞–≤–Ω–æ 512, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏ –¥—Ä—É–≥–∏–º.\n",
    "      *   $D_{model}$ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–ª–∏–Ω—É –≤–µ–∫—Ç–æ—Ä–∞ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
    "\n",
    "  **–ü–æ—á–µ–º—É –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–∏–Ω—É—Å –∏ –∫–æ—Å–∏–Ω—É—Å?**\n",
    "\n",
    "  *   **–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å:** –°–∏–Ω—É—Å –∏ –∫–æ—Å–∏–Ω—É—Å –ø–æ–∑–≤–æ–ª—è—é—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏.\n",
    "  *   **–ü–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å:** –ü–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å —ç—Ç–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ª–µ–≥–∫–æ —Ä–∞–∑–ª–∏—á–∞—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "  *   **–≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è:** –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —á–µ–º —Ç–µ, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –æ–Ω–∞ –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞.\n",
    "  *   **–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏:** –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è–º–∏ –¥–ª—è —Å–æ—Å–µ–¥–Ω–∏—Ö –ø–æ–∑–∏—Ü–∏–π –æ—Å—Ç–∞–µ—Ç—Å—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ –ø–æ–Ω–∏–º–∞—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–æ–∂–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n",
    "  **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–π:**\n",
    "\n",
    "  –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ $pos$ –∏ –∫–∞–∂–¥–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è $i$ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç—Ä–µ—Ö–º–µ—Ä–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ $X$ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $(N, L, D_{model})$ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ. –ü–æ—Å–∫–æ–ª—å–∫—É $i$ –ø—Ä–æ–±–µ–≥–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç 0 –¥–æ $D_{model}/2 - 1$, –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ $pos$ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $D_{model}$. –ü–µ—Ä–≤—ã–µ $D_{model}/2$ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —ç—Ç–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Å–∏–Ω—É—Å–∞, –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ $D_{model}/2$ - —Å –ø–æ–º–æ—â—å—é –∫–æ—Å–∏–Ω—É—Å–∞.\n",
    "\n",
    "  **–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π:**\n",
    "\n",
    "  –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–º–µ—é—Ç —Ç—É –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, —á—Ç–æ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ ($D_{model}$), —á—Ç–æ–±—ã –∏—Ö –º–æ–∂–Ω–æ –±—ã–ª–æ –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ —Å–ª–æ–∂–∏—Ç—å. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—ã–≤–∞—Ç—å –∫–∞–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å–ª–æ–≤–∞ (–∏–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞), —Ç–∞–∫ –∏ –µ–≥–æ –ø–æ–∑–∏—Ü–∏—é –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–∏–∑ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è).\n",
    "\n",
    "### **–í –∏—Ç–æ–≥–µ:**\n",
    "\n",
    "–ò—Ç–æ–≥–æ–≤—ã–π –≤—Ö–æ–¥ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è –∫–æ–¥–µ—Ä–∞ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –ø—É—Ç–µ–º –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–≥–æ —Å–ª–æ–∂–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–ª–æ–≤ $X$ –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–π $PE$: $X_{embedded} = X + PE$.\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä:**\n",
    "\n",
    "–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ $D_{model} = 4$. –¢–æ–≥–¥–∞ –¥–ª—è –ø–æ–∑–∏—Ü–∏–∏ $pos = 1$ –∏ $i = 0$ –º—ã –ø–æ–ª—É—á–∏–º:\n",
    "\n",
    "$$PE_{(1, 0)} = \\sin\\left(\\frac{1}{10000^{0}}\\right) = \\sin(1) \\approx 0.84$$\n",
    "$$PE_{(1, 1)} = \\cos\\left(\\frac{1}{10000^{0}}\\right) = \\cos(1) \\approx 0.54$$\n",
    "\n",
    "–î–ª—è $i = 1$:\n",
    "\n",
    "$$PE_{(1, 2)} = \\sin\\left(\\frac{1}{10000^{2/4}}\\right) = \\sin\\left(\\frac{1}{100}\\right) \\approx 0.01$$\n",
    "$$PE_{(1, 3)} = \\cos\\left(\\frac{1}{10000^{2/4}}\\right) = \\cos\\left(\\frac{1}{100}\\right) \\approx 1$$\n",
    "\n",
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤–µ–∫—Ç–æ—Ä –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø–æ–∑–∏—Ü–∏–∏ 1 –±—É–¥–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ —Ä–∞–≤–µ–Ω [0.84, 0.54, 0.01, 1].\n",
    "\n",
    "- –ò—Ç–æ–≥–æ–≤—ã–π –≤—Ö–æ–¥ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è –∫–æ–¥–µ—Ä–∞: $X_{embedded} = X + PE$.\n",
    "\n",
    "```Python\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def positional_encoding(max_len: int, d_model: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–π –ø–æ —Ñ–æ—Ä–º—É–ª–µ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–∏ Transformer.\n",
    "\n",
    "    Args:\n",
    "        max_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "        d_model: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤).\n",
    "\n",
    "    Returns:\n",
    "        –¢–µ–Ω–∑–æ—Ä –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–π —Ñ–æ—Ä–º—ã (max_len, d_model).\n",
    "\n",
    "    Examples:\n",
    "        >>> pe = positional_encoding(10, 512)\n",
    "        >>> pe.shape\n",
    "        torch.Size([10, 512])\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "    \n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "def print_embeddings(tensor: torch.Tensor, tokens: list, title: str, max_elements: int = 3) -> None:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –º–µ—Ç–∫–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n",
    "    Args:\n",
    "        tensor: –¢–µ–Ω–∑–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n",
    "        tokens: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
    "        title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –≤—ã–≤–æ–¥–∞.\n",
    "        max_elements: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Examples:\n",
    "        >>> embeddings = torch.randn(5, 512)\n",
    "        >>> tokens = [\"token1\", \"token2\", \"token3\", \"token4\", \"token5\"]\n",
    "        >>> print_embeddings(embeddings, tokens, \"–ü—Ä–∏–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\")\n",
    "    \"\"\"\n",
    "    print(f\"\\n{title}:\")\n",
    "    for idx, (vec, token) in enumerate(zip(tensor, tokens)):\n",
    "        elements = vec[:max_elements].detach().numpy().round(4)\n",
    "        print(f\"{idx:2d} {token:15}: [{', '.join(f'{x:7.4f}' for x in elements)}...]\")\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö\n",
    "sentences = [\n",
    "    \"–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –Ø —É–≤–ª–µ–∫–∞—é—Å—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º\",\n",
    "    \"–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\",\n",
    "    \"–ò–ò ‚Äî —ç—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ!\"\n",
    "]\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
    "N = len(sentences)  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
    "L = 9               # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "D_model = 512       # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "\n",
    "# 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –ø–∞–¥–¥–∏–Ω–≥–æ–º\n",
    "batch_tokens = [\n",
    "    [\"[CLS]\", \"–í—Å–µ–º\", \"–ø—Ä–∏–≤–µ—Ç\", \"!\", \"–Ø\", \"—É–≤–ª–µ–∫–∞—é—Å—å\", \"–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º\", \"–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º\", \"[SEP]\"],\n",
    "    [\"[CLS]\", \"–ü—Ä–∏–≤–µ—Ç\", \",\", \"–∫–∞–∫\", \"–¥–µ–ª–∞\", \"?\", \"[SEP]\", \"[PAD]\", \"[PAD]\"],\n",
    "    [\"[CLS]\", \"–ò–ò\", \"‚Äî\", \"—ç—Ç–æ\", \"–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ\", \"!\", \"[SEP]\", \"[PAD]\", \"[PAD]\"]\n",
    "]\n",
    "\n",
    "# 2. –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "embeddings = torch.randn(N, L, D_model)\n",
    "\n",
    "# 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "pe = positional_encoding(L, D_model)\n",
    "\n",
    "# 4. –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è–º–∏\n",
    "X_embedded = embeddings + pe  # Broadcasting –¥–ª—è –±–∞—Ç—á–∞\n",
    "\n",
    "# ================= –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ =================\n",
    "print(\"=\"*60)\n",
    "print(\"–®–∞–≥ 1: –ò—Å—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(f\"–§–æ—Ä–º–∞ —Ç–µ–Ω–∑–æ—Ä–∞: {embeddings.shape}\")\n",
    "for batch_idx in range(N):\n",
    "    print(f\"\\n–ë–∞—Ç—á {batch_idx+1}: '{sentences[batch_idx]}'\")\n",
    "    print_embeddings(embeddings[batch_idx], batch_tokens[batch_idx], \"–ò—Å—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"–®–∞–≥ 2: –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
    "print(f\"–§–æ—Ä–º–∞ —Ç–µ–Ω–∑–æ—Ä–∞: {pe.shape}\")\n",
    "print_embeddings(pe, [f\"–ü–æ–∑–∏—Ü–∏—è {i}\" for i in range(L)], \"–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–π\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"–®–∞–≥ 3: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (X + PE)\")\n",
    "print(f\"–§–æ—Ä–º–∞ —Ç–µ–Ω–∑–æ—Ä–∞: {X_embedded.shape}\")\n",
    "for batch_idx in range(N):\n",
    "    print(f\"\\n–ë–∞—Ç—á {batch_idx+1}: '{sentences[batch_idx]}'\")\n",
    "    print_embeddings(X_embedded[batch_idx], batch_tokens[batch_idx], \"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–ª–æ–∂–µ–Ω–∏—è\")\n",
    "```\n",
    "\n",
    "```Python\n",
    "# ================= –ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ =================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞:\")\n",
    "batch_idx = 0\n",
    "\n",
    "# –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "print(f\"\\n–¢–µ–∫—Å—Ç: '{sentences[batch_idx]}'\")\n",
    "print(f\"–¢–æ–∫–µ–Ω—ã: {batch_tokens[batch_idx]}\")\n",
    "\n",
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–∑–∏—Ü–∏–π\n",
    "for pos in [0, 2, 4, 6, 8]:\n",
    "    print(f\"\\n–ü–æ–∑–∏—Ü–∏—è {pos} ({batch_tokens[batch_idx][pos]}):\")\n",
    "    print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥:  {embeddings[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "    print(f\"–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä:   {pe[pos, :3].detach().numpy().round(4)}\")\n",
    "    print(f\"–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π:     {X_embedded[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "```\n",
    "\n",
    "### 3. **–ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ (Multi-Head Self-Attention):**\n",
    "   - –ù–∞ –≤—Ö–æ–¥–µ –ø–æ–¥—Å–ª–æ—è Multi-Head Attention –Ω–∞—Ö–æ–¥–∏—Ç—Å—è $X_{embedded} = X + PE$.\n",
    "   - **–õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏:** –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ $X_{embedded}$ –ª–∏–Ω–µ–π–Ω–æ –ø—Ä–æ–µ—Ü–∏—Ä—É—é—Ç—Å—è –≤ –∑–∞–ø—Ä–æ—Å—ã $Q$, –∫–ª—é—á–∏ $K$ –∏ –∑–Ω–∞—á–µ–Ω–∏—è $V$ –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã:\n",
    "     $$\n",
    "     Q_i = X_{embedded} W_{Q_i}, \\quad K_i = X_{embedded} W_{K_i}, \\quad V_i = X_{embedded} W_{V_i}\n",
    "     $$\n",
    "     \n",
    "      –≥–¥–µ:\n",
    "      \n",
    "      - $W_{Q_i} \\in \\mathbb{R}^{D_{model} \\times D_k}$\n",
    "      - $W_{K_i} \\in \\mathbb{R}^{D_{model} \\times D_k}$\n",
    "      - $W_{V_i} \\in \\mathbb{R}^{D_{model} \\times D_v}$ - –≤–µ—Å–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –¥–ª—è $i$-–π –≥–æ–ª–æ–≤—ã\n",
    "      - $D_k$ - —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–π –∏ –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "      - $D_v$ - —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "      \n",
    "      –û–±—ã—á–Ω–æ $D_k = D_v = D_{model} / h$, –≥–¥–µ $h$ - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤.\n",
    "\n",
    "      –° —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä—ã, –ª–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è ‚Äî —ç—Ç–æ –ª–∏–Ω–µ–π–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä –∏–∑ –æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤ –¥—Ä—É–≥–æ–µ. –í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, –≤—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä $X_{embedded}$, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—â–∏–π $D_{model}$-–º–µ—Ä–Ω–æ–º—É –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤—É, –ø–æ–¥–≤–µ—Ä–≥–∞–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω—ã–º –ø—Ä–æ–µ–∫—Ü–∏—è–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç—Ä–µ—Ö –Ω–æ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤: $Q_i$, $K_i$ –∏ $V_i$. –≠—Ç–∏ –≤–µ–∫—Ç–æ—Ä—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Å–≤–æ–∏—Ö —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö.\n",
    "\n",
    "      **–§–æ—Ä–º–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:**\n",
    "\n",
    "      1. **$X_{embedded} \\in \\mathbb{R}^{D_{model}}$:** –í—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä –≤ $D_{model}$-–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.\n",
    "      2. **$W_{Q_i} \\in \\mathbb{R}^{D_{model} \\times D_k}$, $W_{K_i} \\in \\mathbb{R}^{D_{model} \\times D_k}$, $W_{V_i} \\in \\mathbb{R}^{D_{model} \\times D_v}$:** –ú–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏–µ –ª–∏–Ω–µ–π–Ω—ã–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n",
    "      3. **–õ–∏–Ω–µ–π–Ω—ã–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ø—Ä–æ–µ–∫—Ü–∏–∏):**\n",
    "        - $Q_i = X_{embedded} W_{Q_i}$: –ü—Ä–æ–µ–∫—Ü–∏—è $X_{embedded}$ –≤ $D_k$-–º–µ—Ä–Ω–æ–µ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤.\n",
    "        - $K_i = X_{embedded} W_{K_i}$: –ü—Ä–æ–µ–∫—Ü–∏—è $X_{embedded}$ –≤ $D_k$-–º–µ—Ä–Ω–æ–µ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∫–ª—é—á–µ–π.\n",
    "        - $V_i = X_{embedded} W_{V_i}$: –ü—Ä–æ–µ–∫—Ü–∏—è $X_{embedded}$ –≤ $D_v$-–º–µ—Ä–Ω–æ–µ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∑–Ω–∞—á–µ–Ω–∏–π.\n",
    "\n",
    "      **–ó–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ?**\n",
    "\n",
    "      - **–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞:** –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ —Å–æ–∑–¥–∞—é—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Ä–∞–∑–Ω—ã—Ö —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è.\n",
    "      - **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:** –ö–∞–∂–¥–æ–µ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–µ—Ç —Å–≤–æ—é —Ä–æ–ª—å: –∑–∞–ø—Ä–æ—Å—ã –∏—â—É—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–ª—é—á–∏, –∞ –∑–Ω–∞—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.\n",
    "      - **–û–±—É—á–∞–µ–º–æ—Å—Ç—å:** –ú–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤ $W_{Q_i}$, $W_{K_i}$ –∏ $W_{V_i}$ —è–≤–ª—è—é—Ç—Å—è –æ–±—É—á–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–µ.\n",
    "      - **–ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ—Å—Ç—å:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≥–æ–ª–æ–≤ (—Ä–∞–∑–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –º–∞—Ç—Ä–∏—Ü) –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —É—á–∏—Ç—ã–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç –µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.\n",
    "\n",
    "      –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ª–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è –≤ –º–µ—Ö–∞–Ω–∏–∑–º–µ –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è ‚Äî —ç—Ç–æ —Å–ø–æ—Å–æ–± –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞, –∫–∞–∂–¥–æ–µ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –∏–º–µ–µ—Ç —Å–≤–æ—é —Ä–æ–ª—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –ø—É—Ç–µ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ª–∏–Ω–µ–π–Ω—ã—Ö –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ–ø—Ä–µ–¥–µ–ª—è–µ–º—ã—Ö –æ–±—É—á–∞–µ–º—ã–º–∏ –º–∞—Ç—Ä–∏—Ü–∞–º–∏ –≤–µ—Å–æ–≤.\n",
    "\n",
    "   - **–í–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã:** –í—ã—á–∏—Å–ª—è–µ—Ç—Å—è –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π, –≥–¥–µ –≤–µ—Å–∞ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏–µ–π softmax –æ—Ç —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∫–ª—é—á–µ–π:\n",
    "     $$\n",
    "     Z_i = \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}\\left(\\frac{Q_i K_i^T}{\\sqrt{D_k}}\\right) V_i\n",
    "     $$\n",
    "\n",
    "   - **–§—É–Ω–∫—Ü–∏—è Softmax:**\n",
    "\n",
    "      **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:** –§—É–Ω–∫—Ü–∏—è softmax ‚Äî —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–∏—Å–µ–ª –≤ –≤–µ–∫—Ç–æ—Ä –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π. –û–Ω–∞ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –≤–µ–∫—Ç–æ—Ä $z = [z_1, z_2, ..., z_n]$ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä $\\sigma(z) = [\\sigma(z_1), \\sigma(z_2), ..., \\sigma(z_n)]$, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç $\\sigma(z_i)$ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –ø–æ —Ñ–æ—Ä–º—É–ª–µ:\n",
    "\n",
    "      $$\n",
    "      \\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\n",
    "      $$\n",
    "\n",
    "      –≥–¥–µ:\n",
    "      - $z_i$ ‚Äî —ç—Ç–æ $i$-–π —ç–ª–µ–º–µ–Ω—Ç –≤—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ $z$.\n",
    "      - $e$ ‚Äî —ç—Ç–æ –æ—Å–Ω–æ–≤–∞–Ω–∏–µ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∞—Ä–∏—Ñ–º–∞ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ 2.71828).\n",
    "      - $n$ ‚Äî —ç—Ç–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ $z$.\n",
    "\n",
    "      **–°–≤–æ–π—Å—Ç–≤–∞ Softmax:**\n",
    "      - **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è:** –°—É–º–º–∞ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ $\\sigma(z)$ —Ä–∞–≤–Ω–∞ 1: $\\sum_{i=1}^{n} \\sigma(z_i) = 1$.\n",
    "      - **–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:** –ö–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ $\\sigma(z_i)$ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 0 –¥–æ 1: $0 \\leq \\sigma(z_i) \\leq 1$.\n",
    "      - **–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ:** Softmax –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —á–∏—Å–ª–∞ –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–µ –ø–æ–ª–µ–∑–Ω–æ–π –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "\n",
    "   - **Softmax –≤ –ú–µ—Ö–∞–Ω–∏–∑–º–µ –í–Ω–∏–º–∞–Ω–∏—è:**\n",
    "\n",
    "      –í –º–µ—Ö–∞–Ω–∏–∑–º–µ –≤–Ω–∏–º–∞–Ω–∏—è softmax –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∞–∂–µ–Ω –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. –í –≤—ã—Ä–∞–∂–µ–Ω–∏–∏ $\\text{softmax}\\left(\\frac{Q_i K_i^T}{\\sqrt{D_k}}\\right) V_i$:\n",
    "\n",
    "      - $Q_i$ ‚Äî —ç—Ç–æ –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ (queries) –¥–ª—è $i$-–π –≥–æ–ª–æ–≤—ã.\n",
    "      - $K_i$ ‚Äî —ç—Ç–æ –º–∞—Ç—Ä–∏—Ü–∞ –∫–ª—é—á–µ–π (keys) –¥–ª—è $i$-–π –≥–æ–ª–æ–≤—ã.\n",
    "      - $V_i$ ‚Äî —ç—Ç–æ –º–∞—Ç—Ä–∏—Ü–∞ –∑–Ω–∞—á–µ–Ω–∏–π (values) –¥–ª—è $i$-–π –≥–æ–ª–æ–≤—ã.\n",
    "      - $D_k$ ‚Äî —ç—Ç–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–π –∏ –∑–∞–ø—Ä–æ—Å–æ–≤.\n",
    "\n",
    "      **–†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è:**\n",
    "      1. **–°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ:** –í—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∫–ª—é—á–µ–π: $Q_i K_i^T$. –≠—Ç–æ –¥–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ \"—Å–æ–≤–ø–∞–¥–∞–µ—Ç\" –∑–∞–ø—Ä–æ—Å —Å –∫–ª—é—á–æ–º.\n",
    "      2. **–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ:** –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –¥–µ–ª–∏—Ç—Å—è –Ω–∞ $\\sqrt{D_k}$. –≠—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–º–æ–≥–∞–µ—Ç —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ, –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—è —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø—Ä–æ–±–ª–µ–º–∞–º —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏.\n",
    "      3. **Softmax:** –§—É–Ω–∫—Ü–∏—è softmax –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è. –≠—Ç–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "\n",
    "   - **–ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≥–æ–ª–æ–≤:** –í—ã—Ö–æ–¥—ã –≤—Å–µ—Ö –≥–æ–ª–æ–≤ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è:\n",
    "      $$\n",
    "      \\text{Concat}(Z_1, Z_2, ..., Z_h)\n",
    "      $$\n",
    "      –≥–¥–µ $Z_i = \\text{Attention}(Q_i, K_i, V_i)$.\n",
    "   - **–õ–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è –≤—ã—Ö–æ–¥–∞:** –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ—Ç—Å—è –æ–±—Ä–∞—Ç–Ω–æ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $D_{model}$:\n",
    "      $$\n",
    "      \\text{MultiHead}(Q, K, V) = \\text{Concat}(Z_1, Z_2, ..., Z_h) W^O\n",
    "      $$\n",
    "      –≥–¥–µ $W^O \\in \\mathbb{R}^{h D_v \\times D_{model}}$ - –≤–µ—Å–æ–≤–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –≤—ã—Ö–æ–¥–Ω–æ–π –ø—Ä–æ–µ–∫—Ü–∏–∏.\n",
    "\n",
    "### **–î–∞–≤–∞–π—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞–∑–±–µ—Ä–µ–º, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç softmax –≤–Ω—É—Ç—Ä–∏ Multi-Head Attention –Ω–∞ –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –∫–æ–¥–∞ –Ω–∏–∂–µ.**\n",
    "\n",
    "–¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ–π–¥–µ–º –∫ —Å–∞–º–æ–º—É –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–º—É ‚Äî Multi-Head Self-Attention, –≥–¥–µ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è softmax.\n",
    "\n",
    "**1. –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏:**\n",
    "\n",
    "  - –ù–∞ –≤—Ö–æ–¥ Multi-Head Attention –ø–æ–¥–∞—é—Ç—Å—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (—Ñ–æ—Ä–º–∞ `torch.Size([3, 9, 512])`).\n",
    "  - –î–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã (–≤ –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ –∏—Ö 8) –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ª–∏–Ω–µ–π–Ω–æ –ø—Ä–æ–µ—Ü–∏—Ä—É—é—Ç—Å—è –≤ —Ç—Ä–∏ –º–∞—Ç—Ä–∏—Ü—ã:\n",
    "    - **Q (–∑–∞–ø—Ä–æ—Å—ã):** `torch.Size([3, 8, 9, 64])`\n",
    "    - **K (–∫–ª—é—á–∏):** `torch.Size([3, 8, 9, 64])`\n",
    "    - **V (–∑–Ω–∞—á–µ–Ω–∏—è):** `torch.Size([3, 8, 9, 64])`\n",
    "  - –≠—Ç–∏ –ø—Ä–æ–µ–∫—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–∞–µ–º—ã—Ö –º–∞—Ç—Ä–∏—Ü –≤–µ—Å–æ–≤ $W_Q$, $W_K$ –∏ $W_V$.\n",
    "  - –í –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ –¥–ª—è –ø–µ—Ä–≤–æ–π –ø–æ–∑–∏—Ü–∏–∏ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ (—Ç–æ–∫–µ–Ω `[CLS]`) –≤—ã –≤–∏–¥–∏—Ç–µ –ø—Ä–∏–º–µ—Ä—ã –º–∞—Ç—Ä–∏—Ü Q, K –∏ V.\n",
    "\n",
    "  **–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ!:**\n",
    "\n",
    "```\n",
    "# –≠—Ç–æ –¥–æ –ø–æ–¥—Å–ª–æ—è Multi-Head Attention!\n",
    "\n",
    "–ü–æ–∑–∏—Ü–∏—è 0 ([CLS]):\n",
    "–ò—Å—Ö–æ–¥–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥   X:  [ 0.9007 -2.1055  0.6784]\n",
    "–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä   PE:  [0. 1. 0.]\n",
    "–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π X + PE:  [ 0.9007 -1.1055  0.6784]\n",
    "\n",
    "# –≠—Ç–æ –ø–æ—Å–ª–µ Multi-Head Attention! \n",
    "  –ü–æ–∑–∏—Ü–∏—è 0 ([CLS]):\n",
    "    Q: [[ 17.2809  14.7748 -11.5202]\n",
    "[ -7.0419 -29.7085  54.2687]\n",
    "[ 29.6011  11.744   -0.8485]\n",
    "[ -6.7934 -25.4389  67.2095]\n",
    "[ 28.5684 -16.4333 -13.8622]\n",
    "[ -4.5139 -61.0905  -0.8532]\n",
    "[-24.339   -9.4282  -5.367 ]\n",
    "[  7.8296 -14.4175  16.9908]]\n",
    "    K: [[  6.637  -10.3847 -29.3882]\n",
    "[-22.8757 -18.3149 -65.0343]\n",
    "[ 18.4063  29.4638 -34.1548]\n",
    "[  5.1229   5.5592  66.0818]\n",
    "[  9.9801 -20.4229  -7.4216]\n",
    "[-22.0776   4.2677 -32.6255]\n",
    "[-40.8423  19.4702   0.3407]\n",
    "[  8.7071  27.0544 -13.8258]]\n",
    "    V: [[ 25.8466  12.3776  -7.7585]\n",
    "[ 32.6146  -2.0634  32.7602]\n",
    "[ 25.009   11.0889  28.2676]\n",
    "[ 19.9813 -11.8157  22.7189]\n",
    "[ 12.4848   6.3136 -28.9884]\n",
    "[ -1.6635  15.4315 -23.0705]\n",
    "[ 14.6102  -1.6098 -15.4584]\n",
    "[ -9.4451 -38.6892  42.4362]]\n",
    "```\n",
    "\n",
    "*   **\"Q –¥–ª—è —Ç–æ–∫–µ–Ω–∞ CLS\" - —ç—Ç–æ –Ω–∞–±–æ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –≤–∏–¥–µ –º–∞—Ç—Ä–∏—Ü—ã.**\n",
    "*   **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ = –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è** (–≤ –ø—Ä–∏–º–µ—Ä–µ 8).\n",
    "*   **–ö–∞–∂–¥—ã–π –≤–µ–∫—Ç–æ—Ä —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–π –≥–æ–ª–æ–≤–µ –≤–Ω–∏–º–∞–Ω–∏—è.**\n",
    "*   **–í–µ–∫—Ç–æ—Ä –ø–æ–ª—É—á–∞–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω–æ–π –ø—Ä–æ–µ–∫—Ü–∏–µ–π:**  –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ CLS —Ç–æ–∫–µ–Ω–∞  **—É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ –º–∞—Ç—Ä–∏—Ü—É  `W_q`**  –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "\n",
    "**2. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è:**\n",
    "\n",
    "  - –î–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã –∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "  - **–°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ:** –°–Ω–∞—á–∞–ª–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∫–ª—é—á–µ–π: $Q_i K_i^T$.\n",
    "    - –í –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ, –¥–ª—è –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã –∏ –ø–µ—Ä–≤–æ–π –ø–æ–∑–∏—Ü–∏–∏ (—Ç–æ–∫–µ–Ω `[CLS]`), —ç—Ç–æ –±—É–¥–µ—Ç —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ Q (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å 64) –Ω–∞ —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä K (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å 64) –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "    - –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –±—É–¥–µ—Ç –º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (9, 9), –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ \"—Å–æ–≤–ø–∞–¥–∞–µ—Ç\" –∑–∞–ø—Ä–æ—Å —Ç–æ–∫–µ–Ω–∞ `[CLS]` —Å –∫–ª—é—á–æ–º –∫–∞–∂–¥–æ–≥–æ –∏–∑ 9 —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "    <div style=\"border: 1px solid #000; padding: 10px; margin: 10px;\">\n",
    "    \n",
    "    **–¶–µ–ª—å —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è:**\n",
    "\n",
    "    –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ ‚Äî **–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ \"—Å–æ–≤–º–µ—Å—Ç–∏–º\" –∏–ª–∏ \"—Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω\" –∑–∞–ø—Ä–æ—Å (Q) –æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∫ –∫–ª—é—á–∞–º (K) –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.**  –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –º—ã –ø–æ–ª—É—á–∞–µ–º \"—Å—ã—Ä—ã–µ\" –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Ç–æ–º –±—É–¥—É—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã Softmax.\n",
    "\n",
    "    **–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∞ `[CLS]` (–ø–µ—Ä–≤–∞—è –ø–æ–∑–∏—Ü–∏—è) –∏ –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã:**\n",
    "\n",
    "    1.  **–ë–µ—Ä–µ–º –≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞ Q –¥–ª—è `[CLS]` –∏–∑ –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã:**\n",
    "        *   –í –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ –¥–ª—è –ø–æ–∑–∏—Ü–∏–∏ 0 ([CLS]) –∏ –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã (–ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ –±–ª–æ–∫–µ Q) —É–∫–∞–∑–∞–Ω –≤–µ–∫—Ç–æ—Ä (–ø–æ–∫–∞–∑–∞–Ω—ã —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 —ç–ª–µ–º–µ–Ω—Ç–∞): `[ 17.2809  14.7748 -11.5202 ...]`.  –ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —ç—Ç–æ –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 64. –û–±–æ–∑–Ω–∞—á–∏–º –µ–≥–æ –∫–∞–∫  `Q_cls_head1`.\n",
    "\n",
    "    2.  **–ë–µ—Ä–µ–º –≤–µ–∫—Ç–æ—Ä—ã –∫–ª—é—á–µ–π K –¥–ª—è *–≤—Å–µ—Ö* –ø–æ–∑–∏—Ü–∏–π (–æ—Ç 0 –¥–æ 8) –∏–∑ –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã:**\n",
    "        *   –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ –æ—Ç 0 –¥–æ 8 –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (—Ç–æ–∫–µ–Ω—ã `[CLS]`, `–í—Å–µ–º`, `–ø—Ä–∏–≤–µ—Ç`, `!`, `–Ø`, `—É–≤–ª–µ–∫–∞—é—Å—å`, `–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º`, `–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º`, `[SEP]`) –µ—Å—Ç—å —Å–≤–æ–π –≤–µ–∫—Ç–æ—Ä –∫–ª—é—á–µ–π K, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –∏–∑ –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã.  –í –ø—Ä–∏–º–µ—Ä–µ –¥–ª—è –ø–æ–∑–∏—Ü–∏–∏ 0 ([CLS]) –∏ –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã (–ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ –±–ª–æ–∫–µ K) —É–∫–∞–∑–∞–Ω –≤–µ–∫—Ç–æ—Ä: `[  6.637  -10.3847 -29.3882 ...]`.  –û–±–æ–∑–Ω–∞—á–∏–º –≤–µ–∫—Ç–æ—Ä—ã –∫–ª—é—á–µ–π –∫–∞–∫ `K_pos0_head1`, `K_pos1_head1`, `K_pos2_head1`, ..., `K_pos8_head1`. –ö–∞–∂–¥—ã–π –∏–∑ –Ω–∏—Ö —Ç–∞–∫–∂–µ –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å 64.\n",
    "\n",
    "    3.  **–í—ã—á–∏—Å–ª—è–µ–º —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –º–µ–∂–¥—É `Q_cls_head1` –∏ –∫–∞–∂–¥—ã–º –≤–µ–∫—Ç–æ—Ä–æ–º –∫–ª—é—á–µ–π `K_pos_j_head1`:**\n",
    "        *   –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ `j` –æ—Ç 0 –¥–æ 8 –º—ã –≤—ã—á–∏—Å–ª—è–µ–º —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ:\n",
    "            *   `score_0 = Q_cls_head1 * (K_pos0_head1)^T`  (–≤–Ω–∏–º–∞–Ω–∏–µ `[CLS]` –Ω–∞ `[CLS]`)\n",
    "            *   `score_1 = Q_cls_head1 * (K_pos1_head1)^T`  (–≤–Ω–∏–º–∞–Ω–∏–µ `[CLS]` –Ω–∞ `–í—Å–µ–º`)\n",
    "            *   `score_2 = Q_cls_head1 * (K_pos2_head1)^T`  (–≤–Ω–∏–º–∞–Ω–∏–µ `[CLS]` –Ω–∞ `–ø—Ä–∏–≤–µ—Ç`)\n",
    "            *   ...\n",
    "            *   `score_8 = Q_cls_head1 * (K_pos8_head1)^T`  (–≤–Ω–∏–º–∞–Ω–∏–µ `[CLS]` –Ω–∞ `[SEP]`)\n",
    "\n",
    "        *   **–ö–∞–∂–¥–æ–µ —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ `score_j` ‚Äî —ç—Ç–æ –æ–¥–Ω–æ —á–∏—Å–ª–æ (—Å–∫–∞–ª—è—Ä).** –û–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ \"—Å–æ–≤–º–µ—Å—Ç–∏–º\" –∑–∞–ø—Ä–æ—Å —Ç–æ–∫–µ–Ω–∞ `[CLS]` —Å –∫–ª—é—á–æ–º —Ç–æ–∫–µ–Ω–∞ –≤ –ø–æ–∑–∏—Ü–∏–∏ `j`. –ß–µ–º –±–æ–ª—å—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ `score_j`, —Ç–µ–º –±–æ–ª—å—à–µ –≤–Ω–∏–º–∞–Ω–∏—è (–ø–æ–∫–∞ –µ—â–µ \"—Å—ã—Ä–æ–≥–æ\")  —Ç–æ–∫–µ–Ω `[CLS]` –¥–æ–ª–∂–µ–Ω —É–¥–µ–ª–∏—Ç—å —Ç–æ–∫–µ–Ω—É –≤ –ø–æ–∑–∏—Ü–∏–∏ `j`.\n",
    "\n",
    "        –í –Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ –¥–ª—è –ø–æ–∑–∏—Ü–∏–∏ [CLS]:\n",
    "          ```python\n",
    "          Q: [[-22.9001 -31.6346  6.0742]    # –≤–µ–∫—Ç–æ—Ä –≥–æ–ª–æ–≤—ã 0\n",
    "              [ 41.4631   5.2998  6.2346]    # –≤–µ–∫—Ç–æ—Ä –≥–æ–ª–æ–≤—ã 1\n",
    "              [ 29.6049 -43.8211 -13.9067]   # –≤–µ–∫—Ç–æ—Ä –≥–æ–ª–æ–≤—ã 2\n",
    "              [ -9.0778  17.0357  -0.9468]   # –≤–µ–∫—Ç–æ—Ä –≥–æ–ª–æ–≤—ã 3\n",
    "              [ 19.0137  -6.5111 -15.9635]   # –≤–µ–∫—Ç–æ—Ä –≥–æ–ª–æ–≤—ã 4\n",
    "              [-42.3292 -31.1711  -1.0993]   # –≤–µ–∫—Ç–æ—Ä –≥–æ–ª–æ–≤—ã 5\n",
    "              [ 22.1916 -19.8376  24.6427]   # –≤–µ–∫—Ç–æ—Ä –≥–æ–ª–æ–≤—ã 6\n",
    "              [-11.865  -57.7867 -35.5895]]  # –≤–µ–∫—Ç–æ—Ä –≥–æ–ª–æ–≤—ã 7\n",
    "          ```\n",
    "\n",
    "          –ü—Ä–æ—Ü–µ—Å—Å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è:\n",
    "          \n",
    "            - –î–ª—è –≥–æ–ª–æ–≤—ã 0:\n",
    "              * –ë–µ—Ä–µ–º –≤–µ–∫—Ç–æ—Ä Q –≥–æ–ª–æ–≤—ã 0: [-22.9001 -31.6346 6.0742]\n",
    "              * –£–º–Ω–æ–∂–∞–µ–º –µ–≥–æ –Ω–∞ –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã K –¢–û–õ–¨–ö–û –≥–æ–ª–æ–≤—ã 0\n",
    "            - –î–ª—è –≥–æ–ª–æ–≤—ã 1:\n",
    "              * –ë–µ—Ä–µ–º –≤–µ–∫—Ç–æ—Ä Q –≥–æ–ª–æ–≤—ã 1: [41.4631 5.2998 6.2346]\n",
    "              * –£–º–Ω–æ–∂–∞–µ–º –µ–≥–æ –Ω–∞ –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã K –¢–û–õ–¨–ö–û –≥–æ–ª–æ–≤—ã 1\n",
    "            - –ò —Ç–∞–∫ –¥–∞–ª–µ–µ –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã\n",
    "\n",
    "    4.  **–†–µ–∑—É–ª—å—Ç–∞—Ç - —Ä—è–¥ —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:**\n",
    "        *   –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —ç—Ç–∏—Ö 9 —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π –º—ã –ø–æ–ª—É—á–∞–µ–º —Ä—è–¥ —á–∏—Å–µ–ª: `[score_0, score_1, score_2, score_3, score_4, score_5, score_6, score_7, score_8]`.\n",
    "        *   **–ò–º–µ–Ω–Ω–æ —ç—Ç–æ—Ç —Ä—è–¥ —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–ø–æ—Å–ª–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏ Softmax) —Å—Ç–∞–Ω–µ—Ç –≤–µ—Å–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —Ç–æ–∫–µ–Ω–∞ `[CLS]` –≤ –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤–µ.**  –í –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ –ø–æ—Å–ª–µ Softmax —ç—Ç–∏ –≤–µ—Å–∞ —Å—Ç–∞–ª–∏ `[1. 0. 0. 0. 0. 0. 0. 0. 0.]`.\n",
    "      </div>\n",
    "\n",
    "  - **–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ:** –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –¥–µ–ª–∏—Ç—Å—è –Ω–∞ $\\sqrt{D_k}$, –≥–¥–µ $D_k$ ‚Äî —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–π –∏ –∑–∞–ø—Ä–æ—Å–æ–≤ (–≤ –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ $D_k = 64$). –≠—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–º–æ–≥–∞–µ—Ç —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ.\n",
    "  - **Softmax:** –§—É–Ω–∫—Ü–∏—è softmax –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
    "    - Softmax –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "    - Softmax –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–µ –º–∞—Ç—Ä–∏—Ü—ã (9, 9), —Ç–æ –µ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "    - **–í–∞–∂–Ω–æ:** Softmax –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤–µ—Å–∞ —Ç–∞–∫, —á—Ç–æ –∏—Ö —Å—É–º–º–∞ —Ä–∞–≤–Ω–∞ 1. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∞–∂–µ–Ω –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞.\n",
    "    - –í –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ, –¥–ª—è –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã –∏ –ø–µ—Ä–≤–æ–π –ø–æ–∑–∏—Ü–∏–∏, –≤—ã –≤–∏–¥–∏—Ç–µ, —á—Ç–æ –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è —Ä–∞–≤–Ω—ã `[1. 0. 0. 0. 0. 0. 0. 0. 0.]`. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞ `[CLS]` (–ø–µ—Ä–≤–∞—è –ø–æ–∑–∏—Ü–∏—è) –Ω–∞–∏–±–æ–ª—å—à–µ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è —Å–∞–º–æ–º—É —Ç–æ–∫–µ–Ω—É `[CLS]`, –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –Ω–µ –∏–º–µ—é—Ç –∑–Ω–∞—á–µ–Ω–∏—è.\n",
    "\n",
    "**3. –í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π:**\n",
    "\n",
    "  - –ü–æ—Å–ª–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è, –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π (–º–∞—Ç—Ä–∏—Ü—ã V).\n",
    "  - –í–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è —É–º–Ω–æ–∂–∞—é—Ç—Å—è –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è.\n",
    "  - –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞.\n",
    "  - –í –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ, –¥–ª—è –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã –∏ –ø–µ—Ä–≤–æ–π –ø–æ–∑–∏—Ü–∏–∏, –≤—ã –≤–∏–¥–∏—Ç–µ –º–∞—Ç—Ä–∏—Ü—É Z, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π.\n",
    "\n",
    "**4. –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≥–æ–ª–æ–≤:**\n",
    "\n",
    "  - –í—ã—Ö–æ–¥—ã –≤—Å–µ—Ö –≥–æ–ª–æ–≤ (–º–∞—Ç—Ä–∏—Ü—ã Z) –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏.\n",
    "  - –í –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ, –∫–∞–∂–¥–∞—è –º–∞—Ç—Ä–∏—Ü–∞ Z –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å `torch.Size([3, 8, 9, 64])`. –ü–æ—Å–ª–µ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –º–∞—Ç—Ä–∏—Ü–∞ `torch.Size([3, 9, 512])`.\n",
    "  - –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –≤–µ–∫—Ç–æ—Ä—ã —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 64 –∏–∑ –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã \"—Å–∫–ª–µ–∏–≤–∞—é—Ç—Å—è\" –≤ –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 512.\n",
    "\n",
    "**5. –õ–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è –≤—ã—Ö–æ–¥–∞:**\n",
    "\n",
    "  - –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ—Ç—Å—è –æ–±—Ä–∞—Ç–Ω–æ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $D_{model}$ (512 –≤ –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ) —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–∞–µ–º–æ–π –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤ $W^O$.\n",
    "  - –≠—Ç–æ –¥–∞–µ—Ç –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥ Multi-Head Attention.\n",
    "\n",
    "    **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã $W^O$:**\n",
    "\n",
    "    –ú–∞—Ç—Ä–∏—Ü–∞ $W^O$ (\"W-output\"\") –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é **–ª–∏–Ω–µ–π–Ω–æ–π –ø—Ä–æ–µ–∫—Ü–∏–∏**, –ø–æ–¥–æ–±–Ω–æ –º–∞—Ç—Ä–∏—Ü–∞–º $W^Q$, $W_K$ –∏ $W_V$, –Ω–æ –Ω–∞ —ç—Ç–æ—Ç —Ä–∞–∑ –æ–Ω–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ **–∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –≤—ã—Ö–æ–¥—É –≤—Å–µ—Ö –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è**.\n",
    "\n",
    "    **–¶–µ–ª—å $W^O$**:\n",
    "\n",
    "    * **–°–≤–µ—Å—Ç–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å:**  –ü–æ—Å–ª–µ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–æ–≤ –≤—Å–µ—Ö –≥–æ–ª–æ–≤, –º—ã –ø–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–æ—Ç–æ—Ä–æ–≥–æ —É–≤–µ–ª–∏—á–µ–Ω–∞ (–≤ –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ —Å 64 –¥–æ 512, —Ç–∞–∫ –∫–∞–∫ 8 –≥–æ–ª–æ–≤ * 64 —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã = 512). –ú–∞—Ç—Ä–∏—Ü–∞ $W^O$ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è **–ø—Ä–æ–µ–∫—Ü–∏–∏ —ç—Ç–æ–≥–æ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –æ–±—Ä–∞—Ç–Ω–æ –∫ –∏—Å—Ö–æ–¥–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏** ($D_{model}$, –≤ –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ 512).  –≠—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –≤—ã—Ö–æ–¥ Multi-Head Attention –∏–º–µ–ª —Ç—É –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, —á—Ç–æ –∏ –≤—Ö–æ–¥, –∏ –º–æ–≥ –±—ã—Ç—å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –≤ –æ—Å—Ç–∞–ª—å–Ω—É—é —á–∞—Å—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ —Å–ª–æ–∏ Feed-Forward Network –∏–ª–∏ Residual Connections).\n",
    "    * **–°–º–µ—à–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç —Ä–∞–∑–Ω—ã—Ö –≥–æ–ª–æ–≤:**  –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –ø—Ä–æ—Å—Ç–æ \"—Å–∫–ª–µ–∏–≤–∞–µ—Ç\" –≤—ã—Ö–æ–¥—ã —Ä–∞–∑–Ω—ã—Ö –≥–æ–ª–æ–≤. –ú–∞—Ç—Ä–∏—Ü–∞ $W^O$ –ø–æ–∑–≤–æ–ª—è–µ—Ç **–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –∏ —Å–º–µ—à–∏–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –ø–æ–ª—É—á–µ–Ω–Ω—É—é –æ—Ç —Ä–∞–∑–Ω—ã—Ö –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è**.  –ö–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –º–æ–≥–ª–∞ —É–ª–æ–≤–∏—Ç—å —Ä–∞–∑–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö, –∏ $W^O$ –ø–æ–º–æ–≥–∞–µ—Ç –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å —ç—Ç–∏ —Ä–∞–∑–Ω—ã–µ \"—Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è\" –≤ –µ–¥–∏–Ω–æ–µ, –æ–±–æ–±—â–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ.\n",
    "\n",
    "    **–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ –ø–µ—Ä–µ–º–Ω–æ–∂–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü:**\n",
    "\n",
    "    –î–∞–≤–∞–π—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –º–∞—Ç—Ä–∏—Ü, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–µ—Ä–µ–º–Ω–æ–∂–µ–Ω–∏–µ.\n",
    "\n",
    "    * **–í—ã—Ö–æ–¥ –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã (Z):**  –í –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ, –ø–æ—Å–ª–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π, –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç –º–∞—Ç—Ä–∏—Ü—É $Z$ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ `torch.Size([3, 8, 9, 64])`.  –ï—Å–ª–∏ –º—ã –≥–æ–≤–æ—Ä–∏–º –æ–± **–æ–¥–Ω–æ–º –±–∞—Ç—á–µ, –æ–¥–Ω–æ–π –≥–æ–ª–æ–≤–µ –∏ –≤—Å–µ—Ö –ø–æ–∑–∏—Ü–∏—è—Ö –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**, —Ç–æ —ç—Ç–æ –±—É–¥–µ—Ç –º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ `[9, 64]` (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ 9 —Ç–æ–∫–µ–Ω–æ–≤, –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 64 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞).\n",
    "    * **–ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≥–æ–ª–æ–≤:**  –í—ã—Ö–æ–¥—ã –≤—Å–µ—Ö –≥–æ–ª–æ–≤ (–≤ –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ 8 –≥–æ–ª–æ–≤) –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è **–ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏**.  –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –≤–µ–∫—Ç–æ—Ä—ã —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 64 –æ—Ç –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã \"—Å–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è\" —Ä—è–¥–æ–º.  –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –¥–ª—è **–æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞ –∏ –≤—Å–µ—Ö –ø–æ–∑–∏—Ü–∏–π –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**, –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã—Ö–æ–¥ –±—É–¥–µ—Ç –∏–º–µ—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å `[9, 512]` (9 —Ç–æ–∫–µ–Ω–æ–≤, –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 512 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, –≥–¥–µ 512 = 8 –≥–æ–ª–æ–≤ * 64 —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≥–æ–ª–æ–≤—ã).  –û–±–æ–∑–Ω–∞—á–∏–º —ç—Ç—É –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –∫–∞–∫ $Z_{concat}$.\n",
    "    * **–ú–∞—Ç—Ä–∏—Ü–∞ $W^O$:** –ú–∞—Ç—Ä–∏—Ü–∞ $W^O$ –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å  $(D_{model} \\times D_{model})$, –≤ –≤–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ $(512 \\times 512)$.  –û–Ω–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $D_{model}$ (512 –ø–æ—Å–ª–µ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏) –æ–±—Ä–∞—Ç–Ω–æ –≤ –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $D_{model}$ (512).\n",
    "\n",
    "    **–ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ:**\n",
    "\n",
    "    –§–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥ Multi-Head Attention –ø–æ–ª—É—á–∞–µ—Ç—Å—è –ø—É—Ç–µ–º **—É–º–Ω–æ–∂–µ–Ω–∏—è –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã $Z_{concat}$ –Ω–∞ –º–∞—Ç—Ä–∏—Ü—É $W^O$**:\n",
    "\n",
    "    $Output_{MHA} = Z_{concat} \\times W^O$\n",
    "\n",
    "    –í —Ç–µ—Ä–º–∏–Ω–∞—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π:\n",
    "\n",
    "    `[batch_size, seq_len, num_heads * head_dim]`  —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞  `[num_heads * head_dim,  D_{model}]`  =  `[batch_size, seq_len, D_{model}]`\n",
    "\n",
    "    –í –Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ:\n",
    "\n",
    "    `[3, 9, 512]`  —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞  `[512, 512]`  =  `[3, 9, 512]`\n",
    "\n",
    "    > –ò–∑–≤–∏–Ω—è—é—Å—å,¬†–Ω–µ —Å–∞–º—ã–π —É–¥–∞—á–Ω—ã–π –ø—Ä–∏–º–µ—Ä, —Ç–∞–∫ –∫–∞–∫ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å. –í–æ—Ç –∫—Ä–∞—Ç–∫–∏–π –ø—Ä–∏–º–µ—Ä —Å –¥—Ä—É–≥–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é\n",
    "\n",
    "    ```\n",
    "    –í—Ö–æ–¥:  [3, 9, 256]  (D_model = 256)\n",
    "    |\n",
    "    |  Multi-Head Attention (num_heads=4, head_dim=128)\n",
    "    |\n",
    "    –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≥–æ–ª–æ–≤: [3, 9, 512]  (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —É–≤–µ–ª–∏—á–∏–ª–∞—Å—å –¥–æ 512 = 4 * 128)\n",
    "    |\n",
    "    |  –õ–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è W^O (–º–∞—Ç—Ä–∏—Ü–∞ 512x256)\n",
    "    |\n",
    "    –í—ã—Ö–æ–¥: [3, 9, 256]  (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ—Ä–Ω—É–ª–∞—Å—å –∫ D_model = 256)\n",
    "    ```\n",
    "\n",
    "    **4. –ò—Ç–æ–≥ –ø–æ $W^O$:**\n",
    "\n",
    "    * **$W^O$ - —ç—Ç–æ –æ–±—É—á–∞–µ–º–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤.** –û–Ω–∞, –∫–∞–∫ –∏ $W^Q$, $W_K$, $W_V$, –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.\n",
    "    * **$W^O$ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ø–æ—Å–ª–µ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–æ–≤ –≤—Å–µ—Ö –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è.**\n",
    "    * **$W^O$ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ª–∏–Ω–µ–π–Ω—É—é –ø—Ä–æ–µ–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è:**\n",
    "        * **–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–π $D_{model}$.**\n",
    "        * **–°–º–µ—à–∏–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –ø–æ–ª—É—á–µ–Ω–Ω—É—é –æ—Ç —Ä–∞–∑–Ω—ã—Ö –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è.**\n",
    "    * **–†–µ–∑—É–ª—å—Ç–∞—Ç —É–º–Ω–æ–∂–µ–Ω–∏—è –Ω–∞ $W^O$ —è–≤–ª—è–µ—Ç—Å—è –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–º –≤—ã—Ö–æ–¥–æ–º —Å–ª–æ—è Multi-Head Attention.**\n",
    "\n",
    "**–í–ª–∏—è–Ω–∏–µ Softmax:**\n",
    "\n",
    "- Softmax –∏–≥—Ä–∞–µ—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ –º–µ—Ö–∞–Ω–∏–∑–º–µ –≤–Ω–∏–º–∞–Ω–∏—è, –ø—Ä–µ–æ–±—Ä–∞–∑—É—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∫–ª—é—á–µ–π –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏.\n",
    "- –≠—Ç–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (–≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è) –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∞–∂–µ–Ω –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞.\n",
    "- Softmax –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç, —á—Ç–æ —Å—É–º–º–∞ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è —Ä–∞–≤–Ω–∞ 1, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏.\n",
    "\n",
    "**–í –∏—Ç–æ–≥–µ:**\n",
    "\n",
    "Softmax –≤–Ω—É—Ç—Ä–∏ Multi-Head Attention –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å, –Ω–∞ –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–µ–¥—É–µ—Ç –æ–±—Ä–∞—â–∞—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç –º–æ–¥–µ–ª—å –±–æ–ª–µ–µ –≥–∏–±–∫–æ–π –∏ —Å–ø–æ—Å–æ–±–Ω–æ–π —É–ª–∞–≤–ª–∏–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "```python\n",
    "# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import math\n",
    "\n",
    "# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def positional_encoding(max_len: int, d_model: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–π –ø–æ —Ñ–æ—Ä–º—É–ª–µ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–∏ Transformer.\n",
    "\n",
    "    Args:\n",
    "        max_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "        d_model: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤).\n",
    "\n",
    "    Returns:\n",
    "        –¢–µ–Ω–∑–æ—Ä –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–π —Ñ–æ—Ä–º—ã (max_len, d_model).\n",
    "\n",
    "    Examples:\n",
    "        >>> pe = positional_encoding(10, 512)\n",
    "        >>> pe.shape\n",
    "        torch.Size([10, 512])\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(\n",
    "        torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "    )\n",
    "\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "\n",
    "def print_embeddings(\n",
    "    tensor: torch.Tensor, tokens: list, title: str, max_elements: int = 3\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –º–µ—Ç–∫–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n",
    "    Args:\n",
    "        tensor: –¢–µ–Ω–∑–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n",
    "        tokens: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
    "        title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –≤—ã–≤–æ–¥–∞.\n",
    "        max_elements: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Examples:\n",
    "        >>> embeddings = torch.randn(5, 512)\n",
    "        >>> tokens = [\"token1\", \"token2\", \"token3\", \"token4\", \"token5\"]\n",
    "        >>> print_embeddings(embeddings, tokens, \"–ü—Ä–∏–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\")\n",
    "    \"\"\"\n",
    "    print(f\"\\n{title}:\")\n",
    "    for idx, (vec, token) in enumerate(zip(tensor, tokens)):\n",
    "        elements = vec[:max_elements].detach().numpy().round(4)\n",
    "        print(f\"{idx:2d} {token:15}: [{', '.join(f'{x:7.4f}' for x in elements)}...]\")\n",
    "\n",
    "\n",
    "def print_attention_details(\n",
    "    batch_idx: int,\n",
    "    head_idx: int,\n",
    "    pos_idx: int,\n",
    "    Q: torch.Tensor,\n",
    "    K: torch.Tensor,\n",
    "    attention_scores: torch.Tensor,\n",
    "    attention_weights: torch.Tensor,\n",
    "    tokens: list,\n",
    "    num_elements: int = 5,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ—Ü–µ—Å—Å–µ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏.\n",
    "\n",
    "    Args:\n",
    "        batch_idx: –ò–Ω–¥–µ–∫—Å –±–∞—Ç—á–∞.\n",
    "        head_idx: –ò–Ω–¥–µ–∫—Å –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "        pos_idx: –ü–æ–∑–∏—Ü–∏—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "        Q: –¢–µ–Ω–∑–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤.\n",
    "        K: –¢–µ–Ω–∑–æ—Ä –∫–ª—é—á–µ–π.\n",
    "        attention_scores: –¢–µ–Ω–∑–æ—Ä —Å—ã—Ä—ã—Ö –æ—Ü–µ–Ω–æ–∫ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "        attention_weights: –¢–µ–Ω–∑–æ—Ä –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ—Å–ª–µ softmax.\n",
    "        tokens: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "        num_elements: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤—ã–≤–æ–¥–∞.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\n",
    "        f\"–î–µ—Ç–∞–ª–∏ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –±–∞—Ç—á–∞ {batch_idx}, –≥–æ–ª–æ–≤—ã {head_idx}, \"\n",
    "        f\"–ø–æ–∑–∏—Ü–∏–∏ {pos_idx} ({tokens[batch_idx][pos_idx]}):\"\n",
    "    )\n",
    "\n",
    "    # –í—ã–≤–æ–¥ Q –≤–µ–∫—Ç–æ—Ä–∞\n",
    "    q_vec = Q[batch_idx, head_idx, pos_idx, :num_elements].detach().numpy()\n",
    "    print(f\"Q –≤–µ–∫—Ç–æ—Ä (–ø–µ—Ä–≤—ã–µ {num_elements} —ç–ª–µ–º–µ–Ω—Ç–æ–≤):\")\n",
    "    print(f\"{q_vec.round(4)}\")\n",
    "\n",
    "    # –í—ã–≤–æ–¥ K –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "    print(f\"K –≤–µ–∫—Ç–æ—Ä–∞ (–ø–µ—Ä–≤—ã–µ {num_elements} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∫–∞–∂–¥–æ–≥–æ):\")\n",
    "    for i, token in enumerate(tokens[batch_idx]):\n",
    "        k_vec = K[batch_idx, head_idx, i, :num_elements].detach().numpy()\n",
    "        print(f\"{i:2d} {token:15}: {k_vec.round(4)}\")\n",
    "\n",
    "    # –†—É—á–Ω–æ–π —Ä–∞—Å—á–µ—Ç —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π\n",
    "    manual_scores = []\n",
    "    q = Q[batch_idx, head_idx, pos_idx]\n",
    "    for i in range(len(tokens[batch_idx])):\n",
    "        k = K[batch_idx, head_idx, i]\n",
    "        score = torch.dot(q, k) / math.sqrt(D_k)\n",
    "        manual_scores.append(score.item())\n",
    "\n",
    "    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫\n",
    "    auto_scores = attention_scores[batch_idx, head_idx, pos_idx].detach().numpy()\n",
    "\n",
    "    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    print(\"\\n–°—ã—Ä—ã–µ –æ—Ü–µ–Ω–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è:\")\n",
    "    print(f\"–†—É—á–Ω–æ–π —Ä–∞—Å—á–µ—Ç:     {np.array(manual_scores).round(4)}\")\n",
    "    print(f\"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π:    {auto_scores.round(4)}\")\n",
    "\n",
    "    # –í—ã–≤–æ–¥ –≤–µ—Å–æ–≤ –ø–æ—Å–ª–µ softmax\n",
    "    weights = attention_weights[batch_idx, head_idx, pos_idx].detach().numpy()\n",
    "    print(f\"\\n–í–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ—Å–ª–µ Softmax:\")\n",
    "    print(f\"{weights.round(4)}\")\n",
    "\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö\n",
    "sentences = [\n",
    "    \"–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –Ø —É–≤–ª–µ–∫–∞—é—Å—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º\",\n",
    "    \"–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\",\n",
    "    \"–ò–ò ‚Äî —ç—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ!\",\n",
    "]\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
    "N = len(sentences)  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
    "L = 9  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "D_model = 512  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "h = 8  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤\n",
    "D_k = D_model // h  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–π –∏ –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "D_v = D_model // h  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "\n",
    "# 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –ø–∞–¥–¥–∏–Ω–≥–æ–º\n",
    "batch_tokens = [\n",
    "    [\"[CLS]\", \"–í—Å–µ–º\", \"–ø—Ä–∏–≤–µ—Ç\", \"!\", \"–Ø\", \"—É–≤–ª–µ–∫–∞—é—Å—å\", \"–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º\", \"–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º\", \"[SEP]\"],\n",
    "    [\"[CLS]\", \"–ü—Ä–∏–≤–µ—Ç\", \",\", \"–∫–∞–∫\", \"–¥–µ–ª–∞\", \"?\", \"[SEP]\", \"[PAD]\", \"[PAD]\"],\n",
    "    [\"[CLS]\", \"–ò–ò\", \"‚Äî\", \"—ç—Ç–æ\", \"–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ\", \"!\", \"[SEP]\", \"[PAD]\", \"[PAD]\"],\n",
    "]\n",
    "\n",
    "# 2. –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "embeddings = torch.randn(N, L, D_model)\n",
    "\n",
    "# 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "pe = positional_encoding(L, D_model)\n",
    "\n",
    "# 4. –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è–º–∏\n",
    "X_embedded = embeddings + pe  # Broadcasting –¥–ª—è –±–∞—Ç—á–∞\n",
    "\n",
    "# ================= –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ =================\n",
    "print(\"=\" * 60)\n",
    "print(\"–®–∞–≥ 1: –ò—Å—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(f\"–§–æ—Ä–º–∞ —Ç–µ–Ω–∑–æ—Ä–∞: {embeddings.shape}\")\n",
    "for batch_idx in range(N):\n",
    "    print(f\"\\n–ë–∞—Ç—á {batch_idx + 1}: '{sentences[batch_idx]}'\")\n",
    "    print_embeddings(embeddings[batch_idx], batch_tokens[batch_idx], \"–ò—Å—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"–®–∞–≥ 2: –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
    "print(f\"–§–æ—Ä–º–∞ —Ç–µ–Ω–∑–æ—Ä–∞: {pe.shape}\")\n",
    "print_embeddings(pe, [f\"–ü–æ–∑–∏—Ü–∏—è {i}\" for i in range(L)], \"–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–π\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"–®–∞–≥ 3: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (X + PE)\")\n",
    "print(f\"–§–æ—Ä–º–∞ —Ç–µ–Ω–∑–æ—Ä–∞: {X_embedded.shape}\")\n",
    "for batch_idx in range(N):\n",
    "    print(f\"\\n–ë–∞—Ç—á {batch_idx + 1}: '{sentences[batch_idx]}'\")\n",
    "    print_embeddings(X_embedded[batch_idx], batch_tokens[batch_idx], \"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–ª–æ–∂–µ–Ω–∏—è\")\n",
    "\n",
    "# ================= –ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ =================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞:\")\n",
    "batch_idx = 0\n",
    "\n",
    "# –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "print(f\"\\n–¢–µ–∫—Å—Ç: '{sentences[batch_idx]}'\")\n",
    "print(f\"–¢–æ–∫–µ–Ω—ã: {batch_tokens[batch_idx]}\")\n",
    "\n",
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–∑–∏—Ü–∏–π\n",
    "for pos in [0, 2, 4, 6, 8]:\n",
    "    print(f\"\\n–ü–æ–∑–∏—Ü–∏—è {pos} ({batch_tokens[batch_idx][pos]}):\")\n",
    "    print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥:  {embeddings[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "    print(f\"–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä:   {pe[pos, :3].detach().numpy().round(4)}\")\n",
    "    print(f\"–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π:     {X_embedded[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "\n",
    "# ================= Multi-Head Self-Attention =================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"–®–∞–≥ 4: Multi-Head Self-Attention\")\n",
    "\n",
    "# 1. –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏\n",
    "W_Q = torch.randn(h, D_model, D_k)\n",
    "W_K = torch.randn(h, D_model, D_k)\n",
    "W_V = torch.randn(h, D_model, D_v)\n",
    "\n",
    "Q = torch.einsum('nlk,hkd->nhld', X_embedded, W_Q)\n",
    "K = torch.einsum('nlk,hkd->nhld', X_embedded, W_K)\n",
    "V = torch.einsum('nlk,hkd->nhld', X_embedded, W_V)\n",
    "\n",
    "print(\"\\n–õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏:\")\n",
    "print(f\"–§–æ—Ä–º–∞ Q: {Q.shape}\")\n",
    "print(f\"–§–æ—Ä–º–∞ K: {K.shape}\")\n",
    "print(f\"–§–æ—Ä–º–∞ V: {V.shape}\")\n",
    "\n",
    "# –í—ã–≤–æ–¥ –ø–µ—Ä–≤—ã—Ö 3 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è Q, K, V\n",
    "for batch_idx in range(N):\n",
    "    print(f\"\\n–ë–∞—Ç—á {batch_idx + 1}: '{sentences[batch_idx]}'\")\n",
    "    for pos in [0, 2, 4]:\n",
    "        print(f\"  –ü–æ–∑–∏—Ü–∏—è {pos} ({batch_tokens[batch_idx][pos]}):\")\n",
    "        print(f\"    Q: {Q[batch_idx, :, pos, :3].detach().numpy().round(4)}\")\n",
    "        print(f\"    K: {K[batch_idx, :, pos, :3].detach().numpy().round(4)}\")\n",
    "        print(f\"    V: {V[batch_idx, :, pos, :3].detach().numpy().round(4)}\")\n",
    "\n",
    "# –ü–æ—Å–ª–µ —Ä–∞—Å—á–µ—Ç–∞ attention_weights –¥–æ–±–∞–≤–ª—è–µ–º:\n",
    "print_attention_details(\n",
    "    batch_idx=0,\n",
    "    head_idx=0,\n",
    "    pos_idx=0,\n",
    "    Q=Q,\n",
    "    K=K,\n",
    "    attention_scores=attention_scores,\n",
    "    attention_weights=attention_weights,\n",
    "    tokens=batch_tokens,\n",
    ")\n",
    "\n",
    "# 2. –í–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã\n",
    "attention_scores = torch.einsum('nhld,nhmd->nhlm', Q, K) / math.sqrt(D_k)\n",
    "\n",
    "# –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–¥–¥–∏–Ω–≥–∞\n",
    "mask = torch.ones(N, 1, L, L, dtype=torch.bool)\n",
    "for batch_idx, tokens in enumerate(batch_tokens):\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == \"[PAD]\":\n",
    "            mask[batch_idx, :, i:, :] = False\n",
    "            mask[batch_idx, :, :, i:] = False\n",
    "attention_scores = attention_scores.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "Z = torch.einsum('nhlm,nhmd->nhld', attention_weights, V)\n",
    "\n",
    "print(\"\\n–í–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã:\")\n",
    "print(f\"–§–æ—Ä–º–∞ Z: {Z.shape}\")\n",
    "\n",
    "# –í—ã–≤–æ–¥ –ø–µ—Ä–≤—ã—Ö 3 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è Z\n",
    "for batch_idx in range(N):\n",
    "    print(f\"\\n–ë–∞—Ç—á {batch_idx + 1}: '{sentences[batch_idx]}'\")\n",
    "    for pos in [0, 2, 4]:\n",
    "        print(f\"  –ü–æ–∑–∏—Ü–∏—è {pos} ({batch_tokens[batch_idx][pos]}):\")\n",
    "        print(f\"    Z: {Z[batch_idx, :, pos, :3].detach().numpy().round(4)}\")\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã –∏ –ø–µ—Ä–≤–æ–π –ø–æ–∑–∏—Ü–∏–∏\n",
    "print(\"\\n–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã –∏ –ø–µ—Ä–≤–æ–π –ø–æ–∑–∏—Ü–∏–∏:\")\n",
    "print(f\"–í–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è (–ø–µ—Ä–≤–∞—è –≥–æ–ª–æ–≤–∞, –ø–µ—Ä–≤–∞—è –ø–æ–∑–∏—Ü–∏—è): {attention_weights[0, 0, 0, :].detach().numpy().round(4)}\")\n",
    "\n",
    "# 3. –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≥–æ–ª–æ–≤\n",
    "Z_concat = Z.transpose(1, 2).reshape(N, L, h * D_v)\n",
    "\n",
    "print(\"\\n–ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≥–æ–ª–æ–≤:\")\n",
    "print(f\"–§–æ—Ä–º–∞ Z_concat: {Z_concat.shape}\")\n",
    "\n",
    "# 4. –õ–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è –≤—ã—Ö–æ–¥–∞\n",
    "W_O = torch.randn(h * D_v, D_model)\n",
    "multi_head_output = torch.einsum('nlk,kd->nld', Z_concat, W_O)\n",
    "\n",
    "print(\"\\n–õ–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è –≤—ã—Ö–æ–¥–∞:\")\n",
    "print(f\"–§–æ—Ä–º–∞ MultiHead Output: {multi_head_output.shape}\")\n",
    "\n",
    "# –í—ã–≤–æ–¥ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞\n",
    "print(\"\\n–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ –ø–æ—Å–ª–µ Multi-Head Attention:\")\n",
    "batch_idx = 0\n",
    "for pos in [0, 2, 4, 6, 8]:\n",
    "    print(f\"\\n–ü–æ–∑–∏—Ü–∏—è {pos} ({batch_tokens[batch_idx][pos]}):\")\n",
    "    print(f\"–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π:     {X_embedded[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "    print(f\"Multi-Head Output:  {multi_head_output[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "```\n",
    "\n",
    "4. **–°–ª–æ–π Add & Norm (–ø–æ—Å–ª–µ Multi-Head Attention):**\n",
    "   - **Add (–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ):** \n",
    "\n",
    "        *   **–°—É—Ç—å –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è:**  –û—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ, —Ç–∞–∫–∂–µ –∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∫–∞–∫ skip-connection –∏–ª–∏ residual connection, –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ **–≤—ã—Ö–æ–¥ —Å–ª–æ—è Multi-Head Attention –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫ –µ–≥–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É –≤—Ö–æ–¥—É**.\n",
    "\n",
    "            –í —Ñ–æ—Ä–º—É–ª–µ —ç—Ç–æ –≤—ã–≥–ª—è–¥–∏—Ç —Ç–∞–∫:\n",
    "            $$\n",
    "            \\text{Output}_{Add1} = X_{embedded} + \\text{MultiHead}(Q, K, V)\n",
    "            $$\n",
    "            –≥–¥–µ:\n",
    "            *   $X_{embedded}$ - —ç—Ç–æ **–≤—Ö–æ–¥** –ø–æ–¥—Å–ª–æ—è Multi-Head Attention. –í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ Transformer, —ç—Ç–æ –º–æ–≥—É—Ç –±—ã—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, –≤–æ–∑–º–æ–∂–Ω–æ, —É–∂–µ –ø—Ä–æ—à–µ–¥—à–∏–µ —á–µ—Ä–µ–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–ª–æ–∏ Transformer.\n",
    "            *   $\\text{MultiHead}(Q, K, V)$ - —ç—Ç–æ **–≤—ã—Ö–æ–¥** —Å–ª–æ—è Multi-Head Attention.\n",
    "            *   $\\text{Output}_{Add1}$ - —ç—Ç–æ **—Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–ª–æ–∂–µ–Ω–∏—è**, –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤—Ö–æ–¥–æ–º –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞ - –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ—è.\n",
    "\n",
    "            > –¢–æ –µ—Å—Ç—å,¬†–ø–æ —Å—É—Ç–∏ —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –æ–±—ã—á–Ω–æ–µ —Å–ª–æ–∂–µ–Ω–∏–µ –∏–∑ –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä—ã –¥–≤—É—Ö –º–∞—Ç—Ä–∏—Ü, –∞ —Ç–æ—á–Ω–µ–µ –¥–≤—É—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "        *   **–ó–∞—á–µ–º –Ω—É–∂–Ω—ã –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è?**\n",
    "\n",
    "            *   **–ë–æ—Ä—å–±–∞ —Å –ø—Ä–æ–±–ª–µ–º–æ–π –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞:**  –í –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ Transformer, –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (—Å–∏–≥–Ω–∞–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è) –º–æ–≥—É—Ç –∑–∞—Ç—É—Ö–∞—Ç—å –ø–æ –º–µ—Ä–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —á–µ—Ä–µ–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤. –û—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –ø–æ–º–æ–≥–∞—é—Ç **\"–ø–µ—Ä–µ–ø—Ä—ã–≥–∏–≤–∞—Ç—å\" —á–µ—Ä–µ–∑ —Å–ª–æ–∏**, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –±–æ–ª–µ–µ –ø—Ä—è–º–æ–π –ø—É—Ç—å –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤. –≠—Ç–æ –æ–±–ª–µ–≥—á–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É—á–∏—Ç—å—Å—è.\n",
    "            *   **–£–ª—É—á—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π:**  –û—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –ø–æ–∑–≤–æ–ª—è—é—Ç –æ–±—É—á–∞—Ç—å **–±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–µ –∏ —Å–ª–æ–∂–Ω—ã–µ –º–æ–¥–µ–ª–∏**. –ë–µ–∑ –Ω–∏—Ö, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–ª–æ–µ–≤ –≤ –≥–ª—É–±–æ–∫—É—é —Å–µ—Ç—å —á–∞—Å—Ç–æ –Ω–µ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∞ –º–æ–∂–µ—Ç –¥–∞–∂–µ —É—Ö—É–¥—à–∏—Ç—å –µ–µ. Residual connections –ø–æ–∑–≤–æ–ª—è—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≥–ª—É–±–æ–∫–∏—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä.\n",
    "            *   **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤—Ö–æ–¥–µ:**  –î–æ–±–∞–≤–ª—è—è –∏—Å—Ö–æ–¥–Ω—ã–π –≤—Ö–æ–¥ –∫ –≤—ã—Ö–æ–¥—É —Å–ª–æ—è –≤–Ω–∏–º–∞–Ω–∏—è, –º—ã **—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ö–æ–¥–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö**.  –°–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ *–∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö* –∏ *—É—Ç–æ—á–Ω–µ–Ω–∏–∏* –≤—Ö–æ–¥–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –∞ –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –∏—Å—Ö–æ–¥–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –±—É–¥–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ—Ç–µ—Ä—è–Ω–∞.\n",
    "\n",
    "   - **Norm (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è):** \n",
    "\n",
    "        *   **–°—É—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ—è:**  –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è - —ç—Ç–æ —Ç–µ—Ö–Ω–∏–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è **–∫ –≤—ã—Ö–æ–¥–∞–º –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ —Å–ª–æ—è –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ –æ–±—É—á–∞—é—â–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞**. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç Batch Normalization, –∫–æ—Ç–æ—Ä–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø–æ –±–∞—Ç—á—É, Layer Normalization –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç **–ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞**.\n",
    "\n",
    "            –í Transformer –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–º–µ–Ω–Ω–æ Layer Normalization. –§–æ—Ä–º—É–ª–∞ –≤—ã–≥–ª—è–¥–∏—Ç —Ç–∞–∫:\n",
    "            $$\n",
    "            \\text{Output}_{Norm1} = \\text{LayerNorm}(\\text{Output}_{Add1}) = \\gamma \\frac{\\text{Output}_{Add1} - \\mu}{\\sigma} + \\beta\n",
    "            $$\n",
    "            –≥–¥–µ:\n",
    "            *   $\\text{Output}_{Add1}$ - —ç—Ç–æ **–≤—Ö–æ–¥** –¥–ª—è —Å–ª–æ—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è.\n",
    "            *   $\\mu$ - —ç—Ç–æ **—Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ** —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤—Ö–æ–¥–∞ $\\text{Output}_{Add1}$ **–ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤** (–¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏).\n",
    "            *   $\\sigma$ - —ç—Ç–æ **—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ** —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤—Ö–æ–¥–∞ $\\text{Output}_{Add1}$ **–ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤** (–¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏).\n",
    "            *   $\\gamma$ (–≥–∞–º–º–∞) –∏ $\\beta$ (–±–µ—Ç–∞) - —ç—Ç–æ **–æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–¥–≤–∏–≥–∞**. –û–Ω–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç —Å–µ—Ç–∏ **–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å** —Å—Ç–µ–ø–µ–Ω—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏. –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ $\\gamma$ –æ–±—ã—á–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –µ–¥–∏–Ω–∏—Ü–∞–º–∏, –∞ $\\beta$ - –Ω—É–ª—è–º–∏.\n",
    "            *   $\\text{Output}_{Norm1}$ - —ç—Ç–æ **–≤—ã—Ö–æ–¥** —Å–ª–æ—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤—Ö–æ–¥–æ–º –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø–æ–¥—Å–ª–æ—è (–≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ, Feed Forward Network).\n",
    "\n",
    "            > –¢–æ –µ—Å—Ç—å, –ø–æ —Å—É—Ç–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è, –∞ —Ç–æ—á–Ω–µ–µ –≤—Å–µ—Ö –≤–µ—Å–æ–≤ –≤—ã—Ö–æ–¥–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã –æ—Å—Ç–∞—Ç–∫–æ–≤ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∞ –Ω–∞ Z-score –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é (—Ç–∞–∫–∂–µ –∏–∑–≤–µ—Å—Ç–Ω—É—é –∫–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è), –∫–æ—Ç–æ—Ä–∞—è –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é. –§–æ—Ä–º—É–ª–∞ Layer Normalization, –∫–æ—Ç–æ—Ä—É—é –º—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª–∏:\n",
    "\n",
    "            $$\n",
    "            \\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta\n",
    "            $$\n",
    "\n",
    "        *   **–ó–∞—á–µ–º –Ω—É–∂–Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è?**\n",
    "\n",
    "            *   **–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è:** –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è **—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è**, –¥–µ–ª–∞—è –µ–≥–æ –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã–º –∏ —É—Å—Ç–æ–π—á–∏–≤—ã–º. –û–Ω–∞ –ø–æ–º–æ–≥–∞–µ—Ç **—É–º–µ–Ω—å—à–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ (internal covariate shift)**, —Ç–æ –µ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ—Ç–æ–º—É, —á—Ç–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–≤–æ–¥–∏—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫ –±–æ–ª–µ–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –¥–∏–∞–ø–∞–∑–æ–Ω—É –∑–Ω–∞—á–µ–Ω–∏–π (–±–ª–∏–∑–∫–æ–º—É –∫ –Ω—É–ª–µ–≤–æ–º—É —Å—Ä–µ–¥–Ω–µ–º—É –∏ –µ–¥–∏–Ω–∏—á–Ω–æ–º—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—é).\n",
    "            *   **–£—Å–∫–æ—Ä–µ–Ω–∏–µ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏:**  –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è (learning rates)** –∏ **—É—Å–∫–æ—Ä—è–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å** –º–æ–¥–µ–ª–∏ –∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º—É —Ä–µ—à–µ–Ω–∏—é.\n",
    "            *   **–£–ª—É—á—à–µ–Ω–∏–µ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏:**  –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è –º–æ–∂–µ—Ç —Ç–∞–∫–∂–µ —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞—Ç—å **–ª—É—á—à–µ–π –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏** –º–æ–¥–µ–ª–∏, —Ç–æ –µ—Å—Ç—å –µ–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ –Ω–æ–≤—ã—Ö, —Ä–∞–Ω–µ–µ –Ω–µ –≤–∏–¥–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "            *   **–ú–µ–Ω—å—à–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞:**  –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç Batch Normalization, Layer Normalization **–Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞**. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–µ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ–π –≤ —Å–∏—Ç—É–∞—Ü–∏—è—Ö, –∫–æ–≥–¥–∞ —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –Ω–µ–±–æ–ª—å—à–æ–π –∏–ª–∏ –∫–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, –≥–¥–µ –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–∂–µ—Ç –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å—Å—è.\n",
    "\n",
    "            **–ê–Ω–∞–ª–æ–≥–∏—è:** –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –≤—ã –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç–µ –≥—Ä–æ–º–∫–æ—Å—Ç—å –∑–≤—É–∫–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö.  –£ –∫–∞–∂–¥–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ —Å–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω –≥—Ä–æ–º–∫–æ—Å—Ç–∏. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è - —ç—Ç–æ –∫–∞–∫ **–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≥—Ä–æ–º–∫–æ—Å—Ç–∏ –∫ –µ–¥–∏–Ω–æ–º—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç—É** –¥–ª—è –≤—Å–µ—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤.  –≠—Ç–æ –æ–±–ª–µ–≥—á–∞–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–≤—É–∫–∞, –¥–µ–ª–∞—è —Å–∏—Å—Ç–µ–º—É –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–π. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã $\\gamma$ –∏ $\\beta$ –ø–æ–∑–≤–æ–ª—è—é—Ç –Ω–µ–º–Ω–æ–≥–æ \"–ø–æ–¥—Å—Ç—Ä–æ–∏—Ç—å\" —ç—Ç–æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç, —á—Ç–æ–±—ã —É—á–µ—Å—Ç—å –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞.\n",
    "\n",
    "5. **–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è (Feed Forward Network):**\n",
    "\n",
    "    **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ FFN:**\n",
    "\n",
    "    FFN - —ç—Ç–æ –∫–ª—é—á–µ–≤–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –≤ –∫–∞–∂–¥–æ–º –±–ª–æ–∫–µ Transformer, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ **–Ω–µ–ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π**.  –í —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ Multi-Head Attention –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç–æ–∫–µ–Ω–∞–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º –∏ —É—á–∏—Ç—ã–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç, FFN –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ **–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ**, –Ω–æ —É–∂–µ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –æ—Ç —Å–ª–æ—è –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "\n",
    "   - –ù–∞ –≤—Ö–æ–¥ –ø–æ–¥—Å–ª–æ—è FFN –ø–æ—Å—Ç—É–ø–∞–µ—Ç $\\text{Output}_{Norm1}$.\n",
    "   - FFN —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ–µ–≤ —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ReLU, GeLU) –º–µ–∂–¥—É –Ω–∏–º–∏:\n",
    "     $$\n",
    "     \\text{FFN}(\\text{Output}_{Norm1}) = \\text{Activation}(\\text{Output}_{Norm1} W_1 + b_1) W_2 + b_2\n",
    "     $$\n",
    "     –≥–¥–µ:\n",
    "\n",
    "        - $W_1 \\in \\mathbb{R}^{D_{model} \\times D_{ff}}$\n",
    "        - $W_2 \\in \\mathbb{R}^{D_{ff} \\times D_{model}}$ - –≤–µ—Å–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã\n",
    "        - $b_1 \\in \\mathbb{R}^{D_{ff}}$, $b_2 \\in \\mathbb{R}^{D_{model}}$ - –≤–µ–∫—Ç–æ—Ä—ã —Å–º–µ—â–µ–Ω–∏–π\n",
    "        - $D_{ff}$ - –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å FFN (–æ–±—ã—á–Ω–æ $4 \\times D_{model}$).\n",
    "\n",
    "        **–í—ã—Ö–æ–¥:** FFN –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ö–æ–¥ –∏ –≤—ã–¥–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä **—Ç–æ–π –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏** $(N, L, D_{model})$, –≥–¥–µ:\n",
    "            \n",
    "        - $N$ ‚Äî —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –±–∞—Ç—á–µ),  \n",
    "        - $L$ ‚Äî –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (—á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤),  \n",
    "        - $D_{model}$ ‚Äî —Å–∫—Ä—ã—Ç–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (—Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤).  \n",
    "        \n",
    "        –≠—Ç–æ—Ç –≤—ã—Ö–æ–¥ –∑–∞—Ç–µ–º –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ —Å–ª–µ–¥—É—é—â–∏–π —Å–ª–æ–π Transformer –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞).\n",
    "\n",
    "    **–°—Ç—Ä—É–∫—Ç—É—Ä–∞ FFN:**\n",
    "\n",
    "    FFN —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ **–¥–≤—É—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ–µ–≤** —Å **—Ñ—É–Ω–∫—Ü–∏–µ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏** –º–µ–∂–¥—É –Ω–∏–º–∏.  –≠—Ç–æ –º–æ–∂–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ –¥–≤—É—Ö—Å–ª–æ–π–Ω—É—é –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å, –ø—Ä–∏–º–µ–Ω—è–µ–º—É—é –∫ –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "    **–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã FFN –∏ —Ñ–æ—Ä–º—É–ª–∞:**\n",
    "\n",
    "    $$\n",
    "    \\text{FFN}(x) = \\text{Activation}(x W_1 + b_1) W_2 + b_2\n",
    "    $$\n",
    "\n",
    "    –†–∞–∑–±–µ—Ä–µ–º –∫–∞–∂–¥—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —Ñ–æ—Ä–º—É–ª—ã:\n",
    "\n",
    "    1.  **–ü–µ—Ä–≤—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π (Expansion Layer):**  `(x W_1 + b_1)`\n",
    "        *   **–í—Ö–æ–¥:**  $x$ - —ç—Ç–æ –≤—Ö–æ–¥ FFN, —Ç–æ –µ—Å—Ç—å $\\text{Output}_{Norm1}$ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ `[batch_size, sequence_length, hidden_size]` ($D_{model}$).\n",
    "        *   **–í–µ—Å–æ–≤–∞—è –º–∞—Ç—Ä–∏—Ü–∞ $W_1$**:  $W_1 \\in \\mathbb{R}^{D_{model} \\times D_{ff}}$ - —ç—Ç–æ **–º–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤ –ø–µ—Ä–≤–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è**.  –û–Ω–∞ —è–≤–ª—è–µ—Ç—Å—è **–æ–±—É—á–∞–µ–º—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º**.\n",
    "        *   **–í–µ–∫—Ç–æ—Ä —Å–º–µ—â–µ–Ω–∏—è $b_1$**: $b_1 \\in \\mathbb{R}^{D_{ff}}$ - —ç—Ç–æ **–≤–µ–∫—Ç–æ—Ä —Å–º–µ—â–µ–Ω–∏—è –ø–µ—Ä–≤–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è**. –û–Ω —Ç–∞–∫–∂–µ —è–≤–ª—è–µ—Ç—Å—è **–æ–±—É—á–∞–µ–º—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º**.\n",
    "        *   **–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å $D_{ff}$**: $D_{ff}$ - —ç—Ç–æ **–≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è (–ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è) —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å FFN**.  –û–±—ã—á–Ω–æ –æ–Ω–∞ **–±–æ–ª—å—à–µ, —á–µ–º $D_{model}$**, —á–∞—Å—Ç–æ –≤ 4 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ ($D_{ff} = 4 \\times D_{model}$).  –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ $D_{model} = 512$, —Ç–æ $D_{ff} = 2048$.  **–£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏** –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è **\"—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º\" (expansion)**.\n",
    "        *   **–û–ø–µ—Ä–∞—Ü–∏—è:**  –ü—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç **–ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ** –≤—Ö–æ–¥–∞ $x$ –ø—É—Ç–µ–º –º–∞—Ç—Ä–∏—á–Ω–æ–≥–æ —É–º–Ω–æ–∂–µ–Ω–∏—è –Ω–∞ $W_1$ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–º–µ—â–µ–Ω–∏—è $b_1$.\n",
    "        *   **–í—ã—Ö–æ–¥ –ø–µ—Ä–≤–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è:**  –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ `[batch_size, sequence_length, D_{ff}]`.  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ **—É–≤–µ–ª–∏—á–∏–ª–∞—Å—å** —Å $D_{model}$ –¥–æ $D_{ff}$.\n",
    "\n",
    "    2.  **–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (Activation Function):**  `Activation(...)`\n",
    "        *   **–í—Ö–æ–¥:**  –í—ã—Ö–æ–¥ –ø–µ—Ä–≤–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ `[batch_size, sequence_length, D_{ff}]`.\n",
    "        *   **–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏:**  $\\text{Activation}$ - —ç—Ç–æ **–Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏**.  –í Transformer –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è:\n",
    "            *   **ReLU (Rectified Linear Unit):**  $\\text{ReLU}(z) = \\max(0, z)$.  –ü—Ä–æ—Å—Ç–∞—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –æ–±–Ω—É–ª—è—é—â–∞—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.\n",
    "            *   **GeLU (Gaussian Error Linear Unit):**  –ë–æ–ª–µ–µ –≥–ª–∞–¥–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —á–µ–º ReLU.  –§–æ—Ä–º—É–ª–∞ GeLU –Ω–µ–º–Ω–æ–≥–æ —Å–ª–æ–∂–Ω–µ–µ, –Ω–æ —Å—É—Ç—å –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω–∞ —Ç–∞–∫–∂–µ –≤–Ω–æ—Å–∏—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å.\n",
    "        *   **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏:**  –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ **–≤–≤–æ–¥–∏—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å** –≤ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ.  –ë–µ–∑ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏, FFN –±—ã–ª –±—ã –ø—Ä–æ—Å—Ç–æ –µ—â–µ –æ–¥–Ω–∏–º –ª–∏–Ω–µ–π–Ω—ã–º —Å–ª–æ–µ–º, –∏ Transformer –≤ —Ü–µ–ª–æ–º –±—ã–ª –±—ã —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–µ–Ω –ª–∏–Ω–µ–π–Ω–æ–π –º–æ–¥–µ–ª–∏, —á—Ç–æ —Å–∏–ª—å–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–ª–æ –±—ã –µ–≥–æ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.  –ù–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å **—Å–ª–æ–∂–Ω—ã–µ, –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏** –≤ –¥–∞–Ω–Ω—ã—Ö.\n",
    "        *   **–í—ã—Ö–æ–¥ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏:**  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Ç–µ–Ω–∑–æ—Ä–∞ **–Ω–µ –º–µ–Ω—è–µ—Ç—Å—è** –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.  –í—ã—Ö–æ–¥ –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å `[batch_size, sequence_length, D_{ff}]`.\n",
    "\n",
    "    3.  **–í—Ç–æ—Ä–æ–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π (Contraction Layer):**  `(... ) W_2 + b_2`\n",
    "        *   **–í—Ö–æ–¥:**  –í—ã—Ö–æ–¥ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ `[batch_size, sequence_length, D_{ff}]`.\n",
    "        *   **–í–µ—Å–æ–≤–∞—è –º–∞—Ç—Ä–∏—Ü–∞ $W_2$**:  $W_2 \\in \\mathbb{R}^{D_{ff} \\times D_{model}}$ - —ç—Ç–æ **–º–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤ –≤—Ç–æ—Ä–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è**.  –¢–∞–∫–∂–µ —è–≤–ª—è–µ—Ç—Å—è **–æ–±—É—á–∞–µ–º—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º**.\n",
    "        *   **–í–µ–∫—Ç–æ—Ä —Å–º–µ—â–µ–Ω–∏—è $b_2$**: $b_2 \\in \\mathbb{R}^{D_{model}}$ - —ç—Ç–æ **–≤–µ–∫—Ç–æ—Ä —Å–º–µ—â–µ–Ω–∏—è –≤—Ç–æ—Ä–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è**.  –¢–∞–∫–∂–µ **–æ–±—É—á–∞–µ–º—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä**.\n",
    "        *   **–û–ø–µ—Ä–∞—Ü–∏—è:**  –ü—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç **–ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ** –≤—ã—Ö–æ–¥–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø—É—Ç–µ–º –º–∞—Ç—Ä–∏—á–Ω–æ–≥–æ —É–º–Ω–æ–∂–µ–Ω–∏—è –Ω–∞ $W_2$ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–º–µ—â–µ–Ω–∏—è $b_2$.\n",
    "        *   **–í—ã—Ö–æ–¥ –≤—Ç–æ—Ä–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è (–∏ FFN –≤ —Ü–µ–ª–æ–º):**  –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ `[batch_size, sequence_length, D_{model}]`.  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ **–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è** –∫ –∏—Å—Ö–æ–¥–Ω–æ–π $D_{model}$.  –≠—Ç–æ **\"—Å–∂–∞—Ç–∏–µ\" (contraction)** —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "    **–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤ FFN –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ:**\n",
    "\n",
    "    –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, $D_{model} = 512$ –∏ $D_{ff} = 4 \\times D_{model} = 2048$.\n",
    "\n",
    "    1.  **–í—Ö–æ–¥ $x$**:  `[batch_size, sequence_length, 512]`\n",
    "    2.  **–ü–µ—Ä–≤—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π $(x W_1 + b_1)$**:\n",
    "        *   $W_1$ –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å `[512, 2048]`\n",
    "        *   –í—ã—Ö–æ–¥: `[batch_size, sequence_length, 2048]` (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å—à–∏—Ä–∏–ª–∞—Å—å)\n",
    "    3.  **–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ $\\text{Activation}$**:\n",
    "        *   –í—Ö–æ–¥: `[batch_size, sequence_length, 2048]`\n",
    "        *   –í—ã—Ö–æ–¥: `[batch_size, sequence_length, 2048]` (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è)\n",
    "    4.  **–í—Ç–æ—Ä–æ–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π $(... ) W_2 + b_2)$**:\n",
    "        *   $W_2$ –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å `[2048, 512]`\n",
    "        *   –í—ã—Ö–æ–¥: `[batch_size, sequence_length, 512]` (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∂–∞–ª–∞—Å—å –æ–±—Ä–∞—Ç–Ω–æ –∫ –∏—Å—Ö–æ–¥–Ω–æ–π)\n",
    "\n",
    "    **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã $W_1$ –∏ $W_2$:**\n",
    "\n",
    "    *   **$W_1$ (–º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è):**  –ú–∞—Ç—Ä–∏—Ü–∞ $W_1$ –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ **–ø—Ä–æ–µ–∫—Ü–∏—é –≤—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $D_{model}$ –≤ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $D_{ff}$**.  –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç FFN **—É–≤–µ–ª–∏—á–∏—Ç—å –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** –∏ \"–∑–∞–ø–æ–º–Ω–∏—Ç—å\" –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–º —ç—Ç–∞–ø–µ.\n",
    "    *   **$W_2$ (–º–∞—Ç—Ä–∏—Ü–∞ —Å–∂–∞—Ç–∏—è):**  –ú–∞—Ç—Ä–∏—Ü–∞ $W_2$ –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ **–ø—Ä–æ–µ–∫—Ü–∏—é –æ–±—Ä–∞—Ç–Ω–æ –∏–∑ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $D_{ff}$ –≤ –∏—Å—Ö–æ–¥–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $D_{model}$**.  –≠—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ, —á—Ç–æ–±—ã –≤—ã—Ö–æ–¥ FFN –∏–º–µ–ª —Ç—É –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, —á—Ç–æ –∏ –≤—Ö–æ–¥, –∏ –º–æ–≥ –±—ã—Ç—å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –≤ –æ—Å—Ç–∞–ª—å–Ω—É—é —á–∞—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Transformer.  –¢–∞–∫–∂–µ, –º–∞—Ç—Ä–∏—Ü–∞ $W_2$ –ø–æ–∑–≤–æ–ª—è–µ—Ç **—Å–º–µ—à–∞—Ç—å –∏ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é**, –ø–æ–ª—É—á–µ–Ω–Ω—É—é –Ω–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–º —ç—Ç–∞–ø–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –±–æ–ª—å—à–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "    **–ó–∞—á–µ–º –Ω—É–∂–µ–Ω FFN –≤ Transformer?**\n",
    "\n",
    "    *   **–í–≤–µ–¥–µ–Ω–∏–µ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏:**  FFN –≤–Ω–æ—Å–∏—Ç **–Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å** –≤ –º–æ–¥–µ–ª—å, —á—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö.\n",
    "    *   **–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–æ–∑–∏—Ü–∏–π:**  FFN –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è **–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –∫ –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏** –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.  –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å **–±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–µ, –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ** –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç –±—ã–ª —É—á—Ç–µ–Ω —Å–ª–æ–µ–º –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "    *   **–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏:**  –ó–∞ —Å—á–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–æ $D_{ff}$ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–∂–∞—Ç–∏—è –æ–±—Ä–∞—Ç–Ω–æ –¥–æ $D_{model}$, FFN –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ **—É–≤–µ–ª–∏—á–∏—Ç—å —Å–≤–æ—é –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±—É—á–µ–Ω–∏—é –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—è–º.  –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –±–æ–ª—å—à–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤—É–µ—Ç –∫–∞–∫ —Å–≤–æ–µ–≥–æ —Ä–æ–¥–∞ \"—Å–∫—Ä—ã—Ç–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ\", –≥–¥–µ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –±–æ–ª–µ–µ –≥–∏–±–∫–æ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏ –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "```python\n",
    "# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import math\n",
    "\n",
    "# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª—å nn –¥–ª—è LayerNorm\n",
    "\n",
    "\n",
    "def positional_encoding(max_len: int, d_model: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–π –ø–æ —Ñ–æ—Ä–º—É–ª–µ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–∏ Transformer.\n",
    "\n",
    "    Args:\n",
    "        max_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "        d_model: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤).\n",
    "\n",
    "    Returns:\n",
    "        –¢–µ–Ω–∑–æ—Ä –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–π —Ñ–æ—Ä–º—ã (max_len, d_model).\n",
    "\n",
    "    Examples:\n",
    "        >>> pe = positional_encoding(10, 512)\n",
    "        >>> pe.shape\n",
    "        torch.Size([10, 512])\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(\n",
    "        torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "    )\n",
    "\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "\n",
    "def print_embeddings(\n",
    "    tensor: torch.Tensor, tokens: list, title: str, max_elements: int = 3\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –º–µ—Ç–∫–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n",
    "    Args:\n",
    "        tensor: –¢–µ–Ω–∑–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n",
    "        tokens: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
    "        title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –≤—ã–≤–æ–¥–∞.\n",
    "        max_elements: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Examples:\n",
    "        >>> embeddings = torch.randn(5, 512)\n",
    "        >>> tokens = [\"token1\", \"token2\", \"token3\", \"token4\", \"token5\"]\n",
    "        >>> print_embeddings(embeddings, tokens, \"–ü—Ä–∏–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\")\n",
    "    \"\"\"\n",
    "    print(f\"\\n{title}:\")\n",
    "    for idx, (vec, token) in enumerate(zip(tensor, tokens)):\n",
    "        elements = vec[:max_elements].detach().numpy().round(4)\n",
    "        print(f\"{idx:2d} {token:15}: [{', '.join(f'{x:7.4f}' for x in elements)}...]\")\n",
    "\n",
    "\n",
    "def print_attention_details(\n",
    "    batch_idx: int,\n",
    "    head_idx: int,\n",
    "    pos_idx: int,\n",
    "    Q: torch.Tensor,\n",
    "    K: torch.Tensor,\n",
    "    attention_scores: torch.Tensor,\n",
    "    attention_weights: torch.Tensor,\n",
    "    tokens: list,\n",
    "    num_elements: int = 5,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        –í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ—Ü–µ—Å—Å–µ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏.\n",
    "\n",
    "    Args:\n",
    "        batch_idx: –ò–Ω–¥–µ–∫—Å –±–∞—Ç—á–∞.\n",
    "        head_idx: –ò–Ω–¥–µ–∫—Å –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "        pos_idx: –ü–æ–∑–∏—Ü–∏—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "        Q: –¢–µ–Ω–∑–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤.\n",
    "        K: –¢–µ–Ω–∑–æ—Ä –∫–ª—é—á–µ–π.\n",
    "        attention_scores: –¢–µ–Ω–∑–æ—Ä —Å—ã—Ä—ã—Ö –æ—Ü–µ–Ω–æ–∫ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "        attention_weights: –¢–µ–Ω–∑–æ—Ä –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ—Å–ª–µ softmax.\n",
    "        tokens: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "        num_elements: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤—ã–≤–æ–¥–∞.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\n",
    "        f\"–î–µ—Ç–∞–ª–∏ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –±–∞—Ç—á–∞ {batch_idx}, –≥–æ–ª–æ–≤—ã {head_idx}, \"\n",
    "        f\"–ø–æ–∑–∏—Ü–∏–∏ {pos_idx} ({tokens[batch_idx][pos_idx]}):\"\n",
    "    )\n",
    "\n",
    "    # –í—ã–≤–æ–¥ Q –≤–µ–∫—Ç–æ—Ä–∞\n",
    "    q_vec = Q[batch_idx, head_idx, pos_idx, :num_elements].detach().numpy()\n",
    "    print(f\"Q –≤–µ–∫—Ç–æ—Ä (–ø–µ—Ä–≤—ã–µ {num_elements} —ç–ª–µ–º–µ–Ω—Ç–æ–≤):\")\n",
    "    print(f\"{q_vec.round(4)}\")\n",
    "\n",
    "    # –í—ã–≤–æ–¥ K –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "    print(f\"K –≤–µ–∫—Ç–æ—Ä–∞ (–ø–µ—Ä–≤—ã–µ {num_elements} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∫–∞–∂–¥–æ–≥–æ):\")\n",
    "    for i, token in enumerate(tokens[batch_idx]):\n",
    "        k_vec = K[batch_idx, head_idx, i, :num_elements].detach().numpy()\n",
    "        print(f\"{i:2d} {token:15}: {k_vec.round(4)}\")\n",
    "\n",
    "    # –†—É—á–Ω–æ–π —Ä–∞—Å—á–µ—Ç —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π\n",
    "    manual_scores = []\n",
    "    q = Q[batch_idx, head_idx, pos_idx]\n",
    "    for i in range(len(tokens[batch_idx])):\n",
    "        k = K[batch_idx, head_idx, i]\n",
    "        score = torch.dot(q, k) / math.sqrt(D_k)\n",
    "        manual_scores.append(score.item())\n",
    "\n",
    "    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫\n",
    "    auto_scores = attention_scores[batch_idx, head_idx, pos_idx].detach().numpy()\n",
    "\n",
    "    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    print(\"\\n–°—ã—Ä—ã–µ –æ—Ü–µ–Ω–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è:\")\n",
    "    print(f\"–†—É—á–Ω–æ–π —Ä–∞—Å—á–µ—Ç:     {np.array(manual_scores).round(4)}\")\n",
    "    print(f\"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π:    {auto_scores.round(4)}\")\n",
    "\n",
    "    # –í—ã–≤–æ–¥ –≤–µ—Å–æ–≤ –ø–æ—Å–ª–µ softmax\n",
    "    weights = attention_weights[batch_idx, head_idx, pos_idx].detach().numpy()\n",
    "    print(f\"\\n–í–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ—Å–ª–µ Softmax:\")\n",
    "    print(f\"{weights.round(4)}\")\n",
    "\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö\n",
    "sentences = [\n",
    "    \"–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –Ø —É–≤–ª–µ–∫–∞—é—Å—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º\",\n",
    "    \"–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\",\n",
    "    \"–ò–ò ‚Äî —ç—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ!\",\n",
    "]\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
    "N = len(sentences)  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
    "L = 9               # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "D_model = 512       # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "h = 8               # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤\n",
    "D_k = D_model // h  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–π –∏ –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "D_v = D_model // h  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "D_ff = 4 * D_model  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å Feed Forward Network\n",
    "\n",
    "# 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –ø–∞–¥–¥–∏–Ω–≥–æ–º\n",
    "batch_tokens = [\n",
    "    [\"[CLS]\", \"–í—Å–µ–º\", \"–ø—Ä–∏–≤–µ—Ç\", \"!\", \"–Ø\", \"—É–≤–ª–µ–∫–∞—é—Å—å\", \"–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º\", \"–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º\", \"[SEP]\"],\n",
    "    [\"[CLS]\", \"–ü—Ä–∏–≤–µ—Ç\", \",\", \"–∫–∞–∫\", \"–¥–µ–ª–∞\", \"?\", \"[SEP]\", \"[PAD]\", \"[PAD]\"],\n",
    "    [\"[CLS]\", \"–ò–ò\", \"‚Äî\", \"—ç—Ç–æ\", \"–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ\", \"!\", \"[SEP]\", \"[PAD]\", \"[PAD]\"],\n",
    "]\n",
    "\n",
    "# 2. –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "embeddings = torch.randn(N, L, D_model)\n",
    "\n",
    "# 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "pe = positional_encoding(L, D_model)\n",
    "\n",
    "# 4. –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è–º–∏\n",
    "X_embedded = embeddings + pe  # Broadcasting –¥–ª—è –±–∞—Ç—á–∞\n",
    "\n",
    "# ================= –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ =================\n",
    "print(\"=\" * 60)\n",
    "print(\"–®–∞–≥ 1: –ò—Å—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(f\"–§–æ—Ä–º–∞ —Ç–µ–Ω–∑–æ—Ä–∞: {embeddings.shape}\")\n",
    "for batch_idx in range(N):\n",
    "    print(f\"\\n–ë–∞—Ç—á {batch_idx + 1}: '{sentences[batch_idx]}'\")\n",
    "    print_embeddings(embeddings[batch_idx], batch_tokens[batch_idx], \"–ò—Å—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"–®–∞–≥ 2: –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
    "print(f\"–§–æ—Ä–º–∞ —Ç–µ–Ω–∑–æ—Ä–∞: {pe.shape}\")\n",
    "print_embeddings(pe, [f\"–ü–æ–∑–∏—Ü–∏—è {i}\" for i in range(L)], \"–ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–π\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"–®–∞–≥ 3: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (X + PE)\")\n",
    "print(f\"–§–æ—Ä–º–∞ —Ç–µ–Ω–∑–æ—Ä–∞: {X_embedded.shape}\")\n",
    "for batch_idx in range(N):\n",
    "    print(f\"\\n–ë–∞—Ç—á {batch_idx + 1}: '{sentences[batch_idx]}'\")\n",
    "    print_embeddings(X_embedded[batch_idx], batch_tokens[batch_idx], \"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–ª–æ–∂–µ–Ω–∏—è\")\n",
    "\n",
    "# ================= –ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ =================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞:\")\n",
    "batch_idx = 0\n",
    "\n",
    "# –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "print(f\"\\n–¢–µ–∫—Å—Ç: '{sentences[batch_idx]}'\")\n",
    "print(f\"–¢–æ–∫–µ–Ω—ã: {batch_tokens[batch_idx]}\")\n",
    "\n",
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–∑–∏—Ü–∏–π\n",
    "for pos in [0, 2, 4, 6, 8]:\n",
    "    print(f\"\\n–ü–æ–∑–∏—Ü–∏—è {pos} ({batch_tokens[batch_idx][pos]}):\")\n",
    "    print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥:  {embeddings[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "    print(f\"–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä:   {pe[pos, :3].detach().numpy().round(4)}\")\n",
    "    print(f\"–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π:     {X_embedded[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "\n",
    "# ================= Multi-Head Self-Attention =================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"–®–∞–≥ 4: Multi-Head Self-Attention\")\n",
    "\n",
    "# 1. –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏\n",
    "W_Q = torch.randn(h, D_model, D_k)\n",
    "W_K = torch.randn(h, D_model, D_k)\n",
    "W_V = torch.randn(h, D_model, D_v)\n",
    "\n",
    "Q = torch.einsum('nlk,hkd->nhld', X_embedded, W_Q)\n",
    "K = torch.einsum('nlk,hkd->nhld', X_embedded, W_K)\n",
    "V = torch.einsum('nlk,hkd->nhld', X_embedded, W_V)\n",
    "\n",
    "print(\"\\n–õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏:\")\n",
    "print(f\"–§–æ—Ä–º–∞ Q: {Q.shape}\")\n",
    "print(f\"–§–æ—Ä–º–∞ K: {K.shape}\")\n",
    "print(f\"–§–æ—Ä–º–∞ V: {V.shape}\")\n",
    "\n",
    "# –í—ã–≤–æ–¥ –ø–µ—Ä–≤—ã—Ö 3 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è Q, K, V\n",
    "for batch_idx in range(N):\n",
    "    print(f\"\\n–ë–∞—Ç—á {batch_idx + 1}: '{sentences[batch_idx]}'\")\n",
    "    for pos in [0, 2, 4]:\n",
    "        print(f\"  –ü–æ–∑–∏—Ü–∏—è {pos} ({batch_tokens[batch_idx][pos]}):\")\n",
    "        print(f\"    Q: {Q[batch_idx, :, pos, :3].detach().numpy().round(4)}\")\n",
    "        print(f\"    K: {K[batch_idx, :, pos, :3].detach().numpy().round(4)}\")\n",
    "        print(f\"    V: {V[batch_idx, :, pos, :3].detach().numpy().round(4)}\")\n",
    "\n",
    "# 2. –í–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã\n",
    "attention_scores = torch.einsum('nhld,nhmd->nhlm', Q, K) / math.sqrt(D_k)\n",
    "\n",
    "# –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–¥–¥–∏–Ω–≥–∞\n",
    "mask = torch.ones(N, 1, L, L, dtype=torch.bool)\n",
    "for batch_idx, tokens in enumerate(batch_tokens):\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == \"[PAD]\":\n",
    "            mask[batch_idx, :, i:, :] = False\n",
    "            mask[batch_idx, :, :, i:] = False\n",
    "attention_scores = attention_scores.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "Z = torch.einsum('nhlm,nhmd->nhld', attention_weights, V)\n",
    "\n",
    "print(\"\\n–í–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã:\")\n",
    "print(f\"–§–æ—Ä–º–∞ Z: {Z.shape}\")\n",
    "\n",
    "# –í—ã–≤–æ–¥ –ø–µ—Ä–≤—ã—Ö 3 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è Z\n",
    "for batch_idx in range(N):\n",
    "    print(f\"\\n–ë–∞—Ç—á {batch_idx + 1}: '{sentences[batch_idx]}'\")\n",
    "    for pos in [0, 2, 4]:\n",
    "        print(f\"  –ü–æ–∑–∏—Ü–∏—è {pos} ({batch_tokens[batch_idx][pos]}):\")\n",
    "        print(f\"    Z: {Z[batch_idx, :, pos, :3].detach().numpy().round(4)}\")\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã –∏ –ø–µ—Ä–≤–æ–π –ø–æ–∑–∏—Ü–∏–∏\n",
    "print(\"\\n–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤—ã –∏ –ø–µ—Ä–≤–æ–π –ø–æ–∑–∏—Ü–∏–∏:\")\n",
    "print(f\"–í–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è (–ø–µ—Ä–≤–∞—è –≥–æ–ª–æ–≤–∞, –ø–µ—Ä–≤–∞—è –ø–æ–∑–∏—Ü–∏—è): {attention_weights[0, 0, 0, :].detach().numpy().round(4)}\")\n",
    "\n",
    "# 3. –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≥–æ–ª–æ–≤\n",
    "Z_concat = Z.transpose(1, 2).reshape(N, L, h * D_v)\n",
    "\n",
    "print(\"\\n–ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≥–æ–ª–æ–≤:\")\n",
    "print(f\"–§–æ—Ä–º–∞ Z_concat: {Z_concat.shape}\")\n",
    "\n",
    "# 4. –õ–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è –≤—ã—Ö–æ–¥–∞\n",
    "W_O = torch.randn(h * D_v, D_model)\n",
    "multi_head_output = torch.einsum('nlk,kd->nld', Z_concat, W_O)\n",
    "\n",
    "print(\"\\n–õ–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è –≤—ã—Ö–æ–¥–∞:\")\n",
    "print(f\"–§–æ—Ä–º–∞ MultiHead Output: {multi_head_output.shape}\")\n",
    "\n",
    "# –í—ã–≤–æ–¥ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞\n",
    "print(\"\\n–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ –ø–æ—Å–ª–µ Multi-Head Attention:\")\n",
    "batch_idx = 0\n",
    "for pos in [0, 2, 4, 6, 8]:\n",
    "    print(f\"\\n–ü–æ–∑–∏—Ü–∏—è {pos} ({batch_tokens[batch_idx][pos]}):\")\n",
    "    print(f\"–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π:     {X_embedded[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "    print(f\"Multi-Head Output:  {multi_head_output[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "\n",
    "# ================= Add & Norm (–ø–æ—Å–ª–µ Multi-Head Attention) =================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"–®–∞–≥ 5: Add & Norm (–ø–æ—Å–ª–µ Multi-Head Attention)\")\n",
    "\n",
    "# Add (–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ)\n",
    "# –í—ã—Ö–æ–¥ —Å–ª–æ—è Multi-Head Attention (multi_head_output) –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫ –≤—Ö–æ–¥—É –ø–æ–¥—Å–ª–æ—è Multi-Head Attention (X_embedded)\n",
    "output_add_norm_1_add = X_embedded + multi_head_output\n",
    "print(\"\\nAdd (–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ):\")\n",
    "print(f\"–§–æ—Ä–º–∞ Output –ø–æ—Å–ª–µ Add: {output_add_norm_1_add.shape}\")\n",
    "\n",
    "# Norm (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è)\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º Layer Normalization –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É —Å–ª–æ–∂–µ–Ω–∏—è\n",
    "layer_norm_1 = nn.LayerNorm(D_model) #  D_model - —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, –ø–æ –∫–æ—Ç–æ—Ä–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º\n",
    "output_add_norm_1_norm = layer_norm_1(output_add_norm_1_add)\n",
    "print(\"\\nNorm (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è):\")\n",
    "print(f\"–§–æ—Ä–º–∞ Output –ø–æ—Å–ª–µ LayerNorm: {output_add_norm_1_norm.shape}\")\n",
    "\n",
    "# –í—ã–≤–æ–¥ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ –ø–æ—Å–ª–µ Add & Norm\n",
    "print(\"\\n–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ –ø–æ—Å–ª–µ Add & Norm:\")\n",
    "batch_idx = 0\n",
    "for pos in [0, 2, 4, 6, 8]:\n",
    "    print(f\"\\n–ü–æ–∑–∏—Ü–∏—è {pos} ({batch_tokens[batch_idx][pos]}):\")\n",
    "    print(f\"Multi-Head Output:      {multi_head_output[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "    print(f\"Output Add:             {output_add_norm_1_add[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "    print(f\"Output Add & Norm:      {output_add_norm_1_norm[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "\n",
    "\n",
    "# ================= Feed Forward Network =================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"–®–∞–≥ 6: Feed Forward Network\")\n",
    "\n",
    "# 1. –ü–µ—Ä–≤—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π (Expansion Layer)\n",
    "W_ff_1 = torch.randn(D_model, D_ff)\n",
    "b_ff_1 = torch.randn(D_ff)\n",
    "output_ffn_layer_1 = torch.relu(torch.einsum('nlk,kd->nld', output_add_norm_1_norm, W_ff_1) + b_ff_1)\n",
    "print(\"\\n–ü–µ—Ä–≤—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π (Expansion Layer):\")\n",
    "print(f\"–§–æ—Ä–º–∞ Output –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è: {output_ffn_layer_1.shape}\")\n",
    "\n",
    "# 2. –í—Ç–æ—Ä–æ–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π (Contraction Layer)\n",
    "W_ff_2 = torch.randn(D_ff, D_model)\n",
    "b_ff_2 = torch.randn(D_model)\n",
    "output_ffn = torch.einsum('nlk,kd->nld', output_ffn_layer_1, W_ff_2) + b_ff_2\n",
    "print(\"\\n–í—Ç–æ—Ä–æ–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π (Contraction Layer):\")\n",
    "print(f\"–§–æ—Ä–º–∞ Output –ø–æ—Å–ª–µ –≤—Ç–æ—Ä–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è (FFN Output): {output_ffn.shape}\")\n",
    "\n",
    "# –í—ã–≤–æ–¥ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ –ø–æ—Å–ª–µ Feed Forward Network\n",
    "print(\"\\n–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ –ø–æ—Å–ª–µ Feed Forward Network:\")\n",
    "batch_idx = 0\n",
    "for pos in [0, 2, 4, 6, 8]:\n",
    "    print(f\"\\n–ü–æ–∑–∏—Ü–∏—è {pos} ({batch_tokens[batch_idx][pos]}):\")\n",
    "    print(f\"Output Add & Norm:      {output_add_norm_1_norm[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "    print(f\"FFN Output:             {output_ffn[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "\n",
    "# ================= Add & Norm (–ø–æ—Å–ª–µ Feed Forward Network) =================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"–®–∞–≥ 7: Add & Norm (–ø–æ—Å–ª–µ Feed Forward Network)\")\n",
    "\n",
    "# Add (–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ)\n",
    "# –í—ã—Ö–æ–¥ FFN (output_ffn) –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫ –≤—Ö–æ–¥—É –ø–æ–¥—Å–ª–æ—è FFN (output_add_norm_1_norm)\n",
    "output_add_norm_2_add = output_add_norm_1_norm + output_ffn\n",
    "print(\"\\nAdd (–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ FFN):\")\n",
    "print(f\"–§–æ—Ä–º–∞ Output –ø–æ—Å–ª–µ Add: {output_add_norm_2_add.shape}\")\n",
    "\n",
    "# Norm (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è)\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º Layer Normalization –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É —Å–ª–æ–∂–µ–Ω–∏—è\n",
    "layer_norm_2 = nn.LayerNorm(D_model) #  D_model - —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, –ø–æ –∫–æ—Ç–æ—Ä–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º\n",
    "output_add_norm_2_norm = layer_norm_2(output_add_norm_2_add)\n",
    "print(\"\\nNorm (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è –ø–æ—Å–ª–µ FFN):\")\n",
    "print(f\"–§–æ—Ä–º–∞ Output –ø–æ—Å–ª–µ LayerNorm: {output_add_norm_2_norm.shape}\")\n",
    "\n",
    "# –í—ã–≤–æ–¥ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ –ø–æ—Å–ª–µ Add & Norm (–ø–æ—Å–ª–µ FFN)\n",
    "print(\"\\n–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ –ø–æ—Å–ª–µ Add & Norm (–ø–æ—Å–ª–µ FFN):\")\n",
    "batch_idx = 0\n",
    "for pos in [0, 2, 4, 6, 8]:\n",
    "    print(f\"\\n–ü–æ–∑–∏—Ü–∏—è {pos} ({batch_tokens[batch_idx][pos]}):\")\n",
    "    print(f\"FFN Output:             {output_ffn[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "    print(f\"Output Add (–ø–æ—Å–ª–µ FFN):   {output_add_norm_2_add[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "    print(f\"Output Add & Norm (–ø–æ—Å–ª–µ FFN): {output_add_norm_2_norm[batch_idx, pos, :3].detach().numpy().round(4)}\")\n",
    "```\n",
    "\n",
    "6. **–°–ª–æ–π Add & Norm (–ø–æ—Å–ª–µ Feed Forward):**\n",
    "   - **Add (–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ):** –í—ã—Ö–æ–¥ FFN –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫ –≤—Ö–æ–¥—É –ø–æ–¥—Å–ª–æ—è:\n",
    "     $$\n",
    "     \\text{Output}_{Add2} = \\text{Output}_{Norm1} + \\text{FFN}(\\text{Output}_{Norm1})\n",
    "     $$\n",
    "   - **Norm (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è):** –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è:\n",
    "     $$\n",
    "     \\text{Output}_{Norm2} = \\text{LayerNorm}(\\text{Output}_{Add2})\n",
    "     $$\n",
    "\n",
    "7. **–í—ã—Ö–æ–¥ –∫–æ–¥–µ—Ä–∞:**\n",
    "   - –í—ã—Ö–æ–¥–æ–º –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è –∫–æ–¥–µ—Ä–∞ —è–≤–ª—è–µ—Ç—Å—è $\\text{Output}_{Norm2}$ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $(N, L, D_{model})$.\n",
    "   - –ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –≤—Å–µ—Ö $N_{layers}$ —Å–ª–æ–µ–≤ –∫–æ–¥–µ—Ä–∞, —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–∞—Ç—Ä–∏—Ü—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $(N, L, D_{model})$.\n",
    "\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–æ–¥–∫–ª—é—á–∏–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T18:51:13.347043Z",
     "iopub.status.busy": "2024-05-27T18:51:13.346519Z",
     "iopub.status.idle": "2024-05-27T18:51:13.374286Z",
     "shell.execute_reply": "2024-05-27T18:51:13.373261Z",
     "shell.execute_reply.started": "2024-05-27T18:51:13.347014Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "–î–∞–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–π –∫–æ–¥ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –∏ –æ–±—É—á–µ–Ω–∏—é\n",
    "–º–æ–¥–µ–ª–∏ —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã GPT (Generative Pre-trained Transformer).\n",
    "\n",
    "–û—Å–Ω–æ–≤–Ω–æ–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∫–æ–¥–∞:\n",
    "- –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö SQLite.\n",
    "- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è —Å–ª–æ–≤.\n",
    "- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏.\n",
    "- –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã GPT, –≤–∫–ª—é—á–∞—è –±–ª–æ–∫–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ\n",
    "  –∏ —Å–ª–æ–∏ –ø—Ä—è–º–æ–π —Å–≤—è–∑–∏.\n",
    "- –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ AdamW –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å CrossEntropyLoss.\n",
    "- –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫ –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏ –∏ –ø–æ—Ç–µ—Ä—å.\n",
    "- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.\n",
    "- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–ö–æ–¥ —è–≤–ª—è–µ—Ç—Å—è —É—á–µ–±–Ω—ã–º –ø—Ä–∏–º–µ—Ä–æ–º –∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ —Ä–∞–±–æ—Ç—ã\n",
    "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.\n",
    "\"\"\"\n",
    "\n",
    "# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import math\n",
    "import sqlite3\n",
    "\n",
    "# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –∏ –º–∞—à–∏–Ω–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞\n",
    "import nltk\n",
    "\n",
    "# –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\n",
    "from collections import Counter\n",
    "from typing import List, Optional\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è –ª—É—á—à–µ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "seaborn.set(palette='summer')\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏–Ω–∞—á–µ CPU)\n",
    "# –ü–æ—á–µ–º—É: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T18:51:13.376281Z",
     "iopub.status.busy": "2024-05-27T18:51:13.375686Z",
     "iopub.status.idle": "2024-05-27T18:51:15.504152Z",
     "shell.execute_reply": "2024-05-27T18:51:15.503387Z",
     "shell.execute_reply.started": "2024-05-27T18:51:13.376248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö SQLite\n",
    "conn = sqlite3.connect('../input/wikibooks.sqlite')\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö SQLite\n",
    "df = pd.read_sql_query(\"SELECT * FROM ru LIMIT 3300\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T18:51:15.506458Z",
     "iopub.status.busy": "2024-05-27T18:51:15.506102Z",
     "iopub.status.idle": "2024-05-27T18:51:25.758070Z",
     "shell.execute_reply": "2024-05-27T18:51:25.757158Z",
     "shell.execute_reply.started": "2024-05-27T18:51:15.506429Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3300/3300 [00:10<00:00, 322.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π 120873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "sentences: List[str] = []\n",
    "\n",
    "# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞\n",
    "for sentence_text in tqdm(df['body_text']):\n",
    "    sentences.extend(\n",
    "        [x.lower() for x in sent_tokenize(sentence_text, language='russian') if len(x) < 256]\n",
    "    )\n",
    "\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T18:51:25.759541Z",
     "iopub.status.busy": "2024-05-27T18:51:25.759238Z",
     "iopub.status.idle": "2024-05-27T18:51:55.296492Z",
     "shell.execute_reply": "2024-05-27T18:51:55.295543Z",
     "shell.execute_reply.started": "2024-05-27T18:51:25.759504Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 120873/120873 [00:29<00:00, 4126.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—Å–µ–≥–æ —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ: 20004\n"
     ]
    }
   ],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—á–µ—Ç—á–∏–∫–∞ —Å–ª–æ–≤\n",
    "# –î–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å —á–∞—Å—Ç–æ—Ç—É –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞.\n",
    "words = Counter()\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ –ø–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞\n",
    "# –∏ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –º–æ–¥–µ–ª–∏.\n",
    "for sentence_item in tqdm(sentences):\n",
    "    for word in nltk.word_tokenize(sentence_item):\n",
    "        words[word] += 1\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤–∞—Ä—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏\n",
    "# –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–ª—è –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤, –Ω–∞—á–∞–ª–∞/–∫–æ–Ω—Ü–∞\n",
    "# –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è (–ø–∞–¥–¥–∏–Ω–≥–∞) –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–ª–∏–Ω –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.\n",
    "vocab = {'<unk>', '<bos>', '<eos>', '<pad>'}\n",
    "VOCAB_SIZE = 20000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "\n",
    "# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä—å\n",
    "# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è –ø–æ–º–æ–≥–∞–µ—Ç —Å–Ω–∏–∑–∏—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏\n",
    "# —É–º–µ–Ω—å—à–∏—Ç—å –æ–±—ä–µ–º –ø–∞–º—è—Ç–∏, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n",
    "for elem in words.most_common(VOCAB_SIZE):\n",
    "    vocab.add(elem[0])\n",
    "\n",
    "print(f\"–í—Å–µ–≥–æ —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T18:51:55.298100Z",
     "iopub.status.busy": "2024-05-27T18:51:55.297746Z",
     "iopub.status.idle": "2024-05-27T18:51:55.309250Z",
     "shell.execute_reply": "2024-05-27T18:51:55.308379Z",
     "shell.execute_reply.started": "2024-05-27T18:51:55.298063Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–π: —Å–ª–æ–≤–æ -> –∏–Ω–¥–µ–∫—Å –∏ –∏–Ω–¥–µ–∫—Å -> —Å–ª–æ–≤–æ\n",
    "# –ü–æ—á–µ–º—É: –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –≤–∏–¥–µ —á–∏—Å–ª–æ–≤—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤.\n",
    "word2ind: dict[str, int] = {char: i for i, char in enumerate(vocab)}\n",
    "ind2word: dict[int, str] = {i: char for char, i in word2ind.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T18:51:55.310873Z",
     "iopub.status.busy": "2024-05-27T18:51:55.310572Z",
     "iopub.status.idle": "2024-05-27T18:51:55.327558Z",
     "shell.execute_reply": "2024-05-27T18:51:55.326771Z",
     "shell.execute_reply.started": "2024-05-27T18:51:55.310848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ.\n",
    "def fit_epoch(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    sheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –ü—Ä–æ–≤–æ–¥–∏—Ç –æ–¥–Ω—É —ç–ø–æ—Ö—É –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –≤—ã—á–∏—Å–ª—è–µ—Ç –ø–æ—Ç–µ—Ä–∏ –∏ –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        model: –û–±—É—á–∞–µ–º–∞—è –º–æ–¥–µ–ª—å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.\n",
    "        train_loader: –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏.\n",
    "        criterion: –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ—à–∏–±–∫–∏.\n",
    "        optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏.\n",
    "        sheduler: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Ç–µ–º–ø–∞ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "    Returns:\n",
    "    ---------------\n",
    "        –ö–æ—Ä—Ç–µ–∂ –∏–∑ —Å—Ä–µ–¥–Ω–µ–π –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏ –∏ —Å—Ä–µ–¥–Ω–µ–π –ø–æ—Ç–µ—Ä–∏ –∑–∞ —ç–ø–æ—Ö—É.\n",
    "\n",
    "    Raises:\n",
    "    ---------------\n",
    "        –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> model = GPT(...)\n",
    "        >>> train_loader = DataLoader(...)\n",
    "        >>> criterion = nn.CrossEntropyLoss()\n",
    "        >>> optimizer = torch.optim.AdamW(model.parameters())\n",
    "        >>> perplexity, loss = fit_epoch(model, train_loader, criterion, optimizer)\n",
    "        >>> print(f\"Perplexity: {perplexity}, Loss: {loss}\")\n",
    "    \"\"\"\n",
    "    model.train()  # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è\n",
    "    losses: List[float] = []\n",
    "    perplexity: List[float] = []\n",
    "\n",
    "    # –ò—Ç–µ—Ä–∞—Ü–∏—è –ø–æ –±–∞—Ç—á–∞–º –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()   # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–µ—Ä–µ–¥ –ø—Ä—è–º—ã–º –ø—Ä–æ—Ö–æ–¥–æ–º\n",
    "        # –ü–æ—á–µ–º—É: –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π.\n",
    "\n",
    "        logits = model(batch['input_ids']) # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥: –ø–æ–ª—É—á–µ–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤\n",
    "        loss = criterion(\n",
    "            logits, batch['target_ids'].flatten()\n",
    "        )  # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å\n",
    "        loss.backward()   # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥: –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "        optimizer.step()  # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏\n",
    "\n",
    "        perplexity.append(torch.exp(loss).item())  # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏\n",
    "        losses.append(loss.item())                 # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ—Ç–µ—Ä–∏\n",
    "\n",
    "    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–π –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏ –∏ –ø–æ—Ç–µ—Ä—å –∑–∞ —ç–ø–æ—Ö—É\n",
    "    avg_perplexity = sum(perplexity) / len(perplexity)\n",
    "    avg_losses = sum(losses) / len(losses)\n",
    "    return avg_perplexity, avg_losses\n",
    "\n",
    "\n",
    "# –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ.\n",
    "def eval_epoch(\n",
    "    model: nn.Module,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –ü—Ä–æ–≤–æ–¥–∏—Ç –æ–¥–Ω—É —ç–ø–æ—Ö—É –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        model: –û–±—É—á–∞–µ–º–∞—è –º–æ–¥–µ–ª—å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.\n",
    "        val_loader: –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏.\n",
    "        criterion: –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ—à–∏–±–∫–∏.\n",
    "\n",
    "    Returns:\n",
    "    ---------------\n",
    "        –ö–æ—Ä—Ç–µ–∂ –∏–∑ —Å—Ä–µ–¥–Ω–µ–π –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏ –∏ —Å—Ä–µ–¥–Ω–µ–π –ø–æ—Ç–µ—Ä–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ.\n",
    "\n",
    "    Raises:\n",
    "    ---------------\n",
    "        –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> model = GPT(...)\n",
    "        >>> val_loader = DataLoader(...)\n",
    "        >>> criterion = nn.CrossEntropyLoss()\n",
    "        >>> perplexity, loss = eval_epoch(model, val_loader, criterion)\n",
    "        >>> print(f\"Validation Perplexity: {perplexity}, Validation Loss: {loss}\")\n",
    "    \"\"\"\n",
    "    # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏\n",
    "    # –ü–æ—á–µ–º—É: –æ—Ç–∫–ª—é—á–∞–µ—Ç Dropout –∏ Batch Normalization –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏.\n",
    "    model.eval()\n",
    "    perplexity: List[float] = []\n",
    "    losses: List[float] = []\n",
    "\n",
    "    # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "    # –ü–æ—á–µ–º—É: —É–º–µ–Ω—å—à–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ —É—Å–∫–æ—Ä—è–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –æ—Ü–µ–Ω–∫–∏.\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            logits = model(batch['input_ids'])\n",
    "            loss = criterion(\n",
    "                logits,\n",
    "                batch['target_ids'].flatten()\n",
    "            )\n",
    "            perplexity.append(torch.exp(loss).item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–π –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏ –∏ –ø–æ—Ç–µ—Ä—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
    "    avg_perplexity = sum(perplexity) / len(perplexity)\n",
    "    avg_losses = sum(losses) / len(losses)\n",
    "    return avg_perplexity, avg_losses\n",
    "\n",
    "\n",
    "# –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –∑–∞–¥–∞–Ω–Ω–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —ç–ø–æ—Ö.\n",
    "def train(\n",
    "    train_dataloader: DataLoader,\n",
    "    eval_dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    epochs: int,\n",
    "    ignore_index: int = word2ind['<pad>'],\n",
    "    optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "    criterion: Optional[nn.Module] = None,\n",
    "    sheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None\n",
    ") -> tuple[nn.Module, List[tuple[float, float, float, float]]]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –∑–∞–¥–∞–Ω–Ω–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —ç–ø–æ—Ö.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        train_dataloader: –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏.\n",
    "        eval_dataloader: –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏.\n",
    "        model: –ú–æ–¥–µ–ª—å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.\n",
    "        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è.\n",
    "        ignore_index: –ò–Ω–¥–µ–∫—Å, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
    "                      (–æ–±—ã—á–Ω–æ —ç—Ç–æ –∏–Ω–¥–µ–∫—Å –ø–∞–¥–¥–∏–Ω–≥–∞).\n",
    "        optimizer: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä. –ï—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è AdamW.\n",
    "        criterion: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å. –ï—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CrossEntropyLoss.\n",
    "        sheduler: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Ç–µ–º–ø–∞ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "    Returns:\n",
    "    ---------------\n",
    "        –ö–æ—Ä—Ç–µ–∂ –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è (–ø–æ—Ç–µ—Ä–∏ –∏ –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏).\n",
    "\n",
    "    Raises:\n",
    "    ---------------\n",
    "        –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> model = GPT(...)\n",
    "        >>> train_dl = DataLoader(...)\n",
    "        >>> eval_dl = DataLoader(...)\n",
    "        >>> trained_model, history = train(train_dl, eval_dl, model, 10)\n",
    "    \"\"\"\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞, –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω–∞\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=ignore_index).to(device)\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ —Ç–µ–º–ø–∞ –æ–±—É—á–µ–Ω–∏—è\n",
    "    # –ü–æ—á–µ–º—É: –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Ç–µ–º–ø–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ —Å—Ö–æ–¥–∏—Ç—å—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ,\n",
    "    # –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–º–µ–Ω—å—à–∞—è —Ç–µ–º–ø –æ–±—É—á–µ–Ω–∏—è.\n",
    "    MIN_LR = 1e-4\n",
    "    INITIAL_LR = 3e-4\n",
    "\n",
    "    lambda_func = lambda epoch: max(0.99 ** epoch, MIN_LR / INITIAL_LR)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_func)\n",
    "\n",
    "    best_model_wts = model.state_dict()  # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏\n",
    "    best_perplexity = float('inf')       # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª—É—á—à–µ–π –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç—å—é\n",
    "\n",
    "    # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "    history: List[tuple[float, float, float, float]] = []\n",
    "    log_template = (\"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \"\n",
    "                    \"val_loss {v_loss:0.4f} train_perplexity {t_acc:0.4f} \"\n",
    "                    \"val_perplexity {v_acc:0.4f}\")\n",
    "\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        for epoch in range(epochs):\n",
    "            # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ\n",
    "            train_perplexity, train_loss = fit_epoch(model, train_dataloader, criterion, optimizer)\n",
    "            scheduler.step()  # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–µ–º–ø–∞ –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
    "            val_perplexity, val_loss = eval_epoch(model, eval_dataloader, criterion)\n",
    "            history.append((train_loss, train_perplexity, val_loss, val_perplexity))\n",
    "\n",
    "            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "            if val_perplexity < best_perplexity:\n",
    "                best_perplexity = val_perplexity\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "            pbar_outer.update(1)  # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞\n",
    "            tqdm.write(log_template.format(ep=epoch + 1, t_loss=train_loss,\n",
    "                                           v_loss=val_loss, t_acc=train_perplexity, v_acc=val_perplexity))\n",
    "\n",
    "    print(f'–õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è: {best_perplexity:.4f}')\n",
    "    model.load_state_dict(best_model_wts)  # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –§—É–Ω–∫—Ü–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏/–∑–∞–≥—Ä—É–∑–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞/–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T21:16:44.064196Z",
     "iopub.status.busy": "2024-05-27T21:16:44.063816Z",
     "iopub.status.idle": "2024-05-27T21:16:44.080003Z",
     "shell.execute_reply": "2024-05-27T21:16:44.079112Z",
     "shell.execute_reply.started": "2024-05-27T21:16:44.064159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–ª–æ–≤.\n",
    "class WordDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –ø—Ä–µ–æ–±—Ä–∞–∑—É—è —Å–ª–æ–≤–∞ –≤ –∏—Ö —á–∏—Å–ª–æ–≤—ã–µ –∏–Ω–¥–µ–∫—Å—ã.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        sentences: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∫–∞–∂–¥–æ–µ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —è–≤–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–∫–æ–º —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "        word2ind: –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å–ª–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å—ã.\n",
    "\n",
    "    Attributes:\n",
    "    ---------------\n",
    "        data: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –≤–∏–¥–µ —Å–ø–∏—Å–∫–æ–≤ –∏–Ω–¥–µ–∫—Å–æ–≤.\n",
    "        word2ind: –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å–ª–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å—ã.\n",
    "        unk_id: –ò–Ω–¥–µ–∫—Å —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤.\n",
    "        bos_id: –ò–Ω–¥–µ–∫—Å —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –Ω–∞—á–∞–ª–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "        eos_id: –ò–Ω–¥–µ–∫—Å —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –∫–æ–Ω—Ü–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "        pad_id: –ò–Ω–¥–µ–∫—Å —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è.\n",
    "\n",
    "    Raises:\n",
    "    ---------------\n",
    "        –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> sentences = [['hello', 'world'], ['foo', 'bar']]\n",
    "        >>> word2ind = {'hello': 0, 'world': 1, 'foo': 2, 'bar': 3, '<bos>': 4, '<eos>': 5, '<unk>': 6, '<pad>': 7}\n",
    "        >>> dataset = WordDataset(sentences, word2ind)\n",
    "        >>> print(len(dataset))\n",
    "        2\n",
    "        >>> print(dataset[0])\n",
    "        [4, 0, 1, 5]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentences: List[List[int]], word2ind: dict[str, int]):\n",
    "        super().__init__()\n",
    "        self.data = sentences\n",
    "        self.word2ind = word2ind\n",
    "        self.unk_id = self.word2ind['<unk>']\n",
    "        self.bos_id = self.word2ind['<bos>']\n",
    "        self.eos_id = self.word2ind['<eos>']\n",
    "        self.pad_id = self.word2ind['<pad>']\n",
    "\n",
    "    def __getitem__(self, idx: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ –∏–Ω–¥–µ–∫—Å—É, –¥–æ–±–∞–≤–ª—è—è —Ç–æ–∫–µ–Ω—ã BOS –∏ EOS.\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            idx: –ò–Ω–¥–µ–∫—Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ.\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ BOS –∏ EOS.\n",
    "\n",
    "        Raises:\n",
    "        ---------------\n",
    "            IndexError: –ï—Å–ª–∏ `idx` –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞.\n",
    "\n",
    "        Examples:\n",
    "        ---------------\n",
    "            >>> dataset = WordDataset([['hello', 'world']], {'hello': 0, 'world': 1, '<bos>': 2, '<eos>': 3, '<unk>': 4, '<pad>': 5})\n",
    "            >>> dataset[0]\n",
    "            [2, 0, 1, 3]\n",
    "        \"\"\"\n",
    "        tokenized_sentence = [self.bos_id]\n",
    "        tokenized_sentence.extend(self.data[idx])  # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "        tokenized_sentence.append(self.eos_id)     # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –∫–æ–Ω—Ü–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        return tokenized_sentence\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ.\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            –ù–µ—Ç.\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.\n",
    "\n",
    "        Raises:\n",
    "        ---------------\n",
    "            –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "        Examples:\n",
    "        ---------------\n",
    "            >>> dataset = WordDataset([['hello', 'world'], ['foo', 'bar']], {})\n",
    "            >>> len(dataset)\n",
    "            2\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "# –í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã.\n",
    "def collate_fn_with_padding(\n",
    "    input_batch: List[List[int]], pad_id: int = word2ind['<pad>'], max_seq_len: int = 96\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –±–∞—Ç—á–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ–º (padding)\n",
    "        –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã. –û–±—Ä–µ–∑–∞–µ—Ç –∏–ª–∏ –¥–æ–ø–æ–ª–Ω—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        input_batch: –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∏–Ω–¥–µ–∫—Å–æ–≤, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "        pad_id: –ò–Ω–¥–µ–∫—Å —Ç–æ–∫–µ–Ω–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è.\n",
    "        max_seq_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è/–æ–±—Ä–µ–∑–∫–∏.\n",
    "\n",
    "    Returns:\n",
    "    ---------------\n",
    "        –°–ª–æ–≤–∞—Ä—å, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Ç–µ–Ω–∑–æ—Ä—ã 'input_ids' –∏ 'target_ids' –¥–ª—è –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "    Raises:\n",
    "    ---------------\n",
    "        –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> batch = [[1, 2, 3], [4, 5]]\n",
    "        >>> padded_batch = collate_fn_with_padding(batch, pad_id=0, max_seq_len=5)\n",
    "        >>> print(padded_batch['input_ids'])\n",
    "        tensor([[1, 2, 3, 0],\n",
    "                [4, 5, 0, 0]])\n",
    "        >>> print(padded_batch['target_ids'])\n",
    "        tensor([[2, 3, 0, 0],\n",
    "                [5, 0, 0, 0]])\n",
    "    \"\"\"\n",
    "    new_batch: List[List[int]] = []\n",
    "    for sequence in input_batch:\n",
    "        if len(sequence) > max_seq_len:\n",
    "            # –û–±—Ä–µ–∑–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –µ—Å–ª–∏ –æ–Ω–∞ –¥–ª–∏–Ω–Ω–µ–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã\n",
    "            # –ü–æ—á–µ–º—É: –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "            sequence = sequence[:max_seq_len - 1] + [sequence[-1]]\n",
    "        else:\n",
    "            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞–¥–¥–∏–Ω–≥–∞, –µ—Å–ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–æ—Ä–æ—á–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã\n",
    "            # –ü–æ—á–µ–º—É: –≤—Å–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –±–∞—Ç—á–µ –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—É—é –¥–ª–∏–Ω—É.\n",
    "            for _ in range(max_seq_len - len(sequence)):\n",
    "                sequence.append(pad_id)\n",
    "        new_batch.append(sequence)\n",
    "\n",
    "    sequences = torch.LongTensor(new_batch).to(device)\n",
    "\n",
    "    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –≤—Ö–æ–¥–Ω—ã–µ –∏ —Ü–µ–ª–µ–≤—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "    # –ü–æ—á–µ–º—É: –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç\n",
    "    # —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω (—Ü–µ–ª–µ–≤–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å).\n",
    "    processed_batch = {\n",
    "        'input_ids': sequences[:, :-1],  # –í—Å–µ —Ç–æ–∫–µ–Ω—ã –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ\n",
    "        'target_ids': sequences[:, 1:]   # –í—Å–µ —Ç–æ–∫–µ–Ω—ã –∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–≥–æ\n",
    "    }\n",
    "\n",
    "    return processed_batch\n",
    "\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å.\n",
    "def generate_sequence(\n",
    "    model: nn.Module,\n",
    "    dict_2ind: dict[str, int],\n",
    "    ind2dict: dict[int, str],\n",
    "    starting_seq: int,\n",
    "    max_seq_len: int = 256\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –Ω–∞—á–∏–Ω–∞—è —Å –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
    "        dict_2ind: –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å–ª–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å—ã.\n",
    "        ind2dict: –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ —Å–ª–æ–≤–∞.\n",
    "        starting_seq: –ò–Ω–¥–µ–∫—Å –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
    "        max_seq_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "    Returns:\n",
    "    ---------------\n",
    "        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–µ–∫—Å—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞.\n",
    "\n",
    "    Raises:\n",
    "    ---------------\n",
    "        –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> model = GPT(...) # –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å\n",
    "        >>> word2ind = {'<bos>': 0, '–ø—Ä–∏–≤–µ—Ç': 1, '–º–∏—Ä': 2, '<eos>': 3}\n",
    "        >>> ind2word = {0: '<bos>', 1: '–ø—Ä–∏–≤–µ—Ç', 2: '–º–∏—Ä', 3: '<eos>'}\n",
    "        >>> generated_text = generate_sequence(model, word2ind, ind2word, word2ind['<bos>'], 10)\n",
    "        >>> print(generated_text)\n",
    "        \"<bos> –ø—Ä–∏–≤–µ—Ç –º–∏—Ä <eos>\"\n",
    "    \"\"\"\n",
    "    # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ CPU –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–∞ GPU\n",
    "    # –ü–æ—á–µ–º—É: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ã—á–Ω–æ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–æ–π –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –º–æ—â–Ω–æ—Å—Ç–∏ GPU –∏ –º–æ–∂–µ—Ç\n",
    "    # –±—ã—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –Ω–∞ CPU –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞.\n",
    "    current_device = 'cpu'  \n",
    "    model = model.to(current_device)\n",
    "\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞—á–∞–ª—å–Ω—ã–º —Ç–æ–∫–µ–Ω–æ–º\n",
    "    idx = torch.zeros((1, 1), dtype=torch.long).to(current_device)\n",
    "    idx[0, 0] = starting_seq\n",
    "\n",
    "    # –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞\n",
    "    # –ü–æ—á–µ–º—É: —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –≤—Ö–æ–¥–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.\n",
    "    BLOCK_SIZE = 256  \n",
    "\n",
    "    model.eval()                # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏\n",
    "    current_len = idx.shape[1]  # –¢–µ–∫—É—â–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "\n",
    "    # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_seq_len):\n",
    "            # –û–±—Ä–µ–∑–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–ª–æ–∫–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "            idx_cond = idx[:, -BLOCK_SIZE:]\n",
    "            logits = model.forward(idx_cond)                    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤\n",
    "            logits = logits.reshape(1, current_len, -1)         # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –ª–æ–≥–∏—Ç–æ–≤\n",
    "            logits = logits[:, -1, :]                           # –í—ã–±–æ—Ä –ª–æ–≥–∏—Ç–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "            probs = F.softmax(logits, dim=1)                    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤ –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # –í—ã–±–æ—Ä —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º\n",
    "            idx = torch.cat((idx, idx_next), dim=1)             # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "\n",
    "            if current_len < BLOCK_SIZE:\n",
    "                current_len += 1\n",
    "\n",
    "            if idx_next.item() == dict_2ind['<eos>']:  # –û—Å—Ç–∞–Ω–æ–≤–∫–∞, –µ—Å–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ç–æ–∫–µ–Ω –∫–æ–Ω—Ü–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "                break\n",
    "\n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ª–æ–≤–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É\n",
    "    words_sequence = ' '.join([ind2dict[i.item()] for i in idx[0]])\n",
    "\n",
    "    return words_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T18:51:55.346235Z",
     "iopub.status.busy": "2024-05-27T18:51:55.345905Z",
     "iopub.status.idle": "2024-05-27T18:51:55.371050Z",
     "shell.execute_reply": "2024-05-27T18:51:55.370280Z",
     "shell.execute_reply.started": "2024-05-27T18:51:55.346205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –†–µ–∞–ª–∏–∑—É–µ—Ç –±–ª–æ–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –†–µ–∞–ª–∏–∑—É–µ—Ç –æ–¥–∏–Ω –±–ª–æ–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –≤–∫–ª—é—á–∞—é—â–∏–π –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ\n",
    "        –∏ —Å–ª–æ–π –ø—Ä—è–º–æ–π —Å–≤—è–∑–∏ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π —Å–ª–æ–µ–≤.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        num_heads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –±–ª–æ–∫–µ MultiHeadSelfAttention.\n",
    "        n_embed: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (—Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è).\n",
    "        block_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º–∞—è –±–ª–æ–∫–æ–º.\n",
    "\n",
    "    Attributes:\n",
    "    ---------------\n",
    "        mhsa: –ú–æ–¥—É–ª—å MultiHeadSelfAttention.\n",
    "        feed_forward: –ú–æ–¥—É–ª—å FeedForward.\n",
    "        norm1: –ü–µ—Ä–≤—ã–π —Å–ª–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
    "        norm2: –í—Ç–æ—Ä–æ–π —Å–ª–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
    "\n",
    "    Raises:\n",
    "    ---------------\n",
    "        –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> block = TransformerBlock(num_heads=4, n_embed=128, block_size=64)\n",
    "        >>> x = torch.randn(1, 64, 128)\n",
    "        >>> output = block(x)\n",
    "        >>> print(output.shape)\n",
    "        torch.Size([1, 64, 128])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        n_embed: int,\n",
    "        block_size: int\n",
    "    ):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # –ü–æ—á–µ–º—É: hidden_dim –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∫–∞–∫ n_embed // num_heads, —Ç–∞–∫ –∫–∞–∫ –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞\n",
    "        # –≤–Ω–∏–º–∞–Ω–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç—å –æ–±—â–µ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞.\n",
    "        hidden_dim = n_embed // num_heads\n",
    "        self.mhsa = MultiHeadSelfAttention(num_heads, hidden_dim, n_embed, block_size)\n",
    "        self.feed_forward = FeedForward(n_embed)\n",
    "        self.norm1 = nn.LayerNorm(n_embed)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è –ø–æ—Å–ª–µ MHSA\n",
    "        self.norm2 = nn.LayerNorm(n_embed)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è –ø–æ—Å–ª–µ FeedForward\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –±–ª–æ–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä.\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –ø–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ –±–ª–æ–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.\n",
    "\n",
    "        Raises:\n",
    "        ---------------\n",
    "            –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "        Examples:\n",
    "        ---------------\n",
    "            >>> block = TransformerBlock(num_heads=4, n_embed=128, block_size=64)\n",
    "            >>> x = torch.randn(1, 64, 128)\n",
    "            >>> output = block(x)\n",
    "        \"\"\"\n",
    "        # –°–Ω–∞—á–∞–ª–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –∑–∞—Ç–µ–º –≤–Ω–∏–º–∞–Ω–∏–µ, –∑–∞—Ç–µ–º —Å–ª–æ–∂–µ–Ω–∏–µ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –≤—Ö–æ–¥–æ–º (residual connection)\n",
    "        x = x + self.mhsa(self.norm1(x))\n",
    "        # –°–Ω–æ–≤–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –∑–∞—Ç–µ–º FeedForward, –∑–∞—Ç–µ–º —Å–ª–æ–∂–µ–Ω–∏–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —à–∞–≥–∞\n",
    "        x = x + self.feed_forward(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# –†–µ–∞–ª–∏–∑—É–µ—Ç —Å–ª–æ–π –ø—Ä—è–º–æ–π —Å–≤—è–∑–∏ (Feed-Forward Network).\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –†–µ–∞–ª–∏–∑—É–µ—Ç —Å–ª–æ–π –ø—Ä—è–º–æ–π —Å–≤—è–∑–∏ (Feed-Forward Network) —Å ReLU –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π\n",
    "        –∏ Dropout.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        n_embed: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω–æ–≥–æ –∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞.\n",
    "        extend_width: –ú–Ω–æ–∂–∏—Ç–µ–ª—å –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 4).\n",
    "        dropout: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥—Ä–æ–ø–∞—É—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.2).\n",
    "\n",
    "    Attributes:\n",
    "    ---------------\n",
    "        layer: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–µ–≤: –ª–∏–Ω–µ–π–Ω—ã–π, ReLU, –ª–∏–Ω–µ–π–Ω—ã–π, Dropout.\n",
    "\n",
    "    Raises:\n",
    "    ---------------\n",
    "        –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> ff = FeedForward(n_embed=128)\n",
    "        >>> x = torch.randn(1, 64, 128)\n",
    "        >>> output = ff(x)\n",
    "        >>> print(output.shape)\n",
    "        torch.Size([1, 64, 128])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        extend_width: int = 4,\n",
    "        dropout: float = 0.2\n",
    "    ):\n",
    "        super(FeedForward, self).__init__()\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Sequential –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–µ–≤ –≤ –æ–¥–∏–Ω –º–æ–¥—É–ª—å.\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(n_embed, extend_width * n_embed),  # –†–∞—Å—à–∏—Ä—è—é—â–∏–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π\n",
    "            nn.ReLU(),                                   # –ê–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è\n",
    "            nn.Linear(extend_width * n_embed, n_embed),  # –°–∂–∏–º–∞—é—â–∏–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π\n",
    "            nn.Dropout(dropout)                          # –°–ª–æ–π –¥—Ä–æ–ø–∞—É—Ç–∞ –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —Å–ª–æ—è –ø—Ä—è–º–æ–π —Å–≤—è–∑–∏.\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä.\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –ø–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Å–ª–æ–π –ø—Ä—è–º–æ–π —Å–≤—è–∑–∏.\n",
    "\n",
    "        Raises:\n",
    "        ---------------\n",
    "            –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "        Examples:\n",
    "        ---------------\n",
    "            >>> ff = FeedForward(n_embed=128)\n",
    "            >>> x = torch.randn(1, 64, 128)\n",
    "            >>> output = ff(x)\n",
    "        \"\"\"\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "# –†–µ–∞–ª–∏–∑—É–µ—Ç –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ.\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –†–µ–∞–ª–∏–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–≥–æ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è (Multi-Head Self-Attention).\n",
    "        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—ã—Ö–æ–¥—ã –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        num_heads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "        hidden_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã.\n",
    "        n_embed: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞.\n",
    "        block_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "        dropout: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥—Ä–æ–ø–∞—É—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.2).\n",
    "\n",
    "    Attributes:\n",
    "    ---------------\n",
    "        num_heads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "        heads: –°–ø–∏—Å–æ–∫ –º–æ–¥—É–ª–µ–π SingleHead.\n",
    "        project: –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –≤—ã—Ö–æ–¥–æ–≤ –≥–æ–ª–æ–≤.\n",
    "        drop: –°–ª–æ–π –¥—Ä–æ–ø–∞—É—Ç–∞.\n",
    "\n",
    "    Raises:\n",
    "    ---------------\n",
    "        –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> mhsa = MultiHeadSelfAttention(num_heads=4, hidden_dim=32, n_embed=128, block_size=64)\n",
    "        >>> x = torch.randn(1, 64, 128)\n",
    "        >>> output = mhsa(x)\n",
    "        >>> print(output.shape)\n",
    "        torch.Size([1, 64, 128])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        n_embed: int,\n",
    "        block_size: int,\n",
    "        dropout: float = 0.2\n",
    "    ):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        self.heads = nn.ModuleList(\n",
    "            [SingleHead(hidden_dim, n_embed, block_size) for _ in range(self.num_heads)]\n",
    "        )\n",
    "        self.project = nn.Linear(n_embed, n_embed)  # –ü—Ä–æ–µ–∫—Ü–∏–æ–Ω–Ω—ã–π —Å–ª–æ–π –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤—ã—Ö–æ–¥–æ–≤\n",
    "        self.drop = nn.Dropout(dropout)             # –°–ª–æ–π –¥—Ä–æ–ø–∞—É—Ç–∞\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–≥–æ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä.\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–≥–æ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "\n",
    "        Raises:\n",
    "        ---------------\n",
    "            –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "        Examples:\n",
    "        ---------------\n",
    "            >>> mhsa = MultiHeadSelfAttention(num_heads=4, hidden_dim=32, n_embed=128, block_size=64)\n",
    "            >>> x = torch.randn(1, 64, 128)\n",
    "            >>> output = mhsa(x)\n",
    "        \"\"\"\n",
    "        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–æ–≤ –≤—Å–µ—Ö –≥–æ–ª–æ–≤ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –∏–∑–º–µ—Ä–µ–Ω–∏—é\n",
    "        out = torch.cat([sh(x) for sh in self.heads], dim=-1)\n",
    "        out = self.project(out)  # –ü—Ä–æ–µ–∫—Ü–∏—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –≤—ã—Ö–æ–¥–æ–≤\n",
    "        out = self.drop(out)     # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥—Ä–æ–ø–∞—É—Ç–∞\n",
    "        return out\n",
    "\n",
    "\n",
    "# –†–µ–∞–ª–∏–∑—É–µ—Ç –æ–¥–Ω—É –≥–æ–ª–æ–≤—É —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "class SingleHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –†–µ–∞–ª–∏–∑—É–µ—Ç –æ–¥–Ω—É –≥–æ–ª–æ–≤—É –º–µ—Ö–∞–Ω–∏–∑–º–∞ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è (Self-Attention) —Å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–º\n",
    "        –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞–≥–ª—è–¥—ã–≤–∞–Ω–∏—è –≤ –±—É–¥—É—â–µ–µ.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        hidden_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –¥–∞–Ω–Ω–æ–π –≥–æ–ª–æ–≤—ã.\n",
    "        n_embed: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞.\n",
    "        block_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "        dropout: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥—Ä–æ–ø–∞—É—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.2).\n",
    "\n",
    "    Attributes:\n",
    "    ---------------\n",
    "        key: –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–ª—é—á–µ–π.\n",
    "        query: –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤.\n",
    "        value: –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π.\n",
    "        drop: –°–ª–æ–π –¥—Ä–æ–ø–∞—É—Ç–∞.\n",
    "        tril: –ù–∏–∂–Ω–µ—Ç—Ä–µ—É–≥–æ–ª—å–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
    "\n",
    "    Raises:\n",
    "    ---------------\n",
    "        –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> sh = SingleHead(hidden_dim=32, n_embed=128, block_size=64)\n",
    "        >>> x = torch.randn(1, 64, 128)\n",
    "        >>> output = sh(x)\n",
    "        >>> print(output.shape)\n",
    "        torch.Size([1, 64, 32])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        n_embed: int,\n",
    "        block_size: int,\n",
    "        dropout: float = 0.2\n",
    "    ):\n",
    "        super(SingleHead, self).__init__()\n",
    "        # –õ–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è Q, K, V –±–µ–∑ —Å–º–µ—â–µ–Ω–∏—è, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–æ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π\n",
    "        self.key   = nn.Linear(n_embed, hidden_dim, bias=False)\n",
    "        self.query = nn.Linear(n_embed, hidden_dim, bias=False)\n",
    "        self.value = nn.Linear(n_embed, hidden_dim, bias=False)\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –±—É—Ñ–µ—Ä–∞ –¥–ª—è –Ω–∏–∂–Ω–µ—Ç—Ä–µ—É–≥–æ–ª—å–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã\n",
    "        # –ü–æ—á–µ–º—É: —ç—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –∑–∞–≥–ª—è–¥—ã–≤–∞–Ω–∏—è –≤ –±—É–¥—É—â–∏–µ —Ç–æ–∫–µ–Ω—ã,\n",
    "        # —á—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ GPT.\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –æ–¥–Ω–æ–π –≥–æ–ª–æ–≤—ã —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä.\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –æ–¥–Ω–æ–π –≥–æ–ª–æ–≤—ã —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "\n",
    "        Raises:\n",
    "        ---------------\n",
    "            –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "        Examples:\n",
    "        ---------------\n",
    "            >>> sh = SingleHead(hidden_dim=32, n_embed=128, block_size=64)\n",
    "            >>> x = torch.randn(1, 64, 128)\n",
    "            >>> output = sh(x)\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length, channels = x.shape\n",
    "        k = self.key(x)    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–π   (Batch, T, Hidden_dim)\n",
    "        q = self.query(x)  # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ (Batch, T, Hidden_dim)\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (dot product attention)\n",
    "        # –ü–æ—á–µ–º—É: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ sqrt(C) —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.\n",
    "        weights = q @ k.transpose(-2, -1) * channels ** (-0.5)\n",
    "\n",
    "        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞–≥–ª—è–¥—ã–≤–∞–Ω–∏—è –≤ –±—É–¥—É—â–µ–µ\n",
    "        masked_weights = weights.masked_fill(self.tril[:sequence_length, :sequence_length] == 0, float(\"-inf\"))\n",
    "        masked_probs = F.softmax(masked_weights, dim=-1)  # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π\n",
    "        masked_probs = self.drop(masked_probs)            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥—Ä–æ–ø–∞—É—Ç–∞\n",
    "\n",
    "        v = self.value(x)       # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π (Batch, T, Hidden_dim)\n",
    "        out = masked_probs @ v  # –£–º–Ω–æ–∂–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# –†–µ–∞–ª–∏–∑—É–µ—Ç –º–æ–¥–µ–ª—å GPT.\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –†–µ–∞–ª–∏–∑—É–µ—Ç –º–æ–¥–µ–ª—å GPT (Generative Pre-trained Transformer) –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        vocab_size: –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è.\n",
    "        block_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–∫–æ–Ω—Ç–µ–∫—Å—Ç).\n",
    "        n_embed: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞.\n",
    "        num_heads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –∫–∞–∂–¥–æ–º –±–ª–æ–∫–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.\n",
    "        n_layers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –±–ª–æ–∫–æ–≤.\n",
    "\n",
    "    Attributes:\n",
    "    ---------------\n",
    "        vocab_size: –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è.\n",
    "        block_size: –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞.\n",
    "        embedding: –°–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "        positional_embedding_table: –°–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –ø–æ–∑–∏—Ü–∏–π.\n",
    "        blocks: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –±–ª–æ–∫–æ–≤.\n",
    "        norm: –°–ª–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –±–ª–æ–∫–æ–≤.\n",
    "        fc: –§–∏–Ω–∞–ª—å–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å–ª–æ–≤–∞—Ä—è.\n",
    "\n",
    "    Raises:\n",
    "    ---------------\n",
    "        –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> model = GPT(vocab_size=10000, block_size=256, n_embed=384, num_heads=6, n_layers=6)\n",
    "        >>> x = torch.randint(0, 10000, (1, 64))\n",
    "        >>> output = model(x)\n",
    "        >>> print(output.shape)\n",
    "        torch.Size([64, 10000])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        block_size: int,\n",
    "        n_embed: int,\n",
    "        num_heads: int,\n",
    "        n_layers: int\n",
    "    ):\n",
    "        super(GPT, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embed)  # –°–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "\n",
    "        # –ü–æ—á–µ–º—É: –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ—Ä—è–¥–∫–µ —Ç–æ–∫–µ–Ω–æ–≤,\n",
    "        # —Ç–∞–∫ –∫–∞–∫ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫.\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[TransformerBlock(num_heads, n_embed, block_size) for _ in range(n_layers)],\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(n_embed)         # –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è\n",
    "        self.fc = nn.Linear(n_embed, vocab_size)  # –§–∏–Ω–∞–ª—å–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –º–æ–¥–µ–ª–∏ GPT.\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∏–Ω–¥–µ–∫—Å—ã —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            –¢–µ–Ω–∑–æ—Ä –ª–æ–≥–∏—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ.\n",
    "\n",
    "        Raises:\n",
    "        ---------------\n",
    "            –ù–µ—Ç —è–≤–Ω—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "\n",
    "        Examples:\n",
    "        ---------------\n",
    "            >>> model = GPT(vocab_size=10000, block_size=256, n_embed=384, num_heads=6, n_layers=6)\n",
    "            >>> x = torch.randint(0, 10000, (1, 64))\n",
    "            >>> output = model(x)\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length = x.shape\n",
    "        token_embeddings = self.embedding(x)  # (Batch, Sequence_Length, Embedding_Dim)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —Ç–µ–∫—É—â–µ–π –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        positional_embedding = self.positional_embedding_table(\n",
    "            torch.arange(sequence_length, device=x.device)\n",
    "        )  # (Sequence_Length, Embedding_Dim)\n",
    "        \n",
    "        # –°–ª–æ–∂–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–Ω—ã—Ö –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        token_embeddings = token_embeddings + positional_embedding  # (Batch, Sequence_Length, Embedding_Dim)\n",
    "        blocks_out = self.blocks(token_embeddings)                  # –ü—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –±–ª–æ–∫–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞\n",
    "        blocks_out = self.norm(blocks_out)                          # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "        logits = self.fc(blocks_out)                                # –ü—Ä–æ–µ–∫—Ü–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å–ª–æ–≤–∞—Ä—è (Batch, Sequence_Length, Vocab_Size)\n",
    "        \n",
    "        # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –ª–æ–≥–∏—Ç–æ–≤ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø–æ—Ç–µ—Ä—å (Batch * Sequence_Length, Vocab_Size)\n",
    "        logits = logits.reshape(batch_size * sequence_length, self.vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T18:51:55.374216Z",
     "iopub.status.busy": "2024-05-27T18:51:55.373943Z",
     "iopub.status.idle": "2024-05-27T18:51:55.420360Z",
     "shell.execute_reply": "2024-05-27T18:51:55.419656Z",
     "shell.execute_reply.started": "2024-05-27T18:51:55.374193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "train_sentences, eval_sentences = train_test_split(sentences, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T18:51:55.421642Z",
     "iopub.status.busy": "2024-05-27T18:51:55.421381Z",
     "iopub.status.idle": "2024-05-27T18:51:55.427328Z",
     "shell.execute_reply": "2024-05-27T18:51:55.426317Z",
     "shell.execute_reply.started": "2024-05-27T18:51:55.421619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤.\n",
    "def sentence_pre(s: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ —Å–ø–∏—Å–æ–∫ —á–∏—Å–ª–æ–≤—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è `word2ind`.\n",
    "        –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –∑–∞–º–µ–Ω—è—é—Ç—Å—è —Ç–æ–∫–µ–Ω–æ–º `<unk>`.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        s: –í—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.\n",
    "\n",
    "    Returns:\n",
    "    ---------------\n",
    "        –°–ø–∏—Å–æ–∫ —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª, –≥–¥–µ –∫–∞–∂–¥–æ–µ —á–∏—Å–ª–æ - —ç—Ç–æ –∏–Ω–¥–µ–∫—Å —Å–ª–æ–≤–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ.\n",
    "    \"\"\"\n",
    "    return [word2ind.get(w, word2ind['<unk>']) for w in nltk.word_tokenize(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T18:51:55.428966Z",
     "iopub.status.busy": "2024-05-27T18:51:55.428611Z",
     "iopub.status.idle": "2024-05-27T18:52:24.356606Z",
     "shell.execute_reply": "2024-05-27T18:52:24.355847Z",
     "shell.execute_reply.started": "2024-05-27T18:51:55.428936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ `sentence_pre` –∫ –æ–±—É—á–∞—é—â–∏–º –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º\n",
    "train_sentences = list(map(sentence_pre, train_sentences))\n",
    "eval_sentences  = list(map(sentence_pre, eval_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T18:52:24.357942Z",
     "iopub.status.busy": "2024-05-27T18:52:24.357661Z",
     "iopub.status.idle": "2024-05-27T18:52:24.363175Z",
     "shell.execute_reply": "2024-05-27T18:52:24.362335Z",
     "shell.execute_reply.started": "2024-05-27T18:52:24.357916Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –∫–ª–∞—Å—Å–æ–≤ WordDataset –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–æ–∫\n",
    "train_dataset = WordDataset(train_sentences, word2ind)\n",
    "eval_dataset  = WordDataset(eval_sentences,  word2ind)\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–æ–∫\n",
    "# –ü–æ—á–µ–º—É: DataLoader'—ã –ø–æ–∑–≤–æ–ª—è—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∞–º–∏,\n",
    "# –ø—Ä–∏–º–µ–Ω—è—Ç—å —Ñ—É–Ω–∫—Ü–∏—é collate_fn –∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ.\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, collate_fn=collate_fn_with_padding, batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, collate_fn=collate_fn_with_padding, batch_size=BATCH_SIZE, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T18:52:24.364580Z",
     "iopub.status.busy": "2024-05-27T18:52:24.364313Z",
     "iopub.status.idle": "2024-05-27T18:52:24.374963Z",
     "shell.execute_reply": "2024-05-27T18:52:24.374216Z",
     "shell.execute_reply.started": "2024-05-27T18:52:24.364557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ GPT\n",
    "# –ü–æ—á–µ–º—É: —ç—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∏ —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏.\n",
    "vocab_size = len(vocab)  # –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è\n",
    "block_size = 256         # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "n_embed = 384            # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞\n",
    "num_heads = 6            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "n_layers = 6             # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T18:52:24.376974Z",
     "iopub.status.busy": "2024-05-27T18:52:24.376102Z",
     "iopub.status.idle": "2024-05-27T18:52:24.910756Z",
     "shell.execute_reply": "2024-05-27T18:52:24.909866Z",
     "shell.execute_reply.started": "2024-05-27T18:52:24.376942Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (embedding): Embedding(20004, 384)\n",
      "  (positional_embedding_table): Embedding(256, 384)\n",
      "  (blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x SingleHead(\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (project): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (drop): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layer): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x SingleHead(\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (project): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (drop): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layer): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x SingleHead(\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (project): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (drop): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layer): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x SingleHead(\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (project): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (drop): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layer): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x SingleHead(\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (project): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (drop): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layer): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (mhsa): MultiHeadSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-5): 6 x SingleHead(\n",
      "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (project): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (drop): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (layer): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc): Linear(in_features=384, out_features=20004, bias=True)\n",
      ")\n",
      "Number of model parameters: 26,122,020\n"
     ]
    }
   ],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ GPT –∏ –ø–µ—Ä–µ–Ω–æ—Å –µ—ë –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (CPU/GPU)\n",
    "model = GPT(vocab_size=vocab_size, block_size=block_size, n_embed=n_embed, num_heads=num_heads, n_layers=n_layers).to(device)\n",
    "\n",
    "# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏\n",
    "# –ü–æ—á–µ–º—É: –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –µ–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(model)\n",
    "print(f\"Number of model parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T18:52:24.912174Z",
     "iopub.status.busy": "2024-05-27T18:52:24.911900Z",
     "iopub.status.idle": "2024-05-27T20:54:30.786360Z",
     "shell.execute_reply": "2024-05-27T20:54:30.785414Z",
     "shell.execute_reply.started": "2024-05-27T18:52:24.912151Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   3%|‚ñé         | 1/30 [04:05<1:58:27, 245.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 001 train_loss: 5.7074     val_loss 5.1998 train_perplexirty 389.2737 val_perplexirty 182.2926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   7%|‚ñã         | 2/30 [08:09<1:54:08, 244.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 002 train_loss: 4.9312     val_loss 4.8145 train_perplexirty 140.1134 val_perplexirty 124.0320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  10%|‚ñà         | 3/30 [12:13<1:49:59, 244.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 003 train_loss: 4.5089     val_loss 4.6039 train_perplexirty 91.4058 val_perplexirty 100.4387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  13%|‚ñà‚ñé        | 4/30 [16:17<1:45:52, 244.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 004 train_loss: 4.1919     val_loss 4.4881 train_perplexirty 66.5211 val_perplexirty 89.4769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  17%|‚ñà‚ñã        | 5/30 [20:21<1:41:46, 244.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 005 train_loss: 3.9345     val_loss 4.4217 train_perplexirty 51.4186 val_perplexirty 83.7435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  20%|‚ñà‚ñà        | 6/30 [24:25<1:37:40, 244.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 006 train_loss: 3.7136     val_loss 4.4016 train_perplexirty 41.2362 val_perplexirty 82.0832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  23%|‚ñà‚ñà‚ñé       | 7/30 [28:30<1:33:35, 244.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 007 train_loss: 3.5176     val_loss 4.4011 train_perplexirty 33.9049 val_perplexirty 82.0706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  27%|‚ñà‚ñà‚ñã       | 8/30 [32:34<1:29:30, 244.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 008 train_loss: 3.3398     val_loss 4.4256 train_perplexirty 28.3729 val_perplexirty 84.1377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  30%|‚ñà‚ñà‚ñà       | 9/30 [36:38<1:25:26, 244.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 009 train_loss: 3.1802     val_loss 4.4597 train_perplexirty 24.2117 val_perplexirty 87.0989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  33%|‚ñà‚ñà‚ñà‚ñé      | 10/30 [40:42<1:21:22, 244.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 010 train_loss: 3.0330     val_loss 4.5091 train_perplexirty 20.8765 val_perplexirty 91.5540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  37%|‚ñà‚ñà‚ñà‚ñã      | 11/30 [44:46<1:17:17, 244.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 011 train_loss: 2.9007     val_loss 4.5579 train_perplexirty 18.3062 val_perplexirty 96.1564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  40%|‚ñà‚ñà‚ñà‚ñà      | 12/30 [48:50<1:13:13, 244.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 012 train_loss: 2.7808     val_loss 4.6251 train_perplexirty 16.2398 val_perplexirty 102.9171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 13/30 [52:54<1:09:09, 244.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 013 train_loss: 2.6690     val_loss 4.6861 train_perplexirty 14.5097 val_perplexirty 109.4391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 14/30 [56:58<1:05:05, 244.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 014 train_loss: 2.5688     val_loss 4.7498 train_perplexirty 13.1302 val_perplexirty 116.7037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 15/30 [1:01:02<1:01:01, 244.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 015 train_loss: 2.4767     val_loss 4.8227 train_perplexirty 11.9716 val_perplexirty 125.6011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 16/30 [1:05:06<56:56, 244.07s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 016 train_loss: 2.3929     val_loss 4.8815 train_perplexirty 11.0103 val_perplexirty 133.2416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 17/30 [1:09:10<52:53, 244.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 017 train_loss: 2.3170     val_loss 4.9483 train_perplexirty 10.2079 val_perplexirty 142.5939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 18/30 [1:13:14<48:48, 244.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 018 train_loss: 2.2474     val_loss 5.0167 train_perplexirty 9.5149 val_perplexirty 152.7303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 19/30 [1:17:18<44:44, 244.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 019 train_loss: 2.1853     val_loss 5.0792 train_perplexirty 8.9442 val_perplexirty 162.6668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 20/30 [1:21:22<40:40, 244.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 020 train_loss: 2.1258     val_loss 5.1391 train_perplexirty 8.4245 val_perplexirty 172.7972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 21/30 [1:25:26<36:36, 244.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 021 train_loss: 2.0700     val_loss 5.2038 train_perplexirty 7.9666 val_perplexirty 184.4470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 22/30 [1:29:30<32:32, 244.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 022 train_loss: 2.0205     val_loss 5.2703 train_perplexirty 7.5809 val_perplexirty 197.2539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 23/30 [1:33:35<28:28, 244.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 023 train_loss: 1.9754     val_loss 5.3235 train_perplexirty 7.2453 val_perplexirty 208.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/30 [1:37:39<24:24, 244.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 024 train_loss: 1.9316     val_loss 5.3808 train_perplexirty 6.9336 val_perplexirty 220.5924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 25/30 [1:41:43<20:20, 244.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 025 train_loss: 1.8909     val_loss 5.4325 train_perplexirty 6.6551 val_perplexirty 232.4053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 26/30 [1:45:47<16:16, 244.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 026 train_loss: 1.8539     val_loss 5.4875 train_perplexirty 6.4134 val_perplexirty 245.7721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 27/30 [1:49:51<12:12, 244.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 027 train_loss: 1.8194     val_loss 5.5330 train_perplexirty 6.1959 val_perplexirty 257.2050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 28/30 [1:53:55<08:08, 244.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 028 train_loss: 1.7853     val_loss 5.5884 train_perplexirty 5.9880 val_perplexirty 272.1051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 29/30 [1:58:00<04:04, 244.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 029 train_loss: 1.7539     val_loss 5.6389 train_perplexirty 5.8000 val_perplexirty 286.3192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [2:02:04<00:00, 244.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 030 train_loss: 1.7252     val_loss 5.6779 train_perplexirty 5.6343 val_perplexirty 297.6578\n",
      "Best val perplexirty: 82.070595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "EPOCHS = 30\n",
    "\n",
    "best_model, losses = train(train_dataloader, eval_dataloader, model, EPOCHS, ignore_index=word2ind[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T21:16:49.249948Z",
     "iopub.status.busy": "2024-05-27T21:16:49.249149Z",
     "iopub.status.idle": "2024-05-27T21:16:49.479558Z",
     "shell.execute_reply": "2024-05-27T21:16:49.478551Z",
     "shell.execute_reply.started": "2024-05-27T21:16:49.249916Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –ª—É—á—à–µ–π –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "torch.save(best_model.state_dict(), \"best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T21:17:55.905577Z",
     "iopub.status.busy": "2024-05-27T21:17:55.904872Z",
     "iopub.status.idle": "2024-05-27T21:17:56.741672Z",
     "shell.execute_reply": "2024-05-27T21:17:56.740766Z",
     "shell.execute_reply.started": "2024-05-27T21:17:55.905543Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–æ–±–ª–∞–∫–æ 1 ) –Ω–∞–ª–∏—á–∏–µ <unk> –≤ –Ω–∞—Ä—É—à–µ–Ω–∏–π –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è ; 2 ) <unk> <unk> –≤ –æ–±—ä–µ–º–µ ; 3 ) –Ω–∞–ª–∏—á–∏–µ —Ö—Ä–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ <unk> –∫—Ä–æ–≤–∏ ; 4 ) —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ–∂–∏–¥–∞–µ–º–æ–π —Å—Ä–µ–¥–Ω–µ–π –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∂–∏–∑–Ω–∏ ; 29 . <eos>'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤, –Ω–∞—á–∏–Ω–∞—è —Å —Ç–æ–∫–µ–Ω–∞ \"–æ–±–ª–∞–∫–æ\"\n",
    "generate_sequence(best_model, word2ind, ind2word,starting_seq=word2ind['–æ–±–ª–∞–∫–æ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í—ã–≤–æ–¥—ã\n",
    "\n",
    "**–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (GPT)**\n",
    "\n",
    "- –í –¥–∞–Ω–Ω–æ–º –ø—É–Ω–∫—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –±—ã–ª–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—É—Ç—É—Ä —Å –Ω—É–ª—è. –†–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Generative Pre-trained Transformer (GPT) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ PyTorch.\n",
    "\n",
    "**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è**\n",
    "\n",
    "- –í –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ª–µ–∂–∏—Ç –∫–ª–∞—Å—Å GPT, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–æ–¥—É–ª—å–Ω—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –±–ª–æ–∫–æ–≤. –í–Ω—É—Ç—Ä–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–ª–∞—Å—Å—ã –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è: SingleHead –¥–ª—è –æ–¥–Ω–æ–π –≥–æ–ª–æ–≤—ã –∏ MultiHeadSelfAttention –¥–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è. –≠—Ç–∏ –∫–ª–∞—Å—Å—ã –ø–æ–∑–≤–æ–ª—è—é—Ç –º–æ–¥–µ–ª–∏ —Å–º–æ—Ç—Ä–µ—Ç—å \"–Ω–∞–∑–∞–¥\" –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç—ã–≤–∞—Ç—å —Ä–∞–∑–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.\n",
    "\n",
    "–¢–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–≤–∞–ª–∏—Å—å –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤ (embeddings), –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∏ –∫–æ–º–±–∏–Ω–∏—Ä—É—é—Ç—Å—è –ø–µ—Ä–µ–¥ –ø–æ–¥–∞—á–µ–π –≤ –º–æ–¥–µ–ª—å\n",
    "\n",
    "**–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏**\n",
    "\n",
    "- –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ, —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –ø–∞–¥–¥–∏–Ω–≥–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–ª–∏–Ω –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∑–∞–Ω—è–ª–æ –ø—Ä–∏–º–µ—Ä–Ω–æ 2 —á–∞—Å–∞, –æ–Ω–∞ –æ–±—É—á–∞–ª–∞—Å—å 30 —ç–ø–æ—Ö.\n",
    "\n",
    "**–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞**\n",
    "\n",
    "- –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª—å —Å–ø–æ—Å–æ–±–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –Ω–∞—á–∏–Ω–∞—è —Å –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞. –ò–∑-–∑–∞ —Ç–æ–≥–æ, —á—Ç–æ –º–æ–¥–µ–ª—å –º–∞–ª–µ–Ω—å–∫–∞—è, –±—ã–ª–æ –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ü–µ–ª–∏–∫–æ–º –æ—Å–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç. –ü–æ–ª—É—á–∞–ª–∏—Å—å —Ç–∞–∫–∂–µ –∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏—è.\n",
    "\n",
    "**–ó–∞–∫–ª—é—á–µ–Ω–∏–µ**\n",
    "\n",
    "- –í –∑–∞–∫–ª—é—á–µ–Ω–∏–∏ –º–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ç–∏–ø–∞ GPT –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è \"—Å –Ω—É–ª—è\" –ø–æ–º–æ–≥–∞–µ—Ç –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º—ã —Ä–∞–±–æ—Ç—ã —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ –¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—É—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–æ–¥ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1167113,
     "sourceId": 2730445,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
