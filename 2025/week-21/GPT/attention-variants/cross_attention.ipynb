{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кросс-внимание (Cross-Attention)\n",
    "\n",
    "Кросс-внимание — это очень простая концепция, такая же, как обычное внимание, но $x$ в $Q = W_q(x)$ и $x$ в $W_k(x)/W_v(x)$ теперь принадлежат разным последовательностям. \"Внимание\", которое запросы (`queries`) уделяют ключам и значениям (`keys/values`), теперь работает **между** последовательностями, а не внутри одной последовательности токенов.  \n",
    "\n",
    "На самом деле, это \"оригинальное\" внимание — именно так оно было представлено в знаменитой статье [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), где механизм внимания изначально разрабатывался для задачи машинного перевода. Там кросс-внимание позволяло любому токену в фразе `\"I am a guy\"` обращать внимание на любой токен в `\"Je suis un mec\"`, что, очевидно, очень полезно для перевода.  \n",
    "\n",
    "В современных языковых моделях (LLM) кросс-внимание **почти не используется** (или не используется вовсе). Однако оно остаётся фундаментальным инструментом во многих других современных подходах:  \n",
    "\n",
    "- **RAG** — для внимания к топ-k эмбеддингам, полученным от модели поиска/реранкера,  \n",
    "- **Мультимодальные LLM** — для взаимодействия токенов разных модальностей,  \n",
    "- **Условные диффузионные модели** — чтобы направлять латентное пространство или делать его функцией от внешней последовательности (контекста).  \n",
    "\n",
    "Это **очень мощный приём**, который стоит хорошо понимать, если вы планируете работать с архитектурами нейросетей.  \n",
    "\n",
    "#### Интерпретация кросс-внимания  \n",
    "\n",
    "Результирующий вектор:  \n",
    "\n",
    "$$  \n",
    "\\text{Output} = \\frac{1}{\\sqrt{D}} \\cdot \\text{Softmax}(QK^T) V  \n",
    "$$  \n",
    "\n",
    "— это линейная комбинация векторов значений (`V`), но теперь они приходят **из другой последовательности**, а их веса (коэффициенты) выбираются **исходной последовательностью**.  \n",
    "\n",
    "#### Личное замечание  \n",
    "\n",
    "Это важный инструмент, если вы экспериментируете с архитектурами. Например, несколько лет назад я пытался улучшить перплексию языковой модели, заставив её обращать внимание на эмбеддинги, сгенерированные **визуальным энкодером**. Идея была в том, что если модель \"увидит\" изображения разных видов цветов, то сможет лучше различать их по текстовым описаниям.  \n",
    "\n",
    "В итоге это **не сработало**, но позже появились похожие идеи, где кросс-внимание используется для улучшения LLM за счёт навыков других моделей (см. [Augmenting LLMs with Knowledge](https://arxiv.org/pdf/2401.02412)).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Этот модуль реализует два варианта механизма внимания Multi-Head Attention (MHA)\n",
    "с использованием библиотеки PyTorch:\n",
    "1.  MHSA (Multi-Head Self-Attention): Стандартный механизм самовнимания, где\n",
    "    запросы (queries), ключи (keys) и значения (values) генерируются из одного\n",
    "    и того же входного тензора.\n",
    "2.  CrossMHSA (Cross Multi-Head Self-Attention): Механизм перекрестного внимания,\n",
    "    где запросы генерируются из одного входного тензора (x1), а ключи и значения -\n",
    "    из другого (x2). Это позволяет модели соотносить информацию между двумя\n",
    "    различными последовательностями.\n",
    "\n",
    "Модуль также включает тестовый блок, который:\n",
    "-   Сравнивает поведение MHSA (в режиме без каузальной маски) и CrossMHSA,\n",
    "    когда на вход подаются идентичные последовательности.\n",
    "-   Демонстрирует ключевую особенность CrossMHSA: способность обрабатывать\n",
    "    входные последовательности (x1 и x2) разной длины, что невозможноЦ\n",
    "    для стандартного MHSA.\n",
    "-   Проверяет, что CrossMHSA выдает различные результаты при обработке\n",
    "    одинаковых и разных последовательностей.\n",
    "\n",
    "Этот код служит наглядным примером реализации и принципов работы\n",
    "перекрестного внимания (Cross-Attention).\n",
    "\"\"\"\n",
    "\n",
    "# Стандартные библиотеки\n",
    "import math\n",
    "from typing import Tuple # Используется для аннотации типов кортежей\n",
    "\n",
    "# Сторонние библиотеки\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение класса для Multi-Head Self-Attention (MHSA)\n",
    "class MHSA(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Реализует механизм Multi-Head Self-Attention (MHSA).\n",
    "        В этом механизме запросы (queries), ключи (keys) и значения (values)\n",
    "        вычисляются из одного и того же входного тензора.\n",
    "        Может работать в каузальном режиме (для декодеров трансформера).\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        head_dim (int): Размерность каждого \"внимания\" (головы).\n",
    "                        По умолчанию 64.\n",
    "        d_model (int): Размерность входного и выходного тензора признаков\n",
    "                       (размерность остаточного потока). По умолчанию 512.\n",
    "        causal (bool): Если True, применяется каузальная маска, чтобы каждая\n",
    "                       позиция могла обращать внимание только на предыдущие\n",
    "                       позиции и на саму себя. По умолчанию True.\n",
    "\n",
    "    Raises:\n",
    "    ---------------\n",
    "        AssertionError: Если `d_model` не делится нацело на `head_dim`.\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> mhsa_layer = MHSA(head_dim=64, d_model=512, causal=True)\n",
    "        >>> input_tensor = torch.randn(2, 10, 512) # (batch, seq_len, d_model)\n",
    "        >>> output_tensor = mhsa_layer(input_tensor)\n",
    "        >>> print(output_tensor.shape)\n",
    "        torch.Size([2, 10, 512])\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim: int = 64, d_model: int = 512, causal: bool = True):\n",
    "        super().__init__()\n",
    "        self.head_dim: int = head_dim\n",
    "        self.d_model: int = d_model\n",
    "        # Проверка, что размерность модели делится на размерность головы\n",
    "        # Это необходимо для корректного разделения на n_heads\n",
    "        assert d_model % head_dim == 0, (\n",
    "            \"Ошибка: размерность головы (head_dim) не делит нацело \"\n",
    "            \"размерность остаточного потока (d_model).\"\n",
    "        )\n",
    "        self.n_heads: int = d_model // head_dim\n",
    "        self.causal: bool = causal\n",
    "\n",
    "        # Линейный слой для проекции входа в объединенное пространство Q, K, V\n",
    "        # Размерность выхода 3 * d_model, так как Q, K, V имеют размерность d_model\n",
    "        self.qkv_proj: nn.Linear = nn.Linear(d_model, 3 * d_model)\n",
    "        # Линейный слой для финальной проекции результата внимания\n",
    "        self.out_proj: nn.Linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    # Прямой проход для MHSA\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Выполняет прямой проход механизма Multi-Head Self-Attention.\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            x (torch.Tensor): Входной тензор с формой (B, S, D), где:\n",
    "                              B - размер пакета (batch size)\n",
    "                              S - длина последовательности (sequence length)\n",
    "                              D - размерность признаков (d_model)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            torch.Tensor: Выходной тензор с той же формой (B, S, D),\n",
    "                          представляющий собой результат применения внимания.\n",
    "\n",
    "        Raises:\n",
    "        ---------------\n",
    "            RuntimeError: Может возникнуть при несоответствии размерностей\n",
    "                          тензоров во время операций PyTorch.\n",
    "        \"\"\"\n",
    "        # B - batch_size, S - sequence_length\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # 1. Проекция входа на Q, K, V\n",
    "        # x: [B, S, D] -> qkv: [B, S, 3*D]\n",
    "        qkv: torch.Tensor = self.qkv_proj(x)\n",
    "\n",
    "        # 2. Разделение на Q, K, V и на n_heads\n",
    "        # qkv: [B, S, 3*D] -> [B, S, 3, n_heads, head_dim]\n",
    "        qkv = qkv.reshape(\n",
    "            batch_size, seq_len, 3, self.n_heads, self.head_dim\n",
    "        )\n",
    "        # Изменение порядка измерений для удобства матричных операций\n",
    "        # qkv: [B, S, 3, n_heads, head_dim] -> [3, B, n_heads, S, head_dim]\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        # Разделение на отдельные тензоры Q, K, V\n",
    "        # Каждый из q, k, v имеет форму: [B, n_heads, S, head_dim]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # 3. Вычисление логитов внимания (матрица схожести)\n",
    "        # Используется операция Эйнштейна для пакетного матричного умножения\n",
    "        # q: [B, n_heads, S, head_dim], k: [B, n_heads, S, head_dim]\n",
    "        # attn_logits: [B, n_heads, S, S]\n",
    "        attn_logits: torch.Tensor = torch.einsum('bnid,bnjd->bnij', q, k)\n",
    "\n",
    "        # 4. Нормализация логитов внимания\n",
    "        # Деление на корень из head_dim для стабилизации градиентов\n",
    "        normalize_factor: float = math.sqrt(self.head_dim)\n",
    "        attn_logits = attn_logits / normalize_factor # Поэлементное деление\n",
    "\n",
    "        # 5. Применение каузальной маски (если требуется)\n",
    "        # Каузальная маска не позволяет позициям \"смотреть вперед\"\n",
    "        if self.causal:\n",
    "            # Создание маски: True для разрешенных позиций, False для запрещенных\n",
    "            # mask: [S, S]\n",
    "            mask: torch.Tensor = torch.arange(seq_len, device=x.device)[:, None] >= \\\n",
    "                                 torch.arange(seq_len, device=x.device)\n",
    "            # Применение маски: запрещенные позиции заменяются на -inf\n",
    "            # Это приведет к нулевым весам внимания после softmax\n",
    "            attn_logits = torch.where(\n",
    "                mask, attn_logits, float('-inf') * torch.ones_like(attn_logits)\n",
    "            )\n",
    "\n",
    "        # 6. Применение Softmax для получения весов внимания\n",
    "        # A (Attention weights): [B, n_heads, S, S]\n",
    "        attention_weights: torch.Tensor = F.softmax(attn_logits, dim=-1)\n",
    "\n",
    "        # 7. Взвешенное суммирование значений (V) с использованием весов внимания\n",
    "        # A: [B, n_heads, S, S], v: [B, n_heads, S, head_dim]\n",
    "        # out: [B, n_heads, S, head_dim]\n",
    "        out: torch.Tensor = torch.einsum('bnij,bnjd->bnid', attention_weights, v)\n",
    "\n",
    "        # 8. Конкатенация выходов всех голов и изменение формы\n",
    "        # out: [B, n_heads, S, head_dim] -> [B, S, n_heads, head_dim]\n",
    "        out = out.transpose(1, 2)\n",
    "        # out: [B, S, n_heads, head_dim] -> [B, S, D] (где D = n_heads * head_dim)\n",
    "        out = out.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        # 9. Финальная линейная проекция\n",
    "        # out_proj: [D, D] @ out: [B, S, D] -> [B, S, D]\n",
    "        return self.out_proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение класса для Cross Multi-Head Attention (CrossMHSA)\n",
    "class CrossMHSA(nn.Module):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ---------------\n",
    "        Реализует механизм Cross Multi-Head Attention (CrossMHSA).\n",
    "        В этом механизме запросы (queries) генерируются из первого входного\n",
    "        тензора (x1), а ключи (keys) и значения (values) - из второго\n",
    "        входного тензора (x2). Это позволяет модели соотносить информацию\n",
    "        между двумя различными последовательностями, которые могут иметь\n",
    "        разную длину.\n",
    "\n",
    "    Args:\n",
    "    ---------------\n",
    "        head_dim (int): Размерность каждого \"внимания\" (головы).\n",
    "                        По умолчанию 64.\n",
    "        d_model (int): Размерность входного и выходного тензора признаков\n",
    "                       (размерность остаточного потока). По умолчанию 512.\n",
    "\n",
    "    Raises:\n",
    "    ---------------\n",
    "        AssertionError: Если `d_model` не делится нацело на `head_dim`.\n",
    "\n",
    "    Examples:\n",
    "    ---------------\n",
    "        >>> cross_mhsa_layer = CrossMHSA(head_dim=64, d_model=512)\n",
    "        >>> x1_tensor = torch.randn(2, 10, 512) # (batch, seq_len1, d_model)\n",
    "        >>> x2_tensor = torch.randn(2, 15, 512) # (batch, seq_len2, d_model)\n",
    "        >>> output_tensor = cross_mhsa_layer(x1_tensor, x2_tensor)\n",
    "        >>> print(output_tensor.shape) # Output shape matches x1's seq_len\n",
    "        torch.Size([2, 10, 512])\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim: int = 64, d_model: int = 512):\n",
    "        super().__init__()\n",
    "        self.head_dim: int = head_dim\n",
    "        self.d_model: int = d_model\n",
    "        # Проверка, что размерность модели делится на размерность головы\n",
    "        assert d_model % head_dim == 0, (\n",
    "            \"Ошибка: размерность головы (head_dim) не делит нацело \"\n",
    "            \"размерность остаточного потока (d_model).\"\n",
    "        )\n",
    "        self.n_heads: int = d_model // head_dim\n",
    "\n",
    "        # Отдельные линейные слои для проекции Q (из x1), K (из x2), V (из x2)\n",
    "        self.q_proj: nn.Linear = nn.Linear(d_model, d_model)\n",
    "        self.k_proj: nn.Linear = nn.Linear(d_model, d_model)\n",
    "        self.v_proj: nn.Linear = nn.Linear(d_model, d_model)\n",
    "        # Линейный слой для финальной проекции результата внимания\n",
    "        self.out_proj: nn.Linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    # Прямой проход для CrossMHSA\n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ---------------\n",
    "            Выполняет прямой проход механизма Cross Multi-Head Attention.\n",
    "            Запросы (Q) генерируются из `x1`.\n",
    "            Ключи (K) и значения (V) генерируются из `x2`.\n",
    "\n",
    "        Args:\n",
    "        ---------------\n",
    "            x1 (torch.Tensor): Входной тензор для генерации запросов (Q).\n",
    "                               Форма (B, S1, D), где:\n",
    "                               B - размер пакета (batch size)\n",
    "                               S1 - длина последовательности для запросов\n",
    "                               D - размерность признаков (d_model)\n",
    "            x2 (torch.Tensor): Входной тензор для генерации ключей (K) и\n",
    "                               значений (V). Форма (B, S2, D), где:\n",
    "                               B - размер пакета (batch size)\n",
    "                               S2 - длина последовательности для ключей/значений\n",
    "                               D - размерность признаков (d_model)\n",
    "\n",
    "        Returns:\n",
    "        ---------------\n",
    "            torch.Tensor: Выходной тензор с формой (B, S1, D),\n",
    "                          представляющий собой результат применения\n",
    "                          перекрестного внимания. Длина последовательности\n",
    "                          соответствует `x1`.\n",
    "\n",
    "        Raises:\n",
    "        ---------------\n",
    "            RuntimeError: Может возникнуть при несоответствии размерностей\n",
    "                          тензоров во время операций PyTorch.\n",
    "        \"\"\"\n",
    "        # B - batch_size, S1 - sequence_length_x1, S2 - sequence_length_x2\n",
    "        batch_size, seq_len1, _ = x1.shape\n",
    "        _, seq_len2, _ = x2.shape\n",
    "\n",
    "        # 1. Проекция входов на Q (из x1), K (из x2), V (из x2)\n",
    "        # q: [B, S1, D]\n",
    "        q: torch.Tensor = self.q_proj(x1)\n",
    "        # k: [B, S2, D], v: [B, S2, D]\n",
    "        k: torch.Tensor = self.k_proj(x2)\n",
    "        v: torch.Tensor = self.v_proj(x2)\n",
    "\n",
    "        # 2. Разделение Q, K, V на n_heads и изменение формы\n",
    "        # q: [B, S1, D] -> [B, S1, n_heads, head_dim] -> [B, n_heads, S1, head_dim]\n",
    "        q = q.reshape(\n",
    "            batch_size, seq_len1, self.n_heads, self.head_dim\n",
    "        ).transpose(1, 2)\n",
    "        # k: [B, S2, D] -> [B, S2, n_heads, head_dim] -> [B, n_heads, S2, head_dim]\n",
    "        k = k.reshape(\n",
    "            batch_size, seq_len2, self.n_heads, self.head_dim\n",
    "        ).transpose(1, 2)\n",
    "        # v: [B, S2, D] -> [B, S2, n_heads, head_dim] -> [B, n_heads, S2, head_dim]\n",
    "        v = v.reshape(\n",
    "            batch_size, seq_len2, self.n_heads, self.head_dim\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        # 3. Вычисление логитов внимания (матрица схожести)\n",
    "        # q: [B, n_heads, S1, head_dim], k: [B, n_heads, S2, head_dim]\n",
    "        # attn_logits: [B, n_heads, S1, S2]\n",
    "        # Каждая позиция из x1 (S1) обращает внимание на все позиции из x2 (S2)\n",
    "        attn_logits: torch.Tensor = torch.einsum('bnid,bnjd->bnij', q, k)\n",
    "\n",
    "        # 4. Нормализация логитов внимания\n",
    "        normalize_factor: float = math.sqrt(self.head_dim)\n",
    "        attn_logits = attn_logits / normalize_factor\n",
    "\n",
    "        # 5. Применение Softmax для получения весов внимания\n",
    "        # A (Attention weights): [B, n_heads, S1, S2]\n",
    "        # Softmax применяется по последнему измерению (S2), т.е. по ключам из x2\n",
    "        attention_weights: torch.Tensor = F.softmax(attn_logits, dim=-1)\n",
    "\n",
    "        # 6. Взвешенное суммирование значений (V) с использованием весов внимания\n",
    "        # A: [B, n_heads, S1, S2], v: [B, n_heads, S2, head_dim]\n",
    "        # out: [B, n_heads, S1, head_dim]\n",
    "        # Результат имеет длину последовательности S1 (от x1)\n",
    "        out: torch.Tensor = torch.einsum('bnij,bnjd->bnid', attention_weights, v)\n",
    "\n",
    "        # 7. Конкатенация выходов всех голов и изменение формы\n",
    "        # out: [B, n_heads, S1, head_dim] -> [B, S1, n_heads, head_dim]\n",
    "        out = out.transpose(1, 2)\n",
    "        # out: [B, S1, n_heads, head_dim] -> [B, S1, D]\n",
    "        out = out.reshape(batch_size, seq_len1, -1)\n",
    "\n",
    "        # 8. Финальная линейная проекция\n",
    "        # out_proj: [D, D] @ out: [B, S1, D] -> [B, S1, D]\n",
    "        return self.out_proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Тест 1: Сравнение MHSA и CrossMHSA (одинаковые входы) ---\n",
      "При идентичных последовательностях и весах: выходы MHSA и CrossMHSA(x,x) совпадают = True\n",
      "\n",
      "--- Тест 2: CrossMHSA с последовательностями разной длины ---\n",
      "CrossMHSA успешно обработал последовательности разной длины (5 и 7).\n",
      "Форма выхода: torch.Size([2, 5, 512]) (соответствует x1).\n",
      "Cross-attention позволяет использовать последовательности разной длины для Q и K/V, в то время как стандартный self-attention требует идентичные последовательности (или одну и ту же).\n",
      "\n",
      "--- Тест 3: CrossMHSA дает разные результаты для разных входов ---\n",
      "CrossMHSA(x1, x1) и CrossMHSA(x1, x2) (где x1 != x2) дают разные результаты: True\n",
      "\n",
      "Все тесты завершены!\n"
     ]
    }
   ],
   "source": [
    "# --- Тестовый блок ---\n",
    "# Этот блок демонстрирует работу MHSA и CrossMHSA,\n",
    "# а также их ключевые различия.\n",
    "\n",
    "# Параметры для тестов\n",
    "BATCH_SIZE: int = 2\n",
    "SEQ_LEN: int = 5\n",
    "DIM: int = 512\n",
    "HEAD_DIM: int = 64\n",
    "\n",
    "# --- Тест 1: Сравнение MHSA и CrossMHSA при одинаковых входных последовательностях ---\n",
    "# Цель: Убедиться, что CrossMHSA(x, x) эквивалентен MHSA(x, causal=False)\n",
    "# при идентичных весах.\n",
    "print(\"--- Тест 1: Сравнение MHSA и CrossMHSA (одинаковые входы) ---\")\n",
    "# Создание случайного входного тензора\n",
    "x_test_identical: torch.Tensor = torch.randn(BATCH_SIZE, SEQ_LEN, DIM)\n",
    "\n",
    "# Инициализация модулей внимания\n",
    "# Для справедливого сравнения, MHSA должен быть без каузальной маски\n",
    "mhsa_module: MHSA = MHSA(head_dim=HEAD_DIM, d_model=DIM, causal=False)\n",
    "cross_attn_module: CrossMHSA = CrossMHSA(head_dim=HEAD_DIM, d_model=DIM)\n",
    "\n",
    "# Копирование весов из CrossMHSA в MHSA для эквивалентности\n",
    "# Это необходимо, чтобы оба модуля выполняли идентичные вычисления\n",
    "# при одинаковых входах.\n",
    "with torch.no_grad(): # Отключаем вычисление градиентов для операций с весами\n",
    "    # Копирование весов проекций Q, K, V\n",
    "    # qkv_proj в MHSA объединяет проекции Q, K, V, поэтому конкатенируем\n",
    "    # веса и смещения из q_proj, k_proj, v_proj из CrossMHSA.\n",
    "    mhsa_module.qkv_proj.weight.copy_(torch.cat([\n",
    "        cross_attn_module.q_proj.weight,\n",
    "        cross_attn_module.k_proj.weight,\n",
    "        cross_attn_module.v_proj.weight\n",
    "    ], dim=0))\n",
    "    mhsa_module.qkv_proj.bias.copy_(torch.cat([\n",
    "        cross_attn_module.q_proj.bias,\n",
    "        cross_attn_module.k_proj.bias,\n",
    "        cross_attn_module.v_proj.bias\n",
    "    ], dim=0))\n",
    "\n",
    "    # Копирование весов выходной проекции\n",
    "    cross_attn_module.out_proj.weight.copy_(mhsa_module.out_proj.weight)\n",
    "    cross_attn_module.out_proj.bias.copy_(mhsa_module.out_proj.bias)\n",
    "\n",
    "# Прямой проход через оба модуля\n",
    "mhsa_output: torch.Tensor = mhsa_module(x_test_identical)\n",
    "# Для CrossMHSA подаем один и тот же тензор x_test_identical в качестве x1 и x2\n",
    "cross_attn_output_same_input: torch.Tensor = cross_attn_module(\n",
    "    x_test_identical, x_test_identical\n",
    ")\n",
    "\n",
    "# Проверка, что выходы практически идентичны (с учетом погрешностей float)\n",
    "# rtol и atol - относительная и абсолютная погрешности\n",
    "are_outputs_close: bool = torch.allclose(\n",
    "    mhsa_output, cross_attn_output_same_input, rtol=1e-4, atol=1e-4\n",
    ")\n",
    "print(\n",
    "    f\"При идентичных последовательностях и весах: выходы MHSA и \"\n",
    "    f\"CrossMHSA(x,x) совпадают = {are_outputs_close}\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- Тест 2: Демонстрация работы CrossMHSA с последовательностями разной длины ---\n",
    "# Цель: Показать, что CrossMHSA корректно обрабатывает x1 и x2 разной длины,\n",
    "# и что выходная размерность по длине последовательности соответствует x1.\n",
    "print(\"\\n--- Тест 2: CrossMHSA с последовательностями разной длины ---\")\n",
    "SEQ_LEN_1: int = 5  # Длина первой последовательности (для Q)\n",
    "SEQ_LEN_2: int = 7  # Длина второй последовательности (для K, V)\n",
    "\n",
    "# Создание случайных входных тензоров разной длины\n",
    "x1_diff_len: torch.Tensor = torch.randn(BATCH_SIZE, SEQ_LEN_1, DIM)\n",
    "x2_diff_len: torch.Tensor = torch.randn(BATCH_SIZE, SEQ_LEN_2, DIM)\n",
    "\n",
    "# Прямой проход CrossMHSA с разными последовательностями\n",
    "# Веса cross_attn_module уже настроены из предыдущего теста\n",
    "cross_attn_output_diff_len: torch.Tensor = cross_attn_module(\n",
    "    x1_diff_len, x2_diff_len\n",
    ")\n",
    "\n",
    "# Проверка формы выходного тензора\n",
    "# Ожидаемая форма: (BATCH_SIZE, SEQ_LEN_1, DIM), так как Q из x1\n",
    "expected_shape: Tuple[int, int, int] = (BATCH_SIZE, SEQ_LEN_1, DIM)\n",
    "assert cross_attn_output_diff_len.shape == expected_shape, (\n",
    "    f\"Ожидаемая форма {expected_shape}, \"\n",
    "    f\"получено {cross_attn_output_diff_len.shape}\"\n",
    ")\n",
    "print(\n",
    "    f\"CrossMHSA успешно обработал последовательности разной длины \"\n",
    "    f\"({SEQ_LEN_1} и {SEQ_LEN_2}).\"\n",
    ")\n",
    "print(f\"Форма выхода: {cross_attn_output_diff_len.shape} (соответствует x1).\")\n",
    "\n",
    "# Попытка использовать MHSA с разными длинами (неявная демонстрация)\n",
    "# Стандартный MHSA ожидает один вход, из которого генерируются Q, K, V.\n",
    "# Попытка передать ему две разные последовательности напрямую невозможна\n",
    "# без модификации самого MHSA или входных данных (например, конкатенации\n",
    "# и маскирования, что усложняет механизм).\n",
    "# Cross-attention специально разработан для таких сценариев.\n",
    "try:\n",
    "    # Это не сработает напрямую с MHSA, так как он ожидает один вход.\n",
    "    # Мы не будем вызывать ошибку, а просто констатируем факт.\n",
    "    print(\n",
    "        \"Cross-attention позволяет использовать последовательности разной длины \"\n",
    "        \"для Q и K/V, в то время как стандартный self-attention \"\n",
    "        \"требует идентичные последовательности (или одну и ту же).\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    # Этот блок не должен выполниться, так как мы не вызываем ошибку.\n",
    "    print(f\"Как и ожидалось, self-attention не может напрямую \"\n",
    "          f\"обработать последовательности разной длины: {e}\")\n",
    "\n",
    "\n",
    "# --- Тест 3: Проверка, что CrossMHSA дает разные результаты для разных входов ---\n",
    "# Цель: Убедиться, что результат CrossMHSA(x1, x2) отличается от\n",
    "# CrossMHSA(x1, x1), если x2 != x1.\n",
    "print(\"\\n--- Тест 3: CrossMHSA дает разные результаты для разных входов ---\")\n",
    "# Используем x1_diff_len из предыдущего теста\n",
    "x1_copy_for_test3: torch.Tensor = x1_diff_len.clone()\n",
    "\n",
    "# Выход CrossMHSA, когда оба входа одинаковы (x1_copy_for_test3, x1_copy_for_test3)\n",
    "cross_attn_output_same_again: torch.Tensor = cross_attn_module(\n",
    "    x1_copy_for_test3, x1_copy_for_test3\n",
    ")\n",
    "\n",
    "# Выход CrossMHSA, когда входы разные (x1_diff_len, x2_diff_len)\n",
    "# Этот результат (cross_attn_output_diff_len) уже вычислен в Тесте 2.\n",
    "\n",
    "# Проверка, что результаты отличаются\n",
    "are_results_different: bool = not torch.allclose(\n",
    "    cross_attn_output_same_again,\n",
    "    cross_attn_output_diff_len,\n",
    "    rtol=1e-4,\n",
    "    atol=1e-4\n",
    ")\n",
    "print(\n",
    "    f\"CrossMHSA(x1, x1) и CrossMHSA(x1, x2) (где x1 != x2) \"\n",
    "    f\"дают разные результаты: {are_results_different}\"\n",
    ")\n",
    "\n",
    "print(\"\\nВсе тесты завершены!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
