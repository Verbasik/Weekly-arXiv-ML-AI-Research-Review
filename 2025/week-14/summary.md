# Qwen2.5-Omni: Мультимодальная модель нового поколения

## Содержание
1. Введение  
2. Архитектура модели  
3. Методы мультимодальной интеграции  
4. Возможности потоковой передачи  
5. Методология обучения  
6. Производительность и бенчмарки  
7. Практическое применение  
8. Заключение  

## Введение

Qwen2.5-Omni представляет собой значительный шаг вперед в мультимодальных AI системах, разработанный командой Qwen в Alibaba Group. Эта модель уникальным образом объединяет возможности обработки языка, визуальной информации и аудио в единую унифицированную архитектуру, поддерживая при этом взаимодействие в режиме реального времени посредством потоковой передачи.

![Figure_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_1.png)  
*Рисунок 1: Обзор мультимодальных возможностей Qwen2.5-Omni, демонстрирующий различные режимы взаимодействия, включая видеочат, чат с изображениями, текстовый чат и аудиочат, которые поддерживаются унифицированной архитектурой Thinker-Talker.*

В отличие от предыдущих подходов, которые часто рассматривают различные модальности как отдельные системы, Qwen2.5-Omni интегрирует их в согласованную структуру, способную понимать и генерировать контент в текстовой, визуальной, аудио и видео областях. Модель разработана не только для обработки этих входных данных, но и для одновременного создания выходных данных как в виде текста, так и в виде естественно звучащей речи, с возможностями потоковой передачи, которые обеспечивают взаимодействие в режиме реального времени.

Основные инновации Qwen2.5-Omni заключаются в ее архитектуре, которая эффективно управляет межмодальной информацией, сохраняя при этом конкурентоспособную производительность в каждой отдельной модальности. В этом обзоре рассматриваются технические основы, методологические подходы и результаты тестов, которые демонстрируют возможности модели.

## Архитектура модели

Основой Qwen2.5-Omni является ее новая архитектура Thinker-Talker, которая разделяет генерацию текста и генерацию речи, поддерживая при этом координацию посредством общих скрытых представлений.

![Figure_2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_2.png)
*Рисунок 2: Подробная архитектура Qwen2.5-Omni, демонстрирующая структуру Thinker-Talker, кодировщики видео и аудио, а также декодер потокового кодека с потоком токенов между компонентами.*

Архитектура состоит из нескольких ключевых компонентов:

1. **Qwen2.5-Omni Thinker:** По сути, большая языковая модель, отвечающая за понимание мультимодальных входных данных и генерацию соответствующих текстовых ответов. Она обрабатывает закодированные представления из визуальных и аудиовходов наряду с текстом.

2. **Qwen2.5-Omni Talker:** Двухдорожечная авторегрессионная модель, которая получает скрытые представления от Thinker и генерирует аудиотокены, которые затем декодируются в речевые волны.

3. **Визуальный кодировщик:** Обрабатывает изображения и видеовходы, преобразуя их в представления, которые могут быть поняты Thinker.

4. **Аудиокодировщик:** Извлекает признаки из речи и других аудиовходов для обработки Thinker.

5. **Декодер потокового кодека:** Преобразует аудиотокены от Talker в реальные формы волны в потоковом режиме, обеспечивая вывод речи в реальном времени.

Такое разделение задач позволяет каждому компоненту специализироваться на своей задаче, а общие скрытые представления обеспечивают согласованность между модальностями. Важно отметить, что архитектура разработана как сквозная, что способствует взаимному улучшению между различными модальностями.

## **Методы мультимодальной интеграции**

Эффективная интеграция нескольких модальностей требует решения нескольких задач, особенно при согласовании временной информации по различным входным данным. Qwen2.5-Omni представляет несколько инновационных методов:

## **Time-Aligned Multimodal RoPE (TMRoPE)**

Одной из наиболее значительных инноваций является Time-Aligned Multimodal RoPE (TMRoPE), метод позиционного встраивания, разработанный для синхронизации аудио- и видеовходов:

![Figure_4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_4.png)
*Рисунок 3: Иллюстрация согласованного по времени мультимодального RoPE (TMRoPE).*

TMRoPE работает следующим образом:

1. Чередование аудио- и видеокадров во временном согласовании;
2. Кодирование 3D-позиционной информации (высота, ширина, время) для визуальных входов;
3. Синхронизация аудиокадров с соответствующими визуальными кадрами на основе информации о времени.

Этот подход гарантирует, что модель может точно связывать аудио- и визуальные события, происходящие одновременно, что имеет решающее значение для таких задач, как понимание видео с сопроводительным звуком.

Математическая формулировка TMRoPE расширяет Rotary Position Embedding для учета временного измерения:

Для последовательности длины L с токенами из нескольких модальностей TMRoPE присваивает каждому токену 3D-позицию (x, y, t), где:

- x представляет горизонтальное положение (актуально для текста и изображений);
- y представляет вертикальное положение (актуально для изображений);
- t представляет временное положение (актуально для аудио и видео).