# Qwen2.5-Omni: Мультимодальная модель нового поколения

## Содержание
1. Введение  
2. Архитектура модели  
3. Методы мультимодальной интеграции  
4. Возможности потоковой передачи  
5. Методология обучения  
6. Производительность и бенчмарки  
7. Практическое применение  
8. Заключение

### **TWRB_FM**

<audio controls>
  <source src="https://example.com/audio.mp3" type="audio/mpeg">
  Ваш браузер не поддерживает аудиоэлемент.
</audio>

## Введение

Qwen2.5-Omni представляет собой значительный шаг вперед в мультимодальных AI системах, разработанный командой Qwen в Alibaba Group. Эта модель уникальным образом объединяет возможности обработки языка, визуальной информации и аудио в единую унифицированную архитектуру, поддерживая при этом взаимодействие в режиме реального времени посредством потоковой передачи.

![Figure_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_1.png)  
*Рисунок 1: Обзор мультимодальных возможностей Qwen2.5-Omni, демонстрирующий различные режимы взаимодействия, включая видеочат, чат с изображениями, текстовый чат и аудиочат, которые поддерживаются унифицированной архитектурой Thinker-Talker.*

В отличие от предыдущих подходов, которые часто рассматривают различные модальности как отдельные системы, Qwen2.5-Omni интегрирует их в согласованную структуру, способную понимать и генерировать контент в текстовой, визуальной, аудио и видео областях. Модель разработана не только для обработки этих входных данных, но и для одновременного создания выходных данных как в виде текста, так и в виде естественно звучащей речи, с возможностями потоковой передачи, которые обеспечивают взаимодействие в режиме реального времени.

Основные инновации Qwen2.5-Omni заключаются в ее архитектуре, которая эффективно управляет межмодальной информацией, сохраняя при этом конкурентоспособную производительность в каждой отдельной модальности. В этом обзоре рассматриваются технические основы, методологические подходы и результаты тестов, которые демонстрируют возможности модели.

## Архитектура модели

Основой Qwen2.5-Omni является ее новая архитектура Thinker-Talker, которая разделяет генерацию текста и генерацию речи, поддерживая при этом координацию посредством общих скрытых представлений.

![Figure_2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_2.png)
*Рисунок 2: Подробная архитектура Qwen2.5-Omni, демонстрирующая структуру Thinker-Talker, кодировщики видео и аудио, а также декодер потокового кодека с потоком токенов между компонентами.*

Архитектура состоит из нескольких ключевых компонентов:

1. **Qwen2.5-Omni Thinker:** По сути, большая языковая модель, отвечающая за понимание мультимодальных входных данных и генерацию соответствующих текстовых ответов. Она обрабатывает закодированные представления из визуальных и аудиовходов наряду с текстом.

2. **Qwen2.5-Omni Talker:** Двухдорожечная авторегрессионная модель, которая получает скрытые представления от Thinker и генерирует аудиотокены, которые затем декодируются в речевые волны.

3. **Визуальный кодировщик:** Обрабатывает изображения и видеовходы, преобразуя их в представления, которые могут быть поняты Thinker.

4. **Аудиокодировщик:** Извлекает признаки из речи и других аудиовходов для обработки Thinker.

5. **Декодер потокового кодека:** Преобразует аудиотокены от Talker в реальные формы волны в потоковом режиме, обеспечивая вывод речи в реальном времени.

Такое разделение задач позволяет каждому компоненту специализироваться на своей задаче, а общие скрытые представления обеспечивают согласованность между модальностями. Важно отметить, что архитектура разработана как сквозная, что способствует взаимному улучшению между различными модальностями.

## **Методы мультимодальной интеграции**

Эффективная интеграция нескольких модальностей требует решения нескольких задач, особенно при согласовании временной информации по различным входным данным. Qwen2.5-Omni представляет несколько инновационных методов:

## **Time-Aligned Multimodal RoPE (TMRoPE)**

Одной из наиболее значительных инноваций является Time-Aligned Multimodal RoPE (TMRoPE), метод позиционного встраивания, разработанный для синхронизации аудио- и видеовходов:

![Figure_4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_4.png)
*Рисунок 3: Иллюстрация согласованного по времени мультимодального RoPE (TMRoPE).*

TMRoPE работает следующим образом:

1. Чередование аудио- и видеокадров во временном согласовании;
2. Кодирование 3D-позиционной информации (высота, ширина, время) для визуальных входов;
3. Синхронизация аудиокадров с соответствующими визуальными кадрами на основе информации о времени.

Этот подход гарантирует, что модель может точно связывать аудио- и визуальные события, происходящие одновременно, что имеет решающее значение для таких задач, как понимание видео с сопроводительным звуком.

Математическая формулировка TMRoPE расширяет Rotary Position Embedding для учета временного измерения:

Для последовательности длины L с токенами из нескольких модальностей TMRoPE присваивает каждому токену 3D-позицию (x, y, t), где:

* x представляет горизонтальное положение (актуально для текста и изображений);
* y представляет вертикальное положение (актуально для изображений);
* t представляет временное положение (актуально для аудио и видео).

## **Блочная обработка**

Для эффективной обработки длинных последовательностей мультимодальных данных Qwen2.5-Omni использует блочную обработку:

![Figure_5](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_5.png)
*Рисунок 4: Подход блочной обработки, показывающий, как прошлые, текущие и будущие блоки управляются во время обработки последовательных данных.*

Этот метод:

1. Разделяет входные последовательности на управляемые блоки;
2. Обрабатывает каждый блок с соответствующим контекстом из соседних блоков;
3. Поддерживает осведомленность о прошлом контексте, ограничивая при этом вычислительные требования.

Этот подход особенно важен для потоковых приложений, поскольку он позволяет модели обрабатывать новые входные данные постепенно, не пересчитывая представления для всех предыдущих входных данных.

## **Возможности потоковой передачи**

Отличительной особенностью Qwen2.5-Omni является ее способность работать в потоковом режиме как для обработки входных данных, так и для генерации выходных данных. Эту возможность поддерживают несколько архитектурных инноваций:

## **Потоковая передача входных данных**

Для потоковой передачи входных данных модель использует:

1. **Предварительное заполнение:** Обработка начальных входных данных и кэширование их представлений для снижения будущей вычислительной нагрузки;
2. **Инкрементное кодирование:** Обработка новых входных данных с повторным использованием кэшированных представлений из предыдущих входных данных;
3. **Маскирование внимания:** Обеспечение надлежащего доступа к контексту и предотвращение утечки информации от будущих токенов.


## **Потоковая передача выходных данных**
Для потоковой передачи речевого вывода Qwen2.5-Omni представляет:

1. DiT-модель со скользящим окном: Diffusion Transformer, который декодирует аудиотокены в формы волн с использованием ограниченного рецептивного поля;
2. Прогрессивная генерация: Генерация аудио небольшими блоками, которые можно немедленно отправить пользователю;
3. Оптимизация задержки начального пакета: Методы минимизации задержки перед тем, как станет доступен первый аудиофрагмент.

Сочетание этих методов позволяет Qwen2.5-Omni достигать мультимодального взаимодействия в реальном времени с минимальной воспринимаемой задержкой, что делает его подходящим для таких приложений, как голосовые помощники и системы перевода в реальном времени.

## **Методология обучения**

Обучение Qwen2.5-Omni следует тщательно разработанному многоэтапному процессу, чтобы обеспечить эффективное обучение во всех модальностях:

## **Многоэтапное обучение**

1. **Начальный этап:** Обучаются энкодеры vision и audio, при этом параметры LLM остаются фиксированными. Этот этап посвящен обучению энкодеров созданию представлений, которые LLM может эффективно использовать;

2. **Промежуточный этап:** Все параметры размораживаются и обучаются с использованием более широкого спектра мультимодальных данных. Это обеспечивает кросс-модальное обучение и адаптацию LLM к мультимодальным входам;

3. **Расширенное обучение:** Длина последовательности увеличена до 32 тысяч токенов, что позволяет модели обрабатывать более длинные контексты, включающие несколько модальностей.

## **Пост-тренировочная доводка**

После основных этапов обучения применяется специализированная доводка:

1. Доводка "Thinker": Использует данные для следования инструкциям в формате ChatML, чтобы улучшить способность модели адекватно отвечать на запросы пользователей;

2. Доводка "Talker": Следует трехэтапному процессу:
   * Обучение продолжению контекста для связного создания речи;
   * Повышение стабильности генерации с помощью Direct Preference Optimization (DPO);
   * Улучшение естественности и управляемости с помощью точной настройки многоязычных инструкций.

Этот поэтапный подход решает проблему балансировки обучения между различными модальностями, не позволяя одной модальности доминировать и вызывать помехи другим.

## **Производительность и тесты**

Qwen2.5-Omni демонстрирует конкурентоспособную производительность в различных модальностях и превосходит другие модели в задачах, требующих мультимодальной интеграции:

## **Производительность по отдельным модальностям**

* Задачи компьютерного зрения: Достигает производительности, сравнимой с Qwen2.5-VL, с особенно сильными результатами в задачах преобразования изображения в текст, таких как MMMU, MathVision, MMBench, TextVQA, DocVQA и ChartQA;
* Аудиозадачи: Превосходит Qwen2-Audio в большинстве тестов понимания аудио;
* Визуальное обоснование: Достигает 42.2 mAP в задачах обнаружения объектов с открытым словарным запасом.

## **Мультимодальные тесты**
* OmniBench: Современная производительность в этом мультимодальном тесте;
* AV-Odyssey Bench: Ведущие результаты в задачах, требующих аудиовизуального понимания.

## **Генерация речи**

Потоковый модуль Talker демонстрирует впечатляющие результаты в:
* Надежности: Низкий уровень ошибок при транскрибировании сгенерированной речи;
* Естественности: Высокие оценки качества речи при оценке человеком;
* Задержке: Минимальная задержка при начальном выводе речи по сравнению с конкурирующими моделями.

## **Следование голосовым инструкциям**

Одним из наиболее заметных достижений является сопоставимая производительность в сквозном следовании голосовым инструкциям по сравнению со следованием текстовым инструкциям, о чем свидетельствуют результаты в сложных тестах, таких как MMLU и GSM8K. Это указывает на то, что модель может понимать устные инструкции так же эффективно, как и письменные, что является значительным шагом на пути к естественному взаимодействию человека с ИИ.

## **Практическое применение**

Унифицированные мультимодальные возможности Qwen2.5-Omni позволяют использовать множество практических приложений:

1. Продвинутые голосовые помощники: Системы, которые могут видеть, слышать и говорить, сохраняя при этом контекстную осведомленность во время взаимодействий;

2. Инструменты доступности: Технологии, которые могут переводить между модальностями в режиме реального времени, например, описывать визуальный контент для пользователей с ослабленным зрением или транскрибировать речь для пользователей с нарушениями слуха;

3. Мультимодальное создание контента: Инструменты для автоматического создания контента, сочетающего текст, изображения и аудио, такие как презентации или учебные материалы;

4. Понимание видео: Приложения, которые могут анализировать и описывать видеоконтент со звуком, извлекая информацию как из визуальной, так и из слуховой информации;

5. Системы перевода в реальном времени: Сервисы, которые могут переводить устную речь, сохраняя при этом контекстную информацию из визуальных сигналов.

Возможности потоковой передачи делают эти приложения особенно привлекательными, поскольку они могут обеспечивать немедленную обратную связь и естественный ход разговора, а не пошаговое взаимодействие, распространенное во многих современных системах искусственного интеллекта.

## **Заключение**

Qwen2.5-Omni представляет собой значительный прогресс в мультимодальном ИИ благодаря своей унифицированной архитектуре, которая эффективно интегрирует обработку текста, зрения и звука, поддерживая потоковые взаимодействия в реальном времени. Дизайн Thinker-Talker, наряду с инновациями, такими как TMRoPE и блочная обработка, позволяет модели понимать и генерировать в различных модальностях с самой современной производительностью.

Хотя модель демонстрирует впечатляющие возможности, авторы признают остающиеся проблемы в таких областях, как видео OCR и аудио-видео совместное понимание. Эти проблемы подчеркивают возможности для будущих исследований и разработок в быстро развивающейся области мультимодального ИИ.

Будучи инициативой с открытым исходным кодом, доступной на таких платформах, как Hugging Face, ModelScope и GitHub, Qwen2.5-Omni вносит вклад в более широкое исследовательское сообщество и способствует дальнейшему прогрессу в создании более естественных и эффективных систем искусственного интеллекта, которые могут взаимодействовать с людьми через несколько каналов связи.