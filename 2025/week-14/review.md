# Qwen2.5-Omni: Мультимодальная модель нового поколения

## **Аннотация**  
Qwen2.5-Omni представляет собой революционную мультимодальную модель, способную обрабатывать текст, изображения, аудио и видео, а также генерировать текстовые и речевые ответы в режиме реального времени. Это универсальное решение, объединяющее передовые технологии для достижения низкой задержки и естественного взаимодействия, что делает его шагом к созданию истинного AGI.

### Ключевые инновации:
1. **Архитектура Thinker-Talker**  
   - **Thinker** (мозг): генерирует текст, анализируя данные через трансформер с мультимодальными кодерами.  
   - **Talker** (речь): преобразует скрытые представления Thinker в аудиопоток с помощью двухдорожечного декодера, избегая конфликтов между модальностями.  
   - Вдохновлено биологией: разделение задач аналогично работе мозга и речевого аппарата человека.

2. **TMRoPE: синхронизация мультимодальных данных**  
   - Новая система позиционного кодирования, выравнивающая временные метки аудио и видео.  
   - Решает проблему рассинхронизации потоков (например, речь и губы в видео).

3. **Потоковая обработка "из коробки"**  
   - Блочное кодирование входных данных и скользящее окно DiT для генерации речи с минимальной задержкой.  
   - Поддержка предварительного заполнения контекста для плавного диалога.

### Производительность и превосходство:
- **Лидер в мультимодальных тестах**: превосходит GPT-4o-mini, Qwen2.5-VL и Qwen2-Audio в задачах ASR, OCR, видеоаналитики.  
- **Генерация речи**: Zero-shot TTS с имитацией голоса и естественностью выше аналогов (включая непотоковые модели).  
- **Сквозное обучение**: точность обработки голосовых команд сопоставима с текстовым вводом (MMLU: 82.1, GSM8K: 86.3).

### Почему это прорыв?
- **Единая архитектура** для всех модальностей вместо наборов узкоспециализированных моделей.  
- **Apache 2.0 лицензия** — открытый доступ для исследований и коммерческого использования.  
- Решает проблему "мультимодального хаоса" через слаженную работу кодеров и декодеров.

Qwen2.5-Omni задаёт новый стандарт для ИИ-ассистентов будущего, сочетая скорость, точность и человеко-подобное взаимодействие. Исследователи и разработчики уже сейчас могут интегрировать модель в свои продукты, используя публичные веса.

## **Краткое содержание**

### Общее представление

Qwen2.5-Omni представляет собой комплексную сквозную мультимодальную модель, способную одновременно обрабатывать входные данные различных форматов (текст, изображения, аудио, видео) и генерировать как текстовые, так и речевые ответы в режиме реального времени. Модель реализует несколько инновационных архитектурных решений, обеспечивающих эффективную синхронизацию и обработку разнородных данных.

### Архитектурные инновации

#### Блочная обработка мультимодальных данных

Для обеспечения потоковой обработки мультимодальной информации в Qwen2.5-Omni применяется метод блочной обработки как для аудио, так и для визуальных кодировщиков. Эта стратегия реализует эффективное разделение:
- Восприятие мультимодальных данных поручается специализированным кодировщикам;
- Моделирование длинных последовательностей возлагается на основную языковую модель;
- Объединение модальностей достигается через механизм общего внимания.

#### TMRoPE: синхронизация временных меток

Для решения проблемы синхронизации временных меток видео и аудио, разработчики Qwen2.5-Omni предложили инновационный метод позиционного кодирования — Time-Aligned Multimodal RoPE (TMRoPE). Особенность данного метода заключается в последовательном чередующемся размещении аудио и видео данных, что обеспечивает точную временную привязку между модальностями.

#### Архитектура "Thinker-Talker" (Мыслитель-Говорящий)

Для одновременной генерации текста и речи без взаимных помех между модальностями в Qwen2.5-Omni реализована двухкомпонентная архитектура:

1. **Мыслитель (Thinker)** — функционирует как основная языковая модель, отвечающая за генерацию текстового содержания
2. **Говорящий (Talker)** — представляет собой двухдорожечную авторегрессивную модель, напрямую использующую скрытые представления из Мыслителя для генерации аудиотокенов

Обе компоненты интегрированы в единую сквозную структуру, что позволяет проводить как обучение, так и вывод данных целостным образом.

#### Потоковое декодирование аудио

Для снижения начальной задержки при декодировании аудиотокенов в Qwen2.5-Omni применяется скользящее окно DiT (Diffusion Transformer) с ограниченным полем восприятия, что критически важно для обеспечения реактивности в голосовом взаимодействии.

### Производительность и сравнительный анализ

Qwen2.5-Omni демонстрирует впечатляющие результаты в сравнительных тестах:

- Сопоставима с аналогичной по размеру моделью Qwen2.5-VL в визуально-текстовых задачах;
- Превосходит специализированную модель Qwen2-Audio в задачах обработки аудио;
- Показывает высокие результаты в комплексных мультимодальных тестах Omni-Bench;
- Производительность при обработке голосовых команд сравнима с текстовым вводом в таких бенчмарках как MMLU и GSM8K;
- Потоковый генератор речи превосходит большинство существующих решений по стабильности и естественности звучания.

### Значимость и потенциал

Qwen2.5-Omni представляет собой значительный шаг к созданию систем искусственного общего интеллекта (AGI), объединяя:
- Комплексную мультимодальную обработку данных;
- Низкую латентность взаимодействия;
- Человекоподобные способности общения;
- Единую интегрированную архитектуру для всех модальностей.

![Обзор мультимодальных возможностей Qwen2.5-Omni](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_1.png)
> Рисунок 1: Qwen2.5-Omni — это унифицированная сквозная модель, которая может обрабатывать различные модальности, такие как текст, аудио, изображения и видео, а также генерировать текстовые или голосовые ответы в реальном времени. Благодаря этим функциям Qwen2.5-Omni поддерживает множество задач, включая, помимо прочего, голосовое общение, видеообщение и видеорассуждение.

## **Введение**

Человеческое восприятие — это сложный многоуровневый процесс. В повседневной жизни мы одновременно воспринимаем многообразную визуальную и слуховую информацию, мгновенно обрабатываем её в мозге и формируем ответную реакцию через речь, письмо или использование инструментов. Этот естественный механизм взаимодействия с миром долгое время оставался недостижимым идеалом для систем искусственного интеллекта.

В последние годы область ИИ совершила значительный прорыв, во многом благодаря стремительному развитию больших языковых моделей (LLM). Эти системы, обучаемые на беспрецедентных объёмах текстовых данных, продемонстрировали впечатляющую способность к решению сложных задач и быстрому обучению. Параллельно развивались и специализированные модели "язык-аудио-язык" (LALM) и "язык-зрение-язык" (LVLM), расширяющие возможности ИИ в области слухового и визуального восприятия.

Однако эффективное сквозное объединение этих разнородных модальностей, использование их полного потенциала и обеспечение естественного человекоподобного взаимодействия через текстовые и голосовые потоки остаётся серьёзным вызовом для современной науки. Разработка по-настоящему универсальной омнимодальной модели требует решения целого комплекса проблем:

1. Создание единого системного подхода к совместному обучению различным модальностям (текст, изображения, видео, аудио);
2. Обеспечение точной временной синхронизации аудио- и видеосигналов;
3. Устранение потенциальных помех между выходными данными разных модальностей;
4. Разработка архитектуры, позволяющей в реальном времени понимать мультимодальную информацию и генерировать потоковые ответы.

В данном брифе мы разберем Qwen2.5-Omni — революционную унифицированную модель, способную одновременно обрабатывать несколько модальностей и генерировать как текстовые, так и естественные речевые ответы в потоковом режиме. Для преодоления вышеуказанных препятствий авторами были разработаны инновационные решения:

1. **TMRoPE (Time-aligned Multimodal RoPE)** — принципиально новый метод позиционного встраивания, который явно включает временную информацию для синхронизации аудио- и видеоданных. Метод размещает аудио и видеокадры в чередующейся структуре, чтобы представить видеопоследовательность в чётком временном порядке.

2. **Архитектура "Thinker-Talker"** — биомиметический подход, вдохновлённый функционированием человеческого мозга. В данной архитектуре "Мыслитель" отвечает за генерацию текста, а "Говорящий" фокусируется на создании потоковых речевых токенов, получая высокоуровневые представления непосредственно от "Мыслителя". Это решение позволяет обеспечить естественную координацию разнородных выходных сигналов.

3. **Потоковая блочная обработка** — модификация всех мультимодальных кодеров для обеспечения понимания сигналов в реальном времени и упрощения предварительного заполнения.

4. **Двухдорожечная авторегрессивная модель** — для потоковой генерации речи, которая преобразует речевые токены в звуковые волны с минимальной начальной задержкой.

![Figure_2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_2.png)
*Рисунок 2: Обзор Qwen2.5-Omni. Qwen2.5-Omni использует архитектуру «Thinker-Talker». Мыслитель отвечает за генерацию текста, в то время как говорящий фокусируется на генерации потоковых речевых маркеров, получая высокоуровневые представления непосредственно от мыслителя.*

При сравнительном тестировании Qwen2.5-Omni демонстрирует выдающиеся результаты. Она сопоставима с Qwen2.5-VL в обработке визуальной информации и значительно превосходит Qwen2-Audio в работе со звуком. В мультимодальных бенчмарках, таких как OmniBench и AV-Odyssey Bench, модель показывает высочайшие показатели производительности. Особенно примечателен тот факт, что Qwen2.5-Omni обрабатывает голосовые команды на уровне, сопоставимом с текстовым вводом, что продемонстрировано в таких тестах, как MMLU и GSM8K.

В области генерации речи модель достигает впечатляющих результатов с показателями Word Error Rate (WER) всего 1,42%, 2,33% и 6,54% в тестовых наборах test-zh, test-en и test-hard seed-tts-eval соответственно, что превосходит достижения таких известных систем, как MaskGCT и CosyVoice 2.

Таким образом, Qwen2.5-Omni представляет собой значительный шаг на пути к созданию истинного AGI (искусственного общего интеллекта), объединяя мультимодальность, низкую задержку и человекоподобное взаимодействие в единой модели. Эта разработка открывает новые горизонты для применения ИИ в различных областях, от повседневной коммуникации до сложных профессиональных задач, требующих интеграции разнородных типов данных и естественного диалога.

Я выступаю в роли эксперта в области мультимодальных языковых моделей и нейросетевых архитектур. Давайте обогатим ваш технический обзор дополнительной информацией о Qwen2.5-Omni.

Для начала, мне необходимо найти более подробную информацию об архитектуре Qwen2.5-Omni и использованных в ней технологиях.

Теперь мне нужно поискать более подробную информацию о TMRoPE и системе восприятия в Qwen2.5-Omni.

Теперь давайте поищем еще информацию о методах восприятия и обработки различных модальностей в Qwen2.5-Omni.

На основе полученной информации, я улучшу и обогащу ваш текст о Qwen2.5-Omni, делая его более детальным и академичным.

## **1. Архитектура**

### **1.1 Обзор**

Qwen2.5-Omni имплементирует инновационную архитектуру типа «Thinker-Talker» (Мыслитель-Говорящий), представляющую собой передовой подход к построению мультимодальных моделей. Thinker функционирует как когнитивный центр системы, отвечающий за обработку и интерпретацию входных данных различных модальностей (текст, аудио, изображения и видео), формирование высокоуровневых репрезентаций и генерацию соответствующего текстового контента. В свою очередь, Talker оперирует как артикуляционная составляющая системы, принимающая потоковым образом высокоуровневые репрезентации и текст, продуцируемые Thinker, и генерирующая дискретные речевые токены.

Архитектурно Thinker реализован как декодер модели Transformer, дополненный специализированными энкодерами для извлечения информации из аудио и визуальных данных. Talker спроектирован как двухдорожечная авторегрессивная архитектура декодера Transformer, основанная на принципах, изложенных в работе Mini-Omni (Xie & Wu, 2024). Существенной особенностью данной архитектуры является то, что на этапах обучения и инференса Talker непосредственно получает многомерные репрезентации от Thinker и имеет доступ ко всей исторической контекстной информации, обработанной компонентом Thinker. Благодаря такой интеграции вся архитектурная система функционирует как единая унифицированная модель, обеспечивающая эффективное сквозное обучение и инференс.

В последующих разделах мы детально рассмотрим механизмы восприятия различных сигналов в Qwen2.5-Omni и проанализируем инновационный алгоритм позиционного кодирования TMRoPE (Time-aligned Multimodal RoPE). Далее будут освещены технические аспекты генерации текста и речи. В заключении мы представим обзор усовершенствований, внедренных в модули понимания и генерации, которые обеспечивают эффективное потоковое рассуждение.

### **Архитектура «Мыслитель-Говорящий» (псевдокод)**

```python
# Стандартные библиотеки
import torch
import torch.nn as nn

# Константы
DEFAULT_NUM_LAYERS = 12
DEFAULT_HIDDEN_SIZE = 768
DEFAULT_NUM_HEADS = 12


class ThinkerTalker(nn.Module):
    """
    Description:
    ---------------
        Модель архитектуры "Мыслитель-Говорящий" (ThinkerTalker), состоящая из
        двух основных компонентов: модуля Thinker для обработки входных данных и
        модуля Talker для генерации аудиотокенов.

    Args:
    ---------------
        config: Конфигурация модели, содержащая параметры:
            - num_layers: Количество слоев в модуле Thinker
            - hidden_size: Размерность скрытого состояния
            - num_heads: Количество голов внимания
            - audio_vocab_size: Размер словаря аудиотокенов

    Returns:
    ---------------
        Экземпляр модели ThinkerTalker

    Raises:
    ---------------
        ValueError: Если параметры конфигурации некорректны
        AttributeError: Если в конфигурации отсутствуют необходимые атрибуты

    Examples:
    ---------------
        >>> from dataclasses import dataclass
        >>> @dataclass
        >>> class ModelConfig:
        ...     num_layers: int = 12
        ...     hidden_size: int = 768
        ...     num_heads: int = 12
        ...     audio_vocab_size: int = 10000
        >>> config = ModelConfig()
        >>> model = ThinkerTalker(config)
    """
    def __init__(self, config) -> None:
        super().__init__()
        
        # Модуль Thinker (основан на TransformerDecoder)
        self.thinker = TransformerDecoder(
            num_layers=config.num_layers,
            hidden_size=config.hidden_size,
            num_heads=config.num_heads
        )
        
        # Модуль Talker (двухслойный трансформер-декодер)
        self.talker = nn.TransformerDecoder(
            decoder_layer=nn.TransformerDecoderLayer(
                d_model=config.hidden_size,
                nhead=config.num_heads
            ),
            num_layers=2  # Фиксированное значение для Talker
        )
        
        # Проекционный слой для преобразования в аудиотокены
        self.audio_proj = nn.Linear(
            config.hidden_size, 
            config.audio_vocab_size
        )

    def forward(
        self, 
        text_input: torch.Tensor, 
        audio_input: torch.Tensor
    ) -> torch.Tensor:
        """
        Description:
        ---------------
            Прямой проход через модель ThinkerTalker. Сначала модуль Thinker
            обрабатывает текстовый ввод, затем модуль Talker генерирует 
            аудиотокены в авторегрессивном режиме.

        Args:
        ---------------
            text_input: Тензор с текстовыми входными данными 
                        [batch, seq_len, dim]
            audio_input: Тензор с аудио входными данными
                        [batch, seq_len, dim]

        Returns:
        ---------------
            Тензор сгенерированных аудиотокенов [batch, seq_len, audio_vocab_size]

        Raises:
        ---------------
            RuntimeError: При несоответствии размерностей входных тензоров

        Examples:
        ---------------
            >>> text_input = torch.randn(2, 10, 768)
            >>> audio_input = torch.randn(2, 5, 768)
            >>> output = model(text_input, audio_input)
            >>> output.shape
            torch.Size([2, 5, 10000])
        """
        # Обработка входных данных через модуль Thinker
        thinker_output = self.thinker(text_input)  # [batch, seq_len, dim]
        
        # Авторегрессивная генерация аудиотокенов через модуль Talker
        audio_tokens = []
        
        # Итерация по шагам генерации (потоковый режим с кешированием)
        for i in range(audio_input.size(1)):
            # Авторегрессивная генерация: p(a_t|a_{<t}, h_{Thinker})
            audio_out = self.talker(
                audio_input[:, :i+1],  # Используем только предыдущие токены
                memory=thinker_output  # Контекст от модуля Thinker
            )
            
            # Получаем следующий токен из выходного слоя
            next_token = self.audio_proj(audio_out[:, -1:])
            
            # Добавляем в список результатов
            audio_tokens.append(next_token)
        
        # Объединяем все сгенерированные токены в один тензор
        return torch.cat(audio_tokens, dim=1)
```

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;"> <p style="margin: 0; font-weight: bold; color: #2c3e50;">First Checkpoint:</p> <p style="margin: 8px 0 0 0; color: #2c3e50;">Qwen2.5-Omni использует инновационную архитектуру «Thinker-Talker»: Thinker обрабатывает мультимодальные данные (текст, аудио, изображения, видео) и формирует высокоуровневые репрезентации, а Talker преобразует их в речь. Система работает как единая модель, обеспечивая эффективное сквозное обучение и генерацию.</p> </div>

### **1.2 Восприятие**

#### **Текст, аудио, изображения и видео**

Компонент Thinker обрабатывает мультимодальные входные данные (текст, аудио, изображения и видео), трансформируя их в векторные репрезентации для последующей обработки. Для сегментации текста используется специализированный токенизатор Qwen (Yang et al., 2024a, arXiv:2407.10671), реализующий кодирование пар байтов на байтовом уровне с лексиконом, содержащим 151 643 стандартных токенов.

Для обработки аудиовходов и аудиодорожек видео Qwen2.5-Omni применяет передискретизацию до частоты 16 кГц с последующим преобразованием исходных волновых форм в Мел-спектрограммы, характеризующиеся 128 каналами, размером окна 25 мс и шагом 10 мс. Для эффективной потоковой обработки аудио и видеоданных модель использует блочно-ориентированный подход (block-wise processing), который позволяет секвенциально обрабатывать фрагменты данных, не требуя полного входного сигнала. Это особенно важно для работы с продолжительными аудио и видеоматериалами в режиме реального времени.

Аудиокодер основан на архитектуре Qwen2-Audio (Chu et al., 2024b), где каждый аудиокадр представляет примерно 40-миллисекундный сегмент исходного аудиосигнала. Для визуальной модальности применяется специализированный визуальный кодер, основанный на архитектуре Vision Transformer (ViT), содержащий около 675 миллионов параметров. Данный кодер обеспечивает эффективную обработку входных изображений и видеоданных благодаря применению инновационных техник, включая оптимизированную оконную систему внимания (Window Attention), которая значительно улучшает вычислительную эффективность при работе с визуальными данными высокого разрешения.

Визуальный кодер обучен на комбинированном датасете изображений и видео с использованием гибридной схемы обучения, что обеспечивает превосходную производительность как при анализе статичных изображений, так и при обработке динамического видеоконтента. Подобно Qwen2.5-VL, визуальный кодер в Qwen2.5-Omni поддерживает динамическое разрешение, что позволяет эффективно обрабатывать изображения различных размеров без стандартной нормализации координат, сохраняя естественное масштабирование визуальных объектов.

Для максимального сохранения видеоинформации и синхронизации с частотой дискретизации аудио, система использует динамическую частоту кадров. Синхронизация между аудио и видео достигается благодаря последовательной организации входных данных в перемежающемся порядке и применению инновационного подхода Time-aligned Multimodal RoPE (TMRoPE). Это позволяет модели точно интерпретировать временные зависимости между тем, что она "видит" и "слышит". Для обеспечения единообразия каждое статичное изображение обрабатывается как последовательность из двух идентичных кадров.

<details> 
    <summary><em><strong>Мел-спектрограмма</strong></em></summary>

---

**Мел-спектрограмма** — это визуальное представление звукового сигнала, которое отражает его частотные характеристики с учетом **мел-шкалы** (шкалы восприятия высоты звука человеком).  

### **Основные понятия**:

1. **Спектрограмма** – это график, показывающий, как частотный состав звука меняется во времени (ось X – время, ось Y – частота, цвет – амплитуда).  
2. **Мел-шкала** – психоакустическая шкала, которая приближает восприятие частот человеческим ухом (люди лучше различают низкие частоты, чем высокие).  
   - Например: разница между 100 Гц и 200 Гц воспринимается значительной, а между 8000 Гц и 8100 Гц – почти незаметной.  

### **Как получают мел-спектрограмму?**

1. **Разбиение сигнала на фреймы** (короткие отрезки);  
2. **Применение БПФ (FFT)** для получения спектра каждого фрейма;
3. **Фильтры мел-банка** – набор треугольных фильтров, распределенных по мел-шкале (больше фильтров на низких частотах, меньше – на высоких);
4. **Логарифмирование энергии** (т.к. человек воспринимает громкость логарифмически).  

### **Формула перевода Герц в меллы**:

$$
m = 2595 \cdot \log_{10}\left(1 + \frac{f}{700}\right)
$$

где:  
- $( f )$ – частота в Герцах,  
- $( m )$ – частота в меллах.  

### **Применение**:
- **Распознавание речи** (ASR, например, в Siri, Google Assistant).  
- **Анализ музыки** (жанровая классификация, выделение тональности).  
- **Генерация звука** (нейросети типа WaveNet, Tacotron).  

### **Пример визуализации**:

![Мел-спектрограмма](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_3.jpeg)

(По горизонтали – время, по вертикали – мел-частоты, цвет – мощность сигнала).  

Если у вас есть аудиофайл, можно построить мел-спектрограмму в Python с помощью `librosa`:  

```python
import librosa
import librosa.display
import matplotlib.pyplot as plt

# Загружаем аудиофайл "audio.wav" с помощью функции librosa.load
# y — это одномерный массив, содержащий аудиосигнал (амплитуды звука)
# sr — частота дискретизации аудиофайла (количество выборок в секунду)
y, sr = librosa.load("audio.wav")

# Вычисляем мел-спектрограмму (Mel Spectrogram) из аудиосигнала
# librosa.feature.melspectrogram преобразует сигнал в представление частотного спектра,
# используя шкалу мелов, которая более адаптирована к восприятию человека.
# Параметры:
#   y — аудиосигнал
#   sr — частота дискретизации
S = librosa.feature.melspectrogram(y=y, sr=sr)

# Преобразуем мощность спектрограммы в децибелы (dB), чтобы улучшить визуализацию
# librosa.power_to_db выполняет логарифмическое преобразование,
# где ref=np.max означает, что максимальное значение мощности используется как точка отсчета.
S_dB = librosa.power_to_db(S, ref=np.max)

# Создаем новое окно для графика размером 10x4 дюймов
plt.figure(figsize=(10, 4))

# Отображаем мел-спектрограмму с помощью функции librosa.display.specshow
# Параметры:
#   S_dB — данные спектрограммы в децибелах
#   sr — частота дискретизации
#   x_axis="time" — ось X будет отображать время
#   y_axis="mel" — ось Y будет использовать шкалу мелов для частот
librosa.display.specshow(S_dB, sr=sr, x_axis="time", y_axis="mel")

# Добавляем цветовую шкалу (colorbar) справа от графика
# format="%+2.0f dB" указывает формат отображения значений в децибелах
plt.colorbar(format="%+2.0f dB")

plt.title("Мел-спектрограмма")
plt.show()
```  

Это мощный инструмент для анализа звука, особенно там, где важно учитывать человеческое восприятие! 🎵

</details>

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;"> <p style="margin: 0; font-weight: bold; color: #2c3e50;">Second Checkpoint:</p> <p style="margin: 8px 0 0 0; color: #2c3e50;">Thinker обрабатывает мультимодальные данные с помощью специализированных энкодеров: текстовый токенизатор Qwen, аудиокодер (Mel-спектрограммы + блочная обработка), ViT-based визуальный кодер с динамическим разрешением. Синхронизация аудио и видео обеспечивается алгоритмом TMRoPE, а статичные изображения адаптируются как двукадровая последовательность.</p> </div>

#### **Видео и TMRoPE**

Авторы предлагают алгоритм временного чередования аудио и видео, а также новый метод позиционного кодирования. Как показано на рисунке 3, TMRoPE кодирует трехмерную информацию о положении мультимодального ввода, а именно мультимодальное встраивание вращательного положения (M-RoPE) с абсолютным временным положением (Bai et al., 2023b). Это достигается путем разложения исходного повернутого вложения на три компонента: время, высоту и ширину. Для ввода текста эти части используют один и тот же идентификатор местоположения, что делает M-RoPE функционально эквивалентным 1D RoPE. Аналогично для аудиовхода также используется тот же идентификатор позиции и вводится кодирование абсолютной временной позиции, при этом каждый идентификатор времени соответствует 40 мс.

При обработке изображения временной идентификатор каждого визуального маркера остается постоянным, тогда как компонентам высоты и ширины присваиваются разные идентификаторы в зависимости от положения маркера на изображении. Если на вход подается видео со звуком, звук по-прежнему кодируется с тем же идентификатором позиции 40 мс на кадр, тогда как видео обрабатывается как серия изображений с увеличивающимися временными идентификаторами для каждого кадра, а части высоты и ширины следуют тому же шаблону назначения идентификаторов, что и изображения. Поскольку частота кадров видео не фиксирована, мы динамически корректируем временной идентификатор между кадрами в соответствии с фактическим временем каждого кадра, чтобы гарантировать, что один временной идентификатор соответствует 40 мс. В случае, когда входные данные модели содержат несколько модальностей, номер позиции каждой модальности инициализируется путем увеличения максимального идентификатора позиции предыдущей модальности на единицу. TMRoPE улучшает моделирование информации о местоположении и максимально увеличивает интеграцию различных модальностей, позволяя Qwen2.5-Omni понимать и анализировать информацию из нескольких модальностей одновременно.

![Figure_4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_4.png)
*Рисунок 3: Иллюстрация согласованного по времени мультимодального RoPE (TMRoPE).*

После включения информации о положении в каждую модальность мы располагаем представления по порядку. Чтобы модель могла одновременно получать как визуальную, так и слуховую информацию, как показано на рисунке 3, мы применяем специальную конструкцию для видео со звуком, называемую методом временного чередования , который делит представление видео со звуком на блоки каждые 2 секунды в соответствии с фактическим временем. Затем в течение 2 секунд визуальное представление помещается спереди, а звуковое представление — сзади, чередуя представление видео со звуком.

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;"> <p style="margin: 0; font-weight: bold; color: #2c3e50;">Third Checkpoint:</p> <p style="margin: 8px 0 0 0; color: #2c3e50;">TMRoPE кодирует трехмерную позиционную информацию мультимодальных данных, применяя различные стратегии кодирования для текста, аудио, изображений и видео. Метод временного чередования интегрирует видео и аудио в двухсекундные блоки, максимизируя взаимодействие между разными модальностями.</p> </div>

<details> 
    <summary><em><strong>Ротационное позиционное кодирование (RoPE)</strong></em></summary>

---

## Ротационное позиционное кодирование (RoPE)

### 1. Основная идея и постановка задачи

RoPE модифицирует векторы запроса $q$ и ключа $k$ так, чтобы их скалярное произведение $\langle q_m, k_n \rangle$ (где $m, n$ - позиции) неявно содержало информацию об их относительном положении $m-n$. 

Математически, мы ищем функцию преобразования $f(\mathbf{x}, p)$, применяемую к вектору $\mathbf{x}$ (будь то $q$ или $k$) с учётом его позиции $p$, такую что:

$$ \langle f(\mathbf{q}_m, m), f(\mathbf{k}_n, n) \rangle = g(\mathbf{q}_m, \mathbf{k}_n, m-n) $$

где $g$ - некоторая функция, зависящая только от исходных векторов и их относительного положения.

### 2. Математическое решение через вращение

#### 2.1. Базовый случай в 2D-пространстве

Для 2-мерных векторов $\mathbf{q}, \mathbf{k} \in \mathbb{R}^2$ RoPE использует матрицу вращения:

$$ \mathbf{R}_{\theta, p} = \begin{pmatrix} \cos p\theta & -\sin p\theta \\ \sin p\theta & \cos p\theta \end{pmatrix} $$

Преобразования векторов:
$$ \mathbf{q}'_m = f(\mathbf{q}_m, m) = \mathbf{R}_{\theta, m} \mathbf{q}_m $$
$$ \mathbf{k}'_n = f(\mathbf{k}_n, n) = \mathbf{R}_{\theta, n} \mathbf{k}_n $$

#### 2.2. Доказательство свойства относительности

Скалярное произведение преобразованных векторов:
$$ (\mathbf{q}'_m)^\top \mathbf{k}'_n = (\mathbf{R}_{\theta, m} \mathbf{q}_m)^\top (\mathbf{R}_{\theta, n} \mathbf{k}_n) = \mathbf{q}_m^\top \mathbf{R}_{\theta, m}^\top \mathbf{R}_{\theta, n} \mathbf{k}_n $$

Поскольку $\mathbf{R}_{\theta, m}^\top = \mathbf{R}_{\theta, -m}$ и $\mathbf{R}_{\theta, -m} \mathbf{R}_{\theta, n} = \mathbf{R}_{\theta, n-m}$, получаем:

$$ (\mathbf{q}'_m)^\top \mathbf{k}'_n = \mathbf{q}_m^\top \mathbf{R}_{\theta, n-m} \mathbf{k}_n $$

Таким образом, скалярное произведение зависит только от исходных векторов и относительной позиции $n-m$.

### 3. Реализация в многомерном пространстве

#### 3.1. Обобщение на произвольную размерность

Для вектора $\mathbf{x} \in \mathbb{R}^d$ (где $d$ обычно четное):

1. Вектор разбивается на $d/2$ пар компонент
2. К каждой паре применяется 2D-вращение с уникальной частотой $\theta_i$

Формально это эквивалентно умножению на блочно-диагональную матрицу:

$$ \mathbf{R}_{\Theta, m} = \bigoplus_{i=1}^{d/2} \mathbf{R}_{\theta_i, m} $$

где $\mathbf{R}_{\theta_i, m} = \begin{pmatrix} \cos m\theta_i & -\sin m\theta_i \\ \sin m\theta_i & \cos m\theta_i \end{pmatrix}$.

#### 3.2. Выбор частот

Частоты образуют геометрическую прогрессию:

$$ \theta_i = \text{base}^{-2i/d} $$

где $\text{base}$ - гиперпараметр (обычно 10000). Такой подход позволяет:
- Первые пары компонент кодируют грубое, крупномасштабное положение (низкие частоты)
- Последующие пары кодируют более тонкое, локальное положение (высокие частоты)

#### 3.3. Эффективная реализация

Вместо явного умножения на матрицу используются векторные операции:

$$ \mathbf{x}' = \mathbf{x} \odot \mathbf{c}_m + \text{rotate\_half}(\mathbf{x}) \odot \mathbf{s}_m $$

где:
- $\mathbf{c}_m$ - вектор косинусов $(\cos m\theta_1, \cos m\theta_1, \cos m\theta_2, \cos m\theta_2, \dots)$
- $\mathbf{s}_m$ - вектор синусов $(\sin m\theta_1, \sin m\theta_1, \sin m\theta_2, \sin m\theta_2, \dots)$
- $\text{rotate\_half}(\mathbf{x})$ - векторная операция, меняющая местами пары элементов с изменением знака: $(-x_2, x_1, -x_4, x_3, \dots)$

### 4. Преимущества RoPE

1. **Экстраполяция длины:** Естественно обобщается на последовательности, превышающие длину обучающих данных, благодаря зависимости только от относительных позиций

2. **Вычислительная эффективность:** Не требует хранения таблиц позиционных эмбеддингов, использует векторные операции, хорошо оптимизированные для GPU/TPU

3. **Интерпретируемость:** Напрямую реализует идею относительного позиционного кодирования через интуитивно понятное вращение векторов

4. **Стабильность обучения:** Сохраняет норму векторов запросов и ключей

</details>

---

## Расширение RoPE до мультимодального случая: TMRoPE

Как эксперт в области глубокого обучения и мультимодальных трансформеров, представляю анализ расширения RoPE для обработки мультимодальной информации.

### 1. Основная идея TMRoPE

TMRoPE (Temporal Multimodal Rotary Position Embedding) расширяет концепцию ротационного позиционного кодирования для синхронизации различных модальностей (аудио, видео, текст) с разными частотами дискретизации в едином пространстве представлений.

### 2. Математическая формализация

#### 2.1. Синхронизация временных шкал

Для разных модальностей с различными частотами дискретизации применяется нормализация временных меток:

$$ t_{video} = t_{audio} \cdot \frac{f_{video}}{f_{audio}} $$

где $f$ — частота дискретизации соответствующей модальности.

#### 2.2. Модификация матрицы вращения

В TMRoPE матрица вращения модифицируется с учетом модальности:

$$ \mathbf{R}_{\theta, p, m} = \begin{pmatrix} 
\cos(p\theta \cdot s_m) & -\sin(p\theta \cdot s_m) \\ 
\sin(p\theta \cdot s_m) & \cos(p\theta \cdot s_m) 
\end{pmatrix} $$

где $s_m$ — масштабирующий коэффициент для модальности $m$.

### 3. Интеграция с блочной обработкой

#### 3.1. Обработка блоков в мультимодальном контексте

Для каждого блока $B_k$ модальности $m$:

$$ z_{k,m} = f_{enc,m}(B_{k,m}) $$

$$ z'_{k,m} = \mathbf{R}_{\Theta, k, m} z_{k,m} $$

#### 3.2. Контекстная синхронизация

Контекстное окно включает блоки разных модальностей, синхронизированные по времени:

$$ C_k = \{z'_{i,m} | t_{start}(i,m) \leq t_k \leq t_{end}(i,m), \forall m \} $$

### 4. Механизм кросс-модального внимания

В TMRoPE позиционное кодирование обеспечивает корректное внимание между различными модальностями:

$$ \text{Attention}(Q_{m_1}, K_{m_2}, V_{m_2}) = \text{softmax}\left(\frac{Q_{m_1}K_{m_2}^T}{\sqrt{d_k}}\right)V_{m_2} $$

Благодаря TMRoPE скалярное произведение $Q_{m_1}K_{m_2}^T$ корректно отражает временные соотношения между токенами разных модальностей.

### 5. Адаптивные частоты для разных модальностей

Частоты $\theta_i$ для разных модальностей выбираются с учетом их характеристик:

$$ \theta_{i,m} = \text{base}_m^{-2i/d} $$

где $\text{base}_m$ — базовый параметр для модальности $m$.

### 6. Преимущества TMRoPE

1. **Унифицированное временное представление:** Обеспечивает единую временную шкалу для всех модальностей

2. **Масштабируемость:** Легко адаптируется к различным частотам дискретизации и характеристикам модальностей

3. **Эффективность в потоковой обработке:** Позволяет обрабатывать непрерывные потоки данных с минимальной задержкой

4. **Синхронизация с блочной обработкой:** Обеспечивает плавное взаимодействие с блочным механизмом обработки, сохраняя темпоральные зависимости между модальностями

5. **Аналитическая дифференцируемость:** Поддерживает сквозное обучение с распространением градиентов через границы блоков и между различными модальностями


### **Кодирование позиции TMRoPE (псевдокод):**

```python
# Стандартные библиотеки
import torch
import torch.nn as nn

# Константы для RoPE
BASE_FREQ = 10000  # Базовая частота для вычисления позиционного кодирования
VIDEO_FPS = 25     # Частота кадров видео (кадров в секунду)
FRAME_TIME = 0.04  # Время одного кадра в секундах (1/25 = 0.04)


class TMRoPE(nn.Module):
    """
    Description:
    ---------------
        Реализация временного RoPE (Rotary Position Embedding) для 
        мультимодальных данных. Метод адаптирует стандартный RoPE для работы
        с различными типами модальностей (текст, аудио, изображения, видео),
        применяя специфичные временные масштабы для каждого типа.

    Args:
    ---------------
        dim: Размерность эмбеддинга
        max_seq_len: Максимальная длина последовательности

    Returns:
    ---------------
        Модуль PyTorch для применения ротационной позиционной кодировки
        
    Raises:
    ---------------
        ValueError: Если указана некорректная размерность эмбеддинга
        TypeError: Если входные тензоры имеют неправильный тип

    Examples:
    ---------------
        >>> model = TMRoPE(dim=64)
        >>> x = torch.randn(2, 10, 64)
        >>> pos_ids = torch.arange(10).expand(2, 10)
        >>> output = model(x, pos_ids, modality_type=3)
    """
    def __init__(self, dim: int, max_seq_len: int = 32768) -> None:
        super().__init__()
        self.dim = dim
        
        # Расчет базовых частот (оригинальный метод RoPE)
        # Формула: inv_freq_i = 1 / (BASE_FREQ^(i / dim)), где i - четные индексы
        inv_freq = 1.0 / (BASE_FREQ ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer("inv_freq", inv_freq)
        
        # Параметр временного масштабирования (каждые 40мс = 1 позиция)
        # Для видео с 25 кадрами в секунду, время кадра = 0.04с
        self.time_scale = 1 / FRAME_TIME  # 25fps для видео

    def forward(
        self, 
        x: torch.Tensor, 
        pos_ids: torch.Tensor, 
        modality_type: int
    ) -> torch.Tensor:
        """
        Description:
        ---------------
            Применяет ротационную позиционную кодировку к входной
            последовательности с учетом типа модальности данных.

        Args:
        ---------------
            x: Входной тензор формы [batch, seq_len, dim]
            pos_ids: Позиционные ID формы [batch, seq_len]
            modality_type: Тип модальности:
                0 - текст
                1 - аудио
                2 - изображение
                3 - видео

        Returns:
        ---------------
            Тензор с примененной ротационной позиционной кодировкой
            
        Raises:
        ---------------
            ValueError: Если указан неподдерживаемый тип модальности
            RuntimeError: При несоответствии размерностей входных тензоров

        Examples:
        ---------------
            >>> model = TMRoPE(64)
            >>> x = torch.randn(2, 10, 64)
            >>> pos_ids = torch.arange(10).expand(2, 10)
            >>> output = model(x, pos_ids, modality_type=3)
        """
        # Получаем длину последовательности из входного тензора
        seq_len = x.size(1)
        
        # Коррекция временной размерности в зависимости от типа модальности
        if modality_type == 3:  # Видео
            # Для видео используем полный временной масштаб
            pos_ids = pos_ids * self.time_scale
        elif modality_type == 1:  # Аудио
            # Для аудио используем половину временного масштаба видео
            pos_ids = pos_ids * self.time_scale / 2
            
        # Расчет углов поворота по формуле: θᵢ = pos · inv_freqᵢ
        # Используем эйнштейновскую нотацию для матричного умножения
        sinusoid = torch.einsum("i,j->ij", pos_ids.float(), self.inv_freq)
        
        # Вычисляем синусы и косинусы для углов поворота
        sin, cos = torch.sin(sinusoid), torch.cos(sinusoid) 
        
        # Применение ротационной позиционной кодировки:
        # x' = x ⊙ cosθ + xᵣₒₜ ⊙ sinθ
        # где xᵣₒₜ - это x с переупорядоченными и инвертированными элементами
        x_rot = torch.cat([-x[..., 1::2], x[..., ::2]], dim=-1)
        
        # Финальное применение ротации через умножение и сложение
        x = x * cos.unsqueeze(-1) + x_rot * sin.unsqueeze(-1)
        return x
```

## **2. Генерация**

### **2.1 Текст**  

Текст генерируется непосредственно модулем Thinker. Логика генерации текста в целом аналогична подходу, используемому в распространенных LLM (Large Language Models), а именно: автопрогрессивная выборка на основе вероятностного распределения словаря. Для повышения разнообразия в процессе генерации могут применяться такие техники, как штраф за повторение и top-p выборка.  

### **2.2 Речь**  

Модуль Speaker получает от Thinker высокоуровневые представления (эмбеддинги) и сэмплированные текстовые токены. Комбинация высокоразмерных представлений и дискретных токенов играет ключевую роль. Поскольку алгоритм работает в потоковом режиме, генерация речи должна предсказывать интонацию и эмоциональную окраску до полного формирования текста. Высокоуровневые представления от Thinker неявно передают эту информацию, делая потоковую генерацию более естественной. Кроме того, эмбеддинги Thinker в основном отражают семантическую, а не фонетическую схожесть. Поэтому даже фонетически разные слова могут иметь близкие высокоуровневые представления, что требует использования дискретных токенов для устранения неоднозначности.  

Авторы разработали эффективный речевой кодек под названием **qwen-tts-tokenizer**, который компактно кодирует ключевую речевую информацию и позволяет потоково декодировать аудиопоток с помощью каузального аудиодекодера. Получив данные, Speaker начинает автопрогрессивно генерировать аудио и текстовые токены. Генерация речи не требует жесткого выравнивания с текстом на уровне слов или временных меток, что значительно упрощает требования к обучающим данным и процессу вывода.

### **2.3 Потоковый дизайн**

В сценариях потокового аудио и видео взаимодействия **начальная задержка пакета** (initial packet latency) является ключевым показателем производительности системы. На нее влияют следующие факторы:

1) Задержка обработки многомодального ввода;  
2) Задержка между получением первого текстового токена и выводом первого речевого токена;  
3) Задержка преобразования первого сегмента речи в аудиосигнал;  
4) Внутренняя задержка архитектуры, зависящая от размера модели, объема вычислений (FLOPs) и других факторов.  

Далее в статье обсуждаются алгоритмические и архитектурные улучшения, направленные на сокращение задержки по этим четырем направлениям.  

**Поддержка предзаполнения (prefill)**

Чанковое предзаполнение (chunked prefill) — широко используемый механизм в современных системах вывода. Для его поддержки в многомодальном взаимодействии мы модифицировали аудио- и визуальные энкодеры, добавив блочное внимание (chunked attention) по временной оси. Аудиоэнкодер теперь обрабатывает данные блоками по 2 секунды вместо полного аудиофайла, а визуальный энкодер использует быстрое внимание (flash attention), объединяя соседние 2×2 токена в один через MLP-слой для эффективности. Размер патча установлен в 14, что позволяет агрегировать изображения разных разрешений в единую последовательность.  

**Потоковая генерация кодеком**

Для удобства потоковой передачи длинных аудиопоследовательностей мы предложили механизм **скользящего оконного блочного внимания** (sliding window chunked attention), ограничивающий контекст текущего токена. В основе лежит DiT-модель на базе **Flow-Matching**: входные коды преобразуются в мел-спектрограммы, которые затем восстанавливаются с помощью модифицированного BigVGAN.

![Figure_5](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_5.png)
*Рисунок 4: Подход блочной обработки, показывающий, как прошлые, текущие и будущие блоки управляются во время обработки последовательных данных.*

Как показано на рисунке 4, для генерации волновых форм из кодов мы группируем смежные коды в блоки и используем эти блоки в качестве масок внимания. Мы ограничиваем рецептивное поле DiT четырьмя блоками, включая взгляд на 2 блока назад и на 1 блок вперед. Во время декодирования мы используем Flow Matching для генерации Mel-спектрограмм по блокам, гарантируя, что каждый блок кода имеет доступ к необходимым контекстным блокам. Такой подход улучшает качество потокового вывода за счет сохранения контекстной информации. Мы также используем этот подход с фиксированным рецептивным полем BigVGAN для достижения потоковой генерации сигналов.

### **Генерация потокового звука DiT** (псевдокод)

```python
# Стандартные библиотеки
import torch
import torch.nn as nn
import torch.nn.functional as F

# Константы
DEFAULT_EMBED_DIM = 256
DEFAULT_NUM_HEADS = 8
DEFAULT_MLP_DIM = 1024
DEFAULT_WINDOW_SIZE = 4
LOOKBACK_SIZE = 2  # Размер окна для просмотра назад
LOOKAHEAD_SIZE = 1  # Размер окна для просмотра вперёд


class StreamingDiT(nn.Module):
    """
    Description:
    ---------------
        Модель StreamingDiT для генерации потокового звука с использованием
        оконного механизма внимания. Модель применяет скользящее окно внимания
        для обработки аудиопоследовательностей в потоковом режиме.

    Args:
    ---------------
        window_size: Общий размер окна внимания (lookback + lookahead + 1)
        embed_dim: Размерность эмбеддинга модели
        num_heads: Количество голов в механизме внимания
        mlp_dim: Размерность скрытого слоя в MLP

    Returns:
    ---------------
        Экземпляр модели StreamingDiT

    Raises:
    ---------------
        ValueError: Если указан некорректный размер окна (должен быть >= 3)

    Examples:
    ---------------
        >>> model = StreamingDiT(window_size=4)
        >>> x = torch.randn(2, 10, 256)
        >>> output = model(x)
        >>> output.shape
        torch.Size([2, 10, 256])
    """
    def __init__(
        self,
        window_size: int = DEFAULT_WINDOW_SIZE,
        embed_dim: int = DEFAULT_EMBED_DIM,
        num_heads: int = DEFAULT_NUM_HEADS,
        mlp_dim: int = DEFAULT_MLP_DIM
    ) -> None:
        super().__init__()
        
        if window_size < 3:
            raise ValueError("Размер окна должен быть не менее 3 (1 текущий + "
                            "минимум 1 назад + минимум 1 вперед)")
        
        self.window_size = window_size
        self.embed_dim = embed_dim
        
        # Механизм внимания со скользящим окном (Attention(Q,K,V)_window)
        self.attention = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            batch_first=True
        )
        
        # MLP блок для обработки после внимания
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, mlp_dim),
            nn.GELU(),
            nn.Linear(mlp_dim, embed_dim)
        )
        
        # Нормализации (добавлены явно вместо функционального вызова)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Description:
        ---------------
            Прямой проход через модель StreamingDiT с механизмом скользящего окна.
            Реализует окно с LOOKBACK_SIZE=2 (просмотр назад) и 
            LOOKAHEAD_SIZE=1 (просмотр вперед).

        Args:
        ---------------
            x: Входной тензор размера [batch, seq_len, dim]

        Returns:
        ---------------
            Выходной тензор той же размерности [batch, seq_len, dim]

        Raises:
        ---------------
            RuntimeError: При несовместимости размерностей тензоров
            ValueError: Если размерность входного тензора не соответствует
                        размерности эмбеддинга модели

        Examples:
        ---------------
            >>> model = StreamingDiT()
            >>> x = torch.randn(1, 8, 256)  # batch=1, seq_len=8, dim=256
            >>> output = model(x)
            >>> output.shape
            torch.Size([1, 8, 256])
        """
        # Проверяем соответствие размерностей
        batch_size, seq_len, dim = x.size()
        if dim != self.embed_dim:
            raise ValueError(
                f"Размерность входа {dim} не соответствует размерности "
                f"модели {self.embed_dim}"
            )
        
        outputs = []
        
        # Обрабатываем каждую позицию в последовательности
        for i in range(seq_len):
            # Определяем границы скользящего окна
            # LOOKBACK_SIZE=2 - количество токенов в прошлом
            # LOOKAHEAD_SIZE=1 - количество токенов в будущем
            start = max(0, i - LOOKBACK_SIZE)
            end = min(seq_len, i + LOOKAHEAD_SIZE + 1)
            
            # Выделяем окно для текущей позиции
            window = x[:, start:end]
            
            # Создаем причинную маску для механизма внимания
            # Маска позволяет видеть только предыдущие и текущую позиции
            window_size = end - start
            attn_mask = torch.triu(
                torch.ones(window_size, window_size), 
                diagonal=1
            ).bool()
            
            # Применяем механизм внимания с маской
            attn_out, _ = self.attention(
                query=x[:, i:i+1],                # Запрос - текущая позиция
                key=window,                       # Ключи - окно контекста
                value=window,                     # Значения - окно контекста
                attn_mask=attn_mask.to(x.device)  # Причинная маска
            )
            
            # Применяем первый остаточный слой: LayerNorm(x + Attention(x))
            norm_out = self.norm1(attn_out + x[:, i:i+1])
            
            # Применяем MLP слой
            mlp_out = self.mlp(norm_out)
            
            # Применяем второй остаточный слой: LayerNorm(norm_out + MLP(norm_out))
            final_out = self.norm2(norm_out + mlp_out)
            
            # Добавляем результат для текущей позиции
            outputs.append(final_out)
        
        # Объединяем результаты всех позиций
        return torch.cat(outputs, dim=1)
```

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;"> <p style="margin: 0; font-weight: bold; color: #2c3e50;">Fourth Checkpoint:</p> <p style="margin: 8px 0 0 0; color: #2c3e50;">Потоковый дизайн минимизирует начальную задержку пакета через блочную обработку данных. Аудио обрабатывается 2-секундными блоками, визуальные данные агрегируются через flash attention, а скользящее оконное внимание с ограниченным рецептивным полем обеспечивает эффективную генерацию, сохраняя контекстную информацию между блоками.</p> </div>


## **3 Предварительная подготовка**

Обучение Qwen2.5-Omni делится на три этапа. На первом этапе авторы фиксируют параметры Большой языковой модели (LLM) и сосредотачиваются на обучении визуального кодировщика и аудиокодировщика с использованием большого количества пар аудио-текст и изображение-текст для улучшения семантического понимания в LLM. На втором этапе авторы размораживаем все параметры и проводим обучение с использованием более обширных мультимодальных данных для достижения более комплексного обучения. На последнем этапе авторы используют данные с длиной последовательностью 32 тыс., чтобы улучшить способность модели понимать сложные длинные данные последовательностей.

Модель предварительно обучена на разнообразном наборе данных, включая корпуса изображений и текста, видео и текста, видео и аудио, аудио и текста, а также простые текстовые корпуса. Авторы заменяют иерархические метки подсказками на естественном языке, следуя Qwen2-Audio, что улучшает возможности обобщения модели и следования инструкциям. На начальном этапе предварительного обучения компонент LLM Qwen2.5-Omni инициализируется с параметрами Qwen2.5, визуальный кодер такой же, как Qwen2.5-VL, а аудиокодер инициализируется с помощью Whisper-large-v3. Два кодировщика обучаются отдельно на фиксированном уровне LLM, сначала уделяя особое внимание обучению соответствующих адаптеров, а затем обучая кодировщиков. Эта базовая подготовка имеет решающее значение для того, чтобы дать модели четкое понимание основных взаимосвязей и соотношений зрительного восприятия и текста, а также аудио-текста. Второй этап предварительной подготовки знаменует собой значительный прогресс за счет введения дополнительных 800 миллиардов размеченных данных для изображений и видео, 300 миллиардов размеченных данных для аудио и 100 миллиардов размеченных данных для видео и аудио. На этом этапе были введены более масштабные смешанные мультимодальные данные и более широкий спектр задач, что улучшило взаимодействие и понимание слуховой, визуальной и текстовой информации. Включение мультимодальных, многозадачных наборов данных имеет решающее значение для разработки возможностей моделей обрабатывать несколько задач и модальностей одновременно, что особенно важно при работе со сложными реальными наборами данных. Кроме того, данные в виде простого текста играют важную роль в поддержании и улучшении уровня владения языком. Для повышения эффективности обучения авторы ограничили максимальную длину токена до 8192 токенов на предыдущем этапе. Затем авторы вводят длинные аудио и видеоданные и расширяют исходные текстовые, аудио, графические и видеоданные до 32 768 токенов для обучения. Экспериментальные результаты показывают, что данные значительно улучшились в плане поддержки данных длинных последовательностей.

### **Мультимодальный процесс обучения** (псевдокод)

```python
# Стандартные библиотеки
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Union, Any

# Константы
TEXT_MODALITY = 0
AUDIO_MODALITY = 1
IMAGE_MODALITY = 2
VIDEO_MODALITY = 3
TEXT_LOSS_WEIGHT = 0.7
AUDIO_LOSS_WEIGHT = 0.3
DPO_LOSS_WEIGHT = 0.1
REWARD_BASELINE = 0.5


def train_step(
    batch: Dict[str, torch.Tensor],
    model: nn.Module,
    optimizer: torch.optim.Optimizer,
    text_embed: nn.Module,
    image_encoder: nn.Module,
    audio_encoder: nn.Module,
    dpo_reference_model: Optional[nn.Module] = None,
    text_labels: Optional[torch.Tensor] = None,
    audio_labels: Optional[torch.Tensor] = None,
    rewards: Optional[torch.Tensor] = None
) -> Dict[str, float]:
    """
    Description:
    ---------------
        Выполняет один шаг обучения мультимодальной модели с использованием
        различных типов входных данных (текст, изображение, аудио, видео).
        Поддерживает дополнительную оптимизацию с помощью DPO (Direct 
        Preference Optimization).

    Args:
    ---------------
        batch: Словарь с входными данными разных модальностей:
            - text: Тензор текстовых данных
            - image: Тензор изображений
            - audio: Тензор аудиоданных
            - video: Тензор видеоданных (опционально)
        model: Основная мультимодальная модель
        optimizer: Оптимизатор для обновления весов модели
        text_embed: Модуль для получения текстовых эмбеддингов
        image_encoder: Энкодер изображений (например, ViT)
        audio_encoder: Энкодер аудио (например, 1D-CNN)
        dpo_reference_model: Опциональная эталонная модель для DPO
        text_labels: Метки для текстовых данных
        audio_labels: Метки для аудиоданных
        rewards: Значения наград для DPO оптимизации

    Returns:
    ---------------
        Словарь с метриками обучения:
            - loss: Общее значение функции потерь
            - loss_text: Значение функции потерь для текста
            - loss_audio: Значение функции потерь для аудио
            - loss_dpo: Значение функции потерь DPO (если применимо)

    Raises:
    ---------------
        ValueError: При некорректных входных данных или несоответствии размерностей
        RuntimeError: При ошибках в вычислениях тензоров

    Examples:
    ---------------
        >>> batch = {
        ...     'text': torch.randint(0, 1000, (8, 64)),  # batch=8, seq_len=64
        ...     'image': torch.randn(8, 3, 224, 224),     # batch=8, RGB изображения
        ...     'audio': torch.randn(8, 1, 16000)         # batch=8, аудио 1 сек
        ... }
        >>> metrics = train_step(
        ...     batch, model, optimizer, text_embed, 
        ...     image_encoder, audio_encoder
        ... )
    """
    # Распаковываем пакет данных
    text, image, audio, video = batch['text'], batch['image'], batch['audio'], batch.get('video')
    
    # Применяем модальные энкодеры без вычисления градиентов
    with torch.no_grad():
        # Кодирование изображения с помощью Vision Transformer
        image_feats = image_encoder(image)  
        
        # Кодирование аудио с помощью 1D-CNN
        audio_feats = audio_encoder(audio)  
    
    # Объединяем мультимодальные входные данные [text; image; audio; video]
    # Применяем усреднение по пространственным/временным измерениям для изображений и аудио
    inputs = torch.cat([
        text_embed(text),              # Эмбеддинги текста
        image_feats.mean(dim=1),       # Усреднение признаков изображения
        audio_feats.mean(dim=1)        # Усреднение признаков аудио
    ], dim=1)
    
    # Создаем позиционные ID для различных модальностей
    # Текстовые позиции начинаются с 0
    # Позиции изображений следуют за текстом
    # Позиции аудио следуют за изображениями
    pos_ids = torch.cat([
        torch.arange(text.size(1)),                       # Позиции для текста
        torch.zeros(image_feats.size(1)) + text.size(1),  # Позиции для изображений
        torch.arange(audio_feats.size(1)) + text.size(1) + 1  # Позиции для аудио
    ])
    
    # Создаем маски типов модальностей
    modality_types = (
        [TEXT_MODALITY] * text.size(1) + 
        [IMAGE_MODALITY] * image_feats.size(1) + 
        [AUDIO_MODALITY] * audio_feats.size(1)
    )
    
    # Применяем ротационную позиционную кодировку с учетом времени (TMRoPE)
    inputs = model.tmrope(inputs, pos_ids, modality_types)
    
    # Прямой проход через модель Thinker-Talker
    text_logits, audio_logits = model(inputs)
    
    # Вычисляем комбинированную функцию потерь
    # L = α * L_text + β * L_audio
    loss_text = F.cross_entropy(text_logits, text_labels)
    loss_audio = F.binary_cross_entropy(audio_logits, audio_labels)
    
    # Применяем весовые коэффициенты к разным компонентам потерь
    loss = TEXT_LOSS_WEIGHT * loss_text + AUDIO_LOSS_WEIGHT * loss_audio
    
    # Метрики для отслеживания
    metrics = {
        'loss_text': loss_text.item(),
        'loss_audio': loss_audio.item(),
        'loss': loss.item()
    }
    
    # Оптимизация DPO (Direct Preference Optimization), если предоставлена эталонная модель
    if dpo_reference_model is not None and rewards is not None:
        with torch.no_grad():
            # Получаем предсказания от эталонной модели
            ref_logits = dpo_reference_model(inputs)
        
        # Вычисляем DPO потери согласно формуле (4) из статьи
        # Отношение логарифмов вероятностей между основной и эталонной моделями
        pi_logratios = torch.log(audio_logits) - torch.log(ref_logits)
        
        # DPO потери на основе наград, смещенных относительно базового уровня
        loss_dpo = -F.logsigmoid(pi_logratios * (rewards - REWARD_BASELINE))
        
        # Добавляем DPO компонент к общим потерям
        loss += DPO_LOSS_WEIGHT * loss_dpo
        metrics['loss_dpo'] = loss_dpo.item()
    
    # Выполняем шаг оптимизации
    optimizer.zero_grad()  # Обнуляем накопленные градиенты
    loss.backward()        # Обратное распространение ошибки
    optimizer.step()       # Обновление весов модели
    
    return metrics
```

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;"> <p style="margin: 0; font-weight: bold; color: #2c3e50;">Fifth Checkpoint:</p> <p style="margin: 8px 0 0 0; color: #2c3e50;">Предварительная подготовка Qwen2.5-Omni реализована в три этапа: сначала обучаются только визуальный и аудиокодировщики при фиксированной LLM, затем размораживаются все параметры для совместного обучения на обширных мультимодальных данных, и наконец, расширяется контекстное окно до 32K токенов для улучшения обработки длинных последовательностей.</p> </div>

## **4. Оценка**

Оценка Qwen2.5-Omni проводилась по двум основным направлениям: понимание (X→Текст) и генерация речи (X→Речь).

В категории X→Текст модель демонстрирует производительность между Qwen2-7B и Qwen2.5-7B, превосходя Qwen2-7B в большинстве текстовых тестов. В задачах аудио→текст Qwen2.5-Omni достигает результатов на уровне или лучше современных специализированных моделей в распознавании речи, переводе и голосовом чате, значительно сокращая разрыв с текстовыми командами.

В обработке изображений модель работает наравне с Qwen2.5-VL-7B и превосходит другие открытые модели, включая GPT-4o-mini. Аналогично, в понимании видео и мультимодальных задачах Qwen2.5-Omni превосходит современные модели с открытым исходным кодом, демонстрируя значительное преимущество в тестах OmniBench.

В генерации речи модель показывает конкурентоспособные результаты как в создании речи с нуля, так и в имитации конкретных дикторов. После оптимизации обучения с подкреплением значительно улучшилась стабильность генерации, а точно настроенная модель обеспечивает качество, близкое к человеческому.

![Figure_6](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_6.png)

## **5 Заключение**

Qwen2.5-Omni представляет собой унифицированную модель, способную понимать и генерировать несколько модальностей, включая текст и речь в реальном времени. Предложенные инновации, такие как TMRoPE и архитектура Thinker-Talker, а также оптимизации для потоковой передачи, позволили достичь значительного прогресса в области мультимодального взаимодействия. Модель демонстрирует сильные результаты на различных бенчмарках, превосходя модели схожего размера, особенно в задачах следования голосовым командам и мультимодального понимания.

Авторы отмечают важность дальнейшей работы над сложными, но часто игнорируемыми задачами, такими как видео OCR и совместное аудио-визуальное понимание, что требует сотрудничества между академическим и индустриальным секторами. Qwen2.5-Omni рассматривается как важный шаг на пути к AGI, и будущие цели включают разработку более robustной и быстрой модели с расширенными возможностями генерации различных модальностей (изображения, видео, музыка).