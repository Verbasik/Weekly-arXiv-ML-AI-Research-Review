# Qwen2.5-Omni: Мультимодальная модель нового поколения

## **Аннотация**  
Qwen2.5-Omni представляет собой революционную мультимодальную модель, способную обрабатывать текст, изображения, аудио и видео, а также генерировать текстовые и речевые ответы в режиме реального времени. Это универсальное решение, объединяющее передовые технологии для достижения низкой задержки и естественного взаимодействия, что делает его шагом к созданию истинного AGI.

### Ключевые инновации:
1. **Архитектура Thinker-Talker**  
   - **Thinker** (мозг): генерирует текст, анализируя данные через трансформер с мультимодальными кодерами.  
   - **Talker** (речь): преобразует скрытые представления Thinker в аудиопоток с помощью двухдорожечного декодера, избегая конфликтов между модальностями.  
   - Вдохновлено биологией: разделение задач аналогично работе мозга и речевого аппарата человека.

2. **TMRoPE: синхронизация мультимодальных данных**  
   - Новая система позиционного кодирования, выравнивающая временные метки аудио и видео.  
   - Решает проблему рассинхронизации потоков (например, речь и губы в видео).

3. **Потоковая обработка "из коробки"**  
   - Блочное кодирование входных данных и скользящее окно DiT для генерации речи с минимальной задержкой.  
   - Поддержка предварительного заполнения контекста для плавного диалога.

### Производительность и превосходство:
- **Лидер в мультимодальных тестах**: превосходит GPT-4o-mini, Qwen2.5-VL и Qwen2-Audio в задачах ASR, OCR, видеоаналитики.  
- **Генерация речи**: Zero-shot TTS с имитацией голоса и естественностью выше аналогов (включая непотоковые модели).  
- **Сквозное обучение**: точность обработки голосовых команд сопоставима с текстовым вводом (MMLU: 82.1, GSM8K: 86.3).

### Почему это прорыв?
- **Единая архитектура** для всех модальностей вместо наборов узкоспециализированных моделей.  
- **Apache 2.0 лицензия** — открытый доступ для исследований и коммерческого использования.  
- Решает проблему "мультимодального хаоса" через слаженную работу кодеров и декодеров.

Qwen2.5-Omni задаёт новый стандарт для ИИ-ассистентов будущего, сочетая скорость, точность и человеко-подобное взаимодействие. Исследователи и разработчики уже сейчас могут интегрировать модель в свои продукты, используя публичные веса.

## **Краткое содержание**

### Общее представление

Qwen2.5-Omni представляет собой комплексную сквозную мультимодальную модель, способную одновременно обрабатывать входные данные различных форматов (текст, изображения, аудио, видео) и генерировать как текстовые, так и речевые ответы в режиме реального времени. Модель реализует несколько инновационных архитектурных решений, обеспечивающих эффективную синхронизацию и обработку разнородных данных.

### Архитектурные инновации

#### Блочная обработка мультимодальных данных

Для обеспечения потоковой обработки мультимодальной информации в Qwen2.5-Omni применяется метод блочной обработки как для аудио, так и для визуальных кодировщиков. Эта стратегия реализует эффективное разделение:
- Восприятие мультимодальных данных поручается специализированным кодировщикам;
- Моделирование длинных последовательностей возлагается на основную языковую модель;
- Объединение модальностей достигается через механизм общего внимания.

#### TMRoPE: синхронизация временных меток

Для решения проблемы синхронизации временных меток видео и аудио, разработчики Qwen2.5-Omni предложили инновационный метод позиционного кодирования — Time-Aligned Multimodal RoPE (TMRoPE). Особенность данного метода заключается в последовательном чередующемся размещении аудио и видео данных, что обеспечивает точную временную привязку между модальностями.

#### Архитектура "Thinker-Talker" (Мыслитель-Говорящий)

Для одновременной генерации текста и речи без взаимных помех между модальностями в Qwen2.5-Omni реализована двухкомпонентная архитектура:

1. **Мыслитель (Thinker)** — функционирует как основная языковая модель, отвечающая за генерацию текстового содержания
2. **Говорящий (Talker)** — представляет собой двухдорожечную авторегрессивную модель, напрямую использующую скрытые представления из Мыслителя для генерации аудиотокенов

Обе компоненты интегрированы в единую сквозную структуру, что позволяет проводить как обучение, так и вывод данных целостным образом.

#### Потоковое декодирование аудио

Для снижения начальной задержки при декодировании аудиотокенов в Qwen2.5-Omni применяется скользящее окно DiT (Diffusion Transformer) с ограниченным полем восприятия, что критически важно для обеспечения реактивности в голосовом взаимодействии.

### Производительность и сравнительный анализ

Qwen2.5-Omni демонстрирует впечатляющие результаты в сравнительных тестах:

- Сопоставима с аналогичной по размеру моделью Qwen2.5-VL в визуально-текстовых задачах;
- Превосходит специализированную модель Qwen2-Audio в задачах обработки аудио;
- Показывает высокие результаты в комплексных мультимодальных тестах Omni-Bench;
- Производительность при обработке голосовых команд сравнима с текстовым вводом в таких бенчмарках как MMLU и GSM8K;
- Потоковый генератор речи превосходит большинство существующих решений по стабильности и естественности звучания.

### Значимость и потенциал

Qwen2.5-Omni представляет собой значительный шаг к созданию систем искусственного общего интеллекта (AGI), объединяя:
- Комплексную мультимодальную обработку данных;
- Низкую латентность взаимодействия;
- Человекоподобные способности общения;
- Единую интегрированную архитектуру для всех модальностей.

![Обзор мультимодальных возможностей Qwen2.5-Omni](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_1.png)
> Рисунок 1: Qwen2.5-Omni — это унифицированная сквозная модель, которая может обрабатывать различные модальности, такие как текст, аудио, изображения и видео, а также генерировать текстовые или голосовые ответы в реальном времени. Благодаря этим функциям Qwen2.5-Omni поддерживает множество задач, включая, помимо прочего, голосовое общение, видеообщение и видеорассуждение.

## Введение

Человеческое восприятие — это сложный многоуровневый процесс. В повседневной жизни мы одновременно воспринимаем многообразную визуальную и слуховую информацию, мгновенно обрабатываем её в мозге и формируем ответную реакцию через речь, письмо или использование инструментов. Этот естественный механизм взаимодействия с миром долгое время оставался недостижимым идеалом для систем искусственного интеллекта.

В последние годы область ИИ совершила значительный прорыв, во многом благодаря стремительному развитию больших языковых моделей (LLM). Эти системы, обучаемые на беспрецедентных объёмах текстовых данных, продемонстрировали впечатляющую способность к решению сложных задач и быстрому обучению. Параллельно развивались и специализированные модели "язык-аудио-язык" (LALM) и "язык-зрение-язык" (LVLM), расширяющие возможности ИИ в области слухового и визуального восприятия.

Однако эффективное сквозное объединение этих разнородных модальностей, использование их полного потенциала и обеспечение естественного человекоподобного взаимодействия через текстовые и голосовые потоки остаётся серьёзным вызовом для современной науки. Разработка по-настоящему универсальной омнимодальной модели требует решения целого комплекса проблем:

1. Создание единого системного подхода к совместному обучению различным модальностям (текст, изображения, видео, аудио);
2. Обеспечение точной временной синхронизации аудио- и видеосигналов;
3. Устранение потенциальных помех между выходными данными разных модальностей;
4. Разработка архитектуры, позволяющей в реальном времени понимать мультимодальную информацию и генерировать потоковые ответы.

В данном брифе мы разберем Qwen2.5-Omni — революционную унифицированную модель, способную одновременно обрабатывать несколько модальностей и генерировать как текстовые, так и естественные речевые ответы в потоковом режиме. Для преодоления вышеуказанных препятствий авторами были разработаны инновационные решения:

1. **TMRoPE (Time-aligned Multimodal RoPE)** — принципиально новый метод позиционного встраивания, который явно включает временную информацию для синхронизации аудио- и видеоданных. Метод размещает аудио и видеокадры в чередующейся структуре, чтобы представить видеопоследовательность в чётком временном порядке.

2. **Архитектура "Мыслитель-Говорящий"** — биомиметический подход, вдохновлённый функционированием человеческого мозга. В данной архитектуре "Мыслитель" отвечает за генерацию текста, а "Говорящий" фокусируется на создании потоковых речевых токенов, получая высокоуровневые представления непосредственно от "Мыслителя". Это решение позволяет обеспечить естественную координацию разнородных выходных сигналов.

3. **Потоковая блочная обработка** — модификация всех мультимодальных кодеров для обеспечения понимания сигналов в реальном времени и упрощения предварительного заполнения.

4. **Двухдорожечная авторегрессивная модель** — для потоковой генерации речи, которая преобразует речевые токены в звуковые волны с минимальной начальной задержкой.

При сравнительном тестировании Qwen2.5-Omni демонстрирует выдающиеся результаты. Она сопоставима с Qwen2.5-VL в обработке визуальной информации и значительно превосходит Qwen2-Audio в работе со звуком. В мультимодальных бенчмарках, таких как OmniBench и AV-Odyssey Bench, модель показывает высочайшие показатели производительности. Особенно примечателен тот факт, что Qwen2.5-Omni обрабатывает голосовые команды на уровне, сопоставимом с текстовым вводом, что продемонстрировано в таких тестах, как MMLU и GSM8K.

В области генерации речи модель достигает впечатляющих результатов с показателями Word Error Rate (WER) всего 1,42%, 2,33% и 6,54% в тестовых наборах test-zh, test-en и test-hard seed-tts-eval соответственно, что превосходит достижения таких известных систем, как MaskGCT и CosyVoice 2.

Таким образом, Qwen2.5-Omni представляет собой значительный шаг на пути к созданию истинного AGI (искусственного общего интеллекта), объединяя мультимодальность, низкую задержку и человекоподобное взаимодействие в единой модели. Эта разработка открывает новые горизонты для применения ИИ в различных областях, от повседневной коммуникации до сложных профессиональных задач, требующих интеграции разнородных типов данных и естественного диалога.