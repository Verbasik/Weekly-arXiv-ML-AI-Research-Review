# Qwen2.5-Omni: Мультимодальная модель нового поколения

## **Аннотация**  
Qwen2.5-Omni представляет собой революционную мультимодальную модель, способную обрабатывать текст, изображения, аудио и видео, а также генерировать текстовые и речевые ответы в режиме реального времени. Это универсальное решение, объединяющее передовые технологии для достижения низкой задержки и естественного взаимодействия, что делает его шагом к созданию истинного AGI.

### Ключевые инновации:
1. **Архитектура Thinker-Talker**  
   - **Thinker** (мозг): генерирует текст, анализируя данные через трансформер с мультимодальными кодерами.  
   - **Talker** (речь): преобразует скрытые представления Thinker в аудиопоток с помощью двухдорожечного декодера, избегая конфликтов между модальностями.  
   - Вдохновлено биологией: разделение задач аналогично работе мозга и речевого аппарата человека.

2. **TMRoPE: синхронизация мультимодальных данных**  
   - Новая система позиционного кодирования, выравнивающая временные метки аудио и видео.  
   - Решает проблему рассинхронизации потоков (например, речь и губы в видео).

3. **Потоковая обработка "из коробки"**  
   - Блочное кодирование входных данных и скользящее окно DiT для генерации речи с минимальной задержкой.  
   - Поддержка предварительного заполнения контекста для плавного диалога.

### Производительность и превосходство:
- **Лидер в мультимодальных тестах**: превосходит GPT-4o-mini, Qwen2.5-VL и Qwen2-Audio в задачах ASR, OCR, видеоаналитики.  
- **Генерация речи**: Zero-shot TTS с имитацией голоса и естественностью выше аналогов (включая непотоковые модели).  
- **Сквозное обучение**: точность обработки голосовых команд сопоставима с текстовым вводом (MMLU: 82.1, GSM8K: 86.3).

### Почему это прорыв?
- **Единая архитектура** для всех модальностей вместо наборов узкоспециализированных моделей.  
- **Apache 2.0 лицензия** — открытый доступ для исследований и коммерческого использования.  
- Решает проблему "мультимодального хаоса" через слаженную работу кодеров и декодеров.

Qwen2.5-Omni задаёт новый стандарт для ИИ-ассистентов будущего, сочетая скорость, точность и человеко-подобное взаимодействие. Исследователи и разработчики уже сейчас могут интегрировать модель в свои продукты, используя публичные веса.

## **Краткое содержание**

### Общее представление

Qwen2.5-Omni представляет собой комплексную сквозную мультимодальную модель, способную одновременно обрабатывать входные данные различных форматов (текст, изображения, аудио, видео) и генерировать как текстовые, так и речевые ответы в режиме реального времени. Модель реализует несколько инновационных архитектурных решений, обеспечивающих эффективную синхронизацию и обработку разнородных данных.

### Архитектурные инновации

#### Блочная обработка мультимодальных данных

Для обеспечения потоковой обработки мультимодальной информации в Qwen2.5-Omni применяется метод блочной обработки как для аудио, так и для визуальных кодировщиков. Эта стратегия реализует эффективное разделение:
- Восприятие мультимодальных данных поручается специализированным кодировщикам;
- Моделирование длинных последовательностей возлагается на основную языковую модель;
- Объединение модальностей достигается через механизм общего внимания.

#### TMRoPE: синхронизация временных меток

Для решения проблемы синхронизации временных меток видео и аудио, разработчики Qwen2.5-Omni предложили инновационный метод позиционного кодирования — Time-Aligned Multimodal RoPE (TMRoPE). Особенность данного метода заключается в последовательном чередующемся размещении аудио и видео данных, что обеспечивает точную временную привязку между модальностями.

#### Архитектура "Thinker-Talker" (Мыслитель-Говорящий)

Для одновременной генерации текста и речи без взаимных помех между модальностями в Qwen2.5-Omni реализована двухкомпонентная архитектура:

1. **Мыслитель (Thinker)** — функционирует как основная языковая модель, отвечающая за генерацию текстового содержания
2. **Говорящий (Talker)** — представляет собой двухдорожечную авторегрессивную модель, напрямую использующую скрытые представления из Мыслителя для генерации аудиотокенов

Обе компоненты интегрированы в единую сквозную структуру, что позволяет проводить как обучение, так и вывод данных целостным образом.

#### Потоковое декодирование аудио

Для снижения начальной задержки при декодировании аудиотокенов в Qwen2.5-Omni применяется скользящее окно DiT (Diffusion Transformer) с ограниченным полем восприятия, что критически важно для обеспечения реактивности в голосовом взаимодействии.

### Производительность и сравнительный анализ

Qwen2.5-Omni демонстрирует впечатляющие результаты в сравнительных тестах:

- Сопоставима с аналогичной по размеру моделью Qwen2.5-VL в визуально-текстовых задачах;
- Превосходит специализированную модель Qwen2-Audio в задачах обработки аудио;
- Показывает высокие результаты в комплексных мультимодальных тестах Omni-Bench;
- Производительность при обработке голосовых команд сравнима с текстовым вводом в таких бенчмарках как MMLU и GSM8K;
- Потоковый генератор речи превосходит большинство существующих решений по стабильности и естественности звучания.

### Значимость и потенциал

Qwen2.5-Omni представляет собой значительный шаг к созданию систем искусственного общего интеллекта (AGI), объединяя:
- Комплексную мультимодальную обработку данных;
- Низкую латентность взаимодействия;
- Человекоподобные способности общения;
- Единую интегрированную архитектуру для всех модальностей.

![Обзор мультимодальных возможностей Qwen2.5-Omni](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_1.png)
> Рисунок 1: Qwen2.5-Omni — это унифицированная сквозная модель, которая может обрабатывать различные модальности, такие как текст, аудио, изображения и видео, а также генерировать текстовые или голосовые ответы в реальном времени. Благодаря этим функциям Qwen2.5-Omni поддерживает множество задач, включая, помимо прочего, голосовое общение, видеообщение и видеорассуждение.

## **Введение**

Человеческое восприятие — это сложный многоуровневый процесс. В повседневной жизни мы одновременно воспринимаем многообразную визуальную и слуховую информацию, мгновенно обрабатываем её в мозге и формируем ответную реакцию через речь, письмо или использование инструментов. Этот естественный механизм взаимодействия с миром долгое время оставался недостижимым идеалом для систем искусственного интеллекта.

В последние годы область ИИ совершила значительный прорыв, во многом благодаря стремительному развитию больших языковых моделей (LLM). Эти системы, обучаемые на беспрецедентных объёмах текстовых данных, продемонстрировали впечатляющую способность к решению сложных задач и быстрому обучению. Параллельно развивались и специализированные модели "язык-аудио-язык" (LALM) и "язык-зрение-язык" (LVLM), расширяющие возможности ИИ в области слухового и визуального восприятия.

Однако эффективное сквозное объединение этих разнородных модальностей, использование их полного потенциала и обеспечение естественного человекоподобного взаимодействия через текстовые и голосовые потоки остаётся серьёзным вызовом для современной науки. Разработка по-настоящему универсальной омнимодальной модели требует решения целого комплекса проблем:

1. Создание единого системного подхода к совместному обучению различным модальностям (текст, изображения, видео, аудио);
2. Обеспечение точной временной синхронизации аудио- и видеосигналов;
3. Устранение потенциальных помех между выходными данными разных модальностей;
4. Разработка архитектуры, позволяющей в реальном времени понимать мультимодальную информацию и генерировать потоковые ответы.

В данном брифе мы разберем Qwen2.5-Omni — революционную унифицированную модель, способную одновременно обрабатывать несколько модальностей и генерировать как текстовые, так и естественные речевые ответы в потоковом режиме. Для преодоления вышеуказанных препятствий авторами были разработаны инновационные решения:

1. **TMRoPE (Time-aligned Multimodal RoPE)** — принципиально новый метод позиционного встраивания, который явно включает временную информацию для синхронизации аудио- и видеоданных. Метод размещает аудио и видеокадры в чередующейся структуре, чтобы представить видеопоследовательность в чётком временном порядке.

2. **Архитектура "Thinker-Talker"** — биомиметический подход, вдохновлённый функционированием человеческого мозга. В данной архитектуре "Мыслитель" отвечает за генерацию текста, а "Говорящий" фокусируется на создании потоковых речевых токенов, получая высокоуровневые представления непосредственно от "Мыслителя". Это решение позволяет обеспечить естественную координацию разнородных выходных сигналов.

3. **Потоковая блочная обработка** — модификация всех мультимодальных кодеров для обеспечения понимания сигналов в реальном времени и упрощения предварительного заполнения.

4. **Двухдорожечная авторегрессивная модель** — для потоковой генерации речи, которая преобразует речевые токены в звуковые волны с минимальной начальной задержкой.

![Figure_2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_2.png)
*Рисунок 2: Обзор Qwen2.5-Omni. Qwen2.5-Omni использует архитектуру «Thinker-Talker». Мыслитель отвечает за генерацию текста, в то время как говорящий фокусируется на генерации потоковых речевых маркеров, получая высокоуровневые представления непосредственно от мыслителя.*

При сравнительном тестировании Qwen2.5-Omni демонстрирует выдающиеся результаты. Она сопоставима с Qwen2.5-VL в обработке визуальной информации и значительно превосходит Qwen2-Audio в работе со звуком. В мультимодальных бенчмарках, таких как OmniBench и AV-Odyssey Bench, модель показывает высочайшие показатели производительности. Особенно примечателен тот факт, что Qwen2.5-Omni обрабатывает голосовые команды на уровне, сопоставимом с текстовым вводом, что продемонстрировано в таких тестах, как MMLU и GSM8K.

В области генерации речи модель достигает впечатляющих результатов с показателями Word Error Rate (WER) всего 1,42%, 2,33% и 6,54% в тестовых наборах test-zh, test-en и test-hard seed-tts-eval соответственно, что превосходит достижения таких известных систем, как MaskGCT и CosyVoice 2.

Таким образом, Qwen2.5-Omni представляет собой значительный шаг на пути к созданию истинного AGI (искусственного общего интеллекта), объединяя мультимодальность, низкую задержку и человекоподобное взаимодействие в единой модели. Эта разработка открывает новые горизонты для применения ИИ в различных областях, от повседневной коммуникации до сложных профессиональных задач, требующих интеграции разнородных типов данных и естественного диалога.

Я выступаю в роли эксперта в области мультимодальных языковых моделей и нейросетевых архитектур. Давайте обогатим ваш технический обзор дополнительной информацией о Qwen2.5-Omni.

Для начала, мне необходимо найти более подробную информацию об архитектуре Qwen2.5-Omni и использованных в ней технологиях.

Теперь мне нужно поискать более подробную информацию о TMRoPE и системе восприятия в Qwen2.5-Omni.

Теперь давайте поищем еще информацию о методах восприятия и обработки различных модальностей в Qwen2.5-Omni.

На основе полученной информации, я улучшу и обогащу ваш текст о Qwen2.5-Omni, делая его более детальным и академичным.

## **2 Архитектура**

### **2.1 Обзор**

Qwen2.5-Omni имплементирует инновационную архитектуру типа «Thinker-Talker» (Мыслитель-Говорящий), представляющую собой передовой подход к построению мультимодальных моделей. Thinker функционирует как когнитивный центр системы, отвечающий за обработку и интерпретацию входных данных различных модальностей (текст, аудио, изображения и видео), формирование высокоуровневых репрезентаций и генерацию соответствующего текстового контента. В свою очередь, Talker оперирует как артикуляционная составляющая системы, принимающая потоковым образом высокоуровневые репрезентации и текст, продуцируемые Thinker, и генерирующая дискретные речевые токены.

Архитектурно Thinker реализован как декодер модели Transformer, дополненный специализированными энкодерами для извлечения информации из аудио и визуальных данных. Talker спроектирован как двухдорожечная авторегрессивная архитектура декодера Transformer, основанная на принципах, изложенных в работе Mini-Omni (Xie & Wu, 2024). Существенной особенностью данной архитектуры является то, что на этапах обучения и инференса Talker непосредственно получает многомерные репрезентации от Thinker и имеет доступ ко всей исторической контекстной информации, обработанной компонентом Thinker. Благодаря такой интеграции вся архитектурная система функционирует как единая унифицированная модель, обеспечивающая эффективное сквозное обучение и инференс.

В последующих разделах мы детально рассмотрим механизмы восприятия различных сигналов в Qwen2.5-Omni и проанализируем инновационный алгоритм позиционного кодирования TMRoPE (Time-aligned Multimodal RoPE). Далее будут освещены технические аспекты генерации текста и речи. В заключении мы представим обзор усовершенствований, внедренных в модули понимания и генерации, которые обеспечивают эффективное потоковое рассуждение.

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;"> <p style="margin: 0; font-weight: bold; color: #2c3e50;">First Checkpoint:</p> <p style="margin: 8px 0 0 0; color: #2c3e50;">Qwen2.5-Omni использует инновационную архитектуру «Thinker-Talker»: Thinker обрабатывает мультимодальные данные (текст, аудио, изображения, видео) и формирует высокоуровневые репрезентации, а Talker преобразует их в речь. Система работает как единая модель, обеспечивая эффективное сквозное обучение и генерацию.</p> </div>

### **2.2 Восприятие**

#### **Текст, аудио, изображения и видео**

Компонент Thinker обрабатывает мультимодальные входные данные (текст, аудио, изображения и видео), трансформируя их в векторные репрезентации для последующей обработки. Для сегментации текста используется специализированный токенизатор Qwen (Yang et al., 2024a, arXiv:2407.10671), реализующий кодирование пар байтов на байтовом уровне с лексиконом, содержащим 151 643 стандартных токенов.

Для обработки аудиовходов и аудиодорожек видео Qwen2.5-Omni применяет передискретизацию до частоты 16 кГц с последующим преобразованием исходных волновых форм в Мел-спектрограммы, характеризующиеся 128 каналами, размером окна 25 мс и шагом 10 мс. Для эффективной потоковой обработки аудио и видеоданных модель использует блочно-ориентированный подход (block-wise processing), который позволяет секвенциально обрабатывать фрагменты данных, не требуя полного входного сигнала. Это особенно важно для работы с продолжительными аудио и видеоматериалами в режиме реального времени.

Аудиокодер основан на архитектуре Qwen2-Audio (Chu et al., 2024b), где каждый аудиокадр представляет примерно 40-миллисекундный сегмент исходного аудиосигнала. Для визуальной модальности применяется специализированный визуальный кодер, основанный на архитектуре Vision Transformer (ViT), содержащий около 675 миллионов параметров. Данный кодер обеспечивает эффективную обработку входных изображений и видеоданных благодаря применению инновационных техник, включая оптимизированную оконную систему внимания (Window Attention), которая значительно улучшает вычислительную эффективность при работе с визуальными данными высокого разрешения.

Визуальный кодер обучен на комбинированном датасете изображений и видео с использованием гибридной схемы обучения, что обеспечивает превосходную производительность как при анализе статичных изображений, так и при обработке динамического видеоконтента. Подобно Qwen2.5-VL, визуальный кодер в Qwen2.5-Omni поддерживает динамическое разрешение, что позволяет эффективно обрабатывать изображения различных размеров без стандартной нормализации координат, сохраняя естественное масштабирование визуальных объектов.

Для максимального сохранения видеоинформации и синхронизации с частотой дискретизации аудио, система использует динамическую частоту кадров. Синхронизация между аудио и видео достигается благодаря последовательной организации входных данных в перемежающемся порядке и применению инновационного подхода Time-aligned Multimodal RoPE (TMRoPE). Это позволяет модели точно интерпретировать временные зависимости между тем, что она "видит" и "слышит". Для обеспечения единообразия каждое статичное изображение обрабатывается как последовательность из двух идентичных кадров.

<details> 
    <summary><em><strong>Мел-спектрограмма</strong></em></summary>

---

**Мел-спектрограмма** — это визуальное представление звукового сигнала, которое отражает его частотные характеристики с учетом **мел-шкалы** (шкалы восприятия высоты звука человеком).  

### **Основные понятия**:

1. **Спектрограмма** – это график, показывающий, как частотный состав звука меняется во времени (ось X – время, ось Y – частота, цвет – амплитуда).  
2. **Мел-шкала** – психоакустическая шкала, которая приближает восприятие частот человеческим ухом (люди лучше различают низкие частоты, чем высокие).  
   - Например: разница между 100 Гц и 200 Гц воспринимается значительной, а между 8000 Гц и 8100 Гц – почти незаметной.  

### **Как получают мел-спектрограмму?**

1. **Разбиение сигнала на фреймы** (короткие отрезки);  
2. **Применение БПФ (FFT)** для получения спектра каждого фрейма;
3. **Фильтры мел-банка** – набор треугольных фильтров, распределенных по мел-шкале (больше фильтров на низких частотах, меньше – на высоких);
4. **Логарифмирование энергии** (т.к. человек воспринимает громкость логарифмически).  

### **Формула перевода Герц в меллы**:

$$
m = 2595 \cdot \log_{10}\left(1 + \frac{f}{700}\right)
$$

где:  
- $( f )$ – частота в Герцах,  
- $( m )$ – частота в меллах.  

### **Применение**:
- **Распознавание речи** (ASR, например, в Siri, Google Assistant).  
- **Анализ музыки** (жанровая классификация, выделение тональности).  
- **Генерация звука** (нейросети типа WaveNet, Tacotron).  

### **Пример визуализации**:

![Мел-спектрограмма](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_3.jpeg)

(По горизонтали – время, по вертикали – мел-частоты, цвет – мощность сигнала).  

Если у вас есть аудиофайл, можно построить мел-спектрограмму в Python с помощью `librosa`:  

```python
import librosa
import librosa.display
import matplotlib.pyplot as plt

# Загружаем аудиофайл "audio.wav" с помощью функции librosa.load
# y — это одномерный массив, содержащий аудиосигнал (амплитуды звука)
# sr — частота дискретизации аудиофайла (количество выборок в секунду)
y, sr = librosa.load("audio.wav")

# Вычисляем мел-спектрограмму (Mel Spectrogram) из аудиосигнала
# librosa.feature.melspectrogram преобразует сигнал в представление частотного спектра,
# используя шкалу мелов, которая более адаптирована к восприятию человека.
# Параметры:
#   y — аудиосигнал
#   sr — частота дискретизации
S = librosa.feature.melspectrogram(y=y, sr=sr)

# Преобразуем мощность спектрограммы в децибелы (dB), чтобы улучшить визуализацию
# librosa.power_to_db выполняет логарифмическое преобразование,
# где ref=np.max означает, что максимальное значение мощности используется как точка отсчета.
S_dB = librosa.power_to_db(S, ref=np.max)

# Создаем новое окно для графика размером 10x4 дюймов
plt.figure(figsize=(10, 4))

# Отображаем мел-спектрограмму с помощью функции librosa.display.specshow
# Параметры:
#   S_dB — данные спектрограммы в децибелах
#   sr — частота дискретизации
#   x_axis="time" — ось X будет отображать время
#   y_axis="mel" — ось Y будет использовать шкалу мелов для частот
librosa.display.specshow(S_dB, sr=sr, x_axis="time", y_axis="mel")

# Добавляем цветовую шкалу (colorbar) справа от графика
# format="%+2.0f dB" указывает формат отображения значений в децибелах
plt.colorbar(format="%+2.0f dB")

plt.title("Мел-спектрограмма")
plt.show()
```  

Это мощный инструмент для анализа звука, особенно там, где важно учитывать человеческое восприятие! 🎵

</details>

<div style="border: 2px solid #3498db; border-radius: 8px; padding: 12px; background-color: #f8f9fa; margin: 10px 0;"> <p style="margin: 0; font-weight: bold; color: #2c3e50;">Second Checkpoint:</p> <p style="margin: 8px 0 0 0; color: #2c3e50;">Thinker обрабатывает мультимодальные данные с помощью специализированных энкодеров: текстовый токенизатор Qwen, аудиокодер (Mel-спектрограммы + блочная обработка), ViT-based визуальный кодер с динамическим разрешением. Синхронизация аудио и видео обеспечивается алгоритмом TMRoPE, а статичные изображения адаптируются как двукадровая последовательность.</p> </div>

#### **Видео и TMRoPE**

Авторы предлагают алгоритм временного чередования аудио и видео, а также новый метод позиционного кодирования. Как показано на рисунке 3, TMRoPE кодирует трехмерную информацию о положении мультимодального ввода, а именно мультимодальное встраивание вращательного положения (M-RoPE) с абсолютным временным положением (Bai et al., 2023b). Это достигается путем разложения исходного повернутого вложения на три компонента: время, высоту и ширину. Для ввода текста эти части используют один и тот же идентификатор местоположения, что делает M-RoPE функционально эквивалентным 1D RoPE. Аналогично для аудиовхода также используется тот же идентификатор позиции и вводится кодирование абсолютной временной позиции, при этом каждый идентификатор времени соответствует 40 мс.

При обработке изображения временной идентификатор каждого визуального маркера остается постоянным, тогда как компонентам высоты и ширины присваиваются разные идентификаторы в зависимости от положения маркера на изображении. Если на вход подается видео со звуком, звук по-прежнему кодируется с тем же идентификатором позиции 40 мс на кадр, тогда как видео обрабатывается как серия изображений с увеличивающимися временными идентификаторами для каждого кадра, а части высоты и ширины следуют тому же шаблону назначения идентификаторов, что и изображения. Поскольку частота кадров видео не фиксирована, мы динамически корректируем временной идентификатор между кадрами в соответствии с фактическим временем каждого кадра, чтобы гарантировать, что один временной идентификатор соответствует 40 мс. В случае, когда входные данные модели содержат несколько модальностей, номер позиции каждой модальности инициализируется путем увеличения максимального идентификатора позиции предыдущей модальности на единицу. TMRoPE улучшает моделирование информации о местоположении и максимально увеличивает интеграцию различных модальностей, позволяя Qwen2.5-Omni понимать и анализировать информацию из нескольких модальностей одновременно.

![Figure_4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/develop/2025/week-14/assets/Figure_4.png)
*Рисунок 3: Иллюстрация согласованного по времени мультимодального RoPE (TMRoPE).*

После включения информации о положении в каждую модальность мы располагаем представления по порядку. Чтобы модель могла одновременно получать как визуальную, так и слуховую информацию, как показано на рисунке 3, мы применяем специальную конструкцию для видео со звуком, называемую методом временного чередования , который делит представление видео со звуком на блоки каждые 2 секунды в соответствии с фактическим временем. Затем в течение 2 секунд визуальное представление помещается спереди, а звуковое представление — сзади, чередуя представление видео со звуком.

