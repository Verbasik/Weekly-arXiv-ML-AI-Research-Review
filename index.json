{
    "years": [
      {
        "year": "2025",
        "weeks": [
          {
            "week": "week-01_&_02",
            "title": "Week 01 & 02",
            "date": "Jan 1-14, 2025",
            "tags": ["LLMs"],
            "description": "Initial exploration of new architectures for language models.",
            "papers": 1,
            "notebooks": 1
          },
          {
            "week": "week-03",
            "title": "Week 03",
            "date": "Jan 15-21, 2025",
            "tags": ["Math Reasoning", "Small LLMs"],
            "description": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking.",
            "papers": 1,
            "notebooks": 1
          },
          {
            "week": "week-04",
            "title": "Week 04",
            "date": "Jan 22-28, 2025",
            "tags": ["Encoders", "Efficiency"],
            "description": "A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference.",
            "papers": 1,
            "notebooks": 2
          },
          {
            "week": "week-05",
            "title": "Week 05",
            "date": "Jan 29-Feb 4, 2025",
            "tags": ["Deep Learning", "NLP"],
            "description": "Advancements in neural network architectures with focus on natural language processing tasks.",
            "papers": 1,
            "notebooks": 1
          },
          {
            "week": "week-06",
            "title": "Week 06",
            "date": "Feb 5-11, 2025",
            "tags": ["DeepSpeed", "MoE", "Efficient Training"],
            "description": "DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale.",
            "papers": 1,
            "notebooks": 1
          },
          {
            "week": "week-07_&_08",
            "title": "Week 07 & 08",
            "date": "Feb 12-25, 2025",
            "tags": ["Transformers", "Architecture"],
            "description": "In-depth analysis of recent transformer model architecture improvements and optimizations.",
            "papers": 2,
            "notebooks": 1
          },
          {
            "week": "week-09",
            "title": "Week 09",
            "date": "Feb 26-Mar 4, 2025",
            "tags": ["Attention Mechanisms", "Efficiency"],
            "description": "Native Sparse Attention: Improving efficiency in transformer models with novel attention mechanisms.",
            "papers": 1,
            "notebooks": 1
          },
          {
            "week": "week-10",
            "title": "Week 10",
            "date": "Mar 5-11, 2025",
            "tags": ["Genomics", "CRISPR"],
            "description": "CRISPR-CAS9 —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –∏ –µ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ. Genome modeling and design across all domains of life with Evo 2.",
            "papers": 2,
            "notebooks": 1
          },
          {
            "week": "week-11",
            "title": "Week 11",
            "date": "Mar 12-18, 2025",
            "tags": ["Distillation", "Scaling Laws"],
            "description": "Distillation Scaling Laws: Understanding the relationship between model size, training, and knowledge transfer.",
            "papers": 1,
            "notebooks": 1
          },
          {
            "week": "week-12",
            "title": "Week 12",
            "date": "Mar 19-25, 2025",
            "tags": ["Multi-modal", "Computer Vision"],
            "description": "Exploration of multi-modal learning approaches combining vision and language understanding.",
            "papers": 1,
            "notebooks": 1
          },
          {
            "week": "week-13",
            "title": "Week 13",
            "date": "Mar 26-Apr 1, 2025",
            "tags": ["Reinforcement Learning", "LLMs"],
            "description": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale.",
            "papers": 1,
            "notebooks": 1
          }
        ]
      },
      {
        "year": "2024",
        "weeks": [
          {
            "week": "week-52",
            "title": "Week 52",
            "date": "Dec 23-31, 2024",
            "tags": ["LLMs", "Weight Analysis"],
            "description": "üî¨ –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ ¬´–°—É–ø–µ—Ä–≤–µ—Å–æ–≤¬ª –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) üî¨üìä –û–±—â–∏–µ —Å–≤–µ–¥–µ–Ω–∏—è–≠—Ç–æ—Ç –±—Ä–∏—Ñ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±–∑–æ—Ä –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç—å—é [arXiv: The Super Weight in Large Language Models](https://arxiv.org/abs/2411.07191) –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å—É–ø–µ—Ä–≤–µ—Å–æ–≤ (¬´super weights¬ª) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). ‚ú®üîó –û—Å–Ω–æ–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è:- –ò—Ö –≤–ª–∏—è–Ω–∏—é –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π üîß- –ú–µ—Ç–æ–¥–∞–º –∏—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ üîç- –ò—Ö —Ä–æ–ª–∏ –≤ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π ‚öñÔ∏è",
            "papers": 1,
            "notebooks": 1
          }
        ]
      }
    ]
  }