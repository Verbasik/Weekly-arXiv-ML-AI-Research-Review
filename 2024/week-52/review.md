## Брифинг-документ: "Сверхвеса" в больших языковых моделях

## Введение

Обзор посвящен исследованию, на тему "сверхвеса" (super weights) в больших языковых моделях (LLM). Авторы обнаружили, что очень небольшое количество параметров (вплоть до одного скаляра!) в LLM играет непропорционально важную роль в их способности генерировать качественный текст. Ресерчеры из Apple утверждают, что крошечное подмножество, максимум шесть масштабирующих факторов, важнее остальных. Авторы называют их супервесами, и их обрезка разрушает качество модели.

Несколько статей в прошлом показали, что в определенном масштабе небольшой набор скрытых признаков состояния содержит выбросы с огромной величиной. Эти выбросы составляют небольшой процент всех активаций, но имеют решающее значение для сохранения качества сжатой модели. В контексте LLM эти выбросы проявляются как "сверх-активации" (super activations) – аномально большие активации, которые также критически важны для качества модели. Удаление этих "сверхвесов" может полностью разрушить модель, снижая точность до уровня случайного угадывания и увеличивая перплексию на несколько порядков.

Исследование также показывает, что эти "сверхвеса" и "сверх-активации" могут быть идентифицированы с помощью простого, не требующего данных метода. Этот метод предлагается для использования в улучшении квантизации моделей, что позволяет сохранить их качество даже при значительном снижении вычислительной сложности.


## Основные результаты и идеи

### Сверхвеса (Super Weights)
- Авторы обнаружили, что один единственный параметр ("сверхвес") в LLM имеет непропорционально большое влияние на качество модели.

- Удаление этого параметра может привести к генерации бессмысленного текста, как качественно, так и количественно (показано на примере Llama-7B на рисунке 1 и в таблице 1).

- Важно отметить, что удаление даже 7000 других самых больших по величине параметров влияет на качество незначительно по сравнению с удалением одного "сверхвеса".

**_Внутри обученных LLM находится группа весов-аутлаеров с большой магнитудой, они могут составлять порядка 0.01% от всех весов модели, что в случае миллиардных моделей всё равно сотни тысяч. Это было известно ранее. В текущей работе показывают, что внутри этой группы находится один единственный вес (тот самый super weight, SW), не обязательно самый большой, важность которого превышает суммарную важность тысяч других аутлаеров. Он необходим для качества, без него LLM не может генерить нормальный текст. Перплексия вырастает на несколько порядков, а точность на zero-shot задачах падает до рандома._**

![Рисунок_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/main/2024/week-52/assets/%D0%A0%D0%B8%D1%81%D1%83%D0%BD%D0%BE%D0%BA_1.png)

![Таблица_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/main/2024/week-52/assets/%D0%A2%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D0%B0_1.png)

> «В Llama-7B удаление сверхвеса, одного скаляра, полностью разрушает способность модели генерировать текст. Средняя точность задач с нулевой выборкой фактически падает до нуля. И наоборот, удаление других 7000 крупнейших выбросов, включая выбросы, которые больше, чем сверхвес, влияет не более чем на несколько процентных пунктов.»


## Идентификация сверхвесов

### Основной метод

Предлагается data-free метод идентификации сверхвесов, который не требует наличия тестового набора данных или примеров использования. Метод основан на следующих принципах:

- Анализ распределения активаций в прямом проходе модели
- Выявление скачков в распределении входов и выходов слоев `mlp.down_proj`
- Использование только одного входного запроса для обнаружения
- Авторы предоставляют каталог координат сверхвесов для нескольких общедоступных LLM (Таблица 2).


**Определение координат сверхвеса:**
  - Строка определяется по индексу канала входного распределения активаций
  - Столбец определяется по индексу канала выходного распределения активаций

**Характеристики сверхвесов:**
  - Не обязательно являются максимальными по абсолютной величине в матрице весов
  - Могут быть обнаружены путем подачи произвольного запроса
  - Для уменьшения активации достаточно обрезать один вес

**Распределение в моделях:**
  - Максимальное количество сверхвесов (шесть) обнаружено в модели Phi-3-mini-4k-instruct
  - Позиции сверхвесов сохраняются при тонкой настройке моделей с помощью инструкций

![Таблица_2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/main/2024/week-52/assets/%D0%A2%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D0%B0_2.png)

> «Основываясь на приведенном выше анализе, мы представляем эффективный способ локализации сверхвесов: SW можно найти путем обнаружения скачков в распределениях входов и выходов down_proj по слоям. Это обнаружение требует только одного входного запроса, а не набора проверочных данных или примеров использования.»


### Сверх-активации (Super Activations)
- Сверхвеса вызывают "сверх-активации" - очень большие активации, которые сохраняются на протяжении многих слоев модели в одном и том же положении, независимо от входных данных.
- Эти сверх-активации играют ключевую роль в функционировании модели.
- Удаление сверхвеса резко снижает величину сверх-активации, подтверждая причинно-следственную связь.

**_Ранее (https://arxiv.org/abs/2402.17762) были найдены супер-активации, критичные для качества. Они существуют в различных слоях, имеют константную магнитуду и всегда обнаруживаются в одинаковой позиции несмотря на вход. Текущая работа находит, что канал активации совпадает с оным для супер веса и сперва активация обнаруживается сразу после супер веса. Прунинг этого супер веса значительно уменьшает активацию, так что вероятно активация вызвана им, а не просто скоррелирована. Такие активации называются супер активациями (super activations, SA)._**

**_Предыдущая работа объясняла супер активации через bias terms, но не объясняла как они получаются и почему на одних и тех же местах. Сейчас авторы эмпирически нашли, что до down проекции (down_proj) произведение Адамара (Hadamard product) gate и up проекций (gate_proj, up_proj) создаёт относительно большую активацию. Супер вес далее усиливает её ещё и даёт супер активацию._**

**Напомню, что MLP блок в Ламе выглядит так:**

```
out = down_proj( act_fn(gate_proj(input)) x up_proj(input) )
```

![Рисунок_4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/main/2024/week-52/assets/%D0%A0%D0%B8%D1%81%D1%83%D0%BD%D0%BE%D0%BA_4.png)

> «Мы обнаруживаем еще одно интригующее свойство: канал активации соответствует нашему сверхвесу, и активация появляется сразу после нашего сверхвеса. Чтобы подтвердить, является ли это корреляцией или причинно-следственной связью, мы удаляем сверхвес и проверяем величину массивной активации. На рисунке 4 мы обнаруживаем, что удаление сверхвеса резко снижает величину массивной активации. Это говорит о том, что массивные активации создаются сверхвесами. Для последовательности мы называем эти массивные активации «сверх-активациями».»


### Механизмы действия сверхвесов
- Сверхвеса, помимо создания сверх-активаций, подавляют вероятность стоп-слов в выходных данных модели (Рисунок 2, 5).
- Удаление сверхвесов приводит к увеличению вероятности стоп-слов и уменьшению вероятности значимых слов.
- Восстановление сверх-активаций частично восстанавливает качество модели после удаления сверхвеса, но не полностью.
- Провели эксперименты по обнулению SW, в том числе с восстановлением SA до исходного значения, чтобы проверить влияние SW на другие активации. Это восстанавливает 42% потери, то есть влияние SW на качество выше, чем просто через SA.
- По анализу 500 различных промптов из Lambaba validation set видно, что при убирании SW вероятности стоп-слов сильно возрастают (а обычные слова соответственно занижаются). Для “the” это 2×, для “.” -- 5×, и для “,” -- 10×. То есть наличие SW как бы подавляет стоп-слова и позволяет генерировать осмысленный текст.

![Рисунок_2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/main/2024/week-52/assets/%D0%A0%D0%B8%D1%81%D1%83%D0%BD%D0%BE%D0%BA_2.png)

![Рисунок_5](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/main/2024/week-52/assets/%D0%A0%D0%B8%D1%81%D1%83%D0%BD%D0%BE%D0%BA_5.png)

> «В частности, когда мы восстанавливаем сверх-активации, средняя точность восстанавливается до 49,94 с 35,14, что указывает на то, что восстановление сверх-активаций спасло примерно 42 % потери качества. Эти результаты показывают, что хотя сверх-активации вносят существенный вклад в производительность модели, они не полностью объясняют общее влияние сверхвеса на качество.»


### Сверх-веса и квантизация
- Сверх-веса и сверх-активации оказывают сильное негативное влияние на квантизацию моделей.
- Предлагается метод квантизации, который явно сохраняет сверх-веса, улучшая качество по сравнению с традиционной квантизацией.
- Для квантизации активаций предлагается заменить сверх-активацию медианным значением, квантовать, а затем восстановить исходное значение.
- Для квантизации весов предлагается клиппировать (ограничивать) выбросы, включая сверхвес, квантовать, а затем восстанавливать сверхвес.


### Экспериментальные результаты
- Эксперименты проведены на различных LLM, включая Llama, Mistral и OLMo.
- Предложенный метод квантизации, учитывающий сверх-веса, показал конкурентоспособные результаты по сравнению с state-of-the-art методами, такими как SmoothQuant.
- Метод позволяет масштабировать блочную квантизацию весов до больших размеров без значительной потери качества.


## Практическое значение
- Выявление критически важных параметров в LLM может привести к более эффективным методам сжатия и оптимизации моделей.
- Предложенный data-free метод идентификации сверхвесов может быть использован для оптимизации квантизации моделей без необходимости в дополнительных обучающих данных.
- Улучшенная квантизация, с учетом сверх-весов, позволяет создавать более компактные и эффективные модели, которые можно использовать в условиях ограниченных ресурсов.


## Заключение
Исследование демонстрирует важность "сверхвесов" и "сверх-активаций" в работе LLM. Эти параметры, несмотря на свою малочисленность, оказывают непропорционально большое влияние на качество модели. Авторы предлагают практические методы для их идентификации и использования для улучшения квантизации моделей. Результаты исследования подчеркивают необходимость дальнейшего изучения и учета сверх-весов для создания более эффективных и надежных LLM.

---

## Глоссарий
- **Большая языковая модель (LLM)**: Модель машинного обучения, обученная на больших объемах текстовых данных и способная генерировать и понимать естественный язык.
- **Супервес**: Отдельный скалярный параметр в LLM, имеющий непропорционально важное значение для работы модели.
- **Суперактивация**: Аномально большое значение активации, возникающее в результате влияния супервеса.
- **Нулевой выстрел (zero-shot)**: Способность модели выполнять задачу без предварительного обучения на этой конкретной задаче.
- **Перплексия**: Мера того, насколько хорошо модель предсказывает следующий токен в последовательности. Чем ниже перплексия, тем лучше модель.
- **Квантование**: Метод снижения точности представления чисел, чтобы уменьшить размер модели и ускорить вычисления.
- **mlp.down_proj**: Слой понижающей проекции в многослойном персептроне (MLP), который является частью архитектуры LLM.
- **Усечение (clipping)**: Метод ограничения диапазона значений, чтобы предотвратить влияние выбросов на процесс квантования.
- **Стоп-слова**: Часто встречающиеся слова (например, "и", "а", "на"), которые обычно не несут значительной семантической информации.
- **Hadamard product**: Поэлементное умножение двух матриц.
- **SmoothQuant**: Метод квантования LLM, использующий масштабирование активаций для снижения влияния выбросов.
- **AWQ (Activation-aware Weight Quantization)**: Метод квантования весов, учитывающий активации, для оптимизации параметров масштабирования.
- **SqueezeLLM**: Метод квантования, использующий разреженную матрицу для сохранения наиболее важных параметров в более высокой точности.
- **Skip connection**: Прямое соединение между слоями, которое пропускает один или несколько промежуточных слоёв.
- **Per-tensor quantization**: Метод квантования, который применяет одинаковые параметры квантования ко всему тензору.
- **Per-token quantization**: Метод квантования, который применяет параметры квантования к каждому токену в отдельности.
- **Gaussian Distribution**: Нормальное распределение, описывающее распределение случайных переменных.
- **Z-score**: Мера того, сколько стандартных отклонений отдельное наблюдение отстоит от среднего.

---

## Краткий Тест

1. **Что такое "супервес" в контексте больших языковых моделей (LLM)?**  
   Супервес - это отдельный скалярный параметр в LLM, который, хотя и не является самым большим по величине, играет непропорционально важную роль в качестве модели. Его удаление может полностью разрушить способность LLM генерировать текст.

2. **Где обычно находятся супервеса в архитектуре LLM, согласно исследованию?**  
   Супервеса обычно обнаруживаются в слое `mlp.down_proj` на ранних этапах архитектуры LLM.

3. **Что такое "суперактивации" и как они связаны с супервесами?**  
   Суперактивации - это аномально большие значения активаций в LLM, которые сохраняются на протяжении многих слоёв. Они возникают как результат усиления входных активаций супервесами.

4. **Каково влияние удаления супервеса на производительность LLM?**  
   Удаление супервеса приводит к резкому падению точности LLM в задачах с нулевым выстрелом (zero-shot), а также к увеличению перплексии на порядки.

5. **Как можно использовать знания о супервесах при квантовании LLM?**  
   Знание о супервесах позволяет их сохранить при квантовании, тогда как другие веса могут быть квантованы с использованием различных методов, таких как усечение (clipping). Это улучшает качество квантованной модели.

6. **Опишите метод идентификации супервесов, представленный в исследовании.**  
   Супервеса можно идентифицировать, обнаруживая пиковые значения в распределениях входов и выходов слоев `mlp.down_proj` при прохождении одного входного запроса.

7. **Как супервеса влияют на распределение вероятностей выходных токенов?**  
   Удаление супервесов приводит к увеличению вероятностей стоп-слов и снижению вероятностей значимых слов, что негативно сказывается на способности модели делать точные и уверенные предсказания.

8. **Как исследование изучает связь между супервесами и суперактивациями?**  
   Исследователи показывают, что удаление супервеса приводит к значительному снижению величины суперактиваций, что говорит о том, что супервеса создают эти аномальные активации.

9. **Каковы результаты экспериментов по восстановлению суперактиваций после удаления супервесов?**  
   Восстановление суперактиваций после удаления супервеса частично восстанавливает качество LLM, но не полностью, что указывает на то, что супервеса влияют на модель не только через суперактивации.

10. **Как результаты исследования относятся к другим методам квантования LLM, таким как SmoothQuant, AWQ, и SqueezeLLM?**  
    Исследование показало, что сохранение супервесов может быть конкурентоспособно с методами, такими как SmoothQuant, при квантовании активаций, а также может позволить использовать большие размеры блоков при квантовании весов, аналогично AWQ и SqueezeLLM, которые также косвенно учитывают важность этих параметров.


---


## **Низкоуровневая математическая формализация (Low-level)**

### 1. Базовые операции и обозначения

1. **Входные данные**:  
   - $X$ — матрица/тензор размерности $(L \times H)$.  
     - $L$ — число позиций во входной последовательности (или размер batch $\times$ длина последовательности).  
     - $H$ — скрытая размерность.  
   - Элемент $X_{i,k}$ означает значение входной активации в строке $i$ и столбце (канале) $k$.  

2. **Матрица весов**:  
   - $W$ — матрица весов размерности $(D \times H)$.  
     - $D$ — выходная размерность, меньше или больше $H$ в зависимости от архитектуры.  
   - Элемент $W_{j,k}$ означает значение веса, которое будет умножаться на $X_{i,k}$ при вычислении соответствующего выхода.  

3. **Выходные активации**:  
   - $Y$ — результат умножения $X$ на $W^\mathsf{T}$, то есть матрица размерности $(L \times D)$.  
   - Элемент $Y_{i,j}$ получается из скалярного произведения $i$-й строки $X$ с $j$-й строкой $W$.  

### 2. Вычисление выходной активации поэлементно

Каждый элемент $Y_{i,j}$ задаётся формулой:

$$
  Y_{i,j} 
  = \sum_{k=1}^{H} \left( X_{i,k} \times W_{j,k} \right).
$$

- Индекс $i$ пробегает $1 \le i \le L$.  
- Индекс $j$ пробегает $1 \le j \le D$.  
- Индекс $k$ пробегает $1 \le k \le H$.

### 3. Что такое «супервес»

1. **Идея**: «Супервес» — это такой элемент $W_{j,m}$, что при умножении на соответствующую активацию $X_{i,m}$ он даёт **непропорционально большой** вклад в выход $Y_{i,j}$.  
2. **Порог доминации**: Для некоего элемента $Y_{i,j}$ смотрим все слагаемые $X_{i,k} \times W_{j,k}$. Если существует $m$ такое, что

$$
  \left|X_{i,m} \times W_{j,m}\right| 
    \gg \sum_{k \neq m} \left| X_{i,k} \times W_{j,k} \right|,
$$

то пара $(j,m)$ «доминирует» в $Y_{i,j}$. 

3. **Обобщение**: Если при разных входных данных $X$ одна и та же пара индексов $(j,m)$ («вес» $W_{j,m}$) многократно даёт такие доминирующие вклады, этот $W_{j,m}$ называется **супервесом**.

### 4. Что такое «суперактивация»

1. **Переход от веса к активации**: Супервес вызывает аномально большой выход $Y_{i,j}$. Этот выход и называют «суперактивацией»: 

$$
  Y_{i,j} \approx X_{i,m} \times W_{j,m},
$$

если вклад одной пары $(k = m)$ во много раз превышает суммарный вклад остальных компонент.

2. **Выброс (outlier)**: Формально можно сказать, что $Y_{i,j}$ есть суперактивация, если $Y_{i,j}$ по модулю выходит за несколько стандартных отклонений от среднего по всем элементам. Например:  

$$
  \left|Y_{i,j}\right| > \mu_Y + \gamma \,\sigma_Y,
$$

где $\mu_Y$ и $\sigma_Y$ — среднее и стандартное отклонение по множеству $\{Y_{i',j'}\}$. Параметр $\gamma$ задаёт, насколько «выброс» должен отличаться от среднего.

### 5. Проверка наличия супервеса

1. **Операция обнуления**: Пусть $\widehat{W}$ — копия $W$, но со сброшенным в 0 кандидатом $W_{j,m}$:  

$$
  \widehat{W}_{j',k'} = 
    \begin{cases}
      0, & \text{если } (j' = j) \text{ и } (k' = m),\\
      W_{j',k'}, & \text{иначе}.
    \end{cases}
$$

2. **Сравнение перплексии**: Если модель с весами $\widehat{W}$ даёт перплексию $PPL_\text{new}$, в разы превышающую исходную $PPL_\text{orig}$, то $W_{j,m}$ — супервес.  

$$
  \text{Если } 
    \frac{PPL_\text{new}}{PPL_\text{orig}} \gg 1,
  \quad 
  \text{тогда } W_{j,m} \text{ — супервес.}
$$

### 6. Распространение суперактивации

В многослойной архитектуре результат $Y$ поступает на вход следующих блоков (через нелинейные функции, skip-connection и т. д.). Суперактивация в $Y_{i,j}$ может частично или полностью сохраниться в дальнейших слоях, вызывая лавинообразный эффект: 

$$
  \tilde{X} = \phi(Y), \quad 
  \tilde{Y} = \tilde{X} \, W_{\text{next}}^\mathsf{T},
$$

где $\phi(\cdot)$ — функция активации (ReLU, GeLU и т. д.).

---

## **Среднеуровневая математическая формализация (Medium-level)**

### 1. Линейная часть MLP и базовое выражение

В блоке MLP есть операция «понижающей проекции» (down_proj), которая берёт вход $X \in \mathbb{R}^{L \times H}$ и умножает его на матрицу весов $W \in \mathbb{R}^{D \times H}$:

$$
  Y = X \, W^\mathsf{T}, 
  \quad \text{где } Y \in \mathbb{R}^{L \times D}.
$$

Элементwise:

$$
  Y_{i,j} = \sum_{k=1}^H X_{i,k} \, W_{j,k}.
$$

### 2. Супервес: определение

**Супервесом** называют вес $W_{j,m}$, при котором вклад в $Y_{i,j}$ от пары $(k=m)$ оказывается значительно больше остальных компонент. Формально, если:

$$
  X_{i,m} \, W_{j,m} 
    \gg 
  \sum_{k \neq m} X_{i,k} \, W_{j,k},
$$

и такая ситуация повторяется для множества $(i,j)$ при разных входных данных, то $W_{j,m}$ — супервес.  

### 3. Суперактивация: определение

Когда вклад $X_{i,m} \, W_{j,m}$ доминирует, результат $Y_{i,j}$ становится **суперактивацией** (то есть аномально высоким значением среди всех $\{Y_{i',j'}\}$). Для оценки можно использовать статистический критерий выброса: сравнить $|\,Y_{i,j}\,|$ с усреднёнными значениями по всему блоку.

### 4. Причинно-следственная проверка

Чтобы проверить, действительно ли $W_{j,m}$ вызывает суперактивацию, в исследовании делают так:

1. «Замораживают» все веса, кроме $W_{j,m}$.  
2. Обнуляют $W_{j,m}$.  
3. Смотрят, как изменится $Y_{i,j}$ и итоговая перплексия модели.  

Если она сильно растёт, а точность (accuracy) в zero-shot задачах падает — значит, $W_{j,m}$ действительно является супервесом.

### 5. Влияние на выходное распределение модели

В языковой модели конечные логиты (перед softmax) зависят от цепочки умножений (всех слоёв). Удаление одного «супервеса» может приводить к сильному искажению вероятностей токенов, росту перплексии и заметному ухудшению генерации.

### 6. Резюме

- **Супервес** — это особо важный параметр $W_{j,m}$, «резонирующий» с некоторой активацией $X_{i,m}$.  
- **Суперактивация** — это пик в $Y_{i,j}$, вызванный доминированием произведения $X_{i,m} \times W_{j,m}$.  
- Удаление такого веса ломает механизм генерации (чаще всего резко поднимает перплексию), что подтверждает «критичность» супервеса.

---

## **Высокоуровневая математическая формализация (High-level)**

### 1. Исходные обозначения и архитектура

Рассмотрим стандартный фрагмент архитектуры большой языковой модели (LLM), а именно многослойный персептрон (MLP) с понижающей проекцией, часто обозначаемой как `mlp.down_proj`. Пусть:

- $X \in \mathbb{R}^{L \times H}$ — матрица (или тензор) входных активаций, где $L$ — длина последовательности (batch size $\times$ количество позиций), а $H$ — размерность скрытого состояния (hidden size).  
- $W \in \mathbb{R}^{D \times H}$ — матрица весов слоя понижающей проекции (down_proj), где $D < H$ (или другое соотношение, зависящее от архитектуры).  
- $Y \in \mathbb{R}^{L \times D}$ — выход слоя понижающей проекции, вычисляется как  

$$
  Y = X\,W^\mathsf{T}.
$$  

Здесь мы для краткости опускаем возможное смещение (bias) и операции нелинейной активации, чтобы сосредоточиться на линейной части, где и возникают «супервеса».

### 2. Математическая формула для выходных активаций

Каждый элемент выходной матрицы $Y$ задаётся формулой:

$$
  Y_{i j} = \sum_{k=1}^{H} \left(X_{i k}\right)\,\left(W_{j k}\right),
  \quad i = 1, \ldots, L,\; j = 1, \ldots, D.
$$

- $i$ индексирует позицию (строка входного тензора $X$),  
- $j$ индексирует позицию (строка в матрице $W$, но обычно соответствует выходному каналу/размерности $D$),  
- $k$ индексирует скрытое измерение $H$.  

### 3. Определение «супервеса» (Super Weight)

#### 3.1. Интуитивное описание

**Супервесом** называется такой элемент $W_{j m}$ в матрице $W$, который, в сочетании со «своим» входным активационным каналом $X_{i m}$, даёт аномально большую компоненту в сумме:

$$
  Y_{i j} = \sum_{k=1}^{H} X_{i k} W_{j k}
  \;\;\approx\;\;
  X_{i m} W_{j m} \quad \text{(если данная пара } (i,m), (j,m) \text{ доминирует в сумме).}
$$

Другими словами, вклад одной пары индексов $(k=m)$ в выходной элемент $Y_{i j}$ может быть настолько большим по модулю, что он на порядки превышает суммарный вклад остальных компонент $k \neq m$. При этом $W_{j m}$ может вовсе не быть самым большим по абсолютной величине среди всех весов (всей матрицы $W$), но при умножении именно на соответствующую активацию $X_{i m}$ даёт резкий всплеск.

#### 3.2. Формальное определение критерия «супервеса»

Пусть $\alpha$ — некоторая пороговая функция (например, функция, вычисляющая выбросы по Z-score или по квантилям), тогда **супервес** формально можно определить так:

1. Считаем вклад пары $(k=m)$ в $Y_{i j}$:

$$
  \Delta_{i j, m} = \left|\,X_{i m} W_{j m}\right|.
$$

2. Пусть $\Delta_{i j}^{(\max)} = \max_{k} \left|\,X_{i k} W_{j k}\right|$.  
   Если

$$
  \Delta_{i j, m} \approx \Delta_{i j}^{(\max)} \quad
  \text{и}
  \quad \Delta_{i j, m} \gg
  \sum_{k \neq m} \left|\,X_{i k} W_{j k}\right|,
$$

то говорим, что в позиции $(i,j)$ «доминирует» пара $(m)$.  

3. Повторяя анализ для всех $(i,j)$, мы находим координаты $(j,m)$ в $W$, которые многократно (или хотя бы в одном важном случае) проявляются как «доминирующие» при разных входах $X$. Если такой $(j,m)$ регулярно даёт резкие выбросы $\Delta_{i j,m}$, то $W_{j,m}$ называют **супервесом**.

#### 3.3. Критическая важность супервеса

- Удаление (обнуление) **одного** такого $W_{j m}$ может привести к резкому ухудшению качества модели: 

$$
  \mathrm{Perplexity}(\text{model with } W_{j m}=0) \gg \mathrm{Perplexity}(\text{original model}),
$$  

  вплоть до выхода «бессмысленного» текста и падения точности в zero-shot задачах.  
- В то же время удаление тысячи или даже нескольких тысяч других (даже крупных) весов может практически **не** сказаться на качестве.  

### 4. Определение «суперактивации» (Super Activation)

#### 4.1. Локальный (поштучный) взгляд

**Суперактивация** возникает на выходах слоя при умножении входа $X$ на вес $W$. Если вектор $X_{i}$ (строка $X$) имеет некоторую компоненту $X_{i,m}$, а соответствующий вес $W_{j,m}$ является «супервесом», то произведение $X_{i,m} \cdot W_{j,m}$ может многократно превышать вклады остальных компонент, формируя «пик» в $\{Y_{i j}\}$:

$$
  Y_{i j} \approx X_{i,m} W_{j,m} \quad \gg \quad \sum_{k \ne m} X_{i,k} W_{j,k}.
$$

Этот «пиковый» элемент $Y_{i j}$ и называется «суперактивацией» на данной позиции $(i,j)$.

#### 4.2. Распространение суперактиваций по слоям

В архитектурах LLM выход $Y$ из одного слоя обычно поступает на вход следующего слоя (с учётом нелинейных функций и/или других преобразований). Если на каком-то слое возникает «суперактивация» (очень большой элемент $Y_{i j}$), она зачастую сохраняется или усиливается при прохождении по следующим слоям из-за пропуска через skip connection или последующие матричные умножения:

$$
  \tilde{X} = \sigma(Y), \quad
  \tilde{Y} = \tilde{X} W_{\text{next}}^\mathsf{T} \quad \dots
$$

где $\sigma$ — некоторая нелинейная функция (например, GeLU, ReLU и т.п.). Если $\sigma$ пропускает большой положительный пик почти без усечения, то «суперактивация» может «просачиваться» дальше. В итоге один «супервес» может порождать целую цепочку сильных выбросов в активациях.

#### 4.3. Причинно-следственная связь

Для проверки, что «суперактивация» возникает именно из-за «супервеса», в исследовании делают следующее:
1. Ищут всплеск $\max_{i,j} \left|\,Y_{i j}\right|$ на выходе слоя.
2. «Обнуляют» подозрительный вес $W_{j m}$.
3. Перезапускают прямой проход (forward pass). Если «пик» сильно уменьшается, то, значит, была прямая причинная связь между этим весом и возникшей «суперактивацией».

### 5. Влияние на перплексию и распределение вероятностей слов

В больших языковых моделях выход каждого блока MLP влияет на итоговое распределение вероятностей токенов (слов) в выходной последовательности. Формально, в авторегрессионной схеме:

$$
  p(\text{next token} \mid \text{context}) = \mathrm{softmax}(\mathrm{transformer\_blocks}(X)).
$$

Если на каком-то этапе вычисления были обнулены «супервеса», мы получаем иной набор выходных векторов $\mathrm{transformer\_blocks}(X)$. Это влияет на «логиты» перед softmax и, в частности, может:

- Увеличить вероятность «стоп-слов» (часто встречающихся слов наподобие «the», «и», «а» и т.п.).  
- Уменьшить вероятность «содержательных» (ключевых) слов.  
- Повысить перплексию ($\mathrm{Perplexity}$), которая, грубо говоря, показывает, насколько модель хорошо предсказывает следующие токены.

### 6. Формальное представление для вклада «супервеса» в активацию

Допустим, мы рассмотрим элемент $(i,j)$ выхода $Y$. В общем виде:

$$
  Y_{i j} = \sum_{k=1}^H X_{i k} W_{j k}.
$$

Если существует такое $m$, что:

$$
  X_{i m} W_{j m} \gg \sum_{k \neq m} X_{i k} W_{j k},
$$

то зададим число, показывающее «относительную доминацию»:

$$
  \rho_{i j} =
    \frac{\left| X_{i m} W_{j m}\right|}{\sum_{k=1}^H \left| X_{i k} W_{j k}\right|}.
$$

Если $\rho_{i j} \approx 1$, мы говорим, что $(m)$ «захватывает» (доминирует) данную активацию.

Аналогично, если для некоторых $(j,m)$ такая ситуация происходит **повсеместно** (или во множестве позиций $i$), то $W_{j,m}$ является «супервесом» и формирует «суперактивации» $Y_{i j}$.

### 7. Критерии «суперактивации»

Можно ввести пороговый критерий для «суперактивации», опираясь на статистику по всем элементам $Y$. Например, если

$$
  \left|\,Y_{i j}\right| > \mu_Y + \gamma \cdot \sigma_Y,
$$

где $\mu_Y$ и $\sigma_Y$ — среднее и стандартное отклонение для всех элементов $Y$ при данном входе, а $\gamma$ — некоторый заданный порог (например, 3 или 5), то $Y_{i j}$ считается аномально большим (outlier), т.е. «суперактивацией». Дальше проверяют, действительно ли она обусловлена определённым $W_{j m}$.

### 8. Заключительные замечания о роли супервесов и суперактиваций

1. **Супервеса** могут не быть крупнейшими по модулю в матрице $W$, но при этом «резонировать» с большими входными значениями $X_{i m}$, формируя «суперактивации».  
2. **Суперактивации** прослеживаются по слоям и способны сохраняться при прохождении через функции активации и skip connection’ы.  
3. Удаление супервеса приводит к исчезновению (или сильному снижению) суперактивации и резкому ухудшению итога (качества генерации, точности в zero-shot задачах и т.д.).  
4. Сохранение супервесов при квантовании (либо особое обращение с ними, например, «разжатием» в более высокую точность) помогает избежать катастрофической деградации модели.  

---

## Итоговое резюме

- **Супервес** (Super Weight) в модели — это вес (скаляр) $W_{j m}$ в матрице $\mathrm{down\_proj}$ (или в другом слое), вклад которого в активации $Y_{i j}$ **доминантен** и ведёт к появлению «суперактиваций».  
- **Суперактивация** (Super Activation) — это чрезвычайно большая активация (элемент $Y_{i j}$), вызванная доминированием пары $(X_{i m}, W_{j m})$.  
- Удаление одного супервеса обрушивает качество модели, поскольку «суперактивации» исчезают или резко уменьшаются, что подтверждает причинно-следственную связь.  
- В формализме линейных преобразований $Y = XW^\mathsf{T}$, супервесы — это пары индексов $(j,m)$, при которых произведение $X_{i m} \cdot W_{j m}$ оказывается в разы выше суммы всех остальных компонент для данной позиции $(i,j)$.  

Таким образом, «супервеса» и «суперактивации» — это феномен взаимодействия большого (или «резонансного») веса и специфических входных активаций, что в конечном итоге критически влияет на производительность всей большой языковой модели.
