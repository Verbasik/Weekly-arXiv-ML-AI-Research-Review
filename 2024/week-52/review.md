## Брифинг-документ: "Сверхвеса" в больших языковых моделях

## Введение

Обзор посвящен исследованию, на тему "сверхвеса" (super weights) в больших языковых моделях (LLM). Авторы обнаружили, что очень небольшое количество параметров (вплоть до одного скаляра!) в LLM играет непропорционально важную роль в их способности генерировать качественный текст. Ресерчеры из Apple утверждают, что крошечное подмножество, максимум шесть масштабирующих факторов, важнее остальных. Авторы называют их супервесами, и их обрезка разрушает качество модели.

Несколько статей в прошлом показали, что в определенном масштабе небольшой набор скрытых признаков состояния содержит выбросы с огромной величиной. Эти выбросы составляют небольшой процент всех активаций, но имеют решающее значение для сохранения качества сжатой модели. В контексте LLM эти выбросы проявляются как "сверх-активации" (super activations) – аномально большие активации, которые также критически важны для качества модели. Удаление этих "сверхвесов" может полностью разрушить модель, снижая точность до уровня случайного угадывания и увеличивая перплексию на несколько порядков.

Исследование также показывает, что эти "сверхвеса" и "сверх-активации" могут быть идентифицированы с помощью простого, не требующего данных метода. Этот метод предлагается для использования в улучшении квантизации моделей, что позволяет сохранить их качество даже при значительном снижении вычислительной сложности.


## Основные результаты и идеи

### Сверхвеса (Super Weights)
- Авторы обнаружили, что один единственный параметр ("сверхвес") в LLM имеет непропорционально большое влияние на качество модели.

- Удаление этого параметра может привести к генерации бессмысленного текста, как качественно, так и количественно (показано на примере Llama-7B на рисунке 1 и в таблице 1).

- Важно отметить, что удаление даже 7000 других самых больших по величине параметров влияет на качество незначительно по сравнению с удалением одного "сверхвеса".

**_Внутри обученных LLM находится группа весов-аутлаеров с большой магнитудой, они могут составлять порядка 0.01% от всех весов модели, что в случае миллиардных моделей всё равно сотни тысяч. Это было известно ранее. В текущей работе показывают, что внутри этой группы находится один единственный вес (тот самый super weight, SW), не обязательно самый большой, важность которого превышает суммарную важность тысяч других аутлаеров. Он необходим для качества, без него LLM не может генерить нормальный текст. Перплексия вырастает на несколько порядков, а точность на zero-shot задачах падает до рандома._**

![Рисунок_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/main/2024/week-52/assets/%D0%A0%D0%B8%D1%81%D1%83%D0%BD%D0%BE%D0%BA_1.png)

![Таблица_1](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/main/2024/week-52/assets/%D0%A2%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D0%B0_1.png)

> «В Llama-7B удаление сверхвеса, одного скаляра, полностью разрушает способность модели генерировать текст. Средняя точность задач с нулевой выборкой фактически падает до нуля. И наоборот, удаление других 7000 крупнейших выбросов, включая выбросы, которые больше, чем сверхвес, влияет не более чем на несколько процентных пунктов.»


## Идентификация сверхвесов

### Основной метод

Предлагается data-free метод идентификации сверхвесов, который не требует наличия тестового набора данных или примеров использования. Метод основан на следующих принципах:

- Анализ распределения активаций в прямом проходе модели
- Выявление скачков в распределении входов и выходов слоев `mlp.down_proj`
- Использование только одного входного запроса для обнаружения
- Авторы предоставляют каталог координат сверхвесов для нескольких общедоступных LLM (Таблица 2).


**Определение координат сверхвеса:**
  - Строка определяется по индексу канала входного распределения активаций
  - Столбец определяется по индексу канала выходного распределения активаций

**Характеристики сверхвесов:**
  - Не обязательно являются максимальными по абсолютной величине в матрице весов
  - Могут быть обнаружены путем подачи произвольного запроса
  - Для уменьшения активации достаточно обрезать один вес

**Распределение в моделях:**
  - Максимальное количество сверхвесов (шесть) обнаружено в модели Phi-3-mini-4k-instruct
  - Позиции сверхвесов сохраняются при тонкой настройке моделей с помощью инструкций

![Таблица_2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/main/2024/week-52/assets/%D0%A2%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D0%B0_2.png)

> «Основываясь на приведенном выше анализе, мы представляем эффективный способ локализации сверхвесов: SW можно найти путем обнаружения скачков в распределениях входов и выходов down_proj по слоям. Это обнаружение требует только одного входного запроса, а не набора проверочных данных или примеров использования.»


### Сверх-активации (Super Activations)
- Сверхвеса вызывают "сверх-активации" - очень большие активации, которые сохраняются на протяжении многих слоев модели в одном и том же положении, независимо от входных данных.
- Эти сверх-активации играют ключевую роль в функционировании модели.
- Удаление сверхвеса резко снижает величину сверх-активации, подтверждая причинно-следственную связь.

**_Ранее (https://arxiv.org/abs/2402.17762) были найдены супер-активации, критичные для качества. Они существуют в различных слоях, имеют константную магнитуду и всегда обнаруживаются в одинаковой позиции несмотря на вход. Текущая работа находит, что канал активации совпадает с оным для супер веса и сперва активация обнаруживается сразу после супер веса. Прунинг этого супер веса значительно уменьшает активацию, так что вероятно активация вызвана им, а не просто скоррелирована. Такие активации называются супер активациями (super activations, SA)._**

**_Предыдущая работа объясняла супер активации через bias terms, но не объясняла как они получаются и почему на одних и тех же местах. Сейчас авторы эмпирически нашли, что до down проекции (down_proj) произведение Адамара (Hadamard product) gate и up проекций (gate_proj, up_proj) создаёт относительно большую активацию. Супер вес далее усиливает её ещё и даёт супер активацию._**

**Напомню, что MLP блок в Ламе выглядит так:**

```
out = down_proj( act_fn(gate_proj(input)) x up_proj(input) )
```

![Рисунок_4](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/main/2024/week-52/assets/%D0%A0%D0%B8%D1%81%D1%83%D0%BD%D0%BE%D0%BA_4.png)

> «Мы обнаруживаем еще одно интригующее свойство: канал активации соответствует нашему сверхвесу, и активация появляется сразу после нашего сверхвеса. Чтобы подтвердить, является ли это корреляцией или причинно-следственной связью, мы удаляем сверхвес и проверяем величину массивной активации. На рисунке 4 мы обнаруживаем, что удаление сверхвеса резко снижает величину массивной активации. Это говорит о том, что массивные активации создаются сверхвесами. Для последовательности мы называем эти массивные активации «сверх-активациями».»


### Механизмы действия сверхвесов
- Сверхвеса, помимо создания сверх-активаций, подавляют вероятность стоп-слов в выходных данных модели (Рисунок 2, 5).
- Удаление сверхвесов приводит к увеличению вероятности стоп-слов и уменьшению вероятности значимых слов.
- Восстановление сверх-активаций частично восстанавливает качество модели после удаления сверхвеса, но не полностью.
- Провели эксперименты по обнулению SW, в том числе с восстановлением SA до исходного значения, чтобы проверить влияние SW на другие активации. Это восстанавливает 42% потери, то есть влияние SW на качество выше, чем просто через SA.
- По анализу 500 различных промптов из Lambaba validation set видно, что при убирании SW вероятности стоп-слов сильно возрастают (а обычные слова соответственно занижаются). Для “the” это 2×, для “.” -- 5×, и для “,” -- 10×. То есть наличие SW как бы подавляет стоп-слова и позволяет генерировать осмысленный текст.

![Рисунок_2](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/main/2024/week-52/assets/%D0%A0%D0%B8%D1%81%D1%83%D0%BD%D0%BE%D0%BA_2.png)

![Рисунок_5](https://raw.githubusercontent.com/Verbasik/Weekly-arXiv-ML-AI-Research-Review/refs/heads/main/2024/week-52/assets/%D0%A0%D0%B8%D1%81%D1%83%D0%BD%D0%BE%D0%BA_5.png)

> «В частности, когда мы восстанавливаем сверх-активации, средняя точность восстанавливается до 49,94 с 35,14, что указывает на то, что восстановление сверх-активаций спасло примерно 42 % потери качества. Эти результаты показывают, что хотя сверх-активации вносят существенный вклад в производительность модели, они не полностью объясняют общее влияние сверхвеса на качество.»


### Сверх-веса и квантизация
- Сверх-веса и сверх-активации оказывают сильное негативное влияние на квантизацию моделей.
- Предлагается метод квантизации, который явно сохраняет сверх-веса, улучшая качество по сравнению с традиционной квантизацией.
- Для квантизации активаций предлагается заменить сверх-активацию медианным значением, квантовать, а затем восстановить исходное значение.
- Для квантизации весов предлагается клиппировать (ограничивать) выбросы, включая сверхвес, квантовать, а затем восстанавливать сверхвес.


### Экспериментальные результаты
- Эксперименты проведены на различных LLM, включая Llama, Mistral и OLMo.
- Предложенный метод квантизации, учитывающий сверх-веса, показал конкурентоспособные результаты по сравнению с state-of-the-art методами, такими как SmoothQuant.
- Метод позволяет масштабировать блочную квантизацию весов до больших размеров без значительной потери качества.


## Практическое значение
- Выявление критически важных параметров в LLM может привести к более эффективным методам сжатия и оптимизации моделей.
- Предложенный data-free метод идентификации сверхвесов может быть использован для оптимизации квантизации моделей без необходимости в дополнительных обучающих данных.
- Улучшенная квантизация, с учетом сверх-весов, позволяет создавать более компактные и эффективные модели, которые можно использовать в условиях ограниченных ресурсов.


## Заключение
Исследование демонстрирует важность "сверхвесов" и "сверх-активаций" в работе LLM. Эти параметры, несмотря на свою малочисленность, оказывают непропорционально большое влияние на качество модели. Авторы предлагают практические методы для их идентификации и использования для улучшения квантизации моделей. Результаты исследования подчеркивают необходимость дальнейшего изучения и учета сверх-весов для создания более эффективных и надежных LLM.

---

## Глоссарий
- **Большая языковая модель (LLM)**: Модель машинного обучения, обученная на больших объемах текстовых данных и способная генерировать и понимать естественный язык.
- **Супервес**: Отдельный скалярный параметр в LLM, имеющий непропорционально важное значение для работы модели.
- **Суперактивация**: Аномально большое значение активации, возникающее в результате влияния супервеса.
- **Нулевой выстрел (zero-shot)**: Способность модели выполнять задачу без предварительного обучения на этой конкретной задаче.
- **Перплексия**: Мера того, насколько хорошо модель предсказывает следующий токен в последовательности. Чем ниже перплексия, тем лучше модель.
- **Квантование**: Метод снижения точности представления чисел, чтобы уменьшить размер модели и ускорить вычисления.
- **mlp.down_proj**: Слой понижающей проекции в многослойном персептроне (MLP), который является частью архитектуры LLM.
- **Усечение (clipping)**: Метод ограничения диапазона значений, чтобы предотвратить влияние выбросов на процесс квантования.
- **Стоп-слова**: Часто встречающиеся слова (например, "и", "а", "на"), которые обычно не несут значительной семантической информации.
- **Hadamard product**: Поэлементное умножение двух матриц.
- **SmoothQuant**: Метод квантования LLM, использующий масштабирование активаций для снижения влияния выбросов.
- **AWQ (Activation-aware Weight Quantization)**: Метод квантования весов, учитывающий активации, для оптимизации параметров масштабирования.
- **SqueezeLLM**: Метод квантования, использующий разреженную матрицу для сохранения наиболее важных параметров в более высокой точности.
- **Skip connection**: Прямое соединение между слоями, которое пропускает один или несколько промежуточных слоёв.
- **Per-tensor quantization**: Метод квантования, который применяет одинаковые параметры квантования ко всему тензору.
- **Per-token quantization**: Метод квантования, который применяет параметры квантования к каждому токену в отдельности.
- **Gaussian Distribution**: Нормальное распределение, описывающее распределение случайных переменных.
- **Z-score**: Мера того, сколько стандартных отклонений отдельное наблюдение отстоит от среднего.

---

## Краткий Тест

1. **Что такое "супервес" в контексте больших языковых моделей (LLM)?**  
   Супервес - это отдельный скалярный параметр в LLM, который, хотя и не является самым большим по величине, играет непропорционально важную роль в качестве модели. Его удаление может полностью разрушить способность LLM генерировать текст.

2. **Где обычно находятся супервеса в архитектуре LLM, согласно исследованию?**  
   Супервеса обычно обнаруживаются в слое `mlp.down_proj` на ранних этапах архитектуры LLM.

3. **Что такое "суперактивации" и как они связаны с супервесами?**  
   Суперактивации - это аномально большие значения активаций в LLM, которые сохраняются на протяжении многих слоёв. Они возникают как результат усиления входных активаций супервесами.

4. **Каково влияние удаления супервеса на производительность LLM?**  
   Удаление супервеса приводит к резкому падению точности LLM в задачах с нулевым выстрелом (zero-shot), а также к увеличению перплексии на порядки.

5. **Как можно использовать знания о супервесах при квантовании LLM?**  
   Знание о супервесах позволяет их сохранить при квантовании, тогда как другие веса могут быть квантованы с использованием различных методов, таких как усечение (clipping). Это улучшает качество квантованной модели.

6. **Опишите метод идентификации супервесов, представленный в исследовании.**  
   Супервеса можно идентифицировать, обнаруживая пиковые значения в распределениях входов и выходов слоев `mlp.down_proj` при прохождении одного входного запроса.

7. **Как супервеса влияют на распределение вероятностей выходных токенов?**  
   Удаление супервесов приводит к увеличению вероятностей стоп-слов и снижению вероятностей значимых слов, что негативно сказывается на способности модели делать точные и уверенные предсказания.

8. **Как исследование изучает связь между супервесами и суперактивациями?**  
   Исследователи показывают, что удаление супервеса приводит к значительному снижению величины суперактиваций, что говорит о том, что супервеса создают эти аномальные активации.

9. **Каковы результаты экспериментов по восстановлению суперактиваций после удаления супервесов?**  
   Восстановление суперактиваций после удаления супервеса частично восстанавливает качество LLM, но не полностью, что указывает на то, что супервеса влияют на модель не только через суперактивации.

10. **Как результаты исследования относятся к другим методам квантования LLM, таким как SmoothQuant, AWQ, и SqueezeLLM?**  
    Исследование показало, что сохранение супервесов может быть конкурентоспособно с методами, такими как SmoothQuant, при квантовании активаций, а также может позволить использовать большие размеры блоков при квантовании весов, аналогично AWQ и SqueezeLLM, которые также косвенно учитывают важность этих параметров.


---


## **Математическая формализация**

### 1. Базовые операции и обозначения

1. **Входные данные**:  
   - $X$ — матрица/тензор размерности $(L \times H)$.  
     - $L$ — число позиций во входной последовательности (или размер batch $\times$ длина последовательности).  
     - $H$ — скрытая размерность.  
   - Элемент $X_{i,k}$ означает значение входной активации в строке $i$ и столбце (канале) $k$.  

2. **Матрица весов**:  
   - $W$ — матрица весов размерности $(D \times H)$.  
     - $D$ — выходная размерность, меньше или больше $H$ в зависимости от архитектуры.  
   - Элемент $W_{j,k}$ означает значение веса, которое будет умножаться на $X_{i,k}$ при вычислении соответствующего выхода.  

3. **Выходные активации**:  
   - $Y$ — результат умножения $X$ на $W^\mathsf{T}$, то есть матрица размерности $(L \times D)$.  
   - Элемент $Y_{i,j}$ получается из скалярного произведения $i$-й строки $X$ с $j$-й строкой $W$.  

### 2. Вычисление выходной активации поэлементно

Каждый элемент $Y_{i,j}$ задаётся формулой:

$$
  Y_{i,j} 
  = \sum_{k=1}^{H} \left( X_{i,k} \times W_{j,k} \right).
$$

- Индекс $i$ пробегает $1 \le i \le L$.  
- Индекс $j$ пробегает $1 \le j \le D$.  
- Индекс $k$ пробегает $1 \le k \le H$.

### **_Давайте разберем эту концепцию на конкретном примере с матрицами 3x3, имитируя вычисления внутри LLM Transformer._**

  **Предположим, у нас есть следующие матрицы:**
  
  **1. Входная матрица X (размерность 3x3):**
  
  Представим, что это активации для трех последовательных слов в предложении, где каждое слово представлено вектором скрытой размерности 3.
  
  ```
  X = [[1, 2, 3],  # Активации для первого слова
       [4, 5, 6],  # Активации для второго слова
       [7, 8, 9]]  # Активации для третьего слова
  ```
  
  Здесь:
  - $L = 3$ (количество позиций/слов)
  - $H = 3$ (скрытая размерность)
  - $X_{1,1} = 1$, $X_{1,2} = 2$, $X_{1,3} = 3$
  - $X_{2,1} = 4$, $X_{2,2} = 5$, $X_{2,3} = 6$
  - $X_{3,1} = 7$, $X_{3,2} = 8$, $X_{3,3} = 9$
  
  **2. Матрица весов W (размерность 3x3):**
  
  Представим, что это веса слоя линейного преобразования в Transformer, который преобразует входные активации в новое пространство той же размерности.
  
  ```
  W = [[0.1, 0.2, 0.3],  # Веса для первого выходного канала
       [0.4, 0.5, 0.6],  # Веса для второго выходного канала
       [0.7, 0.8, 0.9]]  # Веса для третьего выходного канала
  ```
  
  Здесь:
  - $D = 3$ (выходная размерность)
  - $H = 3$ (скрытая размерность)
  - $W_{1,1} = 0.1$, $W_{1,2} = 0.2$, $W_{1,3} = 0.3$
  - $W_{2,1} = 0.4$, $W_{2,2} = 0.5$, $W_{2,3} = 0.6$
  - $W_{3,1} = 0.7$, $W_{3,2} = 0.8$, $W_{3,3} = 0.9$
  
  **3. Вычисление выходной матрицы Y (размерность 3x3):**
  
  Теперь давайте вычислим элементы матрицы $Y$ поэлементно, используя формулу:
  
  $$
    Y_{i,j} = \sum_{k=1}^{H} \left( X_{i,k} \times W_{j,k} \right)
  $$
  
  **Вычисление $Y_{1,1}$:**
  
  Здесь $i=1$, $j=1$. Мы берем первую строку матрицы $X$ и первую строку матрицы $W$:
  
  $Y_{1,1} = (X_{1,1} \times W_{1,1}) + (X_{1,2} \times W_{1,2}) + (X_{1,3} \times W_{1,3})$
  
  $Y_{1,1} = (1 \times 0.1) + (2 \times 0.2) + (3 \times 0.3)$
  
  $Y_{1,1} = 0.1 + 0.4 + 0.9$
  
  $Y_{1,1} = 1.4$
  
  **Вычисление $Y_{1,2}$:**
  
  Здесь $i=1$, $j=2$. Мы берем первую строку матрицы $X$ и вторую строку матрицы $W$:
  
  $Y_{1,2} = (X_{1,1} \times W_{2,1}) + (X_{1,2} \times W_{2,2}) + (X_{1,3} \times W_{2,3})$
  
  $Y_{1,2} = (1 \times 0.4) + (2 \times 0.5) + (3 \times 0.6)$
  
  $Y_{1,2} = 0.4 + 1.0 + 1.8$
  
  $Y_{1,2} = 3.2$
  
  **Вычисление $Y_{1,3}$:**
  
  Здесь $i=1$, $j=3$. Мы берем первую строку матрицы $X$ и третью строку матрицы $W$:
  
  $Y_{1,3} = (X_{1,1} \times W_{3,1}) + (X_{1,2} \times W_{3,2}) + (X_{1,3} \times W_{3,3})$
  
  $Y_{1,3} = (1 \times 0.7) + (2 \times 0.8) + (3 \times 0.9)$
  
  $Y_{1,3} = 0.7 + 1.6 + 2.7$
  
  $Y_{1,3} = 5.0$
  
  **Вычисление $Y_{2,1}$:**
  
  Здесь $i=2$, $j=1$. Мы берем вторую строку матрицы $X$ и первую строку матрицы $W$:
  
  $Y_{2,1} = (X_{2,1} \times W_{1,1}) + (X_{2,2} \times W_{1,2}) + (X_{2,3} \times W_{1,3})$
  
  $Y_{2,1} = (4 \times 0.1) + (5 \times 0.2) + (6 \times 0.3)$
  
  $Y_{2,1} = 0.4 + 1.0 + 1.8$
  
  $Y_{2,1} = 3.2$
  
  **Вычисление $Y_{2,2}$:**
  
  Здесь $i=2$, $j=2$. Мы берем вторую строку матрицы $X$ и вторую строку матрицы $W$:
  
  $Y_{2,2} = (X_{2,1} \times W_{2,1}) + (X_{2,2} \times W_{2,2}) + (X_{2,3} \times W_{2,3})$
  
  $Y_{2,2} = (4 \times 0.4) + (5 \times 0.5) + (6 \times 0.6)$
  
  $Y_{2,2} = 1.6 + 2.5 + 3.6$
  
  $Y_{2,2} = 7.7$
  
  **Вычисление $Y_{2,3}$:**
  
  Здесь $i=2$, $j=3$. Мы берем вторую строку матрицы $X$ и третью строку матрицы $W$:
  
  $Y_{2,3} = (X_{2,1} \times W_{3,1}) + (X_{2,2} \times W_{3,2}) + (X_{2,3} \times W_{3,3})$
  
  $Y_{2,3} = (4 \times 0.7) + (5 \times 0.8) + (6 \times 0.9)$
  
  $Y_{2,3} = 2.8 + 4.0 + 5.4$
  
  $Y_{2,3} = 12.2$
  
  **Вычисление $Y_{3,1}$:**
  
  Здесь $i=3$, $j=1$. Мы берем третью строку матрицы $X$ и первую строку матрицы $W$:
  
  $Y_{3,1} = (X_{3,1} \times W_{1,1}) + (X_{3,2} \times W_{1,2}) + (X_{3,3} \times W_{1,3})$
  
  $Y_{3,1} = (7 \times 0.1) + (8 \times 0.2) + (9 \times 0.3)$
  
  $Y_{3,1} = 0.7 + 1.6 + 2.7$
  
  $Y_{3,1} = 5.0$
  
  **Вычисление $Y_{3,2}$:**
  
  Здесь $i=3$, $j=2$. Мы берем третью строку матрицы $X$ и вторую строку матрицы $W$:
  
  $Y_{3,2} = (X_{3,1} \times W_{2,1}) + (X_{3,2} \times W_{2,2}) + (X_{3,3} \times W_{2,3})$
  
  $Y_{3,2} = (7 \times 0.4) + (8 \times 0.5) + (9 \times 0.6)$
  
  $Y_{3,2} = 2.8 + 4.0 + 5.4$
  
  $Y_{3,2} = 12.2$
  
  **Вычисление $Y_{3,3}$:**
  
  Здесь $i=3$, $j=3$. Мы берем третью строку матрицы $X$ и третью строку матрицы $W$:
  
  $Y_{3,3} = (X_{3,1} \times W_{3,1}) + (X_{3,2} \times W_{3,2}) + (X_{3,3} \times W_{3,3})$
  
  $Y_{3,3} = (7 \times 0.7) + (8 \times 0.8) + (9 \times 0.9)$
  
  $Y_{3,3} = 4.9 + 6.4 + 8.1$
  
  $Y_{3,3} = 19.4$
  
  **4. Результирующая матрица Y:**
  
  После всех вычислений мы получаем выходную матрицу $Y$:
  
  ```
  Y = [[1.4, 3.2, 5.0],
       [3.2, 7.7, 12.2],
       [5.0, 12.2, 19.4]]
  ```

### 3. Что такое «супервес»

1. **Идея**: «Супервес» — это такой элемент $W_{j,m}$, что при умножении на соответствующую активацию $X_{i,m}$ он даёт **непропорционально большой** вклад в выход $Y_{i,j}$.  
2. **Порог доминации**: Для некоего элемента $Y_{i,j}$ смотрим все слагаемые $X_{i,k} \times W_{j,k}$. Если существует $m$ такое, что

$$
  \left|X_{i,m} \times W_{j,m}\right| 
    \gg \sum_{k \neq m} \left| X_{i,k} \times W_{j,k} \right|,
$$

то пара $(j,m)$ «доминирует» в $Y_{i,j}$. 

3. **Обобщение**: Если при разных входных данных $X$ одна и та же пара индексов $(j,m)$ («вес» $W_{j,m}$) многократно даёт такие доминирующие вклады, этот $W_{j,m}$ называется **супервесом**.

## **_Давайте рассмотрим концепцию "супервеса" на примере, используя матрицы 3x3, как и в прошлый раз._**

**Возьмем ту же входную матрицу X:**

```
X = [[1, 2, 3],
     [4, 5, 6],
     [7, 8, 9]]
```

**Теперь модифицируем матрицу весов W, чтобы продемонстрировать "супервес".**  Предположим, мы хотим, чтобы вес $W_{1,1}$ оказывал доминирующее влияние. Сделаем его значительно больше остальных весов в первой строке матрицы $W$:

```
W = [[10.0, 0.2, 0.3],  # Заметно увеличенный вес W_{1,1}
     [0.4, 0.5, 0.6],
     [0.7, 0.8, 0.9]]
```

Здесь $W_{1,1} = 10.0$ значительно больше, чем $W_{1,2} = 0.2$ и $W_{1,3} = 0.3$.

**Вычислим выходную матрицу Y с этой модифицированной матрицей W:**

Давайте сосредоточимся на вычислении элементов первой строки матрицы $Y$, так как именно первая строка $W$ содержит "супервес".

**Вычисление $Y_{1,1}$:**

$Y_{1,1} = (X_{1,1} \times W_{1,1}) + (X_{1,2} \times W_{1,2}) + (X_{1,3} \times W_{1,3})$

$Y_{1,1} = (1 \times 10.0) + (2 \times 0.2) + (3 \times 0.3)$

$Y_{1,1} = 10.0 + 0.4 + 0.9$

$Y_{1,1} = 11.3$

Теперь посмотрим на вклад каждого слагаемого:
- $|X_{1,1} \times W_{1,1}| = |1 \times 10.0| = 10.0$
- $|X_{1,2} \times W_{1,2}| = |2 \times 0.2| = 0.4$
- $|X_{1,3} \times W_{1,3}| = |3 \times 0.3| = 0.9$

Сравним доминирующий вклад с суммой остальных:

$|X_{1,1} \times W_{1,1}| = 10.0$

$\sum_{k \neq 1} |X_{1,k} \times W_{1,k}| = |0.4| + |0.9| = 1.3$

Видно, что $10.0 \gg 1.3$. Таким образом, для $Y_{1,1}$ пара индексов $(j=1, m=1)$ доминирует.

**Вычисление $Y_{1,2}$:**

$Y_{1,2} = (X_{1,1} \times W_{2,1}) + (X_{1,2} \times W_{2,2}) + (X_{1,3} \times W_{2,3})$

$Y_{1,2} = (1 \times 0.4) + (2 \times 0.5) + (3 \times 0.6)$

$Y_{1,2} = 0.4 + 1.0 + 1.8$

$Y_{1,2} = 3.2$

Здесь нет явного доминирования одного слагаемого.

**Вычисление $Y_{1,3}$:**

$Y_{1,3} = (X_{1,1} \times W_{3,1}) + (X_{1,2} \times W_{3,2}) + (X_{1,3} \times W_{3,3})$

$Y_{1,3} = (1 \times 0.7) + (2 \times 0.8) + (3 \times 0.9)$

$Y_{1,3} = 0.7 + 1.6 + 2.7$

$Y_{1,3} = 5.0$

Здесь также нет явного доминирования одного слагаемого.

**Теперь рассмотрим другой элемент, где "супервес" может проявиться, например, $Y_{2,1}$:**

$Y_{2,1} = (X_{2,1} \times W_{1,1}) + (X_{2,2} \times W_{1,2}) + (X_{2,3} \times W_{1,3})$

$Y_{2,1} = (4 \times 10.0) + (5 \times 0.2) + (6 \times 0.3)$

$Y_{2,1} = 40.0 + 1.0 + 1.8$

$Y_{2,1} = 42.8$

Сравним вклады:
- $|X_{2,1} \times W_{1,1}| = |4 \times 10.0| = 40.0$
- $|X_{2,2} \times W_{1,2}| = |5 \times 0.2| = 1.0$
- $|X_{2,3} \times W_{1,3}| = |6 \times 0.3| = 1.8$

Сравним доминирующий вклад с суммой остальных:

$|X_{2,1} \times W_{1,1}| = 40.0$

$\sum_{k \neq 1} |X_{2,k} \times W_{1,k}| = |1.0| + |1.8| = 2.8$

Здесь также $40.0 \gg 2.8$, и пара индексов $(j=1, m=1)$ доминирует в $Y_{2,1}$.

**Обобщение на "супервес":**

В нашем примере, вес $W_{1,1} = 10.0$ является кандидатом на "супервес". Если бы мы подавали на вход разные матрицы $X$, и при вычислении различных элементов $Y_{i,j}$ мы бы многократно наблюдали, что вклад с участием $W_{1,1}$ (то есть слагаемое $X_{i,1} \times W_{1,1}$) значительно превышает сумму остальных слагаемых, то мы бы классифицировали $W_{1,1}$ как "супервес".

**Пример с разными входными данными:**

Предположим, у нас есть другая входная матрица $X'$:

```
X' = [[2, 1, 0],
      [5, 0, 1],
      [9, 2, 3]]
```

Рассмотрим вычисление $Y'_{1,1}$ с использованием той же матрицы $W$:

$Y'_{1,1} = (X'_{1,1} \times W_{1,1}) + (X'_{1,2} \times W_{1,2}) + (X'_{1,3} \times W_{1,3})$

$Y'_{1,1} = (2 \times 10.0) + (1 \times 0.2) + (0 \times 0.3)$

$Y'_{1,1} = 20.0 + 0.2 + 0.0$

$Y'_{1,1} = 20.2$

Вклады:
- $|X'_{1,1} \times W_{1,1}| = |2 \times 10.0| = 20.0$
  
- $|X'_{1,2} \times W_{1,2}| = |1 \times 0.2| = 0.2$
  
- $|X'_{1,3} \times W_{1,3}| = |0 \times 0.3| = 0.0$

Сравнение: $20.0 \gg 0.2 + 0.0$. Пара $(j=1, m=1)$ снова доминирует.

Если такая ситуация повторяется для множества различных входных данных $X$, то вес $W_{1,1}$ будет уверенно классифицирован как "супервес", так как он оказывает непропорционально большое влияние на выходные значения.

### 4. Что такое «суперактивация»

1. **Переход от веса к активации**: Супервес вызывает аномально большой выход $Y_{i,j}$. Этот выход и называют «суперактивацией»: 

$$
  Y_{i,j} \approx X_{i,m} \times W_{j,m},
$$

если вклад одной пары $(k = m)$ во много раз превышает суммарный вклад остальных компонент.

2. **Выброс (outlier)**: Формально можно сказать, что $Y_{i,j}$ есть суперактивация, если $Y_{i,j}$ по модулю выходит за несколько стандартных отклонений от среднего по всем элементам. Например:  

$$
  \left|Y_{i,j}\right| > \mu_Y + \gamma \,\sigma_Y,
$$

где $\mu_Y$ и $\sigma_Y$ — среднее и стандартное отклонение по множеству $\{Y_{i',j'}\}$. Параметр $\gamma$ задаёт, насколько «выброс» должен отличаться от среднего.

## **_Давайте разберем концепцию "суперактивации" на примере, опираясь на предыдущее обсуждение "супервеса"._**

**Используем результаты предыдущего примера с "супервесом" $W_{1,1} = 10.0$.**

Напомню, матрица весов $W$ была:

```
W = [[10.0, 0.2, 0.3],
     [0.4, 0.5, 0.6],
     [0.7, 0.8, 0.9]]
```

А входная матрица $X$:

```
X = [[1, 2, 3],
     [4, 5, 6],
     [7, 8, 9]]
```

Результирующая матрица $Y$, вычисленная с этим "супервесом", была:

```
Y = [[11.3, 3.2, 5.0],
     [42.8, 7.7, 12.2],
     [74.3, 12.2, 19.4]]
```

### 1. Суперактивация как результат доминирующего вклада

Как было показано ранее, при вычислении $Y_{1,1}$ вклад от пары $(j=1, m=1)$ доминировал:

$Y_{1,1} = (1 \times 10.0) + (2 \times 0.2) + (3 \times 0.3) = 10.0 + 0.4 + 0.9 = 11.3$

Здесь $10.0$ значительно больше, чем $0.4 + 0.9 = 1.3$. Таким образом, $Y_{1,1} \approx X_{1,1} \times W_{1,1} = 1 \times 10.0 = 10.0$. В этом контексте $Y_{1,1} = 11.3$ можно рассматривать как **суперактивацию**, поскольку она обусловлена доминирующим вкладом "супервеса" $W_{1,1}$.

Аналогично, при вычислении $Y_{2,1}$:

$Y_{2,1} = (4 \times 10.0) + (5 \times 0.2) + (6 \times 0.3) = 40.0 + 1.0 + 1.8 = 42.8$

Здесь $40.0$ значительно больше, чем $1.0 + 1.8 = 2.8$. Следовательно, $Y_{2,1} \approx X_{2,1} \times W_{1,1} = 4 \times 10.0 = 40.0$. $Y_{2,1} = 42.8$ также является **суперактивацией**.

И при вычислении $Y_{3,1}$:

$Y_{3,1} = (7 \times 10.0) + (8 \times 0.2) + (9 \times 0.3) = 70.0 + 1.6 + 2.7 = 74.3$

Здесь $70.0$ значительно больше, чем $1.6 + 2.7 = 4.3$. Следовательно, $Y_{3,1} \approx X_{3,1} \times W_{1,1} = 7 \times 10.0 = 70.0$. $Y_{3,1} = 74.3$ также является **суперактивацией**.

В этих случаях "супервес" $W_{1,1}$ в сочетании с соответствующими активациями $X_{i,1}$ приводит к выходным значениям $Y_{i,1}$, где вклад одного слагаемого непропорционально велик.

### 2. Суперактивация как выброс (outlier)

Теперь рассмотрим определение "суперактивации" как выброса. Для этого нам нужно вычислить среднее ($\mu_Y$) и стандартное отклонение ($\sigma_Y$) для всех элементов матрицы $Y$:

```
Y = [[11.3, 3.2, 5.0],
     [42.8, 7.7, 12.2],
     [74.3, 12.2, 19.4]]
```

Элементы матрицы $Y$: 11.3, 3.2, 5.0, 42.8, 7.7, 12.2, 74.3, 12.2, 19.4

**Вычисление среднего ($\mu_Y$):**

$\mu_Y = \frac{11.3 + 3.2 + 5.0 + 42.8 + 7.7 + 12.2 + 74.3 + 12.2 + 19.4}{9}$
$\mu_Y = \frac{188.1}{9} \approx 20.9$

**Вычисление стандартного отклонения ($\sigma_Y$):**

Сначала вычислим дисперсию ($\sigma_Y^2$):

$\sigma_Y^2 = \frac{1}{9} \sum_{i,j} (Y_{i,j} - \mu_Y)^2$

$\sigma_Y^2 = \frac{1}{9} [ (11.3-20.9)^2 + (3.2-20.9)^2 + (5.0-20.9)^2 + (42.8-20.9)^2 + (7.7-20.9)^2 + (12.2-20.9)^2 + (74.3-20.9)^2 + (12.2-20.9)^2 + (19.4-20.9)^2 ]$

$\sigma_Y^2 = \frac{1}{9} [ (-9.6)^2 + (-17.7)^2 + (-15.9)^2 + (21.9)^2 + (-13.2)^2 + (-8.7)^2 + (53.4)^2 + (-8.7)^2 + (-1.5)^2 ]$

$\sigma_Y^2 = \frac{1}{9} [ 92.16 + 313.29 + 252.81 + 479.61 + 174.24 + 75.69 + 2851.56 + 75.69 + 2.25 ]$

$\sigma_Y^2 = \frac{1}{9} [ 4317.3 ] \approx 479.7$

Теперь вычислим стандартное отклонение:

$\sigma_Y = \sqrt{479.7} \approx 21.9$

**Определение суперактиваций как выбросов:**

Пусть параметр $\gamma = 2$. Тогда пороговое значение для суперактивации:

$|\text{Суперактивация}| > \mu_Y + \gamma \sigma_Y = 20.9 + 2 \times 21.9 = 20.9 + 43.8 = 64.7$

Теперь проверим элементы матрицы $Y$:

- $|Y_{1,1}| = 11.3 \ngtr 64.7$
- $|Y_{1,2}| = 3.2 \ngtr 64.7$
- $|Y_{1,3}| = 5.0 \ngtr 64.7$
- $|Y_{2,1}| = 42.8 \ngtr 64.7$
- $|Y_{2,2}| = 7.7 \ngtr 64.7$
- $|Y_{2,3}| = 12.2 \ngtr 64.7$
- $|Y_{3,1}| = 74.3 > 64.7$  **Суперактивация**
- $|Y_{3,2}| = 12.2 \ngtr 64.7$
- $|Y_{3,3}| = 19.4 \ngtr 64.7$

Согласно этому критерию, только $Y_{3,1} = 74.3$ является суперактивацией.

Если бы мы выбрали меньшее значение для $\gamma$, например $\gamma = 1$, то пороговое значение было бы:

$|\text{Суперактивация}| > 20.9 + 1 \times 21.9 = 42.8$

В этом случае суперактивациями были бы:

- $|Y_{2,1}| = 42.8 \ngtr 42.8$ (на границе)
- $|Y_{3,1}| = 74.3 > 42.8$  **Суперактивация**

### 5. Проверка наличия супервеса

1. **Операция обнуления**: Пусть $\widehat{W}$ — копия $W$, но со сброшенным в 0 кандидатом $W_{j,m}$:  

$$
  \widehat{W}_{j',k'} = 
  \begin{cases}
  0, & \text{если } (j' = j) \text{ и } (k' = m),\\
  W_{j',k'}, & \text{иначе}.
  \end{cases}
$$

2. **Сравнение перплексии**: Если модель с весами $\widehat{W}$ даёт перплексию $PPL_\text{new}$, в разы превышающую исходную $PPL_\text{orig}$, то $W_{j,m}$ — супервес.  

$$
  \text{Если } 
  \frac{PPL_\text{new}}{PPL_\text{orig}} \gg 1,
  \quad 
  \text{тогда } W_{j,m} \text{ — супервес.}
$$

### 6. Распространение суперактивации

В многослойной архитектуре результат $Y$ поступает на вход следующих блоков (через нелинейные функции, skip-connection и т. д.). Суперактивация в $Y_{i,j}$ может частично или полностью сохраниться в дальнейших слоях, вызывая лавинообразный эффект: 

$$
  \tilde{X} = \phi(Y), \quad 
  \tilde{Y} = \tilde{X} \, W_{\text{next}}^\mathsf{T},
$$

где $\phi(\cdot)$ — функция активации (ReLU, GeLU и т. д.).

---

### Заключительные замечания о роли супервесов и суперактиваций

1. **Супервеса** могут не быть крупнейшими по модулю в матрице $W$, но при этом «резонировать» с большими входными значениями $X_{i m}$, формируя «суперактивации».  
2. **Суперактивации** прослеживаются по слоям и способны сохраняться при прохождении через функции активации и skip connection’ы.  
3. Удаление супервеса приводит к исчезновению (или сильному снижению) суперактивации и резкому ухудшению итога (качества генерации, точности в zero-shot задачах и т.д.).  
4. Сохранение супервесов при квантовании (либо особое обращение с ними, например, «разжатием» в более высокую точность) помогает избежать катастрофической деградации модели.  

---

## Итоговое резюме

- **Супервес** (Super Weight) в модели — это вес (скаляр) $W_{j m}$ в матрице $\mathrm{down\_proj}$ (или в другом слое), вклад которого в активации $Y_{i j}$ **доминантен** и ведёт к появлению «суперактиваций».  
- **Суперактивация** (Super Activation) — это чрезвычайно большая активация (элемент $Y_{i j}$), вызванная доминированием пары $(X_{i m}, W_{j m})$.  
- Удаление одного супервеса обрушивает качество модели, поскольку «суперактивации» исчезают или резко уменьшаются, что подтверждает причинно-следственную связь.  
- В формализме линейных преобразований $Y = XW^\mathsf{T}$, супервесы — это пары индексов $(j,m)$, при которых произведение $X_{i m} \cdot W_{j m}$ оказывается в разы выше суммы всех остальных компонент для данной позиции $(i,j)$.  

Таким образом, «супервеса» и «суперактивации» — это феномен взаимодействия большого (или «резонансного») веса и специфических входных активаций, что в конечном итоге критически влияет на производительность всей большой языковой модели.
